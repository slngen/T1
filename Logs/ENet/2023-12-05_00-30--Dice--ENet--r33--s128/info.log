##########Config##########
{'device': 'cuda:0', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/ENet', 'log_path': 'Logs/ENet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (initial_block): InitialBlock(
    (main_branch): Conv2d(6, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (ext_branch): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (out_activation): PReLU(num_parameters=1)
  )
  (downsample1_0): DownsamplingBottleneck(
    (main_max1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (ext_conv1): Sequential(
      (0): Conv2d(16, 4, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.01, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular1_1): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.01, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular1_2): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.01, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular1_3): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.01, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular1_4): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.01, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (downsample2_0): DownsamplingBottleneck(
    (main_max1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular2_1): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated2_2): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (asymmetric2_3): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated2_4): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular2_5): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated2_6): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (asymmetric2_7): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated2_8): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular3_0): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated3_1): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (asymmetric3_2): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated3_3): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (regular3_4): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated3_5): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (asymmetric3_6): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
      (3): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (dilated3_7): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv2): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_conv3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): PReLU(num_parameters=1)
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): PReLU(num_parameters=1)
  )
  (upsample4_0): UpsamplingBottleneck(
    (main_conv1): Sequential(
      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (main_unpool1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
    (ext_conv1): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_tconv1): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (ext_tconv1_bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ext_tconv1_activation): ReLU()
    (ext_conv2): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): ReLU()
  )
  (regular4_1): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): ReLU()
  )
  (regular4_2): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): ReLU()
  )
  (upsample5_0): UpsamplingBottleneck(
    (main_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (main_unpool1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
    (ext_conv1): Sequential(
      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_tconv1): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (ext_tconv1_bnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ext_tconv1_activation): ReLU()
    (ext_conv2): Sequential(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): ReLU()
  )
  (regular5_1): RegularBottleneck(
    (ext_conv1): Sequential(
      (0): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv2): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_conv3): Sequential(
      (0): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (ext_regul): Dropout2d(p=0.1, inplace=False)
    (out_activation): ReLU()
  )
  (transposed_conv): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.9128150427341462
step 101/334, epoch 1/501 --> loss:0.8784221589565278
step 151/334, epoch 1/501 --> loss:0.8589632797241211
step 201/334, epoch 1/501 --> loss:0.8599348413944244
step 251/334, epoch 1/501 --> loss:0.8586571419239044
step 301/334, epoch 1/501 --> loss:0.8437985384464264

##########train dataset##########
acc--> [74.61838352130208]
F1--> {'F1': [0.23865614348152453], 'precision': [0.13660999372202837], 'recall': [0.9433326009547436]}
##########eval dataset##########
acc--> [75.18100688463244]
F1--> {'F1': [0.24522961907382693], 'precision': [0.14117154127603182], 'recall': [0.9328661984802167]}
save model!
step 51/334, epoch 2/501 --> loss:0.8552538144588471
step 101/334, epoch 2/501 --> loss:0.8365885281562805
step 151/334, epoch 2/501 --> loss:0.8388435423374176
step 201/334, epoch 2/501 --> loss:0.851600843667984
step 251/334, epoch 2/501 --> loss:0.8460929906368255
step 301/334, epoch 2/501 --> loss:0.8456834542751313
step 51/334, epoch 3/501 --> loss:0.844821983575821
step 101/334, epoch 3/501 --> loss:0.8321230483055114
step 151/334, epoch 3/501 --> loss:0.8563949680328369
step 201/334, epoch 3/501 --> loss:0.8435298812389374
step 251/334, epoch 3/501 --> loss:0.8385441815853119
step 301/334, epoch 3/501 --> loss:0.8480969738960266
step 51/334, epoch 4/501 --> loss:0.8328938829898834
step 101/334, epoch 4/501 --> loss:0.8386439597606659
step 151/334, epoch 4/501 --> loss:0.8430808639526367
step 201/334, epoch 4/501 --> loss:0.8439926338195801
step 251/334, epoch 4/501 --> loss:0.838480476140976
step 301/334, epoch 4/501 --> loss:0.8379313635826111
step 51/334, epoch 5/501 --> loss:0.8269589114189148
step 101/334, epoch 5/501 --> loss:0.8383003485202789
step 151/334, epoch 5/501 --> loss:0.8567686712741852
step 201/334, epoch 5/501 --> loss:0.8189619290828705
step 251/334, epoch 5/501 --> loss:0.841981143951416
step 301/334, epoch 5/501 --> loss:0.8379231512546539
step 51/334, epoch 6/501 --> loss:0.844372512102127
step 101/334, epoch 6/501 --> loss:0.8348752450942993
step 151/334, epoch 6/501 --> loss:0.8573132050037384
step 201/334, epoch 6/501 --> loss:0.8349329996109008
step 251/334, epoch 6/501 --> loss:0.8387495398521423
step 301/334, epoch 6/501 --> loss:0.8384330105781556
step 51/334, epoch 7/501 --> loss:0.8433571040630341
step 101/334, epoch 7/501 --> loss:0.8219652009010315
step 151/334, epoch 7/501 --> loss:0.8493974721431732
step 201/334, epoch 7/501 --> loss:0.8455655670166016
step 251/334, epoch 7/501 --> loss:0.8397254312038421
step 301/334, epoch 7/501 --> loss:0.8413734412193299
step 51/334, epoch 8/501 --> loss:0.8398741567134858
step 101/334, epoch 8/501 --> loss:0.841159257888794
step 151/334, epoch 8/501 --> loss:0.8557011389732361
step 201/334, epoch 8/501 --> loss:0.8350213849544526
step 251/334, epoch 8/501 --> loss:0.8420112729072571
step 301/334, epoch 8/501 --> loss:0.8380317771434784
step 51/334, epoch 9/501 --> loss:0.8408660042285919
step 101/334, epoch 9/501 --> loss:0.8369159376621247
step 151/334, epoch 9/501 --> loss:0.8454318761825561
step 201/334, epoch 9/501 --> loss:0.8338940048217773
step 251/334, epoch 9/501 --> loss:0.8377565693855286
step 301/334, epoch 9/501 --> loss:0.8323293423652649
step 51/334, epoch 10/501 --> loss:0.8327867805957794
step 101/334, epoch 10/501 --> loss:0.8399308848381043
step 151/334, epoch 10/501 --> loss:0.8364578938484192
step 201/334, epoch 10/501 --> loss:0.838067387342453
step 251/334, epoch 10/501 --> loss:0.8462205481529236
step 301/334, epoch 10/501 --> loss:0.8218855488300324
step 51/334, epoch 11/501 --> loss:0.8443745684623718
step 101/334, epoch 11/501 --> loss:0.8356415224075318
step 151/334, epoch 11/501 --> loss:0.8343440520763398
step 201/334, epoch 11/501 --> loss:0.8215570104122162
step 251/334, epoch 11/501 --> loss:0.8424561321735382
step 301/334, epoch 11/501 --> loss:0.8278239130973816

##########train dataset##########
acc--> [96.52897849438466]
F1--> {'F1': [0.6744329468385886], 'precision': [0.5578910954915385], 'recall': [0.8525377235123422]}
##########eval dataset##########
acc--> [96.61411609131709]
F1--> {'F1': [0.6870116885936083], 'precision': [0.5720640785136974], 'recall': [0.8597843346728099]}
save model!
step 51/334, epoch 12/501 --> loss:0.8377728021144867
step 101/334, epoch 12/501 --> loss:0.8473495852947235
step 151/334, epoch 12/501 --> loss:0.8306203699111938
step 201/334, epoch 12/501 --> loss:0.8439636063575745
step 251/334, epoch 12/501 --> loss:0.8310437631607056
step 301/334, epoch 12/501 --> loss:0.8371892654895783
step 51/334, epoch 13/501 --> loss:0.8320055079460144
step 101/334, epoch 13/501 --> loss:0.8341708612442017
step 151/334, epoch 13/501 --> loss:0.8463996648788452
step 201/334, epoch 13/501 --> loss:0.8433400523662568
step 251/334, epoch 13/501 --> loss:0.8211781919002533
step 301/334, epoch 13/501 --> loss:0.8239857864379883
step 51/334, epoch 14/501 --> loss:0.8243667590618133
step 101/334, epoch 14/501 --> loss:0.8434455645084381
step 151/334, epoch 14/501 --> loss:0.8364376997947693
step 201/334, epoch 14/501 --> loss:0.8356223630905152
step 251/334, epoch 14/501 --> loss:0.8411807882785797
step 301/334, epoch 14/501 --> loss:0.8337107920646667
step 51/334, epoch 15/501 --> loss:0.8367725706100464
step 101/334, epoch 15/501 --> loss:0.8326126623153687
step 151/334, epoch 15/501 --> loss:0.8461165344715118
step 201/334, epoch 15/501 --> loss:0.829480482339859
step 251/334, epoch 15/501 --> loss:0.8305153024196624
step 301/334, epoch 15/501 --> loss:0.8359240770339966
step 51/334, epoch 16/501 --> loss:0.820349452495575
step 101/334, epoch 16/501 --> loss:0.8436524772644043
step 151/334, epoch 16/501 --> loss:0.8368441009521485
step 201/334, epoch 16/501 --> loss:0.8411618971824646
step 251/334, epoch 16/501 --> loss:0.8258961319923401
step 301/334, epoch 16/501 --> loss:0.8413743329048157
step 51/334, epoch 17/501 --> loss:0.8265082776546478
step 101/334, epoch 17/501 --> loss:0.8254696726799011
step 151/334, epoch 17/501 --> loss:0.8405273151397705
step 201/334, epoch 17/501 --> loss:0.838250813484192
step 251/334, epoch 17/501 --> loss:0.8368589103221893
step 301/334, epoch 17/501 --> loss:0.8377849662303924
step 51/334, epoch 18/501 --> loss:0.8389753484725953
step 101/334, epoch 18/501 --> loss:0.8482557356357574
step 151/334, epoch 18/501 --> loss:0.8250599586963654
step 201/334, epoch 18/501 --> loss:0.8370942211151123
step 251/334, epoch 18/501 --> loss:0.8332833993434906
step 301/334, epoch 18/501 --> loss:0.8321704971790314
step 51/334, epoch 19/501 --> loss:0.8353749978542327
step 101/334, epoch 19/501 --> loss:0.834301255941391
step 151/334, epoch 19/501 --> loss:0.8244342088699341
step 201/334, epoch 19/501 --> loss:0.8429592227935792
step 251/334, epoch 19/501 --> loss:0.8281999135017395
step 301/334, epoch 19/501 --> loss:0.8418398582935334
step 51/334, epoch 20/501 --> loss:0.8313130843639374
step 101/334, epoch 20/501 --> loss:0.8185137224197387
step 151/334, epoch 20/501 --> loss:0.8399342727661133
step 201/334, epoch 20/501 --> loss:0.83483327627182
step 251/334, epoch 20/501 --> loss:0.8340563011169434
step 301/334, epoch 20/501 --> loss:0.8453736102581024
step 51/334, epoch 21/501 --> loss:0.8378704571723938
step 101/334, epoch 21/501 --> loss:0.8343045878410339
step 151/334, epoch 21/501 --> loss:0.8295274472236633
step 201/334, epoch 21/501 --> loss:0.8286893391609191
step 251/334, epoch 21/501 --> loss:0.8283121991157532
step 301/334, epoch 21/501 --> loss:0.8467748093605042

##########train dataset##########
acc--> [93.54210772405013]
F1--> {'F1': [0.5477833788462514], 'precision': [0.3886708568760278], 'recall': [0.9274893597732892]}
##########eval dataset##########
acc--> [93.85311430415663]
F1--> {'F1': [0.560554196584609], 'precision': [0.405607978360687], 'recall': [0.9070823561447552]}
step 51/334, epoch 22/501 --> loss:0.8328500270843506
step 101/334, epoch 22/501 --> loss:0.8399125468730927
step 151/334, epoch 22/501 --> loss:0.8179043698310852
step 201/334, epoch 22/501 --> loss:0.8312837028503418
step 251/334, epoch 22/501 --> loss:0.8388695788383483
step 301/334, epoch 22/501 --> loss:0.8378411328792572
step 51/334, epoch 23/501 --> loss:0.8381761813163757
step 101/334, epoch 23/501 --> loss:0.8337450695037841
step 151/334, epoch 23/501 --> loss:0.8396941471099854
step 201/334, epoch 23/501 --> loss:0.8432151710987091
step 251/334, epoch 23/501 --> loss:0.8183502888679505
step 301/334, epoch 23/501 --> loss:0.8325128829479218
step 51/334, epoch 24/501 --> loss:0.8430579817295074
step 101/334, epoch 24/501 --> loss:0.8361642515659332
step 151/334, epoch 24/501 --> loss:0.8230538034439087
step 201/334, epoch 24/501 --> loss:0.8351428246498108
step 251/334, epoch 24/501 --> loss:0.8482713532447815
step 301/334, epoch 24/501 --> loss:0.8152013945579529
step 51/334, epoch 25/501 --> loss:0.8354622697830201
step 101/334, epoch 25/501 --> loss:0.8204559290409088
step 151/334, epoch 25/501 --> loss:0.8356066477298737
step 201/334, epoch 25/501 --> loss:0.8309344172477722
step 251/334, epoch 25/501 --> loss:0.8324625158309936
step 301/334, epoch 25/501 --> loss:0.8358586013317109
step 51/334, epoch 26/501 --> loss:0.8398948383331298
step 101/334, epoch 26/501 --> loss:0.8294270217418671
step 151/334, epoch 26/501 --> loss:0.8416996324062347
step 201/334, epoch 26/501 --> loss:0.8151262700557709
step 251/334, epoch 26/501 --> loss:0.8221607828140258
step 301/334, epoch 26/501 --> loss:0.8436237347126007
step 51/334, epoch 27/501 --> loss:0.8232558333873748
step 101/334, epoch 27/501 --> loss:0.8425468754768372
step 151/334, epoch 27/501 --> loss:0.8437381398677826
step 201/334, epoch 27/501 --> loss:0.8283819961547851
step 251/334, epoch 27/501 --> loss:0.8443573260307312
step 301/334, epoch 27/501 --> loss:0.8223714637756347
step 51/334, epoch 28/501 --> loss:0.830491054058075
step 101/334, epoch 28/501 --> loss:0.8533324253559113
step 151/334, epoch 28/501 --> loss:0.8362596011161805
step 201/334, epoch 28/501 --> loss:0.8334953022003174
step 251/334, epoch 28/501 --> loss:0.835892106294632
step 301/334, epoch 28/501 --> loss:0.8470602381229401
step 51/334, epoch 29/501 --> loss:0.8454310536384583
step 101/334, epoch 29/501 --> loss:0.8342041742801666
step 151/334, epoch 29/501 --> loss:0.834948023557663
step 201/334, epoch 29/501 --> loss:0.8368625938892365
step 251/334, epoch 29/501 --> loss:0.8416994643211365
step 301/334, epoch 29/501 --> loss:0.8114907670021058
step 51/334, epoch 30/501 --> loss:0.829990291595459
step 101/334, epoch 30/501 --> loss:0.8321745240688324
step 151/334, epoch 30/501 --> loss:0.8237607038021088
step 201/334, epoch 30/501 --> loss:0.8271924972534179
step 251/334, epoch 30/501 --> loss:0.8435049080848693
step 301/334, epoch 30/501 --> loss:0.8319001245498657
step 51/334, epoch 31/501 --> loss:0.8249866199493409
step 101/334, epoch 31/501 --> loss:0.8237203085422515
step 151/334, epoch 31/501 --> loss:0.8408585631847382
step 201/334, epoch 31/501 --> loss:0.8556911754608154
step 251/334, epoch 31/501 --> loss:0.8315944755077362
step 301/334, epoch 31/501 --> loss:0.8276982009410858

##########train dataset##########
acc--> [97.77185063170535]
F1--> {'F1': [0.780469932098923], 'precision': [0.6676327935900177], 'recall': [0.9392199109838992]}
##########eval dataset##########
acc--> [97.69794133703826]
F1--> {'F1': [0.774188419080095], 'precision': [0.6719890541015496], 'recall': [0.9130630042094134]}
save model!
step 51/334, epoch 32/501 --> loss:0.8344671249389648
step 101/334, epoch 32/501 --> loss:0.8344915056228638
step 151/334, epoch 32/501 --> loss:0.823509361743927
step 201/334, epoch 32/501 --> loss:0.8287410807609558
step 251/334, epoch 32/501 --> loss:0.8434039890766144
step 301/334, epoch 32/501 --> loss:0.8291804587841034
step 51/334, epoch 33/501 --> loss:0.8363289678096771
step 101/334, epoch 33/501 --> loss:0.8321961557865143
step 151/334, epoch 33/501 --> loss:0.8267799723148346
step 201/334, epoch 33/501 --> loss:0.8467794191837311
step 251/334, epoch 33/501 --> loss:0.8258425939083099
step 301/334, epoch 33/501 --> loss:0.8231294143199921
step 51/334, epoch 34/501 --> loss:0.8472585022449494
step 101/334, epoch 34/501 --> loss:0.8242872178554534
step 151/334, epoch 34/501 --> loss:0.8290293729305267
step 201/334, epoch 34/501 --> loss:0.8278826379776001
step 251/334, epoch 34/501 --> loss:0.8278749489784241
step 301/334, epoch 34/501 --> loss:0.8354029166698456
step 51/334, epoch 35/501 --> loss:0.8432488822937012
step 101/334, epoch 35/501 --> loss:0.8367684233188629
step 151/334, epoch 35/501 --> loss:0.8350013029575348
step 201/334, epoch 35/501 --> loss:0.8187279379367829
step 251/334, epoch 35/501 --> loss:0.8187021660804749
step 301/334, epoch 35/501 --> loss:0.8352322041988373
step 51/334, epoch 36/501 --> loss:0.8343618535995483
step 101/334, epoch 36/501 --> loss:0.8224652707576752
step 151/334, epoch 36/501 --> loss:0.8320854830741883
step 201/334, epoch 36/501 --> loss:0.8319636869430542
step 251/334, epoch 36/501 --> loss:0.8216051054000855
step 301/334, epoch 36/501 --> loss:0.8358769810199738
step 51/334, epoch 37/501 --> loss:0.8305237448215484
step 101/334, epoch 37/501 --> loss:0.8362306594848633
step 151/334, epoch 37/501 --> loss:0.8404621934890747
step 201/334, epoch 37/501 --> loss:0.8397638249397278
step 251/334, epoch 37/501 --> loss:0.813544659614563
step 301/334, epoch 37/501 --> loss:0.8292686545848846
step 51/334, epoch 38/501 --> loss:0.8287004828453064
step 101/334, epoch 38/501 --> loss:0.8216726005077362
step 151/334, epoch 38/501 --> loss:0.842232837677002
step 201/334, epoch 38/501 --> loss:0.8327168774604797
step 251/334, epoch 38/501 --> loss:0.8211167395114899
step 301/334, epoch 38/501 --> loss:0.8414362382888794
step 51/334, epoch 39/501 --> loss:0.8278829598426819
step 101/334, epoch 39/501 --> loss:0.8285606145858765
step 151/334, epoch 39/501 --> loss:0.8381763136386872
step 201/334, epoch 39/501 --> loss:0.8376080846786499
step 251/334, epoch 39/501 --> loss:0.8309023308753968
step 301/334, epoch 39/501 --> loss:0.8232855498790741
step 51/334, epoch 40/501 --> loss:0.8350825536251069
step 101/334, epoch 40/501 --> loss:0.827692813873291
step 151/334, epoch 40/501 --> loss:0.8310367369651794
step 201/334, epoch 40/501 --> loss:0.8338236737251282
step 251/334, epoch 40/501 --> loss:0.8335772383213044
step 301/334, epoch 40/501 --> loss:0.8219390201568604
step 51/334, epoch 41/501 --> loss:0.8383582675457001
step 101/334, epoch 41/501 --> loss:0.8315565979480743
step 151/334, epoch 41/501 --> loss:0.8291671085357666
step 201/334, epoch 41/501 --> loss:0.8247212874889374
step 251/334, epoch 41/501 --> loss:0.8303325068950653
step 301/334, epoch 41/501 --> loss:0.8387326502799988

##########train dataset##########
acc--> [97.714388160431]
F1--> {'F1': [0.779330303140199], 'precision': [0.657276195695243], 'recall': [0.957066349439414]}
##########eval dataset##########
acc--> [97.61312222751289]
F1--> {'F1': [0.7699527025628847], 'precision': [0.6598412894426057], 'recall': [0.9241889616675081]}
step 51/334, epoch 42/501 --> loss:0.8370452558994294
step 101/334, epoch 42/501 --> loss:0.8262488114833831
step 151/334, epoch 42/501 --> loss:0.8291986477375031
step 201/334, epoch 42/501 --> loss:0.8414237403869629
step 251/334, epoch 42/501 --> loss:0.8486591255664826
step 301/334, epoch 42/501 --> loss:0.8138888680934906
step 51/334, epoch 43/501 --> loss:0.8230024158954621
step 101/334, epoch 43/501 --> loss:0.8273374664783478
step 151/334, epoch 43/501 --> loss:0.8240614008903503
step 201/334, epoch 43/501 --> loss:0.8279675662517547
step 251/334, epoch 43/501 --> loss:0.8459311163425446
step 301/334, epoch 43/501 --> loss:0.832739131450653
step 51/334, epoch 44/501 --> loss:0.8267197954654694
step 101/334, epoch 44/501 --> loss:0.8467171800136566
step 151/334, epoch 44/501 --> loss:0.8214466762542725
step 201/334, epoch 44/501 --> loss:0.8366104781627655
step 251/334, epoch 44/501 --> loss:0.824173607826233
step 301/334, epoch 44/501 --> loss:0.8241139495372772
step 51/334, epoch 45/501 --> loss:0.8309485638141632
step 101/334, epoch 45/501 --> loss:0.8079528951644898
step 151/334, epoch 45/501 --> loss:0.823130875825882
step 201/334, epoch 45/501 --> loss:0.8527177357673645
step 251/334, epoch 45/501 --> loss:0.8470456898212433
step 301/334, epoch 45/501 --> loss:0.8309306287765503
step 51/334, epoch 46/501 --> loss:0.8288857018947602
step 101/334, epoch 46/501 --> loss:0.8301493990421295
step 151/334, epoch 46/501 --> loss:0.8257744574546814
step 201/334, epoch 46/501 --> loss:0.8327299964427948
step 251/334, epoch 46/501 --> loss:0.8218493521213531
step 301/334, epoch 46/501 --> loss:0.8293811273574829
step 51/334, epoch 47/501 --> loss:0.8252616763114929
step 101/334, epoch 47/501 --> loss:0.8344098258018494
step 151/334, epoch 47/501 --> loss:0.8265793287754059
step 201/334, epoch 47/501 --> loss:0.8289951074123383
step 251/334, epoch 47/501 --> loss:0.8136800873279572
step 301/334, epoch 47/501 --> loss:0.8429460608959198
step 51/334, epoch 48/501 --> loss:0.8297398149967193
step 101/334, epoch 48/501 --> loss:0.8324151360988616
step 151/334, epoch 48/501 --> loss:0.8303613018989563
step 201/334, epoch 48/501 --> loss:0.823513525724411
step 251/334, epoch 48/501 --> loss:0.8249187922477722
step 301/334, epoch 48/501 --> loss:0.8361702644824982
step 51/334, epoch 49/501 --> loss:0.839372284412384
step 101/334, epoch 49/501 --> loss:0.8232410860061645
step 151/334, epoch 49/501 --> loss:0.825983704328537
step 201/334, epoch 49/501 --> loss:0.8227548384666443
step 251/334, epoch 49/501 --> loss:0.8387420094013214
step 301/334, epoch 49/501 --> loss:0.8221237874031067
step 51/334, epoch 50/501 --> loss:0.834695372581482
step 101/334, epoch 50/501 --> loss:0.8272162747383117
step 151/334, epoch 50/501 --> loss:0.8154727327823639
step 201/334, epoch 50/501 --> loss:0.8312939584255219
step 251/334, epoch 50/501 --> loss:0.8285931050777435
step 301/334, epoch 50/501 --> loss:0.8351908767223358
step 51/334, epoch 51/501 --> loss:0.8203313267230987
step 101/334, epoch 51/501 --> loss:0.8489974927902222
step 151/334, epoch 51/501 --> loss:0.8282176637649536
step 201/334, epoch 51/501 --> loss:0.8288299107551574
step 251/334, epoch 51/501 --> loss:0.8216105329990387
step 301/334, epoch 51/501 --> loss:0.8279712617397308

##########train dataset##########
acc--> [96.87612939068374]
F1--> {'F1': [0.7196564906951802], 'precision': [0.5789271864012135], 'recall': [0.9507939698267853]}
##########eval dataset##########
acc--> [96.6367679839516]
F1--> {'F1': [0.701140595444177], 'precision': [0.5691688521133798], 'recall': [0.9128020091355801]}
step 51/334, epoch 52/501 --> loss:0.818814388513565
step 101/334, epoch 52/501 --> loss:0.8256532490253449
step 151/334, epoch 52/501 --> loss:0.8250566506385804
step 201/334, epoch 52/501 --> loss:0.8261976277828217
step 251/334, epoch 52/501 --> loss:0.8374950408935546
step 301/334, epoch 52/501 --> loss:0.8337237799167633
step 51/334, epoch 53/501 --> loss:0.8391234850883484
step 101/334, epoch 53/501 --> loss:0.8158873188495636
step 151/334, epoch 53/501 --> loss:0.8223840868473054
step 201/334, epoch 53/501 --> loss:0.8261319768428802
step 251/334, epoch 53/501 --> loss:0.8293275582790375
step 301/334, epoch 53/501 --> loss:0.832808837890625
step 51/334, epoch 54/501 --> loss:0.8335865700244903
step 101/334, epoch 54/501 --> loss:0.8306871342658997
step 151/334, epoch 54/501 --> loss:0.8206224822998047
step 201/334, epoch 54/501 --> loss:0.8311007249355317
step 251/334, epoch 54/501 --> loss:0.8198807001113891
step 301/334, epoch 54/501 --> loss:0.8364450025558472
step 51/334, epoch 55/501 --> loss:0.842899432182312
step 101/334, epoch 55/501 --> loss:0.8295209276676178
step 151/334, epoch 55/501 --> loss:0.8314543735980987
step 201/334, epoch 55/501 --> loss:0.8163188922405243
step 251/334, epoch 55/501 --> loss:0.8246082866191864
step 301/334, epoch 55/501 --> loss:0.8239760482311249
step 51/334, epoch 56/501 --> loss:0.8154134750366211
step 101/334, epoch 56/501 --> loss:0.8255623865127564
step 151/334, epoch 56/501 --> loss:0.8309866881370545
step 201/334, epoch 56/501 --> loss:0.8314252042770386
step 251/334, epoch 56/501 --> loss:0.83384889960289
step 301/334, epoch 56/501 --> loss:0.8328851878643035
step 51/334, epoch 57/501 --> loss:0.8213286030292511
step 101/334, epoch 57/501 --> loss:0.8249647068977356
step 151/334, epoch 57/501 --> loss:0.8405751931667328
step 201/334, epoch 57/501 --> loss:0.8338915455341339
step 251/334, epoch 57/501 --> loss:0.8338133811950683
step 301/334, epoch 57/501 --> loss:0.8100767862796784
step 51/334, epoch 58/501 --> loss:0.8222103953361511
step 101/334, epoch 58/501 --> loss:0.8345301949977875
step 151/334, epoch 58/501 --> loss:0.8230273652076722
step 201/334, epoch 58/501 --> loss:0.8219410705566407
step 251/334, epoch 58/501 --> loss:0.8358187878131866
step 301/334, epoch 58/501 --> loss:0.8373169815540313
step 51/334, epoch 59/501 --> loss:0.8309750509262085
step 101/334, epoch 59/501 --> loss:0.8259075713157654
step 151/334, epoch 59/501 --> loss:0.8212411344051361
step 201/334, epoch 59/501 --> loss:0.83342240691185
step 251/334, epoch 59/501 --> loss:0.8223424744606018
step 301/334, epoch 59/501 --> loss:0.8260998356342316
step 51/334, epoch 60/501 --> loss:0.827125905752182
step 101/334, epoch 60/501 --> loss:0.8187889933586121
step 151/334, epoch 60/501 --> loss:0.8241869306564331
step 201/334, epoch 60/501 --> loss:0.8322154462337494
step 251/334, epoch 60/501 --> loss:0.835227234363556
step 301/334, epoch 60/501 --> loss:0.8221290481090545
step 51/334, epoch 61/501 --> loss:0.8235914897918701
step 101/334, epoch 61/501 --> loss:0.8163266634941101
step 151/334, epoch 61/501 --> loss:0.8200066792964935
step 201/334, epoch 61/501 --> loss:0.8328730952739716
step 251/334, epoch 61/501 --> loss:0.8277469134330749
step 301/334, epoch 61/501 --> loss:0.838567909002304

##########train dataset##########
acc--> [97.719010709596]
F1--> {'F1': [0.7758241212480154], 'precision': [0.6624848679489073], 'recall': [0.9359621292533781]}
##########eval dataset##########
acc--> [97.49529046753025]
F1--> {'F1': [0.7570321726339535], 'precision': [0.6517838467970276], 'recall': [0.9028304529655947]}
step 51/334, epoch 62/501 --> loss:0.8274451851844787
step 101/334, epoch 62/501 --> loss:0.8279227340221404
step 151/334, epoch 62/501 --> loss:0.8250083684921264
step 201/334, epoch 62/501 --> loss:0.8450930273532867
step 251/334, epoch 62/501 --> loss:0.825951693058014
step 301/334, epoch 62/501 --> loss:0.8157370471954346
step 51/334, epoch 63/501 --> loss:0.8321627318859101
step 101/334, epoch 63/501 --> loss:0.8238179934024811
step 151/334, epoch 63/501 --> loss:0.8182559394836426
step 201/334, epoch 63/501 --> loss:0.8219988751411438
step 251/334, epoch 63/501 --> loss:0.830635381937027
step 301/334, epoch 63/501 --> loss:0.8207946276664734
step 51/334, epoch 64/501 --> loss:0.8221164739131928
step 101/334, epoch 64/501 --> loss:0.8249061155319214
step 151/334, epoch 64/501 --> loss:0.8239088201522827
step 201/334, epoch 64/501 --> loss:0.8243498277664184
step 251/334, epoch 64/501 --> loss:0.8153071630001069
step 301/334, epoch 64/501 --> loss:0.8441741633415222
step 51/334, epoch 65/501 --> loss:0.8223634421825409
step 101/334, epoch 65/501 --> loss:0.8357063519954682
step 151/334, epoch 65/501 --> loss:0.8309671795368194
step 201/334, epoch 65/501 --> loss:0.8252195787429809
step 251/334, epoch 65/501 --> loss:0.8163619780540466
step 301/334, epoch 65/501 --> loss:0.8377170157432556
step 51/334, epoch 66/501 --> loss:0.8116297209262848
step 101/334, epoch 66/501 --> loss:0.8337131464481353
step 151/334, epoch 66/501 --> loss:0.8220707106590271
step 201/334, epoch 66/501 --> loss:0.8263806080818177
step 251/334, epoch 66/501 --> loss:0.8365256989002228
step 301/334, epoch 66/501 --> loss:0.8412122642993927
step 51/334, epoch 67/501 --> loss:0.8145801448822021
step 101/334, epoch 67/501 --> loss:0.8270926356315613
step 151/334, epoch 67/501 --> loss:0.8311521911621094
step 201/334, epoch 67/501 --> loss:0.837580224275589
step 251/334, epoch 67/501 --> loss:0.8256995666027069
step 301/334, epoch 67/501 --> loss:0.8202453339099884
step 51/334, epoch 68/501 --> loss:0.8176722884178161
step 101/334, epoch 68/501 --> loss:0.8258974015712738
step 151/334, epoch 68/501 --> loss:0.8297481346130371
step 201/334, epoch 68/501 --> loss:0.8365686798095703
step 251/334, epoch 68/501 --> loss:0.8181508708000184
step 301/334, epoch 68/501 --> loss:0.8277027869224548
step 51/334, epoch 69/501 --> loss:0.8211891460418701
step 101/334, epoch 69/501 --> loss:0.8273578572273255
step 151/334, epoch 69/501 --> loss:0.8188895487785339
step 201/334, epoch 69/501 --> loss:0.8289269828796386
step 251/334, epoch 69/501 --> loss:0.8379022157192231
step 301/334, epoch 69/501 --> loss:0.8246085739135742
step 51/334, epoch 70/501 --> loss:0.8303459107875824
step 101/334, epoch 70/501 --> loss:0.823909363746643
step 151/334, epoch 70/501 --> loss:0.8279399752616883
step 201/334, epoch 70/501 --> loss:0.8304157865047455
step 251/334, epoch 70/501 --> loss:0.832679396867752
step 301/334, epoch 70/501 --> loss:0.8240060341358185
step 51/334, epoch 71/501 --> loss:0.8234597969055176
step 101/334, epoch 71/501 --> loss:0.8345322644710541
step 151/334, epoch 71/501 --> loss:0.8353086376190185
step 201/334, epoch 71/501 --> loss:0.8200825142860413
step 251/334, epoch 71/501 --> loss:0.8111312222480774
step 301/334, epoch 71/501 --> loss:0.8381714403629303

##########train dataset##########
acc--> [98.54826267443826]
F1--> {'F1': [0.8491685572113693], 'precision': [0.7556782978567246], 'recall': [0.9690702704991475]}
##########eval dataset##########
acc--> [98.24122297898751]
F1--> {'F1': [0.8180971405178811], 'precision': [0.7397050086795878], 'recall': [0.9150868742937843]}
save model!
step 51/334, epoch 72/501 --> loss:0.8125749170780182
step 101/334, epoch 72/501 --> loss:0.8242907106876374
step 151/334, epoch 72/501 --> loss:0.8240245449542999
step 201/334, epoch 72/501 --> loss:0.8257441008090973
step 251/334, epoch 72/501 --> loss:0.8307008135318756
step 301/334, epoch 72/501 --> loss:0.8369058692455291
step 51/334, epoch 73/501 --> loss:0.8249125802516937
step 101/334, epoch 73/501 --> loss:0.8260725736618042
step 151/334, epoch 73/501 --> loss:0.8141574478149414
step 201/334, epoch 73/501 --> loss:0.8239329195022583
step 251/334, epoch 73/501 --> loss:0.8280830359458924
step 301/334, epoch 73/501 --> loss:0.8370498991012574
step 51/334, epoch 74/501 --> loss:0.826821186542511
step 101/334, epoch 74/501 --> loss:0.8400016617774964
step 151/334, epoch 74/501 --> loss:0.8417688536643982
step 201/334, epoch 74/501 --> loss:0.8232437741756439
step 251/334, epoch 74/501 --> loss:0.8203289818763733
step 301/334, epoch 74/501 --> loss:0.8075717449188232
step 51/334, epoch 75/501 --> loss:0.8240412223339081
step 101/334, epoch 75/501 --> loss:0.8268054676055908
step 151/334, epoch 75/501 --> loss:0.8230862140655517
step 201/334, epoch 75/501 --> loss:0.8280422532558441
step 251/334, epoch 75/501 --> loss:0.8269501554965973
step 301/334, epoch 75/501 --> loss:0.8275247359275818
step 51/334, epoch 76/501 --> loss:0.8237201309204102
step 101/334, epoch 76/501 --> loss:0.834652408361435
step 151/334, epoch 76/501 --> loss:0.8106670296192169
step 201/334, epoch 76/501 --> loss:0.8288314092159271
step 251/334, epoch 76/501 --> loss:0.8172533440589905
step 301/334, epoch 76/501 --> loss:0.827613433599472
step 51/334, epoch 77/501 --> loss:0.8137489545345307
step 101/334, epoch 77/501 --> loss:0.8322550868988037
step 151/334, epoch 77/501 --> loss:0.8234241211414337
step 201/334, epoch 77/501 --> loss:0.8326697874069214
step 251/334, epoch 77/501 --> loss:0.8322266089916229
step 301/334, epoch 77/501 --> loss:0.8222076141834259
step 51/334, epoch 78/501 --> loss:0.8260994613170624
step 101/334, epoch 78/501 --> loss:0.8199821496009827
step 151/334, epoch 78/501 --> loss:0.8156197714805603
step 201/334, epoch 78/501 --> loss:0.837423847913742
step 251/334, epoch 78/501 --> loss:0.83021723985672
step 301/334, epoch 78/501 --> loss:0.840297394990921
step 51/334, epoch 79/501 --> loss:0.8185716640949249
step 101/334, epoch 79/501 --> loss:0.8219446873664856
step 151/334, epoch 79/501 --> loss:0.814999656677246
step 201/334, epoch 79/501 --> loss:0.8374380993843079
step 251/334, epoch 79/501 --> loss:0.8268862879276275
step 301/334, epoch 79/501 --> loss:0.8429314327239991
step 51/334, epoch 80/501 --> loss:0.824047884941101
step 101/334, epoch 80/501 --> loss:0.8340126383304596
step 151/334, epoch 80/501 --> loss:0.830621087551117
step 201/334, epoch 80/501 --> loss:0.8214271473884582
step 251/334, epoch 80/501 --> loss:0.8274078679084778
step 301/334, epoch 80/501 --> loss:0.8228149116039276
step 51/334, epoch 81/501 --> loss:0.8256977069377899
step 101/334, epoch 81/501 --> loss:0.8224721658229828
step 151/334, epoch 81/501 --> loss:0.8175017201900482
step 201/334, epoch 81/501 --> loss:0.8323105978965759
step 251/334, epoch 81/501 --> loss:0.8332355451583863
step 301/334, epoch 81/501 --> loss:0.8242775428295136

##########train dataset##########
acc--> [98.21477288559474]
F1--> {'F1': [0.8219319895949707], 'precision': [0.7093388725402304], 'recall': [0.9770266897828276]}
##########eval dataset##########
acc--> [97.79683377589843]
F1--> {'F1': [0.7843749155786082], 'precision': [0.6797052608594233], 'recall': [0.9271627611596596]}
step 51/334, epoch 82/501 --> loss:0.8239935564994813
step 101/334, epoch 82/501 --> loss:0.8337391769886017
step 151/334, epoch 82/501 --> loss:0.8290447056293487
step 201/334, epoch 82/501 --> loss:0.8272357892990112
step 251/334, epoch 82/501 --> loss:0.8193522584438324
step 301/334, epoch 82/501 --> loss:0.8363509666919708
step 51/334, epoch 83/501 --> loss:0.8314141499996185
step 101/334, epoch 83/501 --> loss:0.8271607947349549
step 151/334, epoch 83/501 --> loss:0.8237436521053314
step 201/334, epoch 83/501 --> loss:0.8255980789661408
step 251/334, epoch 83/501 --> loss:0.83326873421669
step 301/334, epoch 83/501 --> loss:0.8261478316783905
step 51/334, epoch 84/501 --> loss:0.8188366901874542
step 101/334, epoch 84/501 --> loss:0.8330726981163025
step 151/334, epoch 84/501 --> loss:0.8212676095962524
step 201/334, epoch 84/501 --> loss:0.8260829639434815
step 251/334, epoch 84/501 --> loss:0.8328575694561005
step 301/334, epoch 84/501 --> loss:0.8343111753463746
step 51/334, epoch 85/501 --> loss:0.8237611842155457
step 101/334, epoch 85/501 --> loss:0.8139633369445801
step 151/334, epoch 85/501 --> loss:0.8127569818496704
step 201/334, epoch 85/501 --> loss:0.8350237143039704
step 251/334, epoch 85/501 --> loss:0.8290036487579345
step 301/334, epoch 85/501 --> loss:0.8311505258083344
step 51/334, epoch 86/501 --> loss:0.8244148421287537
step 101/334, epoch 86/501 --> loss:0.8184539544582367
step 151/334, epoch 86/501 --> loss:0.815798362493515
step 201/334, epoch 86/501 --> loss:0.8292279899120331
step 251/334, epoch 86/501 --> loss:0.822152225971222
step 301/334, epoch 86/501 --> loss:0.8266943049430847
step 51/334, epoch 87/501 --> loss:0.8248724400997162
step 101/334, epoch 87/501 --> loss:0.8149583232402802
step 151/334, epoch 87/501 --> loss:0.836748697757721
step 201/334, epoch 87/501 --> loss:0.8254565834999085
step 251/334, epoch 87/501 --> loss:0.8273847258090973
step 301/334, epoch 87/501 --> loss:0.83585813164711
step 51/334, epoch 88/501 --> loss:0.8219834804534912
step 101/334, epoch 88/501 --> loss:0.8189956641197205
step 151/334, epoch 88/501 --> loss:0.8238506066799164
step 201/334, epoch 88/501 --> loss:0.818347008228302
step 251/334, epoch 88/501 --> loss:0.835273050069809
step 301/334, epoch 88/501 --> loss:0.8213356781005859
step 51/334, epoch 89/501 --> loss:0.8281498193740845
step 101/334, epoch 89/501 --> loss:0.8254208648204804
step 151/334, epoch 89/501 --> loss:0.828437649011612
step 201/334, epoch 89/501 --> loss:0.8022423422336579
step 251/334, epoch 89/501 --> loss:0.8298751306533814
step 301/334, epoch 89/501 --> loss:0.8361610162258148
step 51/334, epoch 90/501 --> loss:0.8222796654701233
step 101/334, epoch 90/501 --> loss:0.8237727677822113
step 151/334, epoch 90/501 --> loss:0.826149798631668
step 201/334, epoch 90/501 --> loss:0.8363835775852203
step 251/334, epoch 90/501 --> loss:0.8234098684787751
step 301/334, epoch 90/501 --> loss:0.8177578055858612
step 51/334, epoch 91/501 --> loss:0.8148474764823913
step 101/334, epoch 91/501 --> loss:0.8148360204696655
step 151/334, epoch 91/501 --> loss:0.8319778120517731
step 201/334, epoch 91/501 --> loss:0.8153951716423035
step 251/334, epoch 91/501 --> loss:0.8348096549510956
step 301/334, epoch 91/501 --> loss:0.8387309372425079

##########train dataset##########
acc--> [98.21474513714068]
F1--> {'F1': [0.8216964971929427], 'precision': [0.7098091571795075], 'recall': [0.9754718733282972]}
##########eval dataset##########
acc--> [97.89247554534765]
F1--> {'F1': [0.7898339363300936], 'precision': [0.6940597033703909], 'recall': [0.9162846718042292]}
step 51/334, epoch 92/501 --> loss:0.8149525964260101
step 101/334, epoch 92/501 --> loss:0.8254149913787842
step 151/334, epoch 92/501 --> loss:0.8187723100185395
step 201/334, epoch 92/501 --> loss:0.8199758350849151
step 251/334, epoch 92/501 --> loss:0.8306561875343322
step 301/334, epoch 92/501 --> loss:0.832640506029129
step 51/334, epoch 93/501 --> loss:0.8257500958442688
step 101/334, epoch 93/501 --> loss:0.8166905462741851
step 151/334, epoch 93/501 --> loss:0.833546234369278
step 201/334, epoch 93/501 --> loss:0.8318316125869751
step 251/334, epoch 93/501 --> loss:0.8354830276966095
step 301/334, epoch 93/501 --> loss:0.8233957040309906
step 51/334, epoch 94/501 --> loss:0.831024706363678
step 101/334, epoch 94/501 --> loss:0.8093881344795227
step 151/334, epoch 94/501 --> loss:0.8362370193004608
step 201/334, epoch 94/501 --> loss:0.8268144595623016
step 251/334, epoch 94/501 --> loss:0.8242017853260041
step 301/334, epoch 94/501 --> loss:0.8257360661029816
step 51/334, epoch 95/501 --> loss:0.8465006566047668
step 101/334, epoch 95/501 --> loss:0.8170270001888276
step 151/334, epoch 95/501 --> loss:0.8296702623367309
step 201/334, epoch 95/501 --> loss:0.8124030125141144
step 251/334, epoch 95/501 --> loss:0.8272643673419953
step 301/334, epoch 95/501 --> loss:0.8252033150196075
step 51/334, epoch 96/501 --> loss:0.8149629259109497
step 101/334, epoch 96/501 --> loss:0.8192065954208374
step 151/334, epoch 96/501 --> loss:0.8317259466648101
step 201/334, epoch 96/501 --> loss:0.8292838168144226
step 251/334, epoch 96/501 --> loss:0.8335361647605896
step 301/334, epoch 96/501 --> loss:0.8217945110797882
step 51/334, epoch 97/501 --> loss:0.8389035177230835
step 101/334, epoch 97/501 --> loss:0.8503774571418762
step 151/334, epoch 97/501 --> loss:0.8314261949062347
step 201/334, epoch 97/501 --> loss:0.8151323211193084
step 251/334, epoch 97/501 --> loss:0.8230818784236908
step 301/334, epoch 97/501 --> loss:0.792800921201706
step 51/334, epoch 98/501 --> loss:0.8257423782348633
step 101/334, epoch 98/501 --> loss:0.8184219574928284
step 151/334, epoch 98/501 --> loss:0.8379905486106872
step 201/334, epoch 98/501 --> loss:0.8055336594581604
step 251/334, epoch 98/501 --> loss:0.8258330774307251
step 301/334, epoch 98/501 --> loss:0.8289909672737121
step 51/334, epoch 99/501 --> loss:0.8278513944149017
step 101/334, epoch 99/501 --> loss:0.8251786732673645
step 151/334, epoch 99/501 --> loss:0.8194837379455566
step 201/334, epoch 99/501 --> loss:0.8340950059890747
step 251/334, epoch 99/501 --> loss:0.8325887453556061
step 301/334, epoch 99/501 --> loss:0.819491616487503
step 51/334, epoch 100/501 --> loss:0.825884325504303
step 101/334, epoch 100/501 --> loss:0.8073862338066101
step 151/334, epoch 100/501 --> loss:0.8373027312755584
step 201/334, epoch 100/501 --> loss:0.8258608210086823
step 251/334, epoch 100/501 --> loss:0.8283187186717987
step 301/334, epoch 100/501 --> loss:0.828088926076889
step 51/334, epoch 101/501 --> loss:0.8240918886661529
step 101/334, epoch 101/501 --> loss:0.8261362361907959
step 151/334, epoch 101/501 --> loss:0.8180205309391022
step 201/334, epoch 101/501 --> loss:0.8302261805534363
step 251/334, epoch 101/501 --> loss:0.8256636691093445
step 301/334, epoch 101/501 --> loss:0.8151926934719086

##########train dataset##########
acc--> [98.59176109379477]
F1--> {'F1': [0.8544007579205943], 'precision': [0.7574560302735499], 'recall': [0.9798160119158187]}
##########eval dataset##########
acc--> [98.2171847125611]
F1--> {'F1': [0.8172675441455058], 'precision': [0.7336271916542039], 'recall': [0.9224461632011117]}
step 51/334, epoch 102/501 --> loss:0.8313208079338074
step 101/334, epoch 102/501 --> loss:0.8253364062309265
step 151/334, epoch 102/501 --> loss:0.8284032273292542
step 201/334, epoch 102/501 --> loss:0.816504967212677
step 251/334, epoch 102/501 --> loss:0.8249809432029724
step 301/334, epoch 102/501 --> loss:0.8275971639156342
step 51/334, epoch 103/501 --> loss:0.8334147703647613
step 101/334, epoch 103/501 --> loss:0.8245592713356018
step 151/334, epoch 103/501 --> loss:0.814326069355011
step 201/334, epoch 103/501 --> loss:0.8177675688266755
step 251/334, epoch 103/501 --> loss:0.8291163444519043
step 301/334, epoch 103/501 --> loss:0.8212397038936615
step 51/334, epoch 104/501 --> loss:0.8090654349327088
step 101/334, epoch 104/501 --> loss:0.8290586662292481
step 151/334, epoch 104/501 --> loss:0.8243879914283753
step 201/334, epoch 104/501 --> loss:0.8270977103710174
step 251/334, epoch 104/501 --> loss:0.8249663829803466
step 301/334, epoch 104/501 --> loss:0.824403133392334
step 51/334, epoch 105/501 --> loss:0.8204899632930756
step 101/334, epoch 105/501 --> loss:0.8279053890705108
step 151/334, epoch 105/501 --> loss:0.8353294348716735
step 201/334, epoch 105/501 --> loss:0.835275262594223
step 251/334, epoch 105/501 --> loss:0.8166951179504395
step 301/334, epoch 105/501 --> loss:0.8145089900493622
step 51/334, epoch 106/501 --> loss:0.822341343164444
step 101/334, epoch 106/501 --> loss:0.8313043558597565
step 151/334, epoch 106/501 --> loss:0.8201905834674835
step 201/334, epoch 106/501 --> loss:0.8359717440605163
step 251/334, epoch 106/501 --> loss:0.8215388178825378
step 301/334, epoch 106/501 --> loss:0.8130559432506561
step 51/334, epoch 107/501 --> loss:0.8302429246902466
step 101/334, epoch 107/501 --> loss:0.8217659497261047
step 151/334, epoch 107/501 --> loss:0.8179792737960816
step 201/334, epoch 107/501 --> loss:0.8377414989471436
step 251/334, epoch 107/501 --> loss:0.8254815316200257
step 301/334, epoch 107/501 --> loss:0.820004689693451
step 51/334, epoch 108/501 --> loss:0.8237848520278931
step 101/334, epoch 108/501 --> loss:0.824258040189743
step 151/334, epoch 108/501 --> loss:0.8435645973682404
step 201/334, epoch 108/501 --> loss:0.8200787901878357
step 251/334, epoch 108/501 --> loss:0.8279553914070129
step 301/334, epoch 108/501 --> loss:0.8266818034648895
step 51/334, epoch 109/501 --> loss:0.8233429777622223
step 101/334, epoch 109/501 --> loss:0.8200630605220794
step 151/334, epoch 109/501 --> loss:0.8269035029411316
step 201/334, epoch 109/501 --> loss:0.8330330121517181
step 251/334, epoch 109/501 --> loss:0.8343566536903382
step 301/334, epoch 109/501 --> loss:0.8155999529361725
step 51/334, epoch 110/501 --> loss:0.8437138319015502
step 101/334, epoch 110/501 --> loss:0.8205493772029877
step 151/334, epoch 110/501 --> loss:0.8128872179985046
step 201/334, epoch 110/501 --> loss:0.833500862121582
step 251/334, epoch 110/501 --> loss:0.824502409696579
step 301/334, epoch 110/501 --> loss:0.8185311663150787
step 51/334, epoch 111/501 --> loss:0.8216692745685578
step 101/334, epoch 111/501 --> loss:0.8311327874660492
step 151/334, epoch 111/501 --> loss:0.8049982845783233
step 201/334, epoch 111/501 --> loss:0.8253468561172486
step 251/334, epoch 111/501 --> loss:0.8358598399162293
step 301/334, epoch 111/501 --> loss:0.821392047405243

##########train dataset##########
acc--> [98.55545781996695]
F1--> {'F1': [0.8510070857190382], 'precision': [0.753045616483534], 'recall': [0.9782799175650992]}
##########eval dataset##########
acc--> [98.05842588996398]
F1--> {'F1': [0.8019133626778712], 'precision': [0.7172142534772593], 'recall': [0.9093089993249505]}
step 51/334, epoch 112/501 --> loss:0.8224843430519104
step 101/334, epoch 112/501 --> loss:0.8372007036209106
step 151/334, epoch 112/501 --> loss:0.8094174015522003
step 201/334, epoch 112/501 --> loss:0.8247783029079437
step 251/334, epoch 112/501 --> loss:0.8140372085571289
step 301/334, epoch 112/501 --> loss:0.8224164021015167
step 51/334, epoch 113/501 --> loss:0.8212809026241302
step 101/334, epoch 113/501 --> loss:0.8173524117469788
step 151/334, epoch 113/501 --> loss:0.8222044205665588
step 201/334, epoch 113/501 --> loss:0.827755925655365
step 251/334, epoch 113/501 --> loss:0.8312486946582794
step 301/334, epoch 113/501 --> loss:0.8185101807117462
step 51/334, epoch 114/501 --> loss:0.8315506660938263
step 101/334, epoch 114/501 --> loss:0.8253120970726013
step 151/334, epoch 114/501 --> loss:0.813128126859665
step 201/334, epoch 114/501 --> loss:0.814141411781311
step 251/334, epoch 114/501 --> loss:0.8264311218261718
step 301/334, epoch 114/501 --> loss:0.8253203749656677
step 51/334, epoch 115/501 --> loss:0.8248486709594727
step 101/334, epoch 115/501 --> loss:0.8159823107719422
step 151/334, epoch 115/501 --> loss:0.818888988494873
step 201/334, epoch 115/501 --> loss:0.8297829449176788
step 251/334, epoch 115/501 --> loss:0.8177418518066406
step 301/334, epoch 115/501 --> loss:0.8336710369586945
step 51/334, epoch 116/501 --> loss:0.8195304763317108
step 101/334, epoch 116/501 --> loss:0.8352894067764283
step 151/334, epoch 116/501 --> loss:0.8232178151607513
step 201/334, epoch 116/501 --> loss:0.8295182657241821
step 251/334, epoch 116/501 --> loss:0.8127981066703797
step 301/334, epoch 116/501 --> loss:0.8249977922439575
step 51/334, epoch 117/501 --> loss:0.8300990462303162
step 101/334, epoch 117/501 --> loss:0.8353231239318848
step 151/334, epoch 117/501 --> loss:0.8207704758644104
step 201/334, epoch 117/501 --> loss:0.8091531908512115
step 251/334, epoch 117/501 --> loss:0.8151374280452728
step 301/334, epoch 117/501 --> loss:0.8320778632164001
step 51/334, epoch 118/501 --> loss:0.8220051014423371
step 101/334, epoch 118/501 --> loss:0.833122615814209
step 151/334, epoch 118/501 --> loss:0.8175680541992187
step 201/334, epoch 118/501 --> loss:0.8310809481143951
step 251/334, epoch 118/501 --> loss:0.8219404149055481
step 301/334, epoch 118/501 --> loss:0.8274083709716797
step 51/334, epoch 119/501 --> loss:0.8223609459400177
step 101/334, epoch 119/501 --> loss:0.8296405410766602
step 151/334, epoch 119/501 --> loss:0.8279127407073975
step 201/334, epoch 119/501 --> loss:0.8200723040103912
step 251/334, epoch 119/501 --> loss:0.8291852498054504
step 301/334, epoch 119/501 --> loss:0.8202266156673431
step 51/334, epoch 120/501 --> loss:0.8318339431285858
step 101/334, epoch 120/501 --> loss:0.8222296106815338
step 151/334, epoch 120/501 --> loss:0.8250729513168334
step 201/334, epoch 120/501 --> loss:0.8199494731426239
step 251/334, epoch 120/501 --> loss:0.8121443450450897
step 301/334, epoch 120/501 --> loss:0.8351397538185119
step 51/334, epoch 121/501 --> loss:0.8250414752960205
step 101/334, epoch 121/501 --> loss:0.815520738363266
step 151/334, epoch 121/501 --> loss:0.8159203541278839
step 201/334, epoch 121/501 --> loss:0.8276249015331268
step 251/334, epoch 121/501 --> loss:0.8394389128684998
step 301/334, epoch 121/501 --> loss:0.8187254917621613

##########train dataset##########
acc--> [99.00767752236253]
F1--> {'F1': [0.892488844729216], 'precision': [0.8216359187443767], 'recall': [0.9767267291195959]}
##########eval dataset##########
acc--> [98.63318762857841]
F1--> {'F1': [0.8507633675693794], 'precision': [0.8054997804404397], 'recall': [0.9014280291398662]}
save model!
step 51/334, epoch 122/501 --> loss:0.8342787981033325
step 101/334, epoch 122/501 --> loss:0.8241117572784424
step 151/334, epoch 122/501 --> loss:0.8193689787387848
step 201/334, epoch 122/501 --> loss:0.832130856513977
step 251/334, epoch 122/501 --> loss:0.8195140790939331
step 301/334, epoch 122/501 --> loss:0.8112843632698059
step 51/334, epoch 123/501 --> loss:0.8126589858531952
step 101/334, epoch 123/501 --> loss:0.8320819568634034
step 151/334, epoch 123/501 --> loss:0.8196981215476989
step 201/334, epoch 123/501 --> loss:0.8274774694442749
step 251/334, epoch 123/501 --> loss:0.8397736012935638
step 301/334, epoch 123/501 --> loss:0.808509178161621
step 51/334, epoch 124/501 --> loss:0.828740656375885
step 101/334, epoch 124/501 --> loss:0.834534102678299
step 151/334, epoch 124/501 --> loss:0.8197129797935486
step 201/334, epoch 124/501 --> loss:0.8267888903617859
step 251/334, epoch 124/501 --> loss:0.8308818411827087
step 301/334, epoch 124/501 --> loss:0.8116043686866761
step 51/334, epoch 125/501 --> loss:0.8146757018566132
step 101/334, epoch 125/501 --> loss:0.8330520069599152
step 151/334, epoch 125/501 --> loss:0.8348506700992584
step 201/334, epoch 125/501 --> loss:0.8276711845397949
step 251/334, epoch 125/501 --> loss:0.8333509206771851
step 301/334, epoch 125/501 --> loss:0.800718162059784
step 51/334, epoch 126/501 --> loss:0.8305744683742523
step 101/334, epoch 126/501 --> loss:0.8272254645824433
step 151/334, epoch 126/501 --> loss:0.8048483550548553
step 201/334, epoch 126/501 --> loss:0.8285643029212951
step 251/334, epoch 126/501 --> loss:0.8249866652488709
step 301/334, epoch 126/501 --> loss:0.8305394124984741
step 51/334, epoch 127/501 --> loss:0.8221580123901367
step 101/334, epoch 127/501 --> loss:0.8298393988609314
step 151/334, epoch 127/501 --> loss:0.826421092748642
step 201/334, epoch 127/501 --> loss:0.8124486660957336
step 251/334, epoch 127/501 --> loss:0.8251152610778809
step 301/334, epoch 127/501 --> loss:0.8355485761165619
step 51/334, epoch 128/501 --> loss:0.8439106607437133
step 101/334, epoch 128/501 --> loss:0.8389415526390076
step 151/334, epoch 128/501 --> loss:0.805826553106308
step 201/334, epoch 128/501 --> loss:0.8269794166088105
step 251/334, epoch 128/501 --> loss:0.8071773433685303
step 301/334, epoch 128/501 --> loss:0.8344139528274536
step 51/334, epoch 129/501 --> loss:0.8230992603302002
step 101/334, epoch 129/501 --> loss:0.8223875343799592
step 151/334, epoch 129/501 --> loss:0.8348786318302155
step 201/334, epoch 129/501 --> loss:0.8145019543170929
step 251/334, epoch 129/501 --> loss:0.8238522791862488
step 301/334, epoch 129/501 --> loss:0.8350225269794465
step 51/334, epoch 130/501 --> loss:0.8295624768733978
step 101/334, epoch 130/501 --> loss:0.808296183347702
step 151/334, epoch 130/501 --> loss:0.8336663448810577
step 201/334, epoch 130/501 --> loss:0.8237081849575043
step 251/334, epoch 130/501 --> loss:0.8183211386203766
step 301/334, epoch 130/501 --> loss:0.8300674164295196
step 51/334, epoch 131/501 --> loss:0.8221778929233551
step 101/334, epoch 131/501 --> loss:0.8275476694107056
step 151/334, epoch 131/501 --> loss:0.8177417778968811
step 201/334, epoch 131/501 --> loss:0.8163410758972168
step 251/334, epoch 131/501 --> loss:0.8208734786510468
step 301/334, epoch 131/501 --> loss:0.838322958946228

##########train dataset##########
acc--> [98.67835629509041]
F1--> {'F1': [0.8622790030511335], 'precision': [0.7691163198683292], 'recall': [0.9811345635603003]}
##########eval dataset##########
acc--> [98.278562957944]
F1--> {'F1': [0.8216583465098671], 'precision': [0.7439403129645673], 'recall': [0.9175210780534069]}
step 51/334, epoch 132/501 --> loss:0.8103137946128846
step 101/334, epoch 132/501 --> loss:0.8341708219051361
step 151/334, epoch 132/501 --> loss:0.8038918733596802
step 201/334, epoch 132/501 --> loss:0.8349407923221588
step 251/334, epoch 132/501 --> loss:0.8189782071113586
step 301/334, epoch 132/501 --> loss:0.8367115151882172
step 51/334, epoch 133/501 --> loss:0.8119763505458831
step 101/334, epoch 133/501 --> loss:0.8185497486591339
step 151/334, epoch 133/501 --> loss:0.8285014247894287
step 201/334, epoch 133/501 --> loss:0.8368778800964356
step 251/334, epoch 133/501 --> loss:0.8155326819419861
step 301/334, epoch 133/501 --> loss:0.8399137544631958
step 51/334, epoch 134/501 --> loss:0.8198817455768586
step 101/334, epoch 134/501 --> loss:0.8123281157016754
step 151/334, epoch 134/501 --> loss:0.8182407295703888
step 201/334, epoch 134/501 --> loss:0.8340659582614899
step 251/334, epoch 134/501 --> loss:0.829303662776947
step 301/334, epoch 134/501 --> loss:0.8317318320274353
step 51/334, epoch 135/501 --> loss:0.8312184345722199
step 101/334, epoch 135/501 --> loss:0.8234657144546509
step 151/334, epoch 135/501 --> loss:0.8177601218223571
step 201/334, epoch 135/501 --> loss:0.8260052967071533
step 251/334, epoch 135/501 --> loss:0.8219630682468414
step 301/334, epoch 135/501 --> loss:0.8211675536632538
step 51/334, epoch 136/501 --> loss:0.8302804327011108
step 101/334, epoch 136/501 --> loss:0.8327781653404236
step 151/334, epoch 136/501 --> loss:0.8188339996337891
step 201/334, epoch 136/501 --> loss:0.8149573051929474
step 251/334, epoch 136/501 --> loss:0.8255253326892853
step 301/334, epoch 136/501 --> loss:0.8289317226409912
step 51/334, epoch 137/501 --> loss:0.8285051381587982
step 101/334, epoch 137/501 --> loss:0.8342526113986969
step 151/334, epoch 137/501 --> loss:0.8147658324241638
step 201/334, epoch 137/501 --> loss:0.8335089087486267
step 251/334, epoch 137/501 --> loss:0.8260695207118988
step 301/334, epoch 137/501 --> loss:0.8182694351673127
step 51/334, epoch 138/501 --> loss:0.8298583006858826
step 101/334, epoch 138/501 --> loss:0.8187127554416657
step 151/334, epoch 138/501 --> loss:0.8309692120552064
step 201/334, epoch 138/501 --> loss:0.8174831652641297
step 251/334, epoch 138/501 --> loss:0.8163724970817566
step 301/334, epoch 138/501 --> loss:0.8244250357151032
step 51/334, epoch 139/501 --> loss:0.8148304271697998
step 101/334, epoch 139/501 --> loss:0.8387081909179688
step 151/334, epoch 139/501 --> loss:0.827250953912735
step 201/334, epoch 139/501 --> loss:0.8364793705940247
step 251/334, epoch 139/501 --> loss:0.8143974268436431
step 301/334, epoch 139/501 --> loss:0.8158463418483735
step 51/334, epoch 140/501 --> loss:0.8222047448158264
step 101/334, epoch 140/501 --> loss:0.8231709372997283
step 151/334, epoch 140/501 --> loss:0.8345039355754852
step 201/334, epoch 140/501 --> loss:0.8022177147865296
step 251/334, epoch 140/501 --> loss:0.8203968966007232
step 301/334, epoch 140/501 --> loss:0.8208234620094299
step 51/334, epoch 141/501 --> loss:0.8263510894775391
step 101/334, epoch 141/501 --> loss:0.8105124795436859
step 151/334, epoch 141/501 --> loss:0.8391087317466736
step 201/334, epoch 141/501 --> loss:0.827028751373291
step 251/334, epoch 141/501 --> loss:0.8262169456481934
step 301/334, epoch 141/501 --> loss:0.8184609138965606

##########train dataset##########
acc--> [99.01839529122297]
F1--> {'F1': [0.8942204738116937], 'precision': [0.8195317997862436], 'recall': [0.9838998725601426]}
##########eval dataset##########
acc--> [98.5376279602252]
F1--> {'F1': [0.8432369497900596], 'precision': [0.7855928037733662], 'recall': [0.910022025511867]}
step 51/334, epoch 142/501 --> loss:0.8284113895893097
step 101/334, epoch 142/501 --> loss:0.8197544813156128
step 151/334, epoch 142/501 --> loss:0.8241282594203949
step 201/334, epoch 142/501 --> loss:0.8136718499660492
step 251/334, epoch 142/501 --> loss:0.8192327582836151
step 301/334, epoch 142/501 --> loss:0.8286085510253907
step 51/334, epoch 143/501 --> loss:0.8153005540370941
step 101/334, epoch 143/501 --> loss:0.8376130497455597
step 151/334, epoch 143/501 --> loss:0.8342461979389191
step 201/334, epoch 143/501 --> loss:0.8318728363513946
step 251/334, epoch 143/501 --> loss:0.8244086503982544
step 301/334, epoch 143/501 --> loss:0.8196226620674133
step 51/334, epoch 144/501 --> loss:0.8245249426364899
step 101/334, epoch 144/501 --> loss:0.834595549106598
step 151/334, epoch 144/501 --> loss:0.8110269856452942
step 201/334, epoch 144/501 --> loss:0.8297394871711731
step 251/334, epoch 144/501 --> loss:0.8126398634910583
step 301/334, epoch 144/501 --> loss:0.8208940327167511
step 51/334, epoch 145/501 --> loss:0.8246590399742126
step 101/334, epoch 145/501 --> loss:0.8275575435161591
step 151/334, epoch 145/501 --> loss:0.8195287573337555
step 201/334, epoch 145/501 --> loss:0.8175044023990631
step 251/334, epoch 145/501 --> loss:0.8311121547222138
step 301/334, epoch 145/501 --> loss:0.8281461608409881
step 51/334, epoch 146/501 --> loss:0.8170813298225403
step 101/334, epoch 146/501 --> loss:0.8239312982559204
step 151/334, epoch 146/501 --> loss:0.8234707093238831
step 201/334, epoch 146/501 --> loss:0.8148727023601532
step 251/334, epoch 146/501 --> loss:0.8333076310157775
step 301/334, epoch 146/501 --> loss:0.8185714948177337
step 51/334, epoch 147/501 --> loss:0.8200899982452392
step 101/334, epoch 147/501 --> loss:0.8281430041790009
step 151/334, epoch 147/501 --> loss:0.8183284664154052
step 201/334, epoch 147/501 --> loss:0.8259946954250336
step 251/334, epoch 147/501 --> loss:0.8237101912498475
step 301/334, epoch 147/501 --> loss:0.8328113842010498
step 51/334, epoch 148/501 --> loss:0.8222212851047516
step 101/334, epoch 148/501 --> loss:0.8320297408103943
step 151/334, epoch 148/501 --> loss:0.8246828198432923
step 201/334, epoch 148/501 --> loss:0.8234425854682922
step 251/334, epoch 148/501 --> loss:0.8165308105945587
step 301/334, epoch 148/501 --> loss:0.8237974154949188
step 51/334, epoch 149/501 --> loss:0.8091525495052337
step 101/334, epoch 149/501 --> loss:0.8238651740550995
step 151/334, epoch 149/501 --> loss:0.8327358877658844
step 201/334, epoch 149/501 --> loss:0.8327141809463501
step 251/334, epoch 149/501 --> loss:0.8184097588062287
step 301/334, epoch 149/501 --> loss:0.8374929606914521
step 51/334, epoch 150/501 --> loss:0.8248262786865235
step 101/334, epoch 150/501 --> loss:0.8247658097743988
step 151/334, epoch 150/501 --> loss:0.8108893597126007
step 201/334, epoch 150/501 --> loss:0.8294906544685364
step 251/334, epoch 150/501 --> loss:0.8270388662815094
step 301/334, epoch 150/501 --> loss:0.8263423931598664
step 51/334, epoch 151/501 --> loss:0.8236165559291839
step 101/334, epoch 151/501 --> loss:0.8084116709232331
step 151/334, epoch 151/501 --> loss:0.8242419636249543
step 201/334, epoch 151/501 --> loss:0.8205933356285096
step 251/334, epoch 151/501 --> loss:0.8290455317497254
step 301/334, epoch 151/501 --> loss:0.8206230866909027

##########train dataset##########
acc--> [98.85883395663515]
F1--> {'F1': [0.8793701086458752], 'precision': [0.793328903888963], 'recall': [0.9863574878538748]}
##########eval dataset##########
acc--> [98.25486511069614]
F1--> {'F1': [0.8207695660465877], 'precision': [0.7379519806194542], 'recall': [0.9245381391006071]}
step 51/334, epoch 152/501 --> loss:0.8182883763313293
step 101/334, epoch 152/501 --> loss:0.8263590860366822
step 151/334, epoch 152/501 --> loss:0.8382864618301391
step 201/334, epoch 152/501 --> loss:0.8131125557422638
step 251/334, epoch 152/501 --> loss:0.8180638515949249
step 301/334, epoch 152/501 --> loss:0.8189056897163391
step 51/334, epoch 153/501 --> loss:0.8103353607654572
step 101/334, epoch 153/501 --> loss:0.8174265611171723
step 151/334, epoch 153/501 --> loss:0.8255100452899933
step 201/334, epoch 153/501 --> loss:0.8268030107021331
step 251/334, epoch 153/501 --> loss:0.8315551316738129
step 301/334, epoch 153/501 --> loss:0.8282812559604644
step 51/334, epoch 154/501 --> loss:0.8166263055801392
step 101/334, epoch 154/501 --> loss:0.8315148842334747
step 151/334, epoch 154/501 --> loss:0.8080183804035187
step 201/334, epoch 154/501 --> loss:0.8243436372280121
step 251/334, epoch 154/501 --> loss:0.8133666884899139
step 301/334, epoch 154/501 --> loss:0.8291142117977143
step 51/334, epoch 155/501 --> loss:0.8203228855133057
step 101/334, epoch 155/501 --> loss:0.8310147869586945
step 151/334, epoch 155/501 --> loss:0.8089382731914521
step 201/334, epoch 155/501 --> loss:0.8269319605827331
step 251/334, epoch 155/501 --> loss:0.8176297676563263
step 301/334, epoch 155/501 --> loss:0.8308738613128662
step 51/334, epoch 156/501 --> loss:0.8362664866447449
step 101/334, epoch 156/501 --> loss:0.802645663022995
step 151/334, epoch 156/501 --> loss:0.8264892542362213
step 201/334, epoch 156/501 --> loss:0.8211061549186707
step 251/334, epoch 156/501 --> loss:0.8218321382999421
step 301/334, epoch 156/501 --> loss:0.8271479928493499
step 51/334, epoch 157/501 --> loss:0.8144482386112213
step 101/334, epoch 157/501 --> loss:0.8290562915802002
step 151/334, epoch 157/501 --> loss:0.8334955859184265
step 201/334, epoch 157/501 --> loss:0.8334527945518494
step 251/334, epoch 157/501 --> loss:0.8135008811950684
step 301/334, epoch 157/501 --> loss:0.8143146288394928
step 51/334, epoch 158/501 --> loss:0.8161366605758666
step 101/334, epoch 158/501 --> loss:0.8186141610145569
step 151/334, epoch 158/501 --> loss:0.8320991265773773
step 201/334, epoch 158/501 --> loss:0.8354909658432007
step 251/334, epoch 158/501 --> loss:0.8225619494915009
step 301/334, epoch 158/501 --> loss:0.8201807463169097
step 51/334, epoch 159/501 --> loss:0.8164603233337402
step 101/334, epoch 159/501 --> loss:0.8094896912574768
step 151/334, epoch 159/501 --> loss:0.8356647706031799
step 201/334, epoch 159/501 --> loss:0.8380304396152496
step 251/334, epoch 159/501 --> loss:0.8218696451187134
step 301/334, epoch 159/501 --> loss:0.8262158501148223
step 51/334, epoch 160/501 --> loss:0.8324478149414063
step 101/334, epoch 160/501 --> loss:0.8137700712680817
step 151/334, epoch 160/501 --> loss:0.8110105562210083
step 201/334, epoch 160/501 --> loss:0.8330020749568939
step 251/334, epoch 160/501 --> loss:0.8189183104038239
step 301/334, epoch 160/501 --> loss:0.8298691940307618
step 51/334, epoch 161/501 --> loss:0.8195595300197601
step 101/334, epoch 161/501 --> loss:0.8243243408203125
step 151/334, epoch 161/501 --> loss:0.815105949640274
step 201/334, epoch 161/501 --> loss:0.8202565348148346
step 251/334, epoch 161/501 --> loss:0.8245280873775482
step 301/334, epoch 161/501 --> loss:0.815615314245224

##########train dataset##########
acc--> [98.89218702627099]
F1--> {'F1': [0.8826127020354996], 'precision': [0.7978046100772925], 'recall': [0.9876083414563154]}
##########eval dataset##########
acc--> [98.35227238745378]
F1--> {'F1': [0.8280512692160005], 'precision': [0.7541820095611848], 'recall': [0.9179743446461294]}
step 51/334, epoch 162/501 --> loss:0.8296839690208435
step 101/334, epoch 162/501 --> loss:0.8211178410053254
step 151/334, epoch 162/501 --> loss:0.8275515007972717
step 201/334, epoch 162/501 --> loss:0.8312465536594391
step 251/334, epoch 162/501 --> loss:0.8162135469913483
step 301/334, epoch 162/501 --> loss:0.8143166399002075
step 51/334, epoch 163/501 --> loss:0.8240647649765015
step 101/334, epoch 163/501 --> loss:0.839221304655075
step 151/334, epoch 163/501 --> loss:0.8215335988998413
step 201/334, epoch 163/501 --> loss:0.8134748899936676
step 251/334, epoch 163/501 --> loss:0.8147626507282257
step 301/334, epoch 163/501 --> loss:0.821605681180954
step 51/334, epoch 164/501 --> loss:0.8156945550441742
step 101/334, epoch 164/501 --> loss:0.8163149237632752
step 151/334, epoch 164/501 --> loss:0.8256036925315857
step 201/334, epoch 164/501 --> loss:0.8292276453971863
step 251/334, epoch 164/501 --> loss:0.8226788103580475
step 301/334, epoch 164/501 --> loss:0.8269045507907867
step 51/334, epoch 165/501 --> loss:0.8240944242477417
step 101/334, epoch 165/501 --> loss:0.8129574275016784
step 151/334, epoch 165/501 --> loss:0.8225806379318237
step 201/334, epoch 165/501 --> loss:0.8205184388160706
step 251/334, epoch 165/501 --> loss:0.8256768548488617
step 301/334, epoch 165/501 --> loss:0.8277406442165375
step 51/334, epoch 166/501 --> loss:0.8310365545749664
step 101/334, epoch 166/501 --> loss:0.8145354855060577
step 151/334, epoch 166/501 --> loss:0.8319392895698547
step 201/334, epoch 166/501 --> loss:0.8223956489562988
step 251/334, epoch 166/501 --> loss:0.8275076854228973
step 301/334, epoch 166/501 --> loss:0.8152136266231537
step 51/334, epoch 167/501 --> loss:0.8313349139690399
step 101/334, epoch 167/501 --> loss:0.8161845338344574
step 151/334, epoch 167/501 --> loss:0.819285968542099
step 201/334, epoch 167/501 --> loss:0.8073662757873535
step 251/334, epoch 167/501 --> loss:0.8276839327812194
step 301/334, epoch 167/501 --> loss:0.8365532088279725
step 51/334, epoch 168/501 --> loss:0.8146947956085205
step 101/334, epoch 168/501 --> loss:0.828604885339737
step 151/334, epoch 168/501 --> loss:0.8296395564079284
step 201/334, epoch 168/501 --> loss:0.8203141605854034
step 251/334, epoch 168/501 --> loss:0.8247889602184295
step 301/334, epoch 168/501 --> loss:0.8171566474437714
step 51/334, epoch 169/501 --> loss:0.8242900896072388
step 101/334, epoch 169/501 --> loss:0.8140005493164062
step 151/334, epoch 169/501 --> loss:0.8105634105205536
step 201/334, epoch 169/501 --> loss:0.8242229902744294
step 251/334, epoch 169/501 --> loss:0.8311340725421905
step 301/334, epoch 169/501 --> loss:0.8314604413509369
step 51/334, epoch 170/501 --> loss:0.8233460211753845
step 101/334, epoch 170/501 --> loss:0.8245677042007447
step 151/334, epoch 170/501 --> loss:0.8267444586753845
step 201/334, epoch 170/501 --> loss:0.808173109292984
step 251/334, epoch 170/501 --> loss:0.8188704526424408
step 301/334, epoch 170/501 --> loss:0.8358637154102325
step 51/334, epoch 171/501 --> loss:0.825340461730957
step 101/334, epoch 171/501 --> loss:0.8364447426795959
step 151/334, epoch 171/501 --> loss:0.8196997845172882
step 201/334, epoch 171/501 --> loss:0.8123489964008331
step 251/334, epoch 171/501 --> loss:0.8226158821582794
step 301/334, epoch 171/501 --> loss:0.8313158059120178

##########train dataset##########
acc--> [98.66575592235259]
F1--> {'F1': [0.8611069184632194], 'precision': [0.7674612454601983], 'recall': [0.9807947845096157]}
##########eval dataset##########
acc--> [98.15855518517293]
F1--> {'F1': [0.8105332196792538], 'precision': [0.7298121928110802], 'recall': [0.9113436798561665]}
step 51/334, epoch 172/501 --> loss:0.8377503478527069
step 101/334, epoch 172/501 --> loss:0.8234727251529693
step 151/334, epoch 172/501 --> loss:0.8269854009151458
step 201/334, epoch 172/501 --> loss:0.8250186884403229
step 251/334, epoch 172/501 --> loss:0.8111917281150818
step 301/334, epoch 172/501 --> loss:0.8104675018787384
step 51/334, epoch 173/501 --> loss:0.8320490789413452
step 101/334, epoch 173/501 --> loss:0.8284581112861633
step 151/334, epoch 173/501 --> loss:0.8337474024295807
step 201/334, epoch 173/501 --> loss:0.8256656527519226
step 251/334, epoch 173/501 --> loss:0.8222530841827392
step 301/334, epoch 173/501 --> loss:0.8027536249160767
step 51/334, epoch 174/501 --> loss:0.8189423310756684
step 101/334, epoch 174/501 --> loss:0.8290748417377471
step 151/334, epoch 174/501 --> loss:0.8060825133323669
step 201/334, epoch 174/501 --> loss:0.830721994638443
step 251/334, epoch 174/501 --> loss:0.8257325518131257
step 301/334, epoch 174/501 --> loss:0.8240505003929138
step 51/334, epoch 175/501 --> loss:0.8117839276790619
step 101/334, epoch 175/501 --> loss:0.8448331916332245
step 151/334, epoch 175/501 --> loss:0.8126489400863648
step 201/334, epoch 175/501 --> loss:0.8241172206401824
step 251/334, epoch 175/501 --> loss:0.8333975672721863
step 301/334, epoch 175/501 --> loss:0.81581139087677
step 51/334, epoch 176/501 --> loss:0.8191509616374969
step 101/334, epoch 176/501 --> loss:0.8248462343215942
step 151/334, epoch 176/501 --> loss:0.8076574957370758
step 201/334, epoch 176/501 --> loss:0.8223484218120575
step 251/334, epoch 176/501 --> loss:0.8335563004016876
step 301/334, epoch 176/501 --> loss:0.8263185155391694
step 51/334, epoch 177/501 --> loss:0.8299809491634369
step 101/334, epoch 177/501 --> loss:0.8247758018970489
step 151/334, epoch 177/501 --> loss:0.8163177716732025
step 201/334, epoch 177/501 --> loss:0.8357945299148559
step 251/334, epoch 177/501 --> loss:0.8197686338424682
step 301/334, epoch 177/501 --> loss:0.8235283601284027
step 51/334, epoch 178/501 --> loss:0.8228895449638367
step 101/334, epoch 178/501 --> loss:0.8078969812393189
step 151/334, epoch 178/501 --> loss:0.8268192064762115
step 201/334, epoch 178/501 --> loss:0.8237897670269012
step 251/334, epoch 178/501 --> loss:0.8334522676467896
step 301/334, epoch 178/501 --> loss:0.8268607950210571
step 51/334, epoch 179/501 --> loss:0.8105194473266601
step 101/334, epoch 179/501 --> loss:0.8206892156600952
step 151/334, epoch 179/501 --> loss:0.8292822742462158
step 201/334, epoch 179/501 --> loss:0.8212142336368561
step 251/334, epoch 179/501 --> loss:0.8314970314502717
step 301/334, epoch 179/501 --> loss:0.8190533530712127
step 51/334, epoch 180/501 --> loss:0.8265630352497101
step 101/334, epoch 180/501 --> loss:0.8203335130214691
step 151/334, epoch 180/501 --> loss:0.8126214146614075
step 201/334, epoch 180/501 --> loss:0.8160342562198639
step 251/334, epoch 180/501 --> loss:0.8337771165370941
step 301/334, epoch 180/501 --> loss:0.8246877753734588
step 51/334, epoch 181/501 --> loss:0.8302176880836487
step 101/334, epoch 181/501 --> loss:0.8227846491336822
step 151/334, epoch 181/501 --> loss:0.8230785512924195
step 201/334, epoch 181/501 --> loss:0.8187768745422364
step 251/334, epoch 181/501 --> loss:0.8179568302631378
step 301/334, epoch 181/501 --> loss:0.8183133351802826

##########train dataset##########
acc--> [99.17732785156332]
F1--> {'F1': [0.9100001104802091], 'precision': [0.8446793586482552], 'recall': [0.9862820567689554]}
##########eval dataset##########
acc--> [98.64450089261254]
F1--> {'F1': [0.8522763779771907], 'precision': [0.8055810599497982], 'recall': [0.9047293851714263]}
save model!
step 51/334, epoch 182/501 --> loss:0.8325666213035583
step 101/334, epoch 182/501 --> loss:0.814937949180603
step 151/334, epoch 182/501 --> loss:0.8303232920169831
step 201/334, epoch 182/501 --> loss:0.822908718585968
step 251/334, epoch 182/501 --> loss:0.8264385831356048
step 301/334, epoch 182/501 --> loss:0.8127791237831116
step 51/334, epoch 183/501 --> loss:0.8271972835063934
step 101/334, epoch 183/501 --> loss:0.8243541491031646
step 151/334, epoch 183/501 --> loss:0.8236008560657502
step 201/334, epoch 183/501 --> loss:0.8176102471351624
step 251/334, epoch 183/501 --> loss:0.8346586513519287
step 301/334, epoch 183/501 --> loss:0.8202613461017608
step 51/334, epoch 184/501 --> loss:0.8222684586048126
step 101/334, epoch 184/501 --> loss:0.8215240275859833
step 151/334, epoch 184/501 --> loss:0.8155914735794068
step 201/334, epoch 184/501 --> loss:0.8214558243751526
step 251/334, epoch 184/501 --> loss:0.8136582899093628
step 301/334, epoch 184/501 --> loss:0.8384237587451935
step 51/334, epoch 185/501 --> loss:0.8280745506286621
step 101/334, epoch 185/501 --> loss:0.8091464364528655
step 151/334, epoch 185/501 --> loss:0.832142231464386
step 201/334, epoch 185/501 --> loss:0.8192219138145447
step 251/334, epoch 185/501 --> loss:0.8279724836349487
step 301/334, epoch 185/501 --> loss:0.8281428110599518
step 51/334, epoch 186/501 --> loss:0.8305988776683807
step 101/334, epoch 186/501 --> loss:0.8333782303333283
step 151/334, epoch 186/501 --> loss:0.8060664522647858
step 201/334, epoch 186/501 --> loss:0.8325025880336762
step 251/334, epoch 186/501 --> loss:0.8155772399902343
step 301/334, epoch 186/501 --> loss:0.8259630429744721
step 51/334, epoch 187/501 --> loss:0.8206641387939453
step 101/334, epoch 187/501 --> loss:0.8347477960586548
step 151/334, epoch 187/501 --> loss:0.8355208587646484
step 201/334, epoch 187/501 --> loss:0.8190033316612244
step 251/334, epoch 187/501 --> loss:0.8169346976280213
step 301/334, epoch 187/501 --> loss:0.812308177947998
step 51/334, epoch 188/501 --> loss:0.8347260737419129
step 101/334, epoch 188/501 --> loss:0.8144216752052307
step 151/334, epoch 188/501 --> loss:0.8161658918857575
step 201/334, epoch 188/501 --> loss:0.8309853661060334
step 251/334, epoch 188/501 --> loss:0.8176866149902344
step 301/334, epoch 188/501 --> loss:0.8271186125278472
step 51/334, epoch 189/501 --> loss:0.8293585193157196
step 101/334, epoch 189/501 --> loss:0.8257671678066254
step 151/334, epoch 189/501 --> loss:0.8157588243484497
step 201/334, epoch 189/501 --> loss:0.8364371621608734
step 251/334, epoch 189/501 --> loss:0.8158976829051972
step 301/334, epoch 189/501 --> loss:0.8239121317863465
step 51/334, epoch 190/501 --> loss:0.8275850641727448
step 101/334, epoch 190/501 --> loss:0.8237709045410156
step 151/334, epoch 190/501 --> loss:0.8110620474815369
step 201/334, epoch 190/501 --> loss:0.8263466119766235
step 251/334, epoch 190/501 --> loss:0.8304275155067444
step 301/334, epoch 190/501 --> loss:0.8341298902034759
step 51/334, epoch 191/501 --> loss:0.8370666909217834
step 101/334, epoch 191/501 --> loss:0.8154625248908997
step 151/334, epoch 191/501 --> loss:0.8083194804191589
step 201/334, epoch 191/501 --> loss:0.8295828878879548
step 251/334, epoch 191/501 --> loss:0.8267464363574981
step 301/334, epoch 191/501 --> loss:0.8212227010726929

##########train dataset##########
acc--> [99.25514195445133]
F1--> {'F1': [0.9176628788633374], 'precision': [0.8594666758269537], 'recall': [0.9843241045791048]}
##########eval dataset##########
acc--> [98.74778073394462]
F1--> {'F1': [0.8606181797465462], 'precision': [0.8292288551859976], 'recall': [0.8944881855701314]}
save model!
step 51/334, epoch 192/501 --> loss:0.8168535768985749
step 101/334, epoch 192/501 --> loss:0.8306556272506714
step 151/334, epoch 192/501 --> loss:0.816640453338623
step 201/334, epoch 192/501 --> loss:0.8286063826084137
step 251/334, epoch 192/501 --> loss:0.8147944414615631
step 301/334, epoch 192/501 --> loss:0.8172313106060028
step 51/334, epoch 193/501 --> loss:0.8432138466835022
step 101/334, epoch 193/501 --> loss:0.8171959400177002
step 151/334, epoch 193/501 --> loss:0.8218626093864441
step 201/334, epoch 193/501 --> loss:0.831851818561554
step 251/334, epoch 193/501 --> loss:0.8258725416660309
step 301/334, epoch 193/501 --> loss:0.8041357004642486
step 51/334, epoch 194/501 --> loss:0.8214752626419067
step 101/334, epoch 194/501 --> loss:0.8146295595169067
step 151/334, epoch 194/501 --> loss:0.8237917447090148
step 201/334, epoch 194/501 --> loss:0.822713166475296
step 251/334, epoch 194/501 --> loss:0.827190066576004
step 301/334, epoch 194/501 --> loss:0.831595938205719
step 51/334, epoch 195/501 --> loss:0.8132323884963989
step 101/334, epoch 195/501 --> loss:0.8285158967971802
step 151/334, epoch 195/501 --> loss:0.8211149930953979
step 201/334, epoch 195/501 --> loss:0.8164516484737396
step 251/334, epoch 195/501 --> loss:0.8337944221496582
step 301/334, epoch 195/501 --> loss:0.8211401748657227
step 51/334, epoch 196/501 --> loss:0.8180667841434479
step 101/334, epoch 196/501 --> loss:0.8353980350494384
step 151/334, epoch 196/501 --> loss:0.8203951644897461
step 201/334, epoch 196/501 --> loss:0.8285843253135681
step 251/334, epoch 196/501 --> loss:0.8176086556911468
step 301/334, epoch 196/501 --> loss:0.8202306592464447
step 51/334, epoch 197/501 --> loss:0.8131545257568359
step 101/334, epoch 197/501 --> loss:0.8214911592006683
step 151/334, epoch 197/501 --> loss:0.8242986106872559
step 201/334, epoch 197/501 --> loss:0.8181152462959289
step 251/334, epoch 197/501 --> loss:0.8260158240795136
step 301/334, epoch 197/501 --> loss:0.8246580719947815
step 51/334, epoch 198/501 --> loss:0.819810950756073
step 101/334, epoch 198/501 --> loss:0.8199996376037597
step 151/334, epoch 198/501 --> loss:0.8182421135902405
step 201/334, epoch 198/501 --> loss:0.8381585919857025
step 251/334, epoch 198/501 --> loss:0.8264369928836822
step 301/334, epoch 198/501 --> loss:0.8124238896369934
step 51/334, epoch 199/501 --> loss:0.825974566936493
step 101/334, epoch 199/501 --> loss:0.8221358108520508
step 151/334, epoch 199/501 --> loss:0.8155072760581971
step 201/334, epoch 199/501 --> loss:0.8319372296333313
step 251/334, epoch 199/501 --> loss:0.8243202865123749
step 301/334, epoch 199/501 --> loss:0.8168445110321045
step 51/334, epoch 200/501 --> loss:0.8125329899787903
step 101/334, epoch 200/501 --> loss:0.8136431729793548
step 151/334, epoch 200/501 --> loss:0.8295993208885193
step 201/334, epoch 200/501 --> loss:0.8295971238613129
step 251/334, epoch 200/501 --> loss:0.8105489563941956
step 301/334, epoch 200/501 --> loss:0.8303290522098541
step 51/334, epoch 201/501 --> loss:0.8140531170368195
step 101/334, epoch 201/501 --> loss:0.8188395047187805
step 151/334, epoch 201/501 --> loss:0.8140183198451996
step 201/334, epoch 201/501 --> loss:0.8395188629627228
step 251/334, epoch 201/501 --> loss:0.8127043175697327
step 301/334, epoch 201/501 --> loss:0.8298262345790863

##########train dataset##########
acc--> [99.15685664413735]
F1--> {'F1': [0.9080113498367757], 'precision': [0.840876307826421], 'recall': [0.9868083106869453]}
##########eval dataset##########
acc--> [98.58929157673602]
F1--> {'F1': [0.8471192220016355], 'precision': [0.7967406808309075], 'recall': [0.9043100942687887]}
step 51/334, epoch 202/501 --> loss:0.8215509819984436
step 101/334, epoch 202/501 --> loss:0.8224633014202118
step 151/334, epoch 202/501 --> loss:0.8244467580318451
step 201/334, epoch 202/501 --> loss:0.8376954770088196
step 251/334, epoch 202/501 --> loss:0.8132548666000367
step 301/334, epoch 202/501 --> loss:0.8185463619232177
step 51/334, epoch 203/501 --> loss:0.8377744793891907
step 101/334, epoch 203/501 --> loss:0.815129029750824
step 151/334, epoch 203/501 --> loss:0.8241878092288971
step 201/334, epoch 203/501 --> loss:0.8191349387168885
step 251/334, epoch 203/501 --> loss:0.8070524454116821
step 301/334, epoch 203/501 --> loss:0.8201117146015168
step 51/334, epoch 204/501 --> loss:0.8312885177135467
step 101/334, epoch 204/501 --> loss:0.8171834564208984
step 151/334, epoch 204/501 --> loss:0.832149465084076
step 201/334, epoch 204/501 --> loss:0.8110075783729553
step 251/334, epoch 204/501 --> loss:0.8263086223602295
step 301/334, epoch 204/501 --> loss:0.8129049408435821
step 51/334, epoch 205/501 --> loss:0.8176705169677735
step 101/334, epoch 205/501 --> loss:0.8215356171131134
step 151/334, epoch 205/501 --> loss:0.8186015367507935
step 201/334, epoch 205/501 --> loss:0.8300316512584687
step 251/334, epoch 205/501 --> loss:0.8230908060073853
step 301/334, epoch 205/501 --> loss:0.8258200097084045
step 51/334, epoch 206/501 --> loss:0.8101627039909363
step 101/334, epoch 206/501 --> loss:0.8281109082698822
step 151/334, epoch 206/501 --> loss:0.8190035915374756
step 201/334, epoch 206/501 --> loss:0.8293705630302429
step 251/334, epoch 206/501 --> loss:0.8226913440227509
step 301/334, epoch 206/501 --> loss:0.8321445083618164
step 51/334, epoch 207/501 --> loss:0.8237001836299896
step 101/334, epoch 207/501 --> loss:0.8220249724388122
step 151/334, epoch 207/501 --> loss:0.8289754462242126
step 201/334, epoch 207/501 --> loss:0.8185538458824158
step 251/334, epoch 207/501 --> loss:0.8158228003978729
step 301/334, epoch 207/501 --> loss:0.8269842970371246
step 51/334, epoch 208/501 --> loss:0.8349923157691955
step 101/334, epoch 208/501 --> loss:0.8167517757415772
step 151/334, epoch 208/501 --> loss:0.8185274493694306
step 201/334, epoch 208/501 --> loss:0.8126140880584717
step 251/334, epoch 208/501 --> loss:0.8251320779323578
step 301/334, epoch 208/501 --> loss:0.818114482164383
step 51/334, epoch 209/501 --> loss:0.8263983249664306
step 101/334, epoch 209/501 --> loss:0.8276649510860443
step 151/334, epoch 209/501 --> loss:0.8207582068443299
step 201/334, epoch 209/501 --> loss:0.832613308429718
step 251/334, epoch 209/501 --> loss:0.8086526095867157
step 301/334, epoch 209/501 --> loss:0.8200881719589234
step 51/334, epoch 210/501 --> loss:0.8280367290973664
step 101/334, epoch 210/501 --> loss:0.8145172250270843
step 151/334, epoch 210/501 --> loss:0.8187604010105133
step 201/334, epoch 210/501 --> loss:0.8288388514518737
step 251/334, epoch 210/501 --> loss:0.8226819956302642
step 301/334, epoch 210/501 --> loss:0.8303500473499298
step 51/334, epoch 211/501 --> loss:0.8205619776248931
step 101/334, epoch 211/501 --> loss:0.8268653357028961
step 151/334, epoch 211/501 --> loss:0.8170189380645752
step 201/334, epoch 211/501 --> loss:0.8148365581035614
step 251/334, epoch 211/501 --> loss:0.8218493771553039
step 301/334, epoch 211/501 --> loss:0.8291616117954255

##########train dataset##########
acc--> [99.22014600469579]
F1--> {'F1': [0.9144383452536493], 'precision': [0.8508985291629557], 'recall': [0.9882450964870154]}
##########eval dataset##########
acc--> [98.66234886989785]
F1--> {'F1': [0.8535885803949411], 'precision': [0.8099521319608234], 'recall': [0.9022057635728987]}
step 51/334, epoch 212/501 --> loss:0.8236770117282868
step 101/334, epoch 212/501 --> loss:0.8171255934238434
step 151/334, epoch 212/501 --> loss:0.8220210230350494
step 201/334, epoch 212/501 --> loss:0.822195234298706
step 251/334, epoch 212/501 --> loss:0.8201432228088379
step 301/334, epoch 212/501 --> loss:0.8261398744583129
step 51/334, epoch 213/501 --> loss:0.8159801018238068
step 101/334, epoch 213/501 --> loss:0.8305785155296326
step 151/334, epoch 213/501 --> loss:0.8059075808525086
step 201/334, epoch 213/501 --> loss:0.8324055647850037
step 251/334, epoch 213/501 --> loss:0.8179227983951569
step 301/334, epoch 213/501 --> loss:0.8276420617103577
step 51/334, epoch 214/501 --> loss:0.829552503824234
step 101/334, epoch 214/501 --> loss:0.8191044425964356
step 151/334, epoch 214/501 --> loss:0.8200757193565369
step 201/334, epoch 214/501 --> loss:0.8228003537654877
step 251/334, epoch 214/501 --> loss:0.8099338138103485
step 301/334, epoch 214/501 --> loss:0.8293768513202667
step 51/334, epoch 215/501 --> loss:0.8365910828113556
step 101/334, epoch 215/501 --> loss:0.8210965526103974
step 151/334, epoch 215/501 --> loss:0.814833960533142
step 201/334, epoch 215/501 --> loss:0.8154939615726471
step 251/334, epoch 215/501 --> loss:0.8384278726577759
step 301/334, epoch 215/501 --> loss:0.8190096545219422
step 51/334, epoch 216/501 --> loss:0.842769433259964
step 101/334, epoch 216/501 --> loss:0.8319493150711059
step 151/334, epoch 216/501 --> loss:0.8226905488967895
step 201/334, epoch 216/501 --> loss:0.8084558951854706
step 251/334, epoch 216/501 --> loss:0.8169767701625824
step 301/334, epoch 216/501 --> loss:0.8145362687110901
step 51/334, epoch 217/501 --> loss:0.8147788286209107
step 101/334, epoch 217/501 --> loss:0.832287620306015
step 151/334, epoch 217/501 --> loss:0.8288185274600983
step 201/334, epoch 217/501 --> loss:0.8317463159561157
step 251/334, epoch 217/501 --> loss:0.8233288204669953
step 301/334, epoch 217/501 --> loss:0.8094654071331024
step 51/334, epoch 218/501 --> loss:0.8231034231185913
step 101/334, epoch 218/501 --> loss:0.827314932346344
step 151/334, epoch 218/501 --> loss:0.8133356487751007
step 201/334, epoch 218/501 --> loss:0.834886919260025
step 251/334, epoch 218/501 --> loss:0.8399570143222809
step 301/334, epoch 218/501 --> loss:0.8083587050437927
step 51/334, epoch 219/501 --> loss:0.823065140247345
step 101/334, epoch 219/501 --> loss:0.8259418296813965
step 151/334, epoch 219/501 --> loss:0.8158297324180603
step 201/334, epoch 219/501 --> loss:0.8193173038959504
step 251/334, epoch 219/501 --> loss:0.830308449268341
step 301/334, epoch 219/501 --> loss:0.8225607466697693
step 51/334, epoch 220/501 --> loss:0.8081518948078156
step 101/334, epoch 220/501 --> loss:0.8133679580688477
step 151/334, epoch 220/501 --> loss:0.8263902485370636
step 201/334, epoch 220/501 --> loss:0.8251537215709687
step 251/334, epoch 220/501 --> loss:0.8300717508792878
step 301/334, epoch 220/501 --> loss:0.8203284370899201
step 51/334, epoch 221/501 --> loss:0.830780736207962
step 101/334, epoch 221/501 --> loss:0.8365181052684784
step 151/334, epoch 221/501 --> loss:0.8260009527206421
step 201/334, epoch 221/501 --> loss:0.8074079477787017
step 251/334, epoch 221/501 --> loss:0.8009712040424347
step 301/334, epoch 221/501 --> loss:0.8161532044410705

##########train dataset##########
acc--> [99.13736292606694]
F1--> {'F1': [0.9062720476559564], 'precision': [0.8363321346154245], 'recall': [0.9889890287786621]}
##########eval dataset##########
acc--> [98.54262277730982]
F1--> {'F1': [0.8441327258270163], 'precision': [0.784864440217332], 'recall': [0.9130949722450842]}
step 51/334, epoch 222/501 --> loss:0.8334128665924072
step 101/334, epoch 222/501 --> loss:0.8182100701332092
step 151/334, epoch 222/501 --> loss:0.8236109519004822
step 201/334, epoch 222/501 --> loss:0.816412273645401
step 251/334, epoch 222/501 --> loss:0.8135073471069336
step 301/334, epoch 222/501 --> loss:0.8198026311397553
step 51/334, epoch 223/501 --> loss:0.8134649872779847
step 101/334, epoch 223/501 --> loss:0.8143750238418579
step 151/334, epoch 223/501 --> loss:0.8279062259197235
step 201/334, epoch 223/501 --> loss:0.8286232912540435
step 251/334, epoch 223/501 --> loss:0.8254234671592713
step 301/334, epoch 223/501 --> loss:0.8323169267177581
step 51/334, epoch 224/501 --> loss:0.8168465113639831
step 101/334, epoch 224/501 --> loss:0.8190871810913086
step 151/334, epoch 224/501 --> loss:0.8238777649402619
step 201/334, epoch 224/501 --> loss:0.8191218733787536
step 251/334, epoch 224/501 --> loss:0.8221104669570923
step 301/334, epoch 224/501 --> loss:0.8216373145580291
step 51/334, epoch 225/501 --> loss:0.8184033751487731
step 101/334, epoch 225/501 --> loss:0.8253339290618896
step 151/334, epoch 225/501 --> loss:0.8133426213264465
step 201/334, epoch 225/501 --> loss:0.8226223170757294
step 251/334, epoch 225/501 --> loss:0.8299280524253845
step 301/334, epoch 225/501 --> loss:0.8134282028675079
step 51/334, epoch 226/501 --> loss:0.8233489346504211
step 101/334, epoch 226/501 --> loss:0.824890251159668
step 151/334, epoch 226/501 --> loss:0.8336813151836395
step 201/334, epoch 226/501 --> loss:0.8186490821838379
step 251/334, epoch 226/501 --> loss:0.8202216923236847
step 301/334, epoch 226/501 --> loss:0.813421083688736
step 51/334, epoch 227/501 --> loss:0.8288936495780945
step 101/334, epoch 227/501 --> loss:0.819113336801529
step 151/334, epoch 227/501 --> loss:0.8306991910934448
step 201/334, epoch 227/501 --> loss:0.8229229164123535
step 251/334, epoch 227/501 --> loss:0.803049874305725
step 301/334, epoch 227/501 --> loss:0.8253356373310089
step 51/334, epoch 228/501 --> loss:0.815668637752533
step 101/334, epoch 228/501 --> loss:0.8265946555137634
step 151/334, epoch 228/501 --> loss:0.8078171420097351
step 201/334, epoch 228/501 --> loss:0.8157713556289673
step 251/334, epoch 228/501 --> loss:0.8363280618190765
step 301/334, epoch 228/501 --> loss:0.8340533363819123
step 51/334, epoch 229/501 --> loss:0.8262433588504792
step 101/334, epoch 229/501 --> loss:0.8268925368785858
step 151/334, epoch 229/501 --> loss:0.8246559071540832
step 201/334, epoch 229/501 --> loss:0.8192030119895936
step 251/334, epoch 229/501 --> loss:0.8187341046333313
step 301/334, epoch 229/501 --> loss:0.8253890335559845
step 51/334, epoch 230/501 --> loss:0.8405979764461518
step 101/334, epoch 230/501 --> loss:0.8051184165477753
step 151/334, epoch 230/501 --> loss:0.8174132299423218
step 201/334, epoch 230/501 --> loss:0.8117056357860565
step 251/334, epoch 230/501 --> loss:0.8133507716655731
step 301/334, epoch 230/501 --> loss:0.8333727824687958
step 51/334, epoch 231/501 --> loss:0.8363001096248627
step 101/334, epoch 231/501 --> loss:0.8250932383537293
step 151/334, epoch 231/501 --> loss:0.8052102351188659
step 201/334, epoch 231/501 --> loss:0.8176065111160278
step 251/334, epoch 231/501 --> loss:0.8288508784770966
step 301/334, epoch 231/501 --> loss:0.8203080809116363

##########train dataset##########
acc--> [99.16681633667578]
F1--> {'F1': [0.9091203190716418], 'precision': [0.8417264844483541], 'recall': [0.9882571030535898]}
##########eval dataset##########
acc--> [98.42676144363132]
F1--> {'F1': [0.8350918583448789], 'precision': [0.7633928331579243], 'recall': [0.9216671932884397]}
step 51/334, epoch 232/501 --> loss:0.8272965276241302
step 101/334, epoch 232/501 --> loss:0.8278996014595031
step 151/334, epoch 232/501 --> loss:0.8181772613525391
step 201/334, epoch 232/501 --> loss:0.8220015037059784
step 251/334, epoch 232/501 --> loss:0.8271770560741425
step 301/334, epoch 232/501 --> loss:0.8124563527107239
step 51/334, epoch 233/501 --> loss:0.819728125333786
step 101/334, epoch 233/501 --> loss:0.8214031445980072
step 151/334, epoch 233/501 --> loss:0.831166752576828
step 201/334, epoch 233/501 --> loss:0.8362934923171997
step 251/334, epoch 233/501 --> loss:0.8117111027240753
step 301/334, epoch 233/501 --> loss:0.8225349235534668
step 51/334, epoch 234/501 --> loss:0.812881246805191
step 101/334, epoch 234/501 --> loss:0.8348979473114013
step 151/334, epoch 234/501 --> loss:0.8081160843372345
step 201/334, epoch 234/501 --> loss:0.8276303470134735
step 251/334, epoch 234/501 --> loss:0.8190022146701813
step 301/334, epoch 234/501 --> loss:0.8311735570430756
step 51/334, epoch 235/501 --> loss:0.8189737939834595
step 101/334, epoch 235/501 --> loss:0.817820280790329
step 151/334, epoch 235/501 --> loss:0.8330099952220916
step 201/334, epoch 235/501 --> loss:0.8320361697673797
step 251/334, epoch 235/501 --> loss:0.8178707718849182
step 301/334, epoch 235/501 --> loss:0.8224342262744904
step 51/334, epoch 236/501 --> loss:0.8316839098930359
step 101/334, epoch 236/501 --> loss:0.8215954172611236
step 151/334, epoch 236/501 --> loss:0.812960022687912
step 201/334, epoch 236/501 --> loss:0.8273336637020111
step 251/334, epoch 236/501 --> loss:0.8231575512886047
step 301/334, epoch 236/501 --> loss:0.8122067022323608
step 51/334, epoch 237/501 --> loss:0.8125904595851898
step 101/334, epoch 237/501 --> loss:0.8187998068332673
step 151/334, epoch 237/501 --> loss:0.8229971611499787
step 201/334, epoch 237/501 --> loss:0.8305277907848359
step 251/334, epoch 237/501 --> loss:0.8119833612442017
step 301/334, epoch 237/501 --> loss:0.8292339885234833
step 51/334, epoch 238/501 --> loss:0.8247927749156951
step 101/334, epoch 238/501 --> loss:0.8274967682361603
step 151/334, epoch 238/501 --> loss:0.8264262521266937
step 201/334, epoch 238/501 --> loss:0.8144233858585358
step 251/334, epoch 238/501 --> loss:0.8189542627334595
step 301/334, epoch 238/501 --> loss:0.8259091925621033
step 51/334, epoch 239/501 --> loss:0.8212977993488312
step 101/334, epoch 239/501 --> loss:0.8229884135723115
step 151/334, epoch 239/501 --> loss:0.8218613767623901
step 201/334, epoch 239/501 --> loss:0.8200223529338837
step 251/334, epoch 239/501 --> loss:0.829978609085083
step 301/334, epoch 239/501 --> loss:0.80992809176445
step 51/334, epoch 240/501 --> loss:0.8243297696113586
step 101/334, epoch 240/501 --> loss:0.8108971893787384
step 151/334, epoch 240/501 --> loss:0.8232473301887512
step 201/334, epoch 240/501 --> loss:0.8232659721374511
step 251/334, epoch 240/501 --> loss:0.819213547706604
step 301/334, epoch 240/501 --> loss:0.8263338136672974
step 51/334, epoch 241/501 --> loss:0.8248278844356537
step 101/334, epoch 241/501 --> loss:0.8214596092700959
step 151/334, epoch 241/501 --> loss:0.8291436088085175
step 201/334, epoch 241/501 --> loss:0.8122325587272644
step 251/334, epoch 241/501 --> loss:0.8260227811336517
step 301/334, epoch 241/501 --> loss:0.8253364109992981

##########train dataset##########
acc--> [99.28887806717968]
F1--> {'F1': [0.9212991666093794], 'precision': [0.8637617891700031], 'recall': [0.9870604485850077]}
##########eval dataset##########
acc--> [98.72237144596377]
F1--> {'F1': [0.8597530116259214], 'precision': [0.8179284005436088], 'recall': [0.9060965978274306]}
step 51/334, epoch 242/501 --> loss:0.8178140783309936
step 101/334, epoch 242/501 --> loss:0.827655211687088
step 151/334, epoch 242/501 --> loss:0.8187590396404266
step 201/334, epoch 242/501 --> loss:0.8310083305835724
step 251/334, epoch 242/501 --> loss:0.8197430789470672
step 301/334, epoch 242/501 --> loss:0.8241643273830414
step 51/334, epoch 243/501 --> loss:0.8128061032295227
step 101/334, epoch 243/501 --> loss:0.8120633101463318
step 151/334, epoch 243/501 --> loss:0.8168083655834198
step 201/334, epoch 243/501 --> loss:0.8367658174037933
step 251/334, epoch 243/501 --> loss:0.8343250787258149
step 301/334, epoch 243/501 --> loss:0.8230648505687713
step 51/334, epoch 244/501 --> loss:0.8233791828155518
step 101/334, epoch 244/501 --> loss:0.8207883882522583
step 151/334, epoch 244/501 --> loss:0.8193420016765595
step 201/334, epoch 244/501 --> loss:0.8080797314643859
step 251/334, epoch 244/501 --> loss:0.8259905517101288
step 301/334, epoch 244/501 --> loss:0.827062429189682
step 51/334, epoch 245/501 --> loss:0.8299491882324219
step 101/334, epoch 245/501 --> loss:0.8161661279201508
step 151/334, epoch 245/501 --> loss:0.8138085687160492
step 201/334, epoch 245/501 --> loss:0.8187862074375153
step 251/334, epoch 245/501 --> loss:0.8263061845302582
step 301/334, epoch 245/501 --> loss:0.8217585575580597
step 51/334, epoch 246/501 --> loss:0.8314803743362427
step 101/334, epoch 246/501 --> loss:0.8181415271759033
step 151/334, epoch 246/501 --> loss:0.8121235871315002
step 201/334, epoch 246/501 --> loss:0.8346521735191346
step 251/334, epoch 246/501 --> loss:0.8337468028068542
step 301/334, epoch 246/501 --> loss:0.8080160343647003
step 51/334, epoch 247/501 --> loss:0.8281271290779114
step 101/334, epoch 247/501 --> loss:0.8129252409934997
step 151/334, epoch 247/501 --> loss:0.8172558903694153
step 201/334, epoch 247/501 --> loss:0.8222428870201111
step 251/334, epoch 247/501 --> loss:0.8188475942611695
step 301/334, epoch 247/501 --> loss:0.8352143681049347
step 51/334, epoch 248/501 --> loss:0.8350743210315704
step 101/334, epoch 248/501 --> loss:0.8324430274963379
step 151/334, epoch 248/501 --> loss:0.8058466267585754
step 201/334, epoch 248/501 --> loss:0.8196244502067566
step 251/334, epoch 248/501 --> loss:0.8176734066009521
step 301/334, epoch 248/501 --> loss:0.8254760134220124
step 51/334, epoch 249/501 --> loss:0.8208356988430023
step 101/334, epoch 249/501 --> loss:0.8198627030849457
step 151/334, epoch 249/501 --> loss:0.832853342294693
step 201/334, epoch 249/501 --> loss:0.8183514618873596
step 251/334, epoch 249/501 --> loss:0.8301154863834381
step 301/334, epoch 249/501 --> loss:0.8184648036956788
step 51/334, epoch 250/501 --> loss:0.8176447963714599
step 101/334, epoch 250/501 --> loss:0.8387689530849457
step 151/334, epoch 250/501 --> loss:0.8107912755012512
step 201/334, epoch 250/501 --> loss:0.8273183286190033
step 251/334, epoch 250/501 --> loss:0.8232380306720734
step 301/334, epoch 250/501 --> loss:0.8203675651550293
step 51/334, epoch 251/501 --> loss:0.8288093209266663
step 101/334, epoch 251/501 --> loss:0.8244648277759552
step 151/334, epoch 251/501 --> loss:0.8243619060516357
step 201/334, epoch 251/501 --> loss:0.8029135501384735
step 251/334, epoch 251/501 --> loss:0.8169345223903656
step 301/334, epoch 251/501 --> loss:0.8314531111717224

##########train dataset##########
acc--> [99.31463092107099]
F1--> {'F1': [0.9240174686440792], 'precision': [0.867627883404038], 'recall': [0.9882577813906844]}
##########eval dataset##########
acc--> [98.72575361062606]
F1--> {'F1': [0.8600439637606093], 'precision': [0.8186294376560369], 'recall': [0.905883168719716]}
step 51/334, epoch 252/501 --> loss:0.8203814113140107
step 101/334, epoch 252/501 --> loss:0.8215248155593872
step 151/334, epoch 252/501 --> loss:0.8255444955825806
step 201/334, epoch 252/501 --> loss:0.8089269161224365
step 251/334, epoch 252/501 --> loss:0.8289522457122803
step 301/334, epoch 252/501 --> loss:0.8274017095565795
step 51/334, epoch 253/501 --> loss:0.8217481565475464
step 101/334, epoch 253/501 --> loss:0.8303053045272827
step 151/334, epoch 253/501 --> loss:0.8174612414836884
step 201/334, epoch 253/501 --> loss:0.8177934217453003
step 251/334, epoch 253/501 --> loss:0.8193159186840058
step 301/334, epoch 253/501 --> loss:0.8232408058643341
step 51/334, epoch 254/501 --> loss:0.8163656544685364
step 101/334, epoch 254/501 --> loss:0.8163987624645234
step 151/334, epoch 254/501 --> loss:0.8235586416721344
step 201/334, epoch 254/501 --> loss:0.8095473504066467
step 251/334, epoch 254/501 --> loss:0.8312050259113312
step 301/334, epoch 254/501 --> loss:0.8398162209987641
step 51/334, epoch 255/501 --> loss:0.8137973475456238
step 101/334, epoch 255/501 --> loss:0.8349630475044251
step 151/334, epoch 255/501 --> loss:0.8090059852600098
step 201/334, epoch 255/501 --> loss:0.8219934689998627
step 251/334, epoch 255/501 --> loss:0.8239515578746796
step 301/334, epoch 255/501 --> loss:0.8163297760486603
step 51/334, epoch 256/501 --> loss:0.8194628202915192
step 101/334, epoch 256/501 --> loss:0.8064946687221527
step 151/334, epoch 256/501 --> loss:0.8247360646724701
step 201/334, epoch 256/501 --> loss:0.8281752908229828
step 251/334, epoch 256/501 --> loss:0.8237126612663269
step 301/334, epoch 256/501 --> loss:0.8206163203716278
step 51/334, epoch 257/501 --> loss:0.8211290812492371
step 101/334, epoch 257/501 --> loss:0.822662353515625
step 151/334, epoch 257/501 --> loss:0.8221656095981598
step 201/334, epoch 257/501 --> loss:0.8107800269126892
step 251/334, epoch 257/501 --> loss:0.8259221041202545
step 301/334, epoch 257/501 --> loss:0.8190454518795014
step 51/334, epoch 258/501 --> loss:0.8231495094299316
step 101/334, epoch 258/501 --> loss:0.834314204454422
step 151/334, epoch 258/501 --> loss:0.8398694980144501
step 201/334, epoch 258/501 --> loss:0.7975739598274231
step 251/334, epoch 258/501 --> loss:0.8224027001857758
step 301/334, epoch 258/501 --> loss:0.8134598469734192
step 51/334, epoch 259/501 --> loss:0.8208550024032593
step 101/334, epoch 259/501 --> loss:0.8203576016426086
step 151/334, epoch 259/501 --> loss:0.8323300743103027
step 201/334, epoch 259/501 --> loss:0.8190145981311798
step 251/334, epoch 259/501 --> loss:0.8261582207679748
step 301/334, epoch 259/501 --> loss:0.8248068141937256
step 51/334, epoch 260/501 --> loss:0.8228225970268249
step 101/334, epoch 260/501 --> loss:0.8341666889190674
step 151/334, epoch 260/501 --> loss:0.8241903400421142
step 201/334, epoch 260/501 --> loss:0.808988811969757
step 251/334, epoch 260/501 --> loss:0.8172367465496063
step 301/334, epoch 260/501 --> loss:0.8245215344429017
step 51/334, epoch 261/501 --> loss:0.8203544509410858
step 101/334, epoch 261/501 --> loss:0.810542471408844
step 151/334, epoch 261/501 --> loss:0.8249274826049805
step 201/334, epoch 261/501 --> loss:0.8388580596446991
step 251/334, epoch 261/501 --> loss:0.818813658952713
step 301/334, epoch 261/501 --> loss:0.8245699572563171

##########train dataset##########
acc--> [99.28424664995204]
F1--> {'F1': [0.9209553177056865], 'precision': [0.8618333148960942], 'recall': [0.9887978055516947]}
##########eval dataset##########
acc--> [98.68307973038355]
F1--> {'F1': [0.8570604405344162], 'precision': [0.8071990530473967], 'recall': [0.9134986652172739]}
step 51/334, epoch 262/501 --> loss:0.8002522516250611
step 101/334, epoch 262/501 --> loss:0.8324096846580505
step 151/334, epoch 262/501 --> loss:0.811968514919281
step 201/334, epoch 262/501 --> loss:0.8171019244194031
step 251/334, epoch 262/501 --> loss:0.8262138938903809
step 301/334, epoch 262/501 --> loss:0.8271336686611176
step 51/334, epoch 263/501 --> loss:0.8387823569774627
step 101/334, epoch 263/501 --> loss:0.8271123909950256
step 151/334, epoch 263/501 --> loss:0.8139054274559021
step 201/334, epoch 263/501 --> loss:0.8254980385303498
step 251/334, epoch 263/501 --> loss:0.818349734544754
step 301/334, epoch 263/501 --> loss:0.8104244768619537
step 51/334, epoch 264/501 --> loss:0.8226910150051117
step 101/334, epoch 264/501 --> loss:0.8099569630622864
step 151/334, epoch 264/501 --> loss:0.8119881558418274
step 201/334, epoch 264/501 --> loss:0.8284452271461487
step 251/334, epoch 264/501 --> loss:0.8393812084197998
step 301/334, epoch 264/501 --> loss:0.8236173522472382
step 51/334, epoch 265/501 --> loss:0.8128125727176666
step 101/334, epoch 265/501 --> loss:0.8223737549781799
step 151/334, epoch 265/501 --> loss:0.8296390020847321
step 201/334, epoch 265/501 --> loss:0.80658717751503
step 251/334, epoch 265/501 --> loss:0.8240207266807557
step 301/334, epoch 265/501 --> loss:0.8286912095546722
step 51/334, epoch 266/501 --> loss:0.8152826976776123
step 101/334, epoch 266/501 --> loss:0.8228740000724792
step 151/334, epoch 266/501 --> loss:0.8250474095344543
step 201/334, epoch 266/501 --> loss:0.81359450340271
step 251/334, epoch 266/501 --> loss:0.8363177442550659
step 301/334, epoch 266/501 --> loss:0.8153297579288483
step 51/334, epoch 267/501 --> loss:0.8239288711547852
step 101/334, epoch 267/501 --> loss:0.8280641293525696
step 151/334, epoch 267/501 --> loss:0.8290366959571839
step 201/334, epoch 267/501 --> loss:0.8185404431819916
step 251/334, epoch 267/501 --> loss:0.815624384880066
step 301/334, epoch 267/501 --> loss:0.8220100939273834
step 51/334, epoch 268/501 --> loss:0.8223271250724793
step 101/334, epoch 268/501 --> loss:0.8268970441818237
step 151/334, epoch 268/501 --> loss:0.8261888813972473
step 201/334, epoch 268/501 --> loss:0.8176316118240357
step 251/334, epoch 268/501 --> loss:0.8240883529186249
step 301/334, epoch 268/501 --> loss:0.8277603375911713
step 51/334, epoch 269/501 --> loss:0.8187684559822083
step 101/334, epoch 269/501 --> loss:0.8298102641105651
step 151/334, epoch 269/501 --> loss:0.8208571827411651
step 201/334, epoch 269/501 --> loss:0.8276796615123749
step 251/334, epoch 269/501 --> loss:0.8184715521335602
step 301/334, epoch 269/501 --> loss:0.8125292146205902
step 51/334, epoch 270/501 --> loss:0.8164947402477264
step 101/334, epoch 270/501 --> loss:0.8178158617019653
step 151/334, epoch 270/501 --> loss:0.8222839140892029
step 201/334, epoch 270/501 --> loss:0.8289521741867065
step 251/334, epoch 270/501 --> loss:0.825577597618103
step 301/334, epoch 270/501 --> loss:0.8162171363830566
step 51/334, epoch 271/501 --> loss:0.8306249761581421
step 101/334, epoch 271/501 --> loss:0.8286656844615936
step 151/334, epoch 271/501 --> loss:0.8297861278057098
step 201/334, epoch 271/501 --> loss:0.8209003841876984
step 251/334, epoch 271/501 --> loss:0.8142719781398773
step 301/334, epoch 271/501 --> loss:0.8109404098987579

##########train dataset##########
acc--> [99.01469645090467]
F1--> {'F1': [0.8944083356751336], 'precision': [0.8159500740565232], 'recall': [0.9895723308463079]}
##########eval dataset##########
acc--> [98.39678653372627]
F1--> {'F1': [0.8319727996595117], 'precision': [0.760455973263369], 'recall': [0.9183496215866117]}
step 51/334, epoch 272/501 --> loss:0.8212186372280121
step 101/334, epoch 272/501 --> loss:0.8260356914997101
step 151/334, epoch 272/501 --> loss:0.8188992881774902
step 201/334, epoch 272/501 --> loss:0.8184479832649231
step 251/334, epoch 272/501 --> loss:0.8165170419216156
step 301/334, epoch 272/501 --> loss:0.8283373034000396
step 51/334, epoch 273/501 --> loss:0.8319809591770172
step 101/334, epoch 273/501 --> loss:0.8044864094257355
step 151/334, epoch 273/501 --> loss:0.8171617698669433
step 201/334, epoch 273/501 --> loss:0.8280500519275665
step 251/334, epoch 273/501 --> loss:0.8152133679389953
step 301/334, epoch 273/501 --> loss:0.8307505309581756
step 51/334, epoch 274/501 --> loss:0.8212668967247009
step 101/334, epoch 274/501 --> loss:0.8260371005535125
step 151/334, epoch 274/501 --> loss:0.832919237613678
step 201/334, epoch 274/501 --> loss:0.8166661608219147
step 251/334, epoch 274/501 --> loss:0.8090167927742005
step 301/334, epoch 274/501 --> loss:0.825052375793457
step 51/334, epoch 275/501 --> loss:0.8083061826229095
step 101/334, epoch 275/501 --> loss:0.8266847455501556
step 151/334, epoch 275/501 --> loss:0.8251852250099182
step 201/334, epoch 275/501 --> loss:0.819366911649704
step 251/334, epoch 275/501 --> loss:0.8281171083450317
step 301/334, epoch 275/501 --> loss:0.8198313367366791
step 51/334, epoch 276/501 --> loss:0.8252134490013122
step 101/334, epoch 276/501 --> loss:0.8268909585475922
step 151/334, epoch 276/501 --> loss:0.8253104734420776
step 201/334, epoch 276/501 --> loss:0.8266323375701904
step 251/334, epoch 276/501 --> loss:0.8231172013282776
step 301/334, epoch 276/501 --> loss:0.8130121254920959
step 51/334, epoch 277/501 --> loss:0.8203538405895233
step 101/334, epoch 277/501 --> loss:0.819302133321762
step 151/334, epoch 277/501 --> loss:0.8234244382381439
step 201/334, epoch 277/501 --> loss:0.8424592220783234
step 251/334, epoch 277/501 --> loss:0.8027479588985443
step 301/334, epoch 277/501 --> loss:0.8185765337944031
step 51/334, epoch 278/501 --> loss:0.8330572354793548
step 101/334, epoch 278/501 --> loss:0.8124159586429596
step 151/334, epoch 278/501 --> loss:0.8235793495178223
step 201/334, epoch 278/501 --> loss:0.8252194750308991
step 251/334, epoch 278/501 --> loss:0.8336446785926819
step 301/334, epoch 278/501 --> loss:0.804115023612976
step 51/334, epoch 279/501 --> loss:0.8149491775035859
step 101/334, epoch 279/501 --> loss:0.8257830667495728
step 151/334, epoch 279/501 --> loss:0.8090269005298615
step 201/334, epoch 279/501 --> loss:0.8229742980003357
step 251/334, epoch 279/501 --> loss:0.8291893804073334
step 301/334, epoch 279/501 --> loss:0.8294945144653321
step 51/334, epoch 280/501 --> loss:0.8205916082859039
step 101/334, epoch 280/501 --> loss:0.8284445035457612
step 151/334, epoch 280/501 --> loss:0.8122455024719238
step 201/334, epoch 280/501 --> loss:0.8214999032020569
step 251/334, epoch 280/501 --> loss:0.8143576049804687
step 301/334, epoch 280/501 --> loss:0.8315033507347107
step 51/334, epoch 281/501 --> loss:0.8035075151920319
step 101/334, epoch 281/501 --> loss:0.8258321607112884
step 151/334, epoch 281/501 --> loss:0.8226699125766754
step 201/334, epoch 281/501 --> loss:0.8262128388881683
step 251/334, epoch 281/501 --> loss:0.8140481400489807
step 301/334, epoch 281/501 --> loss:0.8293003296852112

##########train dataset##########
acc--> [99.33034255346094]
F1--> {'F1': [0.9256993044058758], 'precision': [0.8698232425796765], 'recall': [0.989258328605218]}
##########eval dataset##########
acc--> [98.72646849090103]
F1--> {'F1': [0.8596612795265489], 'precision': [0.8207106444419531], 'recall': [0.9025042863407804]}
step 51/334, epoch 282/501 --> loss:0.8199570143222809
step 101/334, epoch 282/501 --> loss:0.8096870565414429
step 151/334, epoch 282/501 --> loss:0.8552540922164917
step 201/334, epoch 282/501 --> loss:0.8165695297718049
step 251/334, epoch 282/501 --> loss:0.8034339785575867
step 301/334, epoch 282/501 --> loss:0.8267529618740082
step 51/334, epoch 283/501 --> loss:0.8298948991298676
step 101/334, epoch 283/501 --> loss:0.80413543343544
step 151/334, epoch 283/501 --> loss:0.8173606431484223
step 201/334, epoch 283/501 --> loss:0.8213126802444458
step 251/334, epoch 283/501 --> loss:0.819522807598114
step 301/334, epoch 283/501 --> loss:0.8401317369937896
step 51/334, epoch 284/501 --> loss:0.8359738683700562
step 101/334, epoch 284/501 --> loss:0.8033238458633423
step 151/334, epoch 284/501 --> loss:0.8171117043495179
step 201/334, epoch 284/501 --> loss:0.8153099226951599
step 251/334, epoch 284/501 --> loss:0.8372678112983704
step 301/334, epoch 284/501 --> loss:0.8192931973934173
step 51/334, epoch 285/501 --> loss:0.8149576008319854
step 101/334, epoch 285/501 --> loss:0.8244685924053192
step 151/334, epoch 285/501 --> loss:0.8301368522644043
step 201/334, epoch 285/501 --> loss:0.8130902993679047
step 251/334, epoch 285/501 --> loss:0.8395482575893403
step 301/334, epoch 285/501 --> loss:0.7978957211971283
step 51/334, epoch 286/501 --> loss:0.8162376296520233
step 101/334, epoch 286/501 --> loss:0.8138999140262604
step 151/334, epoch 286/501 --> loss:0.8306839799880982
step 201/334, epoch 286/501 --> loss:0.8396500730514527
step 251/334, epoch 286/501 --> loss:0.8201562142372132
step 301/334, epoch 286/501 --> loss:0.8020176017284393
step 51/334, epoch 287/501 --> loss:0.8318466234207154
step 101/334, epoch 287/501 --> loss:0.8051078748703003
step 151/334, epoch 287/501 --> loss:0.8256130301952362
step 201/334, epoch 287/501 --> loss:0.835762835741043
step 251/334, epoch 287/501 --> loss:0.8250135493278503
step 301/334, epoch 287/501 --> loss:0.8253388547897339
step 51/334, epoch 288/501 --> loss:0.8208326673507691
step 101/334, epoch 288/501 --> loss:0.8151977252960205
step 151/334, epoch 288/501 --> loss:0.8174536192417144
step 201/334, epoch 288/501 --> loss:0.8277249479293823
step 251/334, epoch 288/501 --> loss:0.824141821861267
step 301/334, epoch 288/501 --> loss:0.8333119893074036
step 51/334, epoch 289/501 --> loss:0.8210081017017364
step 101/334, epoch 289/501 --> loss:0.8148838496208191
step 151/334, epoch 289/501 --> loss:0.8272831916809082
step 201/334, epoch 289/501 --> loss:0.8200980687141418
step 251/334, epoch 289/501 --> loss:0.8198365986347198
step 301/334, epoch 289/501 --> loss:0.8177452576160431
step 51/334, epoch 290/501 --> loss:0.8148393833637237
step 101/334, epoch 290/501 --> loss:0.8333506095409393
step 151/334, epoch 290/501 --> loss:0.8219884312152863
step 201/334, epoch 290/501 --> loss:0.810273928642273
step 251/334, epoch 290/501 --> loss:0.825617071390152
step 301/334, epoch 290/501 --> loss:0.8114877307415008
step 51/334, epoch 291/501 --> loss:0.8234845042228699
step 101/334, epoch 291/501 --> loss:0.827640962600708
step 151/334, epoch 291/501 --> loss:0.8203290438652039
step 201/334, epoch 291/501 --> loss:0.826889271736145
step 251/334, epoch 291/501 --> loss:0.8229912042617797
step 301/334, epoch 291/501 --> loss:0.8127714741230011

##########train dataset##########
acc--> [99.32783861306604]
F1--> {'F1': [0.9255248722483518], 'precision': [0.8686007042164149], 'recall': [0.9904448080173811]}
##########eval dataset##########
acc--> [98.76126934327861]
F1--> {'F1': [0.8631344587604597], 'precision': [0.8260164118697185], 'recall': [0.9037562905204061]}
save model!
step 51/334, epoch 292/501 --> loss:0.8237676644325256
step 101/334, epoch 292/501 --> loss:0.8191841459274292
step 151/334, epoch 292/501 --> loss:0.828153600692749
step 201/334, epoch 292/501 --> loss:0.8195039463043213
step 251/334, epoch 292/501 --> loss:0.82297119140625
step 301/334, epoch 292/501 --> loss:0.8190792560577392
step 51/334, epoch 293/501 --> loss:0.8121926748752594
step 101/334, epoch 293/501 --> loss:0.8294268572330474
step 151/334, epoch 293/501 --> loss:0.826240257024765
step 201/334, epoch 293/501 --> loss:0.8192992150783539
step 251/334, epoch 293/501 --> loss:0.8350379681587219
step 301/334, epoch 293/501 --> loss:0.8097529196739197
step 51/334, epoch 294/501 --> loss:0.813753856420517
step 101/334, epoch 294/501 --> loss:0.8266742396354675
step 151/334, epoch 294/501 --> loss:0.8233850967884063
step 201/334, epoch 294/501 --> loss:0.823193974494934
step 251/334, epoch 294/501 --> loss:0.8236098468303681
step 301/334, epoch 294/501 --> loss:0.8226481950283051
step 51/334, epoch 295/501 --> loss:0.8304768550395966
step 101/334, epoch 295/501 --> loss:0.8227244174480438
step 151/334, epoch 295/501 --> loss:0.8272248613834381
step 201/334, epoch 295/501 --> loss:0.8221660375595092
step 251/334, epoch 295/501 --> loss:0.8139810574054718
step 301/334, epoch 295/501 --> loss:0.8269465887546539
step 51/334, epoch 296/501 --> loss:0.8221076321601868
step 101/334, epoch 296/501 --> loss:0.8142354607582092
step 151/334, epoch 296/501 --> loss:0.8215635919570923
step 201/334, epoch 296/501 --> loss:0.8346332836151124
step 251/334, epoch 296/501 --> loss:0.817766945362091
step 301/334, epoch 296/501 --> loss:0.8135146307945251
step 51/334, epoch 297/501 --> loss:0.812156252861023
step 101/334, epoch 297/501 --> loss:0.8227818846702576
step 151/334, epoch 297/501 --> loss:0.833270652294159
step 201/334, epoch 297/501 --> loss:0.8196961522102356
step 251/334, epoch 297/501 --> loss:0.8079125225543976
step 301/334, epoch 297/501 --> loss:0.8231645512580872
step 51/334, epoch 298/501 --> loss:0.813539113998413
step 101/334, epoch 298/501 --> loss:0.8074977731704712
step 151/334, epoch 298/501 --> loss:0.8332790887355804
step 201/334, epoch 298/501 --> loss:0.8214863502979278
step 251/334, epoch 298/501 --> loss:0.8262179362773895
step 301/334, epoch 298/501 --> loss:0.8376538777351379
step 51/334, epoch 299/501 --> loss:0.8262024188041687
step 101/334, epoch 299/501 --> loss:0.8304630136489868
step 151/334, epoch 299/501 --> loss:0.8081888628005981
step 201/334, epoch 299/501 --> loss:0.8149754357337952
step 251/334, epoch 299/501 --> loss:0.8157445764541627
step 301/334, epoch 299/501 --> loss:0.8295738029479981
step 51/334, epoch 300/501 --> loss:0.8299968039989472
step 101/334, epoch 300/501 --> loss:0.8257701408863067
step 151/334, epoch 300/501 --> loss:0.830854309797287
step 201/334, epoch 300/501 --> loss:0.8181444299221039
step 251/334, epoch 300/501 --> loss:0.814052574634552
step 301/334, epoch 300/501 --> loss:0.8192548978328705
step 51/334, epoch 301/501 --> loss:0.7975396347045899
step 101/334, epoch 301/501 --> loss:0.8253454661369324
step 151/334, epoch 301/501 --> loss:0.8304031014442443
step 201/334, epoch 301/501 --> loss:0.8235336852073669
step 251/334, epoch 301/501 --> loss:0.8322106742858887
step 301/334, epoch 301/501 --> loss:0.8110034954547882

##########train dataset##########
acc--> [99.05368989444273]
F1--> {'F1': [0.8980174982868185], 'precision': [0.8230534027622047], 'recall': [0.9880175143917774]}
##########eval dataset##########
acc--> [98.3541333456299]
F1--> {'F1': [0.8281895783067671], 'precision': [0.7545096833385794], 'recall': [0.9178290213535393]}
step 51/334, epoch 302/501 --> loss:0.8141899418830871
step 101/334, epoch 302/501 --> loss:0.8331949830055236
step 151/334, epoch 302/501 --> loss:0.8211764538288117
step 201/334, epoch 302/501 --> loss:0.8248393762111664
step 251/334, epoch 302/501 --> loss:0.8143517279624939
step 301/334, epoch 302/501 --> loss:0.8273028874397278
step 51/334, epoch 303/501 --> loss:0.8316744899749756
step 101/334, epoch 303/501 --> loss:0.8213508713245392
step 151/334, epoch 303/501 --> loss:0.8279190373420715
step 201/334, epoch 303/501 --> loss:0.8161218357086182
step 251/334, epoch 303/501 --> loss:0.8316061413288116
step 301/334, epoch 303/501 --> loss:0.8166018724441528
step 51/334, epoch 304/501 --> loss:0.8174843966960907
step 101/334, epoch 304/501 --> loss:0.8107542169094085
step 151/334, epoch 304/501 --> loss:0.8104913365840912
step 201/334, epoch 304/501 --> loss:0.8347664332389831
step 251/334, epoch 304/501 --> loss:0.8275018191337585
step 301/334, epoch 304/501 --> loss:0.8223411750793457
step 51/334, epoch 305/501 --> loss:0.828824018239975
step 101/334, epoch 305/501 --> loss:0.8074195683002472
step 151/334, epoch 305/501 --> loss:0.8246713268756867
step 201/334, epoch 305/501 --> loss:0.8192280268669129
step 251/334, epoch 305/501 --> loss:0.8227944672107697
step 301/334, epoch 305/501 --> loss:0.8167928457260132
step 51/334, epoch 306/501 --> loss:0.8129078996181488
step 101/334, epoch 306/501 --> loss:0.8150266921520233
step 151/334, epoch 306/501 --> loss:0.8174728822708129
step 201/334, epoch 306/501 --> loss:0.824696637392044
step 251/334, epoch 306/501 --> loss:0.8189599573612213
step 301/334, epoch 306/501 --> loss:0.8336617219448089
step 51/334, epoch 307/501 --> loss:0.8254097211360931
step 101/334, epoch 307/501 --> loss:0.816475795507431
step 151/334, epoch 307/501 --> loss:0.8220762825012207
step 201/334, epoch 307/501 --> loss:0.821350427865982
step 251/334, epoch 307/501 --> loss:0.8220720672607422
step 301/334, epoch 307/501 --> loss:0.8199079084396362
step 51/334, epoch 308/501 --> loss:0.810155165195465
step 101/334, epoch 308/501 --> loss:0.8142508709430695
step 151/334, epoch 308/501 --> loss:0.8373682844638825
step 201/334, epoch 308/501 --> loss:0.8089848101139069
step 251/334, epoch 308/501 --> loss:0.8270287811756134
step 301/334, epoch 308/501 --> loss:0.8221588885784149
step 51/334, epoch 309/501 --> loss:0.8155009591579437
step 101/334, epoch 309/501 --> loss:0.8182793653011322
step 151/334, epoch 309/501 --> loss:0.8106377005577088
step 201/334, epoch 309/501 --> loss:0.8176475811004639
step 251/334, epoch 309/501 --> loss:0.8384675478935242
step 301/334, epoch 309/501 --> loss:0.8304447257518768
step 51/334, epoch 310/501 --> loss:0.8157842779159545
step 101/334, epoch 310/501 --> loss:0.8297471463680267
step 151/334, epoch 310/501 --> loss:0.8220257604122162
step 201/334, epoch 310/501 --> loss:0.8097408878803253
step 251/334, epoch 310/501 --> loss:0.817070541381836
step 301/334, epoch 310/501 --> loss:0.8320472705364227
step 51/334, epoch 311/501 --> loss:0.8015320169925689
step 101/334, epoch 311/501 --> loss:0.8359665834903717
step 151/334, epoch 311/501 --> loss:0.8278414273262024
step 201/334, epoch 311/501 --> loss:0.8319918811321259
step 251/334, epoch 311/501 --> loss:0.8058538496494293
step 301/334, epoch 311/501 --> loss:0.821456425189972

##########train dataset##########
acc--> [99.41362739279498]
F1--> {'F1': [0.9343784731564865], 'precision': [0.8846856267183587], 'recall': [0.9899972412023647]}
##########eval dataset##########
acc--> [98.74883469842099]
F1--> {'F1': [0.8605803212227568], 'precision': [0.8300497759689226], 'recall': [0.8934533169371388]}
step 51/334, epoch 312/501 --> loss:0.8068972182273865
step 101/334, epoch 312/501 --> loss:0.8264376068115235
step 151/334, epoch 312/501 --> loss:0.8174378979206085
step 201/334, epoch 312/501 --> loss:0.8233836090564728
step 251/334, epoch 312/501 --> loss:0.8181895911693573
step 301/334, epoch 312/501 --> loss:0.8305620658397674
step 51/334, epoch 313/501 --> loss:0.8188980627059936
step 101/334, epoch 313/501 --> loss:0.8269575047492981
step 151/334, epoch 313/501 --> loss:0.8149623882770538
step 201/334, epoch 313/501 --> loss:0.8401540040969848
step 251/334, epoch 313/501 --> loss:0.8219005036354065
step 301/334, epoch 313/501 --> loss:0.8027000141143799
step 51/334, epoch 314/501 --> loss:0.8148535466194153
step 101/334, epoch 314/501 --> loss:0.8289649403095245
step 151/334, epoch 314/501 --> loss:0.8112482464313507
step 201/334, epoch 314/501 --> loss:0.8133190560340882
step 251/334, epoch 314/501 --> loss:0.8219845509529113
step 301/334, epoch 314/501 --> loss:0.834862585067749
step 51/334, epoch 315/501 --> loss:0.8168018853664398
step 101/334, epoch 315/501 --> loss:0.8124982821941376
step 151/334, epoch 315/501 --> loss:0.8268010640144348
step 201/334, epoch 315/501 --> loss:0.821648633480072
step 251/334, epoch 315/501 --> loss:0.836663669347763
step 301/334, epoch 315/501 --> loss:0.8196856510639191
step 51/334, epoch 316/501 --> loss:0.8204938209056855
step 101/334, epoch 316/501 --> loss:0.8103096425533295
step 151/334, epoch 316/501 --> loss:0.8178914034366608
step 201/334, epoch 316/501 --> loss:0.8128226101398468
step 251/334, epoch 316/501 --> loss:0.8297988402843476
step 301/334, epoch 316/501 --> loss:0.8308690667152405
step 51/334, epoch 317/501 --> loss:0.8100408613681793
step 101/334, epoch 317/501 --> loss:0.8276843369007111
step 151/334, epoch 317/501 --> loss:0.8225565195083618
step 201/334, epoch 317/501 --> loss:0.8300172317028046
step 251/334, epoch 317/501 --> loss:0.8170867669582367
step 301/334, epoch 317/501 --> loss:0.8418944442272186
step 51/334, epoch 318/501 --> loss:0.8241840624809265
step 101/334, epoch 318/501 --> loss:0.810810180902481
step 151/334, epoch 318/501 --> loss:0.8069928622245789
step 201/334, epoch 318/501 --> loss:0.8372550296783448
step 251/334, epoch 318/501 --> loss:0.8089396893978119
step 301/334, epoch 318/501 --> loss:0.8453647303581238
step 51/334, epoch 319/501 --> loss:0.8258270263671875
step 101/334, epoch 319/501 --> loss:0.8352975296974182
step 151/334, epoch 319/501 --> loss:0.8114462018013
step 201/334, epoch 319/501 --> loss:0.8164581835269928
step 251/334, epoch 319/501 --> loss:0.8202034115791321
step 301/334, epoch 319/501 --> loss:0.8143875551223755
step 51/334, epoch 320/501 --> loss:0.8242993557453155
step 101/334, epoch 320/501 --> loss:0.8095817184448242
step 151/334, epoch 320/501 --> loss:0.8209609377384186
step 201/334, epoch 320/501 --> loss:0.8295677363872528
step 251/334, epoch 320/501 --> loss:0.8240077304840088
step 301/334, epoch 320/501 --> loss:0.8190916585922241
step 51/334, epoch 321/501 --> loss:0.8202873313426972
step 101/334, epoch 321/501 --> loss:0.8159207332134247
step 151/334, epoch 321/501 --> loss:0.8301244258880616
step 201/334, epoch 321/501 --> loss:0.8142137789726257
step 251/334, epoch 321/501 --> loss:0.82361598610878
step 301/334, epoch 321/501 --> loss:0.8195403826236725

##########train dataset##########
acc--> [99.12401506146897]
F1--> {'F1': [0.9051160109768229], 'precision': [0.8330878186317778], 'recall': [0.9907898780974036]}
##########eval dataset##########
acc--> [98.32735770851473]
F1--> {'F1': [0.8267190440161732], 'precision': [0.7485024159419066], 'recall': [0.9232024311754088]}
step 51/334, epoch 322/501 --> loss:0.8181130802631378
step 101/334, epoch 322/501 --> loss:0.8284225738048554
step 151/334, epoch 322/501 --> loss:0.8176116037368775
step 201/334, epoch 322/501 --> loss:0.8245226132869721
step 251/334, epoch 322/501 --> loss:0.8122528839111328
step 301/334, epoch 322/501 --> loss:0.8179752624034882
step 51/334, epoch 323/501 --> loss:0.8230545735359192
step 101/334, epoch 323/501 --> loss:0.8159759843349457
step 151/334, epoch 323/501 --> loss:0.828923659324646
step 201/334, epoch 323/501 --> loss:0.8303295242786407
step 251/334, epoch 323/501 --> loss:0.8169674384593963
step 301/334, epoch 323/501 --> loss:0.824543114900589
step 51/334, epoch 324/501 --> loss:0.8233181941509247
step 101/334, epoch 324/501 --> loss:0.8297181606292725
step 151/334, epoch 324/501 --> loss:0.8156400990486145
step 201/334, epoch 324/501 --> loss:0.8259165954589843
step 251/334, epoch 324/501 --> loss:0.8142842936515808
step 301/334, epoch 324/501 --> loss:0.8119116246700286
step 51/334, epoch 325/501 --> loss:0.8128606462478638
step 101/334, epoch 325/501 --> loss:0.8343297636508942
step 151/334, epoch 325/501 --> loss:0.8261254847049713
step 201/334, epoch 325/501 --> loss:0.8103895843029022
step 251/334, epoch 325/501 --> loss:0.8236464655399323
step 301/334, epoch 325/501 --> loss:0.8256748449802399
step 51/334, epoch 326/501 --> loss:0.817728523015976
step 101/334, epoch 326/501 --> loss:0.8257474827766419
step 151/334, epoch 326/501 --> loss:0.8182548081874848
step 201/334, epoch 326/501 --> loss:0.8015884292125702
step 251/334, epoch 326/501 --> loss:0.8256003952026367
step 301/334, epoch 326/501 --> loss:0.8268055367469788
step 51/334, epoch 327/501 --> loss:0.8365619206428527
step 101/334, epoch 327/501 --> loss:0.8096753585338593
step 151/334, epoch 327/501 --> loss:0.8120741510391235
step 201/334, epoch 327/501 --> loss:0.8125196385383606
step 251/334, epoch 327/501 --> loss:0.8320435523986817
step 301/334, epoch 327/501 --> loss:0.8203511679172516
step 51/334, epoch 328/501 --> loss:0.8134496986865998
step 101/334, epoch 328/501 --> loss:0.8301616764068603
step 151/334, epoch 328/501 --> loss:0.8113223540782929
step 201/334, epoch 328/501 --> loss:0.8169158041477204
step 251/334, epoch 328/501 --> loss:0.813757553100586
step 301/334, epoch 328/501 --> loss:0.8314725852012634
step 51/334, epoch 329/501 --> loss:0.8111863052845001
step 101/334, epoch 329/501 --> loss:0.8106154870986938
step 151/334, epoch 329/501 --> loss:0.8328768348693848
step 201/334, epoch 329/501 --> loss:0.8421948647499085
step 251/334, epoch 329/501 --> loss:0.8158411800861358
step 301/334, epoch 329/501 --> loss:0.8161021280288696
step 51/334, epoch 330/501 --> loss:0.829471275806427
step 101/334, epoch 330/501 --> loss:0.8224466586112976
step 151/334, epoch 330/501 --> loss:0.8173199999332428
step 201/334, epoch 330/501 --> loss:0.8243290233612061
step 251/334, epoch 330/501 --> loss:0.8234668171405792
step 301/334, epoch 330/501 --> loss:0.8123180818557739
step 51/334, epoch 331/501 --> loss:0.8245855486392974
step 101/334, epoch 331/501 --> loss:0.8103643250465393
step 151/334, epoch 331/501 --> loss:0.8209725856781006
step 201/334, epoch 331/501 --> loss:0.8320658230781555
step 251/334, epoch 331/501 --> loss:0.827415337562561
step 301/334, epoch 331/501 --> loss:0.8213491237163544

##########train dataset##########
acc--> [99.42752364694363]
F1--> {'F1': [0.9358733741351537], 'precision': [0.8868448086738084], 'recall': [0.9906513616626865]}
##########eval dataset##########
acc--> [98.77603419078791]
F1--> {'F1': [0.8647184181814155], 'precision': [0.8277970373514202], 'recall': [0.9050980214088467]}
save model!
step 51/334, epoch 332/501 --> loss:0.8230790340900421
step 101/334, epoch 332/501 --> loss:0.8285976421833038
step 151/334, epoch 332/501 --> loss:0.824053499698639
step 201/334, epoch 332/501 --> loss:0.8321583950519562
step 251/334, epoch 332/501 --> loss:0.8223306894302368
step 301/334, epoch 332/501 --> loss:0.8121193778514862
step 51/334, epoch 333/501 --> loss:0.836451872587204
step 101/334, epoch 333/501 --> loss:0.815543065071106
step 151/334, epoch 333/501 --> loss:0.8132610642910003
step 201/334, epoch 333/501 --> loss:0.8203422796726226
step 251/334, epoch 333/501 --> loss:0.8311505913734436
step 301/334, epoch 333/501 --> loss:0.8106830990314484
step 51/334, epoch 334/501 --> loss:0.8210747897624969
step 101/334, epoch 334/501 --> loss:0.8238094782829285
step 151/334, epoch 334/501 --> loss:0.8217678833007812
step 201/334, epoch 334/501 --> loss:0.827907943725586
step 251/334, epoch 334/501 --> loss:0.8046731925010682
step 301/334, epoch 334/501 --> loss:0.8239172673225403
step 51/334, epoch 335/501 --> loss:0.8227589201927185
step 101/334, epoch 335/501 --> loss:0.8326931750774383
step 151/334, epoch 335/501 --> loss:0.8341215372085571
step 201/334, epoch 335/501 --> loss:0.8105758571624756
step 251/334, epoch 335/501 --> loss:0.8136020517349243
step 301/334, epoch 335/501 --> loss:0.824410719871521
step 51/334, epoch 336/501 --> loss:0.825829267501831
step 101/334, epoch 336/501 --> loss:0.8234605979919434
step 151/334, epoch 336/501 --> loss:0.8287682127952576
step 201/334, epoch 336/501 --> loss:0.8136427938938141
step 251/334, epoch 336/501 --> loss:0.8118614232540131
step 301/334, epoch 336/501 --> loss:0.819034264087677
step 51/334, epoch 337/501 --> loss:0.8503999161720276
step 101/334, epoch 337/501 --> loss:0.822755446434021
step 151/334, epoch 337/501 --> loss:0.821515474319458
step 201/334, epoch 337/501 --> loss:0.8001293790340424
step 251/334, epoch 337/501 --> loss:0.8152595400810242
step 301/334, epoch 337/501 --> loss:0.8175860810279846
step 51/334, epoch 338/501 --> loss:0.8273043310642243
step 101/334, epoch 338/501 --> loss:0.8199644422531128
step 151/334, epoch 338/501 --> loss:0.8245774590969086
step 201/334, epoch 338/501 --> loss:0.8212335729598998
step 251/334, epoch 338/501 --> loss:0.8275053310394287
step 301/334, epoch 338/501 --> loss:0.8037376856803894
step 51/334, epoch 339/501 --> loss:0.8209247493743896
step 101/334, epoch 339/501 --> loss:0.8289277458190918
step 151/334, epoch 339/501 --> loss:0.8096975982189178
step 201/334, epoch 339/501 --> loss:0.8309058511257171
step 251/334, epoch 339/501 --> loss:0.8177996623516083
step 301/334, epoch 339/501 --> loss:0.818914829492569
step 51/334, epoch 340/501 --> loss:0.83501620054245
step 101/334, epoch 340/501 --> loss:0.8208919179439544
step 151/334, epoch 340/501 --> loss:0.827501208782196
step 201/334, epoch 340/501 --> loss:0.8084177374839783
step 251/334, epoch 340/501 --> loss:0.8271545624732971
step 301/334, epoch 340/501 --> loss:0.8184735226631165
step 51/334, epoch 341/501 --> loss:0.83404736161232
step 101/334, epoch 341/501 --> loss:0.8213738012313843
step 151/334, epoch 341/501 --> loss:0.8162096977233887
step 201/334, epoch 341/501 --> loss:0.8170828485488891
step 251/334, epoch 341/501 --> loss:0.8092750859260559
step 301/334, epoch 341/501 --> loss:0.8289093935489654

##########train dataset##########
acc--> [99.39774926968073]
F1--> {'F1': [0.9327391678193739], 'precision': [0.881526350439876], 'recall': [0.9902807182741976]}
##########eval dataset##########
acc--> [98.64341956110418]
F1--> {'F1': [0.8525847483013015], 'precision': [0.8038091252417406], 'recall': [0.9076735331522311]}
step 51/334, epoch 342/501 --> loss:0.8359288406372071
step 101/334, epoch 342/501 --> loss:0.8202885568141938
step 151/334, epoch 342/501 --> loss:0.8316793656349182
step 201/334, epoch 342/501 --> loss:0.816063220500946
step 251/334, epoch 342/501 --> loss:0.8120356750488281
step 301/334, epoch 342/501 --> loss:0.8126790571212769
step 51/334, epoch 343/501 --> loss:0.8111696314811706
step 101/334, epoch 343/501 --> loss:0.8233085250854493
step 151/334, epoch 343/501 --> loss:0.8261593198776245
step 201/334, epoch 343/501 --> loss:0.8286734366416931
step 251/334, epoch 343/501 --> loss:0.8251371014118195
step 301/334, epoch 343/501 --> loss:0.8182002878189087
step 51/334, epoch 344/501 --> loss:0.8050881659984589
step 101/334, epoch 344/501 --> loss:0.827330025434494
step 151/334, epoch 344/501 --> loss:0.8076320278644562
step 201/334, epoch 344/501 --> loss:0.8246375918388367
step 251/334, epoch 344/501 --> loss:0.8392761886119843
step 301/334, epoch 344/501 --> loss:0.8107655596733093
step 51/334, epoch 345/501 --> loss:0.8460598361492156
step 101/334, epoch 345/501 --> loss:0.8061806571483612
step 151/334, epoch 345/501 --> loss:0.8207386100292205
step 201/334, epoch 345/501 --> loss:0.8223876082897186
step 251/334, epoch 345/501 --> loss:0.8191439568996429
step 301/334, epoch 345/501 --> loss:0.8104303014278412
step 51/334, epoch 346/501 --> loss:0.8242665922641754
step 101/334, epoch 346/501 --> loss:0.8248951363563538
step 151/334, epoch 346/501 --> loss:0.822815693616867
step 201/334, epoch 346/501 --> loss:0.820943797826767
step 251/334, epoch 346/501 --> loss:0.8161978876590729
step 301/334, epoch 346/501 --> loss:0.8183111882209778
step 51/334, epoch 347/501 --> loss:0.8248927319049835
step 101/334, epoch 347/501 --> loss:0.8083136081695557
step 151/334, epoch 347/501 --> loss:0.8087104272842407
step 201/334, epoch 347/501 --> loss:0.833581473827362
step 251/334, epoch 347/501 --> loss:0.818268209695816
step 301/334, epoch 347/501 --> loss:0.8227364993095398
step 51/334, epoch 348/501 --> loss:0.8205305886268616
step 101/334, epoch 348/501 --> loss:0.8165390956401825
step 151/334, epoch 348/501 --> loss:0.8234389460086823
step 201/334, epoch 348/501 --> loss:0.8247317159175873
step 251/334, epoch 348/501 --> loss:0.8232503831386566
step 301/334, epoch 348/501 --> loss:0.8158366799354553
step 51/334, epoch 349/501 --> loss:0.8473879301548004
step 101/334, epoch 349/501 --> loss:0.8172210657596588
step 151/334, epoch 349/501 --> loss:0.82770587682724
step 201/334, epoch 349/501 --> loss:0.8097868788242341
step 251/334, epoch 349/501 --> loss:0.8042002248764039
step 301/334, epoch 349/501 --> loss:0.8232326126098632
step 51/334, epoch 350/501 --> loss:0.8168209135532379
step 101/334, epoch 350/501 --> loss:0.8176652777194977
step 151/334, epoch 350/501 --> loss:0.8215910792350769
step 201/334, epoch 350/501 --> loss:0.830546760559082
step 251/334, epoch 350/501 --> loss:0.8223392498493195
step 301/334, epoch 350/501 --> loss:0.828562593460083
step 51/334, epoch 351/501 --> loss:0.8265317952632905
step 101/334, epoch 351/501 --> loss:0.8231812202930451
step 151/334, epoch 351/501 --> loss:0.8311082553863526
step 201/334, epoch 351/501 --> loss:0.8061749863624573
step 251/334, epoch 351/501 --> loss:0.821199004650116
step 301/334, epoch 351/501 --> loss:0.8235019814968109

##########train dataset##########
acc--> [99.49053695329425]
F1--> {'F1': [0.9424909981229265], 'precision': [0.8993282441011546], 'recall': [0.9900167773106892]}
##########eval dataset##########
acc--> [98.91445063125603]
F1--> {'F1': [0.8763059261412276], 'precision': [0.8633099098818858], 'recall': [0.8897095047597012]}
save model!
step 51/334, epoch 352/501 --> loss:0.8153206646442414
step 101/334, epoch 352/501 --> loss:0.8051103150844574
step 151/334, epoch 352/501 --> loss:0.8174067151546478
step 201/334, epoch 352/501 --> loss:0.8261680972576141
step 251/334, epoch 352/501 --> loss:0.8288713419437408
step 301/334, epoch 352/501 --> loss:0.8211158239841461
step 51/334, epoch 353/501 --> loss:0.8149308967590332
step 101/334, epoch 353/501 --> loss:0.8289615583419799
step 151/334, epoch 353/501 --> loss:0.826210824251175
step 201/334, epoch 353/501 --> loss:0.8210296905040741
step 251/334, epoch 353/501 --> loss:0.808186513185501
step 301/334, epoch 353/501 --> loss:0.816389627456665
step 51/334, epoch 354/501 --> loss:0.8237232005596161
step 101/334, epoch 354/501 --> loss:0.8192274987697601
step 151/334, epoch 354/501 --> loss:0.8155080127716064
step 201/334, epoch 354/501 --> loss:0.8251752626895904
step 251/334, epoch 354/501 --> loss:0.8287396693229675
step 301/334, epoch 354/501 --> loss:0.8099093627929688
step 51/334, epoch 355/501 --> loss:0.833618916273117
step 101/334, epoch 355/501 --> loss:0.8270327115058899
step 151/334, epoch 355/501 --> loss:0.823104475736618
step 201/334, epoch 355/501 --> loss:0.8149736177921295
step 251/334, epoch 355/501 --> loss:0.817205491065979
step 301/334, epoch 355/501 --> loss:0.8092117285728455
step 51/334, epoch 356/501 --> loss:0.8141455614566803
step 101/334, epoch 356/501 --> loss:0.8361099445819855
step 151/334, epoch 356/501 --> loss:0.825819970369339
step 201/334, epoch 356/501 --> loss:0.8063912498950958
step 251/334, epoch 356/501 --> loss:0.8192201220989227
step 301/334, epoch 356/501 --> loss:0.8182303440570832
step 51/334, epoch 357/501 --> loss:0.8205414712429047
step 101/334, epoch 357/501 --> loss:0.8167478227615357
step 151/334, epoch 357/501 --> loss:0.8192803275585174
step 201/334, epoch 357/501 --> loss:0.8194855725765229
step 251/334, epoch 357/501 --> loss:0.8196361172199249
step 301/334, epoch 357/501 --> loss:0.8167096257209778
step 51/334, epoch 358/501 --> loss:0.8112306594848633
step 101/334, epoch 358/501 --> loss:0.8267626357078552
step 151/334, epoch 358/501 --> loss:0.8142156398296356
step 201/334, epoch 358/501 --> loss:0.8358287835121154
step 251/334, epoch 358/501 --> loss:0.8195736873149871
step 301/334, epoch 358/501 --> loss:0.8133202683925629
step 51/334, epoch 359/501 --> loss:0.8185149514675141
step 101/334, epoch 359/501 --> loss:0.8353733384609222
step 151/334, epoch 359/501 --> loss:0.8299065220355988
step 201/334, epoch 359/501 --> loss:0.8321313464641571
step 251/334, epoch 359/501 --> loss:0.8175766170024872
step 301/334, epoch 359/501 --> loss:0.798688600063324
step 51/334, epoch 360/501 --> loss:0.8215845012664795
step 101/334, epoch 360/501 --> loss:0.8140390121936798
step 151/334, epoch 360/501 --> loss:0.8296412873268127
step 201/334, epoch 360/501 --> loss:0.8231889116764068
step 251/334, epoch 360/501 --> loss:0.8256330132484436
step 301/334, epoch 360/501 --> loss:0.8124893486499787
step 51/334, epoch 361/501 --> loss:0.8351994132995606
step 101/334, epoch 361/501 --> loss:0.8200119817256928
step 151/334, epoch 361/501 --> loss:0.8159735369682312
step 201/334, epoch 361/501 --> loss:0.807818410396576
step 251/334, epoch 361/501 --> loss:0.8212866175174713
step 301/334, epoch 361/501 --> loss:0.8242821860313415

##########train dataset##########
acc--> [99.4087448091482]
F1--> {'F1': [0.9339563661312767], 'precision': [0.8828042909052718], 'recall': [0.9914120488805698]}
##########eval dataset##########
acc--> [98.81204853741377]
F1--> {'F1': [0.8683670751549101], 'precision': [0.8332154603372158], 'recall': [0.906626155287889]}
step 51/334, epoch 362/501 --> loss:0.8233690404891968
step 101/334, epoch 362/501 --> loss:0.809656720161438
step 151/334, epoch 362/501 --> loss:0.8261283421516419
step 201/334, epoch 362/501 --> loss:0.8351595115661621
step 251/334, epoch 362/501 --> loss:0.8101698565483093
step 301/334, epoch 362/501 --> loss:0.8159070777893066
step 51/334, epoch 363/501 --> loss:0.8065058100223541
step 101/334, epoch 363/501 --> loss:0.8212379229068756
step 151/334, epoch 363/501 --> loss:0.8307499504089355
step 201/334, epoch 363/501 --> loss:0.8223268139362335
step 251/334, epoch 363/501 --> loss:0.8095309317111969
step 301/334, epoch 363/501 --> loss:0.8260823225975037
step 51/334, epoch 364/501 --> loss:0.8065797626972199
step 101/334, epoch 364/501 --> loss:0.8214553928375244
step 151/334, epoch 364/501 --> loss:0.8286163890361786
step 201/334, epoch 364/501 --> loss:0.815671820640564
step 251/334, epoch 364/501 --> loss:0.8299861407279968
step 301/334, epoch 364/501 --> loss:0.8210547816753387
step 51/334, epoch 365/501 --> loss:0.826780127286911
step 101/334, epoch 365/501 --> loss:0.8215097260475158
step 151/334, epoch 365/501 --> loss:0.806796383857727
step 201/334, epoch 365/501 --> loss:0.8188770222663879
step 251/334, epoch 365/501 --> loss:0.8345441889762878
step 301/334, epoch 365/501 --> loss:0.8132578825950623
step 51/334, epoch 366/501 --> loss:0.8287547862529755
step 101/334, epoch 366/501 --> loss:0.8298088634014129
step 151/334, epoch 366/501 --> loss:0.8305820453166962
step 201/334, epoch 366/501 --> loss:0.810471225976944
step 251/334, epoch 366/501 --> loss:0.8248514318466187
step 301/334, epoch 366/501 --> loss:0.7999706375598907
step 51/334, epoch 367/501 --> loss:0.837215222120285
step 101/334, epoch 367/501 --> loss:0.8068653237819672
step 151/334, epoch 367/501 --> loss:0.8292739224433899
step 201/334, epoch 367/501 --> loss:0.8140070962905884
step 251/334, epoch 367/501 --> loss:0.8288208425045014
step 301/334, epoch 367/501 --> loss:0.819902480840683
step 51/334, epoch 368/501 --> loss:0.826611111164093
step 101/334, epoch 368/501 --> loss:0.8192146956920624
step 151/334, epoch 368/501 --> loss:0.825526784658432
step 201/334, epoch 368/501 --> loss:0.8271834945678711
step 251/334, epoch 368/501 --> loss:0.8180245423316955
step 301/334, epoch 368/501 --> loss:0.8202883124351501
step 51/334, epoch 369/501 --> loss:0.8232013702392578
step 101/334, epoch 369/501 --> loss:0.8021298265457153
step 151/334, epoch 369/501 --> loss:0.8113737571239471
step 201/334, epoch 369/501 --> loss:0.8266805183887481
step 251/334, epoch 369/501 --> loss:0.8265135657787323
step 301/334, epoch 369/501 --> loss:0.8311930859088897
step 51/334, epoch 370/501 --> loss:0.8414197421073913
step 101/334, epoch 370/501 --> loss:0.8174667859077454
step 151/334, epoch 370/501 --> loss:0.8190292775630951
step 201/334, epoch 370/501 --> loss:0.8126383578777313
step 251/334, epoch 370/501 --> loss:0.8139246881008149
step 301/334, epoch 370/501 --> loss:0.8131252467632294
step 51/334, epoch 371/501 --> loss:0.8329475772380829
step 101/334, epoch 371/501 --> loss:0.8252695214748382
step 151/334, epoch 371/501 --> loss:0.8017004084587097
step 201/334, epoch 371/501 --> loss:0.8147849774360657
step 251/334, epoch 371/501 --> loss:0.81946413397789
step 301/334, epoch 371/501 --> loss:0.8275487637519836

##########train dataset##########
acc--> [99.46948360046628]
F1--> {'F1': [0.9403136292503289], 'precision': [0.894550617936527], 'recall': [0.9910224120534321]}
##########eval dataset##########
acc--> [98.84618857609179]
F1--> {'F1': [0.8710387215960969], 'precision': [0.8425085897151388], 'recall': [0.9015795298306535]}
step 51/334, epoch 372/501 --> loss:0.8197797572612763
step 101/334, epoch 372/501 --> loss:0.8257816708087922
step 151/334, epoch 372/501 --> loss:0.8264478981494904
step 201/334, epoch 372/501 --> loss:0.8064236521720887
step 251/334, epoch 372/501 --> loss:0.8210714793205262
step 301/334, epoch 372/501 --> loss:0.8284895586967468
step 51/334, epoch 373/501 --> loss:0.8197368514537812
step 101/334, epoch 373/501 --> loss:0.8199479269981385
step 151/334, epoch 373/501 --> loss:0.8312101912498474
step 201/334, epoch 373/501 --> loss:0.8103017914295196
step 251/334, epoch 373/501 --> loss:0.8172250652313232
step 301/334, epoch 373/501 --> loss:0.8330212664604187
step 51/334, epoch 374/501 --> loss:0.8311254155635833
step 101/334, epoch 374/501 --> loss:0.8235208261013031
step 151/334, epoch 374/501 --> loss:0.8156247758865356
step 201/334, epoch 374/501 --> loss:0.8188786351680756
step 251/334, epoch 374/501 --> loss:0.8329076111316681
step 301/334, epoch 374/501 --> loss:0.8122134101390839
step 51/334, epoch 375/501 --> loss:0.8101432299613953
step 101/334, epoch 375/501 --> loss:0.8026920962333679
step 151/334, epoch 375/501 --> loss:0.8366983354091644
step 201/334, epoch 375/501 --> loss:0.8329085099697113
step 251/334, epoch 375/501 --> loss:0.8227446413040161
step 301/334, epoch 375/501 --> loss:0.827415292263031
step 51/334, epoch 376/501 --> loss:0.8284941446781159
step 101/334, epoch 376/501 --> loss:0.8161557745933533
step 151/334, epoch 376/501 --> loss:0.8152254557609558
step 201/334, epoch 376/501 --> loss:0.8244610846042633
step 251/334, epoch 376/501 --> loss:0.82165407538414
step 301/334, epoch 376/501 --> loss:0.8200436913967133
step 51/334, epoch 377/501 --> loss:0.8222977471351623
step 101/334, epoch 377/501 --> loss:0.8094118976593018
step 151/334, epoch 377/501 --> loss:0.8277969920635223
step 201/334, epoch 377/501 --> loss:0.8236168968677521
step 251/334, epoch 377/501 --> loss:0.8256927275657654
step 301/334, epoch 377/501 --> loss:0.8145158898830414
step 51/334, epoch 378/501 --> loss:0.8140995872020721
step 101/334, epoch 378/501 --> loss:0.8147963809967042
step 151/334, epoch 378/501 --> loss:0.8253825223445892
step 201/334, epoch 378/501 --> loss:0.828711965084076
step 251/334, epoch 378/501 --> loss:0.8127990007400513
step 301/334, epoch 378/501 --> loss:0.8252656424045562
step 51/334, epoch 379/501 --> loss:0.8317690861225128
step 101/334, epoch 379/501 --> loss:0.8266129434108734
step 151/334, epoch 379/501 --> loss:0.8240254044532775
step 201/334, epoch 379/501 --> loss:0.821099442243576
step 251/334, epoch 379/501 --> loss:0.8216742241382599
step 301/334, epoch 379/501 --> loss:0.8011462533473969
step 51/334, epoch 380/501 --> loss:0.8277172648906708
step 101/334, epoch 380/501 --> loss:0.8198568725585937
step 151/334, epoch 380/501 --> loss:0.809310816526413
step 201/334, epoch 380/501 --> loss:0.816314195394516
step 251/334, epoch 380/501 --> loss:0.8138318240642548
step 301/334, epoch 380/501 --> loss:0.8306240832805634
step 51/334, epoch 381/501 --> loss:0.8278436100482941
step 101/334, epoch 381/501 --> loss:0.8230420029163361
step 151/334, epoch 381/501 --> loss:0.8280171120166778
step 201/334, epoch 381/501 --> loss:0.8158548319339752
step 251/334, epoch 381/501 --> loss:0.7974665665626526
step 301/334, epoch 381/501 --> loss:0.8220997607707977

##########train dataset##########
acc--> [99.43245057090357]
F1--> {'F1': [0.9364077594449409], 'precision': [0.887569516989816], 'recall': [0.9909447424561005]}
##########eval dataset##########
acc--> [98.72628893647156]
F1--> {'F1': [0.8606504508110704], 'precision': [0.8163185895419551], 'recall': [0.9100850349734788]}
step 51/334, epoch 382/501 --> loss:0.830092933177948
step 101/334, epoch 382/501 --> loss:0.8238461685180664
step 151/334, epoch 382/501 --> loss:0.8300584745407105
step 201/334, epoch 382/501 --> loss:0.8136347281932831
step 251/334, epoch 382/501 --> loss:0.8315626001358032
step 301/334, epoch 382/501 --> loss:0.8228502404689789
step 51/334, epoch 383/501 --> loss:0.8205506098270416
step 101/334, epoch 383/501 --> loss:0.824650571346283
step 151/334, epoch 383/501 --> loss:0.8232764637470246
step 201/334, epoch 383/501 --> loss:0.8129209899902343
step 251/334, epoch 383/501 --> loss:0.8270233476161957
step 301/334, epoch 383/501 --> loss:0.8258104991912841
step 51/334, epoch 384/501 --> loss:0.8243446278572083
step 101/334, epoch 384/501 --> loss:0.8239022493362427
step 151/334, epoch 384/501 --> loss:0.8267053508758545
step 201/334, epoch 384/501 --> loss:0.816052815914154
step 251/334, epoch 384/501 --> loss:0.8236270391941071
step 301/334, epoch 384/501 --> loss:0.8137032377719879
step 51/334, epoch 385/501 --> loss:0.8203819942474365
step 101/334, epoch 385/501 --> loss:0.8275189423561096
step 151/334, epoch 385/501 --> loss:0.812339733839035
step 201/334, epoch 385/501 --> loss:0.8218979370594025
step 251/334, epoch 385/501 --> loss:0.8159567868709564
step 301/334, epoch 385/501 --> loss:0.814488480091095
step 51/334, epoch 386/501 --> loss:0.8014211165904999
step 101/334, epoch 386/501 --> loss:0.8132914650440216
step 151/334, epoch 386/501 --> loss:0.829370710849762
step 201/334, epoch 386/501 --> loss:0.8256211149692535
step 251/334, epoch 386/501 --> loss:0.8298824942111969
step 301/334, epoch 386/501 --> loss:0.8206450343132019
step 51/334, epoch 387/501 --> loss:0.8454044938087464
step 101/334, epoch 387/501 --> loss:0.8296140110492707
step 151/334, epoch 387/501 --> loss:0.7995519673824311
step 201/334, epoch 387/501 --> loss:0.8103237557411194
step 251/334, epoch 387/501 --> loss:0.8218614435195923
step 301/334, epoch 387/501 --> loss:0.839766696691513
step 51/334, epoch 388/501 --> loss:0.8190327787399292
step 101/334, epoch 388/501 --> loss:0.8355269598960876
step 151/334, epoch 388/501 --> loss:0.8093123424053192
step 201/334, epoch 388/501 --> loss:0.8288059723377228
step 251/334, epoch 388/501 --> loss:0.8073575019836425
step 301/334, epoch 388/501 --> loss:0.8236545038223266
step 51/334, epoch 389/501 --> loss:0.8343816733360291
step 101/334, epoch 389/501 --> loss:0.8288009297847748
step 151/334, epoch 389/501 --> loss:0.8290365874767304
step 201/334, epoch 389/501 --> loss:0.8159850692749023
step 251/334, epoch 389/501 --> loss:0.8324501729011535
step 301/334, epoch 389/501 --> loss:0.7925287747383117
step 51/334, epoch 390/501 --> loss:0.8110224282741547
step 101/334, epoch 390/501 --> loss:0.8275859117507934
step 151/334, epoch 390/501 --> loss:0.7990751516819
step 201/334, epoch 390/501 --> loss:0.8251672863960267
step 251/334, epoch 390/501 --> loss:0.8272697782516479
step 301/334, epoch 390/501 --> loss:0.8257379174232483
step 51/334, epoch 391/501 --> loss:0.8130915379524231
step 101/334, epoch 391/501 --> loss:0.8220292043685913
step 151/334, epoch 391/501 --> loss:0.8167110311985016
step 201/334, epoch 391/501 --> loss:0.8226242697238922
step 251/334, epoch 391/501 --> loss:0.8267270874977112
step 301/334, epoch 391/501 --> loss:0.8194090366363526

##########train dataset##########
acc--> [99.48473695426459]
F1--> {'F1': [0.9419236282864802], 'precision': [0.8975631740942985], 'recall': [0.9909079765855733]}
##########eval dataset##########
acc--> [98.80476156452683]
F1--> {'F1': [0.8676345392621586], 'precision': [0.8320802203984933], 'recall': [0.9063738085715317]}
step 51/334, epoch 392/501 --> loss:0.8178923726081848
step 101/334, epoch 392/501 --> loss:0.8235093080997467
step 151/334, epoch 392/501 --> loss:0.8160075199604034
step 201/334, epoch 392/501 --> loss:0.8178524351119996
step 251/334, epoch 392/501 --> loss:0.820191844701767
step 301/334, epoch 392/501 --> loss:0.8313049471378327
step 51/334, epoch 393/501 --> loss:0.8262534308433532
step 101/334, epoch 393/501 --> loss:0.8228727626800537
step 151/334, epoch 393/501 --> loss:0.821192501783371
step 201/334, epoch 393/501 --> loss:0.8246793150901794
step 251/334, epoch 393/501 --> loss:0.8082525300979614
step 301/334, epoch 393/501 --> loss:0.8293452596664429
step 51/334, epoch 394/501 --> loss:0.8196619296073914
step 101/334, epoch 394/501 --> loss:0.8116808831691742
step 151/334, epoch 394/501 --> loss:0.8126051497459411
step 201/334, epoch 394/501 --> loss:0.824202092885971
step 251/334, epoch 394/501 --> loss:0.8357394456863403
step 301/334, epoch 394/501 --> loss:0.8249890220165252
step 51/334, epoch 395/501 --> loss:0.8161874091625214
step 101/334, epoch 395/501 --> loss:0.8208808517456054
step 151/334, epoch 395/501 --> loss:0.815772647857666
step 201/334, epoch 395/501 --> loss:0.8249174296855927
step 251/334, epoch 395/501 --> loss:0.8122284960746765
step 301/334, epoch 395/501 --> loss:0.8262544298171997
step 51/334, epoch 396/501 --> loss:0.8080513286590576
step 101/334, epoch 396/501 --> loss:0.8164257824420929
step 151/334, epoch 396/501 --> loss:0.8325162076950073
step 201/334, epoch 396/501 --> loss:0.8128984940052032
step 251/334, epoch 396/501 --> loss:0.8170939385890961
step 301/334, epoch 396/501 --> loss:0.8211355173587799
step 51/334, epoch 397/501 --> loss:0.8277106177806854
step 101/334, epoch 397/501 --> loss:0.8166062045097351
step 151/334, epoch 397/501 --> loss:0.8211600363254548
step 201/334, epoch 397/501 --> loss:0.8349259877204895
step 251/334, epoch 397/501 --> loss:0.8154161429405212
step 301/334, epoch 397/501 --> loss:0.8066821312904358
step 51/334, epoch 398/501 --> loss:0.8313007426261901
step 101/334, epoch 398/501 --> loss:0.8157713651657105
step 151/334, epoch 398/501 --> loss:0.8164199829101563
step 201/334, epoch 398/501 --> loss:0.8300043189525604
step 251/334, epoch 398/501 --> loss:0.8150377380847931
step 301/334, epoch 398/501 --> loss:0.82216606259346
step 51/334, epoch 399/501 --> loss:0.8226753461360932
step 101/334, epoch 399/501 --> loss:0.8217471408843994
step 151/334, epoch 399/501 --> loss:0.8217794251441956
step 201/334, epoch 399/501 --> loss:0.8099116063117981
step 251/334, epoch 399/501 --> loss:0.8349526011943817
step 301/334, epoch 399/501 --> loss:0.8249529576301575
step 51/334, epoch 400/501 --> loss:0.8241290140151978
step 101/334, epoch 400/501 --> loss:0.8107416486740112
step 151/334, epoch 400/501 --> loss:0.818316844701767
step 201/334, epoch 400/501 --> loss:0.8281384027004242
step 251/334, epoch 400/501 --> loss:0.8106250166893005
step 301/334, epoch 400/501 --> loss:0.8259121453762055
step 51/334, epoch 401/501 --> loss:0.8168470644950867
step 101/334, epoch 401/501 --> loss:0.8255011081695557
step 151/334, epoch 401/501 --> loss:0.8125650465488434
step 201/334, epoch 401/501 --> loss:0.8173792433738708
step 251/334, epoch 401/501 --> loss:0.8209551858901978
step 301/334, epoch 401/501 --> loss:0.8321154177188873

##########train dataset##########
acc--> [99.46387812668151]
F1--> {'F1': [0.9396848889604023], 'precision': [0.8939276300441047], 'recall': [0.9903902697149753]}
##########eval dataset##########
acc--> [98.79167011090117]
F1--> {'F1': [0.8659152878389546], 'precision': [0.8319697417146144], 'recall': [0.9027595673212814]}
step 51/334, epoch 402/501 --> loss:0.8230224645137787
step 101/334, epoch 402/501 --> loss:0.8105791401863098
step 151/334, epoch 402/501 --> loss:0.8246310675144195
step 201/334, epoch 402/501 --> loss:0.8111570513248444
step 251/334, epoch 402/501 --> loss:0.8252087128162384
step 301/334, epoch 402/501 --> loss:0.8258626019954681
step 51/334, epoch 403/501 --> loss:0.8260314381122589
step 101/334, epoch 403/501 --> loss:0.8312474584579468
step 151/334, epoch 403/501 --> loss:0.8127769744396209
step 201/334, epoch 403/501 --> loss:0.8151154601573944
step 251/334, epoch 403/501 --> loss:0.8082641470432281
step 301/334, epoch 403/501 --> loss:0.8214004135131836
step 51/334, epoch 404/501 --> loss:0.8127953898906708
step 101/334, epoch 404/501 --> loss:0.8257281959056855
step 151/334, epoch 404/501 --> loss:0.8252124488353729
step 201/334, epoch 404/501 --> loss:0.8132213115692138
step 251/334, epoch 404/501 --> loss:0.8175157964229584
step 301/334, epoch 404/501 --> loss:0.8258656251430512
step 51/334, epoch 405/501 --> loss:0.8159177708625793
step 101/334, epoch 405/501 --> loss:0.8158888125419617
step 151/334, epoch 405/501 --> loss:0.8353108847141266
step 201/334, epoch 405/501 --> loss:0.8190832889080047
step 251/334, epoch 405/501 --> loss:0.8234942817687988
step 301/334, epoch 405/501 --> loss:0.8187647807598114
step 51/334, epoch 406/501 --> loss:0.8034377062320709
step 101/334, epoch 406/501 --> loss:0.8272108340263367
step 151/334, epoch 406/501 --> loss:0.8342342472076416
step 201/334, epoch 406/501 --> loss:0.8162538707256317
step 251/334, epoch 406/501 --> loss:0.8297083795070648
step 301/334, epoch 406/501 --> loss:0.8190836691856385
step 51/334, epoch 407/501 --> loss:0.8363691651821137
step 101/334, epoch 407/501 --> loss:0.8228366315364838
step 151/334, epoch 407/501 --> loss:0.8238975214958191
step 201/334, epoch 407/501 --> loss:0.8222015774250031
step 251/334, epoch 407/501 --> loss:0.8033545088768005
step 301/334, epoch 407/501 --> loss:0.8189130210876465
step 51/334, epoch 408/501 --> loss:0.823127658367157
step 101/334, epoch 408/501 --> loss:0.8329488790035248
step 151/334, epoch 408/501 --> loss:0.8343628203868866
step 201/334, epoch 408/501 --> loss:0.8199519336223602
step 251/334, epoch 408/501 --> loss:0.8036266148090363
step 301/334, epoch 408/501 --> loss:0.8116289305686951
step 51/334, epoch 409/501 --> loss:0.8229478228092194
step 101/334, epoch 409/501 --> loss:0.8198186731338502
step 151/334, epoch 409/501 --> loss:0.8215288782119751
step 201/334, epoch 409/501 --> loss:0.8163433003425599
step 251/334, epoch 409/501 --> loss:0.811076613664627
step 301/334, epoch 409/501 --> loss:0.8342231428623199
step 51/334, epoch 410/501 --> loss:0.8217150855064392
step 101/334, epoch 410/501 --> loss:0.8190472531318664
step 151/334, epoch 410/501 --> loss:0.823641119003296
step 201/334, epoch 410/501 --> loss:0.8219638907909393
step 251/334, epoch 410/501 --> loss:0.8303632485866547
step 301/334, epoch 410/501 --> loss:0.814912132024765
step 51/334, epoch 411/501 --> loss:0.8155814504623413
step 101/334, epoch 411/501 --> loss:0.8220193290710449
step 151/334, epoch 411/501 --> loss:0.8242081272602081
step 201/334, epoch 411/501 --> loss:0.8282551181316375
step 251/334, epoch 411/501 --> loss:0.8252370369434356
step 301/334, epoch 411/501 --> loss:0.8080796468257904

##########train dataset##########
acc--> [99.51912873149482]
F1--> {'F1': [0.9456154714516992], 'precision': [0.9038636720351843], 'recall': [0.9914222917706983]}
##########eval dataset##########
acc--> [98.90715631599465]
F1--> {'F1': [0.8764802472271543], 'precision': [0.8567688933404215], 'recall': [0.8971304132140461]}
save model!
step 51/334, epoch 412/501 --> loss:0.8173803567886353
step 101/334, epoch 412/501 --> loss:0.8051998031139374
step 151/334, epoch 412/501 --> loss:0.8360948634147644
step 201/334, epoch 412/501 --> loss:0.8266096997261048
step 251/334, epoch 412/501 --> loss:0.8304713988304138
step 301/334, epoch 412/501 --> loss:0.8217509090900421
step 51/334, epoch 413/501 --> loss:0.8296816349029541
step 101/334, epoch 413/501 --> loss:0.8199050533771515
step 151/334, epoch 413/501 --> loss:0.8149095702171326
step 201/334, epoch 413/501 --> loss:0.8337250363826751
step 251/334, epoch 413/501 --> loss:0.7993834114074707
step 301/334, epoch 413/501 --> loss:0.8224670362472534
step 51/334, epoch 414/501 --> loss:0.8147565972805023
step 101/334, epoch 414/501 --> loss:0.816002471446991
step 151/334, epoch 414/501 --> loss:0.8298143291473389
step 201/334, epoch 414/501 --> loss:0.8264693260192871
step 251/334, epoch 414/501 --> loss:0.8205824995040893
step 301/334, epoch 414/501 --> loss:0.8278706657886505
step 51/334, epoch 415/501 --> loss:0.8301522922515869
step 101/334, epoch 415/501 --> loss:0.8326219975948334
step 151/334, epoch 415/501 --> loss:0.82593532204628
step 201/334, epoch 415/501 --> loss:0.8113355886936188
step 251/334, epoch 415/501 --> loss:0.8152542436122894
step 301/334, epoch 415/501 --> loss:0.8081521677970886
step 51/334, epoch 416/501 --> loss:0.8078073847293854
step 101/334, epoch 416/501 --> loss:0.8272884202003479
step 151/334, epoch 416/501 --> loss:0.8108741664886474
step 201/334, epoch 416/501 --> loss:0.8220487654209137
step 251/334, epoch 416/501 --> loss:0.8362729215621948
step 301/334, epoch 416/501 --> loss:0.8152512359619141
step 51/334, epoch 417/501 --> loss:0.8192268276214599
step 101/334, epoch 417/501 --> loss:0.8194003689289093
step 151/334, epoch 417/501 --> loss:0.8252597546577454
step 201/334, epoch 417/501 --> loss:0.8259001469612122
step 251/334, epoch 417/501 --> loss:0.8191873812675476
step 301/334, epoch 417/501 --> loss:0.8181885039806366
step 51/334, epoch 418/501 --> loss:0.8253631484508515
step 101/334, epoch 418/501 --> loss:0.8070545101165771
step 151/334, epoch 418/501 --> loss:0.8431570363044739
step 201/334, epoch 418/501 --> loss:0.8199091613292694
step 251/334, epoch 418/501 --> loss:0.8176262998580932
step 301/334, epoch 418/501 --> loss:0.8225700414180755
step 51/334, epoch 419/501 --> loss:0.8132244396209717
step 101/334, epoch 419/501 --> loss:0.8144324791431427
step 151/334, epoch 419/501 --> loss:0.8271613895893097
step 201/334, epoch 419/501 --> loss:0.8219305992126464
step 251/334, epoch 419/501 --> loss:0.8348500883579254
step 301/334, epoch 419/501 --> loss:0.8214077019691467
step 51/334, epoch 420/501 --> loss:0.8115147817134857
step 101/334, epoch 420/501 --> loss:0.8220201909542084
step 151/334, epoch 420/501 --> loss:0.8386929476261139
step 201/334, epoch 420/501 --> loss:0.817255232334137
step 251/334, epoch 420/501 --> loss:0.8233449625968933
step 301/334, epoch 420/501 --> loss:0.8175138652324676
step 51/334, epoch 421/501 --> loss:0.8275997889041901
step 101/334, epoch 421/501 --> loss:0.8292542934417725
step 151/334, epoch 421/501 --> loss:0.8317738533020019
step 201/334, epoch 421/501 --> loss:0.814509414434433
step 251/334, epoch 421/501 --> loss:0.8098811984062195
step 301/334, epoch 421/501 --> loss:0.8182847261428833

##########train dataset##########
acc--> [99.52225086167533]
F1--> {'F1': [0.9459410605174261], 'precision': [0.9045943331958438], 'recall': [0.9912594908679945]}
##########eval dataset##########
acc--> [98.91693168632801]
F1--> {'F1': [0.8771180086782698], 'precision': [0.8605279979969008], 'recall': [0.8943706605694289]}
save model!
step 51/334, epoch 422/501 --> loss:0.8229169023036956
step 101/334, epoch 422/501 --> loss:0.8153151929378509
step 151/334, epoch 422/501 --> loss:0.8168448293209076
step 201/334, epoch 422/501 --> loss:0.8156200110912323
step 251/334, epoch 422/501 --> loss:0.8332449698448181
step 301/334, epoch 422/501 --> loss:0.8200863778591156
step 51/334, epoch 423/501 --> loss:0.8201533257961273
step 101/334, epoch 423/501 --> loss:0.8255100584030152
step 151/334, epoch 423/501 --> loss:0.8210710215568543
step 201/334, epoch 423/501 --> loss:0.8210093748569488
step 251/334, epoch 423/501 --> loss:0.819896856546402
step 301/334, epoch 423/501 --> loss:0.8163015604019165
step 51/334, epoch 424/501 --> loss:0.8149645328521729
step 101/334, epoch 424/501 --> loss:0.8206665194034577
step 151/334, epoch 424/501 --> loss:0.8120161151885986
step 201/334, epoch 424/501 --> loss:0.8254820585250855
step 251/334, epoch 424/501 --> loss:0.8244638574123383
step 301/334, epoch 424/501 --> loss:0.8221355676651001
step 51/334, epoch 425/501 --> loss:0.8282565283775329
step 101/334, epoch 425/501 --> loss:0.8146986401081086
step 151/334, epoch 425/501 --> loss:0.8166855001449584
step 201/334, epoch 425/501 --> loss:0.8319890284538269
step 251/334, epoch 425/501 --> loss:0.8215555322170257
step 301/334, epoch 425/501 --> loss:0.8322107410430908
step 51/334, epoch 426/501 --> loss:0.8176225459575653
step 101/334, epoch 426/501 --> loss:0.8071397507190704
step 151/334, epoch 426/501 --> loss:0.831129573583603
step 201/334, epoch 426/501 --> loss:0.8349544012546539
step 251/334, epoch 426/501 --> loss:0.803848946094513
step 301/334, epoch 426/501 --> loss:0.8179067718982697
step 51/334, epoch 427/501 --> loss:0.8223378133773803
step 101/334, epoch 427/501 --> loss:0.8225736570358276
step 151/334, epoch 427/501 --> loss:0.826824301481247
step 201/334, epoch 427/501 --> loss:0.8221156144142151
step 251/334, epoch 427/501 --> loss:0.8217709040641785
step 301/334, epoch 427/501 --> loss:0.8126580679416656
step 51/334, epoch 428/501 --> loss:0.8207574248313904
step 101/334, epoch 428/501 --> loss:0.823143218755722
step 151/334, epoch 428/501 --> loss:0.8265150260925292
step 201/334, epoch 428/501 --> loss:0.8123422861099243
step 251/334, epoch 428/501 --> loss:0.8249851667881012
step 301/334, epoch 428/501 --> loss:0.8262142443656921
step 51/334, epoch 429/501 --> loss:0.8164729022979736
step 101/334, epoch 429/501 --> loss:0.8195814394950867
step 151/334, epoch 429/501 --> loss:0.8234224998950959
step 201/334, epoch 429/501 --> loss:0.8194124603271484
step 251/334, epoch 429/501 --> loss:0.8182895958423615
step 301/334, epoch 429/501 --> loss:0.824313622713089
step 51/334, epoch 430/501 --> loss:0.819983742237091
step 101/334, epoch 430/501 --> loss:0.8237084150314331
step 151/334, epoch 430/501 --> loss:0.8227710497379302
step 201/334, epoch 430/501 --> loss:0.8220459187030792
step 251/334, epoch 430/501 --> loss:0.8097014427185059
step 301/334, epoch 430/501 --> loss:0.8321059620380402
step 51/334, epoch 431/501 --> loss:0.8237244534492493
step 101/334, epoch 431/501 --> loss:0.8198495078086853
step 151/334, epoch 431/501 --> loss:0.8238399755954743
step 201/334, epoch 431/501 --> loss:0.8124004006385803
step 251/334, epoch 431/501 --> loss:0.8241157567501068
step 301/334, epoch 431/501 --> loss:0.8228901374340057

##########train dataset##########
acc--> [99.57378460397854]
F1--> {'F1': [0.9514485232258865], 'precision': [0.9154604262816239], 'recall': [0.990392711728516]}
##########eval dataset##########
acc--> [98.93498057767752]
F1--> {'F1': [0.8787320244313896], 'precision': [0.8650989847125494], 'recall': [0.8928119485693103]}
save model!
step 51/334, epoch 432/501 --> loss:0.8236952483654022
step 101/334, epoch 432/501 --> loss:0.8074684607982635
step 151/334, epoch 432/501 --> loss:0.8250969636440277
step 201/334, epoch 432/501 --> loss:0.8185282635688782
step 251/334, epoch 432/501 --> loss:0.8183632111549377
step 301/334, epoch 432/501 --> loss:0.8271537303924561
step 51/334, epoch 433/501 --> loss:0.8275215005874634
step 101/334, epoch 433/501 --> loss:0.819637063741684
step 151/334, epoch 433/501 --> loss:0.812123566865921
step 201/334, epoch 433/501 --> loss:0.8188285648822784
step 251/334, epoch 433/501 --> loss:0.8274525451660156
step 301/334, epoch 433/501 --> loss:0.8142662179470063
step 51/334, epoch 434/501 --> loss:0.8217738509178162
step 101/334, epoch 434/501 --> loss:0.8249047064781189
step 151/334, epoch 434/501 --> loss:0.8268607866764068
step 201/334, epoch 434/501 --> loss:0.8188032901287079
step 251/334, epoch 434/501 --> loss:0.8162930381298065
step 301/334, epoch 434/501 --> loss:0.8179757034778595
step 51/334, epoch 435/501 --> loss:0.8321115851402283
step 101/334, epoch 435/501 --> loss:0.8152401232719422
step 151/334, epoch 435/501 --> loss:0.8186986255645752
step 201/334, epoch 435/501 --> loss:0.8307174098491669
step 251/334, epoch 435/501 --> loss:0.8183446049690246
step 301/334, epoch 435/501 --> loss:0.8147597432136535
step 51/334, epoch 436/501 --> loss:0.8119163715839386
step 101/334, epoch 436/501 --> loss:0.8162454128265381
step 151/334, epoch 436/501 --> loss:0.8166975903511048
step 201/334, epoch 436/501 --> loss:0.8254044508934021
step 251/334, epoch 436/501 --> loss:0.8271217405796051
step 301/334, epoch 436/501 --> loss:0.8232174432277679
step 51/334, epoch 437/501 --> loss:0.8273884761333465
step 101/334, epoch 437/501 --> loss:0.8108293223381042
step 151/334, epoch 437/501 --> loss:0.8342238354682923
step 201/334, epoch 437/501 --> loss:0.8233679103851318
step 251/334, epoch 437/501 --> loss:0.8131368422508239
step 301/334, epoch 437/501 --> loss:0.8255176496505737
step 51/334, epoch 438/501 --> loss:0.8310290646553039
step 101/334, epoch 438/501 --> loss:0.8396577644348144
step 151/334, epoch 438/501 --> loss:0.8239985024929046
step 201/334, epoch 438/501 --> loss:0.8155156314373017
step 251/334, epoch 438/501 --> loss:0.8099776387214661
step 301/334, epoch 438/501 --> loss:0.8120736110210419
step 51/334, epoch 439/501 --> loss:0.8299770605564117
step 101/334, epoch 439/501 --> loss:0.8168232083320618
step 151/334, epoch 439/501 --> loss:0.830671694278717
step 201/334, epoch 439/501 --> loss:0.8346595394611359
step 251/334, epoch 439/501 --> loss:0.8122899222373963
step 301/334, epoch 439/501 --> loss:0.8222352647781372
step 51/334, epoch 440/501 --> loss:0.827417002916336
step 101/334, epoch 440/501 --> loss:0.8257376146316528
step 151/334, epoch 440/501 --> loss:0.8228578507900238
step 201/334, epoch 440/501 --> loss:0.821039400100708
step 251/334, epoch 440/501 --> loss:0.8226342904567718
step 301/334, epoch 440/501 --> loss:0.814093121290207
step 51/334, epoch 441/501 --> loss:0.8128963840007782
step 101/334, epoch 441/501 --> loss:0.8231697857379914
step 151/334, epoch 441/501 --> loss:0.8225901496410369
step 201/334, epoch 441/501 --> loss:0.8213559544086456
step 251/334, epoch 441/501 --> loss:0.826871931552887
step 301/334, epoch 441/501 --> loss:0.8172819006443024

##########train dataset##########
acc--> [99.54603700812748]
F1--> {'F1': [0.9484605130834727], 'precision': [0.9097803574220851], 'recall': [0.9905866483038618]}
##########eval dataset##########
acc--> [98.9311425183114]
F1--> {'F1': [0.8787291451657467], 'precision': [0.8621163157262525], 'recall': [0.8960052001324188]}
step 51/334, epoch 442/501 --> loss:0.8228355693817139
step 101/334, epoch 442/501 --> loss:0.8240739536285401
step 151/334, epoch 442/501 --> loss:0.8046380341053009
step 201/334, epoch 442/501 --> loss:0.8260195589065552
step 251/334, epoch 442/501 --> loss:0.8294482934474945
step 301/334, epoch 442/501 --> loss:0.8257295203208923
step 51/334, epoch 443/501 --> loss:0.821092211008072
step 101/334, epoch 443/501 --> loss:0.8213379251956939
step 151/334, epoch 443/501 --> loss:0.8112445640563964
step 201/334, epoch 443/501 --> loss:0.8130652117729187
step 251/334, epoch 443/501 --> loss:0.831401195526123
step 301/334, epoch 443/501 --> loss:0.822825837135315
step 51/334, epoch 444/501 --> loss:0.8196380507946014
step 101/334, epoch 444/501 --> loss:0.8156234526634216
step 151/334, epoch 444/501 --> loss:0.825589942932129
step 201/334, epoch 444/501 --> loss:0.8213357412815094
step 251/334, epoch 444/501 --> loss:0.815913702249527
step 301/334, epoch 444/501 --> loss:0.8334057307243348
step 51/334, epoch 445/501 --> loss:0.803023738861084
step 101/334, epoch 445/501 --> loss:0.8272322523593902
step 151/334, epoch 445/501 --> loss:0.8405857896804809
step 201/334, epoch 445/501 --> loss:0.8134781050682068
step 251/334, epoch 445/501 --> loss:0.8311819243431091
step 301/334, epoch 445/501 --> loss:0.8211316537857055
step 51/334, epoch 446/501 --> loss:0.8311300849914551
step 101/334, epoch 446/501 --> loss:0.812054363489151
step 151/334, epoch 446/501 --> loss:0.812712253332138
step 201/334, epoch 446/501 --> loss:0.8181563842296601
step 251/334, epoch 446/501 --> loss:0.8273066306114196
step 301/334, epoch 446/501 --> loss:0.8171920776367188
step 51/334, epoch 447/501 --> loss:0.8229018366336822
step 101/334, epoch 447/501 --> loss:0.8085338771343231
step 151/334, epoch 447/501 --> loss:0.8395684778690338
step 201/334, epoch 447/501 --> loss:0.818259060382843
step 251/334, epoch 447/501 --> loss:0.8230308258533477
step 301/334, epoch 447/501 --> loss:0.812747563123703
step 51/334, epoch 448/501 --> loss:0.8137706398963929
step 101/334, epoch 448/501 --> loss:0.8095549273490906
step 151/334, epoch 448/501 --> loss:0.839620305299759
step 201/334, epoch 448/501 --> loss:0.812808301448822
step 251/334, epoch 448/501 --> loss:0.813136236667633
step 301/334, epoch 448/501 --> loss:0.8249093246459961
step 51/334, epoch 449/501 --> loss:0.82063507437706
step 101/334, epoch 449/501 --> loss:0.8145696878433227
step 151/334, epoch 449/501 --> loss:0.830164053440094
step 201/334, epoch 449/501 --> loss:0.8200147128105164
step 251/334, epoch 449/501 --> loss:0.8394357776641845
step 301/334, epoch 449/501 --> loss:0.805341341495514
step 51/334, epoch 450/501 --> loss:0.8260151517391204
step 101/334, epoch 450/501 --> loss:0.8052498865127563
step 151/334, epoch 450/501 --> loss:0.8292599368095398
step 201/334, epoch 450/501 --> loss:0.8139914059638977
step 251/334, epoch 450/501 --> loss:0.8314433884620667
step 301/334, epoch 450/501 --> loss:0.8323479902744293
step 51/334, epoch 451/501 --> loss:0.813740360736847
step 101/334, epoch 451/501 --> loss:0.8117244958877563
step 151/334, epoch 451/501 --> loss:0.8195921230316162
step 201/334, epoch 451/501 --> loss:0.8383873975276948
step 251/334, epoch 451/501 --> loss:0.8167525577545166
step 301/334, epoch 451/501 --> loss:0.8221429502964019

##########train dataset##########
acc--> [99.491535325507]
F1--> {'F1': [0.9426895310010176], 'precision': [0.8982971209416986], 'recall': [0.991708685692038]}
##########eval dataset##########
acc--> [98.83888958841032]
F1--> {'F1': [0.8701430946732226], 'precision': [0.8421265333633838], 'recall': [0.9000986530478201]}
step 51/334, epoch 452/501 --> loss:0.8175960791110992
step 101/334, epoch 452/501 --> loss:0.8316612124443055
step 151/334, epoch 452/501 --> loss:0.8254328215122223
step 201/334, epoch 452/501 --> loss:0.8106475067138672
step 251/334, epoch 452/501 --> loss:0.8222122693061829
step 301/334, epoch 452/501 --> loss:0.8249747836589814
step 51/334, epoch 453/501 --> loss:0.8186853504180909
step 101/334, epoch 453/501 --> loss:0.8301519048213959
step 151/334, epoch 453/501 --> loss:0.8131633830070496
step 201/334, epoch 453/501 --> loss:0.8213353633880616
step 251/334, epoch 453/501 --> loss:0.8240698122978211
step 301/334, epoch 453/501 --> loss:0.8178778898715973
step 51/334, epoch 454/501 --> loss:0.8172187459468842
step 101/334, epoch 454/501 --> loss:0.8171233248710632
step 151/334, epoch 454/501 --> loss:0.8276593196392059
step 201/334, epoch 454/501 --> loss:0.8266616284847259
step 251/334, epoch 454/501 --> loss:0.8198107993602752
step 301/334, epoch 454/501 --> loss:0.8220712852478027
step 51/334, epoch 455/501 --> loss:0.819687147140503
step 101/334, epoch 455/501 --> loss:0.8213672256469726
step 151/334, epoch 455/501 --> loss:0.8153937649726868
step 201/334, epoch 455/501 --> loss:0.8223674607276916
step 251/334, epoch 455/501 --> loss:0.8225680112838745
step 301/334, epoch 455/501 --> loss:0.8160549747943878
step 51/334, epoch 456/501 --> loss:0.8197185897827148
step 101/334, epoch 456/501 --> loss:0.8175689256191254
step 151/334, epoch 456/501 --> loss:0.8155877327919007
step 201/334, epoch 456/501 --> loss:0.8327507400512695
step 251/334, epoch 456/501 --> loss:0.8363536858558654
step 301/334, epoch 456/501 --> loss:0.8062366998195648
step 51/334, epoch 457/501 --> loss:0.8326980912685394
step 101/334, epoch 457/501 --> loss:0.8180210196971893
step 151/334, epoch 457/501 --> loss:0.8144396352767944
step 201/334, epoch 457/501 --> loss:0.8200274872779846
step 251/334, epoch 457/501 --> loss:0.8172278368473053
step 301/334, epoch 457/501 --> loss:0.8320282554626465
step 51/334, epoch 458/501 --> loss:0.8163837134838104
step 101/334, epoch 458/501 --> loss:0.8097843301296234
step 151/334, epoch 458/501 --> loss:0.8337577152252197
step 201/334, epoch 458/501 --> loss:0.809503401517868
step 251/334, epoch 458/501 --> loss:0.8332686364650727
step 301/334, epoch 458/501 --> loss:0.8185430026054382
step 51/334, epoch 459/501 --> loss:0.822605322599411
step 101/334, epoch 459/501 --> loss:0.8169119489192963
step 151/334, epoch 459/501 --> loss:0.8140652203559875
step 201/334, epoch 459/501 --> loss:0.820493358373642
step 251/334, epoch 459/501 --> loss:0.8362136566638947
step 301/334, epoch 459/501 --> loss:0.8175525379180908
step 51/334, epoch 460/501 --> loss:0.816066701412201
step 101/334, epoch 460/501 --> loss:0.8117963194847106
step 151/334, epoch 460/501 --> loss:0.8264746034145355
step 201/334, epoch 460/501 --> loss:0.8191592049598694
step 251/334, epoch 460/501 --> loss:0.8244181382656097
step 301/334, epoch 460/501 --> loss:0.8190405070781708
step 51/334, epoch 461/501 --> loss:0.8177409791946411
step 101/334, epoch 461/501 --> loss:0.8279732573032379
step 151/334, epoch 461/501 --> loss:0.8208394110202789
step 201/334, epoch 461/501 --> loss:0.8173433935642243
step 251/334, epoch 461/501 --> loss:0.8233406519889832
step 301/334, epoch 461/501 --> loss:0.8281072044372558

##########train dataset##########
acc--> [99.5820470637553]
F1--> {'F1': [0.9523632826484381], 'precision': [0.9168095638760309], 'recall': [0.9907965936346401]}
##########eval dataset##########
acc--> [98.94307854919573]
F1--> {'F1': [0.8797367723445896], 'precision': [0.8655117131692651], 'recall': [0.8944475691769846]}
save model!
step 51/334, epoch 462/501 --> loss:0.823909227848053
step 101/334, epoch 462/501 --> loss:0.8101565551757812
step 151/334, epoch 462/501 --> loss:0.8296431601047516
step 201/334, epoch 462/501 --> loss:0.819707772731781
step 251/334, epoch 462/501 --> loss:0.8307041311264038
step 301/334, epoch 462/501 --> loss:0.8047167468070984
step 51/334, epoch 463/501 --> loss:0.8278758704662323
step 101/334, epoch 463/501 --> loss:0.8327340793609619
step 151/334, epoch 463/501 --> loss:0.8179806208610535
step 201/334, epoch 463/501 --> loss:0.8175073111057282
step 251/334, epoch 463/501 --> loss:0.8188681781291962
step 301/334, epoch 463/501 --> loss:0.8073711693286896
step 51/334, epoch 464/501 --> loss:0.819863349199295
step 101/334, epoch 464/501 --> loss:0.8203781056404114
step 151/334, epoch 464/501 --> loss:0.8345479881763458
step 201/334, epoch 464/501 --> loss:0.822178829908371
step 251/334, epoch 464/501 --> loss:0.808483659029007
step 301/334, epoch 464/501 --> loss:0.819170572757721
step 51/334, epoch 465/501 --> loss:0.8192955923080444
step 101/334, epoch 465/501 --> loss:0.8108001565933227
step 151/334, epoch 465/501 --> loss:0.821168246269226
step 201/334, epoch 465/501 --> loss:0.822355797290802
step 251/334, epoch 465/501 --> loss:0.8304714894294739
step 301/334, epoch 465/501 --> loss:0.8197905766963959
step 51/334, epoch 466/501 --> loss:0.8226058673858643
step 101/334, epoch 466/501 --> loss:0.8055079066753388
step 151/334, epoch 466/501 --> loss:0.8205572104454041
step 201/334, epoch 466/501 --> loss:0.8092003881931304
step 251/334, epoch 466/501 --> loss:0.8388140058517456
step 301/334, epoch 466/501 --> loss:0.8193970048427581
step 51/334, epoch 467/501 --> loss:0.8162614524364471
step 101/334, epoch 467/501 --> loss:0.8152583849430084
step 151/334, epoch 467/501 --> loss:0.8296995031833648
step 201/334, epoch 467/501 --> loss:0.8147380781173706
step 251/334, epoch 467/501 --> loss:0.8178287792205811
step 301/334, epoch 467/501 --> loss:0.8273619997501374
step 51/334, epoch 468/501 --> loss:0.8131775021553039
step 101/334, epoch 468/501 --> loss:0.8363708066940307
step 151/334, epoch 468/501 --> loss:0.8062324666976929
step 201/334, epoch 468/501 --> loss:0.8163111615180969
step 251/334, epoch 468/501 --> loss:0.8268393421173096
step 301/334, epoch 468/501 --> loss:0.8335271537303924
step 51/334, epoch 469/501 --> loss:0.8127886211872101
step 101/334, epoch 469/501 --> loss:0.8276710641384125
step 151/334, epoch 469/501 --> loss:0.8232937693595886
step 201/334, epoch 469/501 --> loss:0.8280411064624786
step 251/334, epoch 469/501 --> loss:0.8141702818870544
step 301/334, epoch 469/501 --> loss:0.8166506540775299
step 51/334, epoch 470/501 --> loss:0.8266604661941528
step 101/334, epoch 470/501 --> loss:0.8178282678127289
step 151/334, epoch 470/501 --> loss:0.8211400043964386
step 201/334, epoch 470/501 --> loss:0.8108211314678192
step 251/334, epoch 470/501 --> loss:0.8186468172073365
step 301/334, epoch 470/501 --> loss:0.8264603245258332
step 51/334, epoch 471/501 --> loss:0.8204064440727233
step 101/334, epoch 471/501 --> loss:0.8204311287403107
step 151/334, epoch 471/501 --> loss:0.8348651146888733
step 201/334, epoch 471/501 --> loss:0.8081789827346801
step 251/334, epoch 471/501 --> loss:0.830759449005127
step 301/334, epoch 471/501 --> loss:0.8168396723270416

##########train dataset##########
acc--> [99.52314825240066]
F1--> {'F1': [0.946077575052658], 'precision': [0.9041898904413181], 'recall': [0.9920457513943443]}
##########eval dataset##########
acc--> [98.86612779511519]
F1--> {'F1': [0.8729226245915597], 'precision': [0.8464818107526183], 'recall': [0.901079160576677]}
step 51/334, epoch 472/501 --> loss:0.8252454483509064
step 101/334, epoch 472/501 --> loss:0.8101480436325074
step 151/334, epoch 472/501 --> loss:0.816257621049881
step 201/334, epoch 472/501 --> loss:0.8148025703430176
step 251/334, epoch 472/501 --> loss:0.8330002129077911
step 301/334, epoch 472/501 --> loss:0.82534144282341
step 51/334, epoch 473/501 --> loss:0.8336470329761505
step 101/334, epoch 473/501 --> loss:0.8223872780799866
step 151/334, epoch 473/501 --> loss:0.8213145220279694
step 201/334, epoch 473/501 --> loss:0.8134617352485657
step 251/334, epoch 473/501 --> loss:0.8168798661231995
step 301/334, epoch 473/501 --> loss:0.8125905847549438
step 51/334, epoch 474/501 --> loss:0.8206055569648742
step 101/334, epoch 474/501 --> loss:0.8233476102352142
step 151/334, epoch 474/501 --> loss:0.8226640069484711
step 201/334, epoch 474/501 --> loss:0.8139728355407715
step 251/334, epoch 474/501 --> loss:0.8277519917488099
step 301/334, epoch 474/501 --> loss:0.8281273305416107
step 51/334, epoch 475/501 --> loss:0.8270569717884064
step 101/334, epoch 475/501 --> loss:0.8073256254196167
step 151/334, epoch 475/501 --> loss:0.8267198240756989
step 201/334, epoch 475/501 --> loss:0.829007363319397
step 251/334, epoch 475/501 --> loss:0.8140312993526458
step 301/334, epoch 475/501 --> loss:0.815887005329132
step 51/334, epoch 476/501 --> loss:0.825383061170578
step 101/334, epoch 476/501 --> loss:0.807026789188385
step 151/334, epoch 476/501 --> loss:0.8146911513805389
step 201/334, epoch 476/501 --> loss:0.8266230988502502
step 251/334, epoch 476/501 --> loss:0.8139080572128295
step 301/334, epoch 476/501 --> loss:0.8281448459625245
step 51/334, epoch 477/501 --> loss:0.8314795434474945
step 101/334, epoch 477/501 --> loss:0.8208259296417236
step 151/334, epoch 477/501 --> loss:0.8066852247714996
step 201/334, epoch 477/501 --> loss:0.8248685443401337
step 251/334, epoch 477/501 --> loss:0.8278742694854736
step 301/334, epoch 477/501 --> loss:0.8197302591800689
step 51/334, epoch 478/501 --> loss:0.8171496212482452
step 101/334, epoch 478/501 --> loss:0.8129228961467743
step 151/334, epoch 478/501 --> loss:0.8330545461177826
step 201/334, epoch 478/501 --> loss:0.8248408174514771
step 251/334, epoch 478/501 --> loss:0.826655900478363
step 301/334, epoch 478/501 --> loss:0.8183788871765136
step 51/334, epoch 479/501 --> loss:0.8351837193965912
step 101/334, epoch 479/501 --> loss:0.8267424058914185
step 151/334, epoch 479/501 --> loss:0.813832927942276
step 201/334, epoch 479/501 --> loss:0.8144952762126922
step 251/334, epoch 479/501 --> loss:0.8081604611873626
step 301/334, epoch 479/501 --> loss:0.8266183435916901
step 51/334, epoch 480/501 --> loss:0.8242449975013733
step 101/334, epoch 480/501 --> loss:0.8203315126895905
step 151/334, epoch 480/501 --> loss:0.818709270954132
step 201/334, epoch 480/501 --> loss:0.8157488906383514
step 251/334, epoch 480/501 --> loss:0.8201201570034027
step 301/334, epoch 480/501 --> loss:0.8192328655719757
step 51/334, epoch 481/501 --> loss:0.8119868540763855
step 101/334, epoch 481/501 --> loss:0.8101842427253723
step 151/334, epoch 481/501 --> loss:0.8127586913108825
step 201/334, epoch 481/501 --> loss:0.8282176291942597
step 251/334, epoch 481/501 --> loss:0.8302316665649414
step 301/334, epoch 481/501 --> loss:0.8275825130939484

##########train dataset##########
acc--> [99.54970666965905]
F1--> {'F1': [0.9488907765215019], 'precision': [0.9099694406069424], 'recall': [0.9913012764330218]}
##########eval dataset##########
acc--> [98.87941015047629]
F1--> {'F1': [0.8731199408405376], 'precision': [0.854930762495562], 'recall': [0.8921103505690587]}
step 51/334, epoch 482/501 --> loss:0.8136356520652771
step 101/334, epoch 482/501 --> loss:0.8081164932250977
step 151/334, epoch 482/501 --> loss:0.8328697454929351
step 201/334, epoch 482/501 --> loss:0.8089985764026641
step 251/334, epoch 482/501 --> loss:0.8220293736457824
step 301/334, epoch 482/501 --> loss:0.8281475687026978
step 51/334, epoch 483/501 --> loss:0.8118289256095886
step 101/334, epoch 483/501 --> loss:0.8220062923431396
step 151/334, epoch 483/501 --> loss:0.8243267214298249
step 201/334, epoch 483/501 --> loss:0.8184136617183685
step 251/334, epoch 483/501 --> loss:0.8282632124423981
step 301/334, epoch 483/501 --> loss:0.8168981909751892
step 51/334, epoch 484/501 --> loss:0.8260584592819213
step 101/334, epoch 484/501 --> loss:0.8177745282649994
step 151/334, epoch 484/501 --> loss:0.8276233720779419
step 201/334, epoch 484/501 --> loss:0.8243420588970184
step 251/334, epoch 484/501 --> loss:0.8185204923152923
step 301/334, epoch 484/501 --> loss:0.8252995419502258
step 51/334, epoch 485/501 --> loss:0.8218512690067291
step 101/334, epoch 485/501 --> loss:0.8260085165500641
step 151/334, epoch 485/501 --> loss:0.8210496509075165
step 201/334, epoch 485/501 --> loss:0.8349659645557403
step 251/334, epoch 485/501 --> loss:0.8182909083366394
step 301/334, epoch 485/501 --> loss:0.7969741153717042
step 51/334, epoch 486/501 --> loss:0.8331225574016571
step 101/334, epoch 486/501 --> loss:0.8446291744709015
step 151/334, epoch 486/501 --> loss:0.8019807827472687
step 201/334, epoch 486/501 --> loss:0.8168148684501648
step 251/334, epoch 486/501 --> loss:0.8136905598640441
step 301/334, epoch 486/501 --> loss:0.8176025903224945
step 51/334, epoch 487/501 --> loss:0.8220443212985993
step 101/334, epoch 487/501 --> loss:0.8244487917423249
step 151/334, epoch 487/501 --> loss:0.8227485716342926
step 201/334, epoch 487/501 --> loss:0.8226450276374817
step 251/334, epoch 487/501 --> loss:0.8218748891353607
step 301/334, epoch 487/501 --> loss:0.817358351945877
step 51/334, epoch 488/501 --> loss:0.811840295791626
step 101/334, epoch 488/501 --> loss:0.8231491506099701
step 151/334, epoch 488/501 --> loss:0.8311729347705841
step 201/334, epoch 488/501 --> loss:0.8117570543289184
step 251/334, epoch 488/501 --> loss:0.8261140739917755
step 301/334, epoch 488/501 --> loss:0.8195878851413727
step 51/334, epoch 489/501 --> loss:0.8393182587623597
step 101/334, epoch 489/501 --> loss:0.8234404134750366
step 151/334, epoch 489/501 --> loss:0.8333962571620941
step 201/334, epoch 489/501 --> loss:0.8125250589847565
step 251/334, epoch 489/501 --> loss:0.8052397954463959
step 301/334, epoch 489/501 --> loss:0.8121460556983948
step 51/334, epoch 490/501 --> loss:0.8298893892765045
step 101/334, epoch 490/501 --> loss:0.8243724811077118
step 151/334, epoch 490/501 --> loss:0.8271662819385529
step 201/334, epoch 490/501 --> loss:0.8177392673492432
step 251/334, epoch 490/501 --> loss:0.8095535171031952
step 301/334, epoch 490/501 --> loss:0.8094117891788483
step 51/334, epoch 491/501 --> loss:0.8133180606365203
step 101/334, epoch 491/501 --> loss:0.8193705546855926
step 151/334, epoch 491/501 --> loss:0.8094422447681427
step 201/334, epoch 491/501 --> loss:0.8284490847587586
step 251/334, epoch 491/501 --> loss:0.8300389850139618
step 301/334, epoch 491/501 --> loss:0.8297866058349609

##########train dataset##########
acc--> [99.52856091733976]
F1--> {'F1': [0.9466632844907878], 'precision': [0.9051566758819708], 'recall': [0.9921704297523315]}
##########eval dataset##########
acc--> [98.85762265555988]
F1--> {'F1': [0.8719676390329305], 'precision': [0.8455670469822807], 'recall': [0.9000805841580931]}
step 51/334, epoch 492/501 --> loss:0.8280327105522156
step 101/334, epoch 492/501 --> loss:0.8101930510997772
step 151/334, epoch 492/501 --> loss:0.8266977226734161
step 201/334, epoch 492/501 --> loss:0.8187892830371857
step 251/334, epoch 492/501 --> loss:0.81874884724617
step 301/334, epoch 492/501 --> loss:0.8258534443378448
step 51/334, epoch 493/501 --> loss:0.8255692398548127
step 101/334, epoch 493/501 --> loss:0.8104150807857513
step 151/334, epoch 493/501 --> loss:0.8274910700321197
step 201/334, epoch 493/501 --> loss:0.8151788508892059
step 251/334, epoch 493/501 --> loss:0.8186502861976623
step 301/334, epoch 493/501 --> loss:0.828124508857727
step 51/334, epoch 494/501 --> loss:0.8196941900253296
step 101/334, epoch 494/501 --> loss:0.8196043848991394
step 151/334, epoch 494/501 --> loss:0.8285317873954773
step 201/334, epoch 494/501 --> loss:0.818228394985199
step 251/334, epoch 494/501 --> loss:0.7999762225151063
step 301/334, epoch 494/501 --> loss:0.8373583221435547
step 51/334, epoch 495/501 --> loss:0.8284811460971833
step 101/334, epoch 495/501 --> loss:0.8100933456420898
step 151/334, epoch 495/501 --> loss:0.817828369140625
step 201/334, epoch 495/501 --> loss:0.8470415878295898
step 251/334, epoch 495/501 --> loss:0.8156509327888489
step 301/334, epoch 495/501 --> loss:0.8096757543087005
step 51/334, epoch 496/501 --> loss:0.8191636073589325
step 101/334, epoch 496/501 --> loss:0.8239518177509307
step 151/334, epoch 496/501 --> loss:0.8123840022087098
step 201/334, epoch 496/501 --> loss:0.8246916091442108
step 251/334, epoch 496/501 --> loss:0.824806946516037
step 301/334, epoch 496/501 --> loss:0.8111199307441711
step 51/334, epoch 497/501 --> loss:0.8088720965385438
step 101/334, epoch 497/501 --> loss:0.8107037198543549
step 151/334, epoch 497/501 --> loss:0.829362952709198
step 201/334, epoch 497/501 --> loss:0.8307529771327973
step 251/334, epoch 497/501 --> loss:0.8194301271438599
step 301/334, epoch 497/501 --> loss:0.8307746493816376
step 51/334, epoch 498/501 --> loss:0.828333580493927
step 101/334, epoch 498/501 --> loss:0.8241775333881378
step 151/334, epoch 498/501 --> loss:0.8027491211891175
step 201/334, epoch 498/501 --> loss:0.8220776808261872
step 251/334, epoch 498/501 --> loss:0.8256275117397308
step 301/334, epoch 498/501 --> loss:0.8211100888252258
step 51/334, epoch 499/501 --> loss:0.824374303817749
step 101/334, epoch 499/501 --> loss:0.8205608642101287
step 151/334, epoch 499/501 --> loss:0.8196511828899383
step 201/334, epoch 499/501 --> loss:0.8105329263210297
step 251/334, epoch 499/501 --> loss:0.8405740773677826
step 301/334, epoch 499/501 --> loss:0.8179732024669647
step 51/334, epoch 500/501 --> loss:0.8148401618003845
step 101/334, epoch 500/501 --> loss:0.8234877181053162
step 151/334, epoch 500/501 --> loss:0.8136450552940369
step 201/334, epoch 500/501 --> loss:0.8309751200675964
step 251/334, epoch 500/501 --> loss:0.8145982372760773
step 301/334, epoch 500/501 --> loss:0.825801076889038
step 51/334, epoch 501/501 --> loss:0.8367826688289642
step 101/334, epoch 501/501 --> loss:0.8077493262290955
step 151/334, epoch 501/501 --> loss:0.8161632120609283
step 201/334, epoch 501/501 --> loss:0.8177253031730651
step 251/334, epoch 501/501 --> loss:0.818187701702118
step 301/334, epoch 501/501 --> loss:0.8197111868858338

##########train dataset##########
acc--> [99.57774748371013]
F1--> {'F1': [0.9519186028243648], 'precision': [0.9155836250785199], 'recall': [0.9912674952457108]}
##########eval dataset##########
acc--> [98.90435353142452]
F1--> {'F1': [0.8746951375184244], 'precision': [0.8648171662031673], 'recall': [0.8848116001640643]}
