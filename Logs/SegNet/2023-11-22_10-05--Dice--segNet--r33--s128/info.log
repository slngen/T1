##########Config##########
{'device': 'cuda:1', 'class_nums': 2, 'data_path': '/Code/T1/Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': '/Code/T1/Models/SegNet', 'log_path': '/Code/T1/Logs/SegNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (encoder): Encoder(
    (stage_1): Sequential(
      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
    (stage_2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
    (stage_3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
    )
    (stage_4): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
    )
    (stage_5): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
    )
  )
  (upsample_1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (upsample_2): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (upsample_3): Sequential(
    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (upsample_4): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
  )
  (upsample_5): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.896076694726944
step 101/334, epoch 1/501 --> loss:0.8602516806125641
step 151/334, epoch 1/501 --> loss:0.8486166143417359
step 201/334, epoch 1/501 --> loss:0.8459971857070923
step 251/334, epoch 1/501 --> loss:0.8650929272174835
step 301/334, epoch 1/501 --> loss:0.8692119455337525

##########train dataset##########
acc--> [76.12512429018665]
F1--> {'F1': [0.23371440990564737], 'precision': [0.13515163048487874], 'recall': [0.8633564540003923]}
##########eval dataset##########
acc--> [76.9363910611586]
F1--> {'F1': [0.2427249423916254], 'precision': [0.1414352359478617], 'recall': [0.8551957632918997]}
save model!
step 51/334, epoch 2/501 --> loss:0.8478967308998108
step 101/334, epoch 2/501 --> loss:0.8537708842754363
step 151/334, epoch 2/501 --> loss:0.8454384636878968
step 201/334, epoch 2/501 --> loss:0.8562965524196625
step 251/334, epoch 2/501 --> loss:0.841623022556305
step 301/334, epoch 2/501 --> loss:0.8560931313037873
step 51/334, epoch 3/501 --> loss:0.8481223785877228
step 101/334, epoch 3/501 --> loss:0.8387316346168519
step 151/334, epoch 3/501 --> loss:0.8476134741306305
step 201/334, epoch 3/501 --> loss:0.8415844082832337
step 251/334, epoch 3/501 --> loss:0.8435877430438995
step 301/334, epoch 3/501 --> loss:0.8519166386127472
step 51/334, epoch 4/501 --> loss:0.8450989401340485
step 101/334, epoch 4/501 --> loss:0.8340546977519989
step 151/334, epoch 4/501 --> loss:0.8363812184333801
step 201/334, epoch 4/501 --> loss:0.8454780662059784
step 251/334, epoch 4/501 --> loss:0.8324471855163574
step 301/334, epoch 4/501 --> loss:0.853336398601532
step 51/334, epoch 5/501 --> loss:0.847000560760498
step 101/334, epoch 5/501 --> loss:0.8331759548187256
step 151/334, epoch 5/501 --> loss:0.8419813632965087
step 201/334, epoch 5/501 --> loss:0.8304533088207244
step 251/334, epoch 5/501 --> loss:0.8492556393146515
step 301/334, epoch 5/501 --> loss:0.8663795125484467
step 51/334, epoch 6/501 --> loss:0.8511609315872193
step 101/334, epoch 6/501 --> loss:0.8418310523033142
step 151/334, epoch 6/501 --> loss:0.8385239470005036
step 201/334, epoch 6/501 --> loss:0.8387381207942962
step 251/334, epoch 6/501 --> loss:0.8465563464164734
step 301/334, epoch 6/501 --> loss:0.8369126307964325
step 51/334, epoch 7/501 --> loss:0.8422302973270416
step 101/334, epoch 7/501 --> loss:0.8484584808349609
step 151/334, epoch 7/501 --> loss:0.8341530668735504
step 201/334, epoch 7/501 --> loss:0.8465864074230194
step 251/334, epoch 7/501 --> loss:0.8410183846950531
step 301/334, epoch 7/501 --> loss:0.835782778263092
step 51/334, epoch 8/501 --> loss:0.8408867764472961
step 101/334, epoch 8/501 --> loss:0.8368282294273377
step 151/334, epoch 8/501 --> loss:0.8578061640262604
step 201/334, epoch 8/501 --> loss:0.8359638774394988
step 251/334, epoch 8/501 --> loss:0.849660040140152
step 301/334, epoch 8/501 --> loss:0.8387187623977661
step 51/334, epoch 9/501 --> loss:0.8520411586761475
step 101/334, epoch 9/501 --> loss:0.8487945544719696
step 151/334, epoch 9/501 --> loss:0.8309242010116578
step 201/334, epoch 9/501 --> loss:0.8299927508831024
step 251/334, epoch 9/501 --> loss:0.8484859716892242
step 301/334, epoch 9/501 --> loss:0.8487613070011139
step 51/334, epoch 10/501 --> loss:0.8342440211772919
step 101/334, epoch 10/501 --> loss:0.8435342454910278
step 151/334, epoch 10/501 --> loss:0.8436560761928559
step 201/334, epoch 10/501 --> loss:0.8492773747444153
step 251/334, epoch 10/501 --> loss:0.832815682888031
step 301/334, epoch 10/501 --> loss:0.8257078814506531
step 51/334, epoch 11/501 --> loss:0.8440014481544494
step 101/334, epoch 11/501 --> loss:0.8340945625305176
step 151/334, epoch 11/501 --> loss:0.8571776413917541
step 201/334, epoch 11/501 --> loss:0.8333112704753876
step 251/334, epoch 11/501 --> loss:0.833819477558136
step 301/334, epoch 11/501 --> loss:0.8396929240226746

##########train dataset##########
acc--> [94.72872602121363]
F1--> {'F1': [0.5888407603973353], 'precision': [0.43873989596164364], 'recall': [0.8950758357123892]}
##########eval dataset##########
acc--> [94.78107028536692]
F1--> {'F1': [0.5937947291001386], 'precision': [0.4474082966476756], 'recall': [0.8825758453215281]}
save model!
step 51/334, epoch 12/501 --> loss:0.839610641002655
step 101/334, epoch 12/501 --> loss:0.8615879499912262
step 151/334, epoch 12/501 --> loss:0.8400965869426728
step 201/334, epoch 12/501 --> loss:0.8456237769126892
step 251/334, epoch 12/501 --> loss:0.8468602061271667
step 301/334, epoch 12/501 --> loss:0.831784051656723
step 51/334, epoch 13/501 --> loss:0.8524080610275269
step 101/334, epoch 13/501 --> loss:0.8396309673786163
step 151/334, epoch 13/501 --> loss:0.8398398959636688
step 201/334, epoch 13/501 --> loss:0.8289558589458466
step 251/334, epoch 13/501 --> loss:0.8466477227210999
step 301/334, epoch 13/501 --> loss:0.8289875602722168
step 51/334, epoch 14/501 --> loss:0.8345725131034851
step 101/334, epoch 14/501 --> loss:0.8290606880187988
step 151/334, epoch 14/501 --> loss:0.8473096311092376
step 201/334, epoch 14/501 --> loss:0.82591015458107
step 251/334, epoch 14/501 --> loss:0.8508281981945038
step 301/334, epoch 14/501 --> loss:0.8351240921020507
step 51/334, epoch 15/501 --> loss:0.8357841455936432
step 101/334, epoch 15/501 --> loss:0.8407545495033264
step 151/334, epoch 15/501 --> loss:0.8419736850261689
step 201/334, epoch 15/501 --> loss:0.8340370583534241
step 251/334, epoch 15/501 --> loss:0.8525762069225311
step 301/334, epoch 15/501 --> loss:0.8488340842723846
step 51/334, epoch 16/501 --> loss:0.8435507595539093
step 101/334, epoch 16/501 --> loss:0.8391298580169678
step 151/334, epoch 16/501 --> loss:0.8338508796691895
step 201/334, epoch 16/501 --> loss:0.835636956691742
step 251/334, epoch 16/501 --> loss:0.8452123665809631
step 301/334, epoch 16/501 --> loss:0.8345148777961731
step 51/334, epoch 17/501 --> loss:0.8327527368068695
step 101/334, epoch 17/501 --> loss:0.8308803904056549
step 151/334, epoch 17/501 --> loss:0.8402504992485046
step 201/334, epoch 17/501 --> loss:0.8299878919124604
step 251/334, epoch 17/501 --> loss:0.8472578227519989
step 301/334, epoch 17/501 --> loss:0.8491259884834289
step 51/334, epoch 18/501 --> loss:0.8492835521697998
step 101/334, epoch 18/501 --> loss:0.8296546590328217
step 151/334, epoch 18/501 --> loss:0.8393871760368348
step 201/334, epoch 18/501 --> loss:0.8467600119113922
step 251/334, epoch 18/501 --> loss:0.8357414126396179
step 301/334, epoch 18/501 --> loss:0.8369742286205292
step 51/334, epoch 19/501 --> loss:0.8452447807788849
step 101/334, epoch 19/501 --> loss:0.8279777395725251
step 151/334, epoch 19/501 --> loss:0.8375472664833069
step 201/334, epoch 19/501 --> loss:0.8314374554157257
step 251/334, epoch 19/501 --> loss:0.8355862855911255
step 301/334, epoch 19/501 --> loss:0.8430761802196503
step 51/334, epoch 20/501 --> loss:0.8371202528476716
step 101/334, epoch 20/501 --> loss:0.8376252007484436
step 151/334, epoch 20/501 --> loss:0.8408652365207672
step 201/334, epoch 20/501 --> loss:0.8471201896667481
step 251/334, epoch 20/501 --> loss:0.8257419860363007
step 301/334, epoch 20/501 --> loss:0.84306112408638
step 51/334, epoch 21/501 --> loss:0.8407120478153228
step 101/334, epoch 21/501 --> loss:0.8419205582141877
step 151/334, epoch 21/501 --> loss:0.8451845359802246
step 201/334, epoch 21/501 --> loss:0.8314828610420227
step 251/334, epoch 21/501 --> loss:0.8452161431312561
step 301/334, epoch 21/501 --> loss:0.8376566219329834

##########train dataset##########
acc--> [94.5256771377977]
F1--> {'F1': [0.5869381831972419], 'precision': [0.4304366408268353], 'recall': [0.9222825799025661]}
##########eval dataset##########
acc--> [94.5402804530657]
F1--> {'F1': [0.5893773007140254], 'precision': [0.43661913899459287], 'recall': [0.906568087744835]}
step 51/334, epoch 22/501 --> loss:0.8216768968105316
step 101/334, epoch 22/501 --> loss:0.8329806888103485
step 151/334, epoch 22/501 --> loss:0.8375937497615814
step 201/334, epoch 22/501 --> loss:0.8426589393615722
step 251/334, epoch 22/501 --> loss:0.8369806253910065
step 301/334, epoch 22/501 --> loss:0.8522079932689667
step 51/334, epoch 23/501 --> loss:0.84558807015419
step 101/334, epoch 23/501 --> loss:0.8354715371131897
step 151/334, epoch 23/501 --> loss:0.8345408618450165
step 201/334, epoch 23/501 --> loss:0.8305936443805695
step 251/334, epoch 23/501 --> loss:0.8318079745769501
step 301/334, epoch 23/501 --> loss:0.8478995764255524
step 51/334, epoch 24/501 --> loss:0.8438006186485291
step 101/334, epoch 24/501 --> loss:0.8360622382164001
step 151/334, epoch 24/501 --> loss:0.8103207325935364
step 201/334, epoch 24/501 --> loss:0.8333127748966217
step 251/334, epoch 24/501 --> loss:0.8378912091255188
step 301/334, epoch 24/501 --> loss:0.8545886468887329
step 51/334, epoch 25/501 --> loss:0.8356028139591217
step 101/334, epoch 25/501 --> loss:0.8385831606388092
step 151/334, epoch 25/501 --> loss:0.8375460290908814
step 201/334, epoch 25/501 --> loss:0.8349221646785736
step 251/334, epoch 25/501 --> loss:0.8376148533821106
step 301/334, epoch 25/501 --> loss:0.8382289135456085
step 51/334, epoch 26/501 --> loss:0.8381333839893341
step 101/334, epoch 26/501 --> loss:0.8267948627471924
step 151/334, epoch 26/501 --> loss:0.840152564048767
step 201/334, epoch 26/501 --> loss:0.8304743635654449
step 251/334, epoch 26/501 --> loss:0.8389752125740051
step 301/334, epoch 26/501 --> loss:0.8427449321746826
step 51/334, epoch 27/501 --> loss:0.842008695602417
step 101/334, epoch 27/501 --> loss:0.8291682779788971
step 151/334, epoch 27/501 --> loss:0.83637721657753
step 201/334, epoch 27/501 --> loss:0.8386451184749604
step 251/334, epoch 27/501 --> loss:0.8410205531120301
step 301/334, epoch 27/501 --> loss:0.8338772797584534
step 51/334, epoch 28/501 --> loss:0.839749755859375
step 101/334, epoch 28/501 --> loss:0.8270617461204529
step 151/334, epoch 28/501 --> loss:0.8279280316829681
step 201/334, epoch 28/501 --> loss:0.8520529317855835
step 251/334, epoch 28/501 --> loss:0.8396179652214051
step 301/334, epoch 28/501 --> loss:0.8257160782814026
step 51/334, epoch 29/501 --> loss:0.8303846585750579
step 101/334, epoch 29/501 --> loss:0.8471994352340698
step 151/334, epoch 29/501 --> loss:0.8327389943599701
step 201/334, epoch 29/501 --> loss:0.8304535257816315
step 251/334, epoch 29/501 --> loss:0.8471719110012055
step 301/334, epoch 29/501 --> loss:0.8432778418064117
step 51/334, epoch 30/501 --> loss:0.831510751247406
step 101/334, epoch 30/501 --> loss:0.8218899428844452
step 151/334, epoch 30/501 --> loss:0.8393050122261048
step 201/334, epoch 30/501 --> loss:0.834258337020874
step 251/334, epoch 30/501 --> loss:0.8343418347835541
step 301/334, epoch 30/501 --> loss:0.8479103291034699
step 51/334, epoch 31/501 --> loss:0.8417402243614197
step 101/334, epoch 31/501 --> loss:0.8334060418605804
step 151/334, epoch 31/501 --> loss:0.8453941237926483
step 201/334, epoch 31/501 --> loss:0.8274941873550415
step 251/334, epoch 31/501 --> loss:0.8345399475097657
step 301/334, epoch 31/501 --> loss:0.843144199848175

##########train dataset##########
acc--> [96.61522555270524]
F1--> {'F1': [0.6902766441868843], 'precision': [0.5620140794363306], 'recall': [0.8944109296922632]}
##########eval dataset##########
acc--> [96.6690056801612]
F1--> {'F1': [0.6938396884758613], 'precision': [0.5755679380581726], 'recall': [0.8733044972371999]}
save model!
step 51/334, epoch 32/501 --> loss:0.8414607286453247
step 101/334, epoch 32/501 --> loss:0.8477996337413788
step 151/334, epoch 32/501 --> loss:0.8289410519599915
step 201/334, epoch 32/501 --> loss:0.8328147685527801
step 251/334, epoch 32/501 --> loss:0.8366913640499115
step 301/334, epoch 32/501 --> loss:0.831514151096344
step 51/334, epoch 33/501 --> loss:0.8422644293308258
step 101/334, epoch 33/501 --> loss:0.8287761437892914
step 151/334, epoch 33/501 --> loss:0.8347351062297821
step 201/334, epoch 33/501 --> loss:0.8362792217731476
step 251/334, epoch 33/501 --> loss:0.8356369853019714
step 301/334, epoch 33/501 --> loss:0.8440800631046295
step 51/334, epoch 34/501 --> loss:0.832571165561676
step 101/334, epoch 34/501 --> loss:0.8413424861431121
step 151/334, epoch 34/501 --> loss:0.8346807622909546
step 201/334, epoch 34/501 --> loss:0.824208744764328
step 251/334, epoch 34/501 --> loss:0.8367703890800476
step 301/334, epoch 34/501 --> loss:0.8420858252048492
step 51/334, epoch 35/501 --> loss:0.8432174491882324
step 101/334, epoch 35/501 --> loss:0.8173990023136138
step 151/334, epoch 35/501 --> loss:0.8285961461067199
step 201/334, epoch 35/501 --> loss:0.8389501512050629
step 251/334, epoch 35/501 --> loss:0.8321500408649445
step 301/334, epoch 35/501 --> loss:0.8612725389003754
step 51/334, epoch 36/501 --> loss:0.8349577641487121
step 101/334, epoch 36/501 --> loss:0.8396532118320466
step 151/334, epoch 36/501 --> loss:0.8324148142337799
step 201/334, epoch 36/501 --> loss:0.8377977240085602
step 251/334, epoch 36/501 --> loss:0.8359176802635193
step 301/334, epoch 36/501 --> loss:0.8341387283802032
step 51/334, epoch 37/501 --> loss:0.836962205171585
step 101/334, epoch 37/501 --> loss:0.837399137020111
step 151/334, epoch 37/501 --> loss:0.8389793002605438
step 201/334, epoch 37/501 --> loss:0.8347717177867889
step 251/334, epoch 37/501 --> loss:0.8305017483234406
step 301/334, epoch 37/501 --> loss:0.8388940572738648
step 51/334, epoch 38/501 --> loss:0.8254418396949768
step 101/334, epoch 38/501 --> loss:0.8503466522693635
step 151/334, epoch 38/501 --> loss:0.8328030431270599
step 201/334, epoch 38/501 --> loss:0.8441075229644776
step 251/334, epoch 38/501 --> loss:0.8284628069400788
step 301/334, epoch 38/501 --> loss:0.8312387132644653
step 51/334, epoch 39/501 --> loss:0.8369303345680237
step 101/334, epoch 39/501 --> loss:0.8426011526584625
step 151/334, epoch 39/501 --> loss:0.830915448665619
step 201/334, epoch 39/501 --> loss:0.8363587248325348
step 251/334, epoch 39/501 --> loss:0.8345067453384399
step 301/334, epoch 39/501 --> loss:0.8444621694087983
step 51/334, epoch 40/501 --> loss:0.8368764269351959
step 101/334, epoch 40/501 --> loss:0.84536829829216
step 151/334, epoch 40/501 --> loss:0.8418156480789185
step 201/334, epoch 40/501 --> loss:0.8267674005031586
step 251/334, epoch 40/501 --> loss:0.8283157432079316
step 301/334, epoch 40/501 --> loss:0.8303406929969788
step 51/334, epoch 41/501 --> loss:0.813084911108017
step 101/334, epoch 41/501 --> loss:0.8422663545608521
step 151/334, epoch 41/501 --> loss:0.8348912191390991
step 201/334, epoch 41/501 --> loss:0.835539608001709
step 251/334, epoch 41/501 --> loss:0.8455078840255738
step 301/334, epoch 41/501 --> loss:0.8324149632453919

##########train dataset##########
acc--> [97.40136786261424]
F1--> {'F1': [0.7483134477550482], 'precision': [0.6324957646651643], 'recall': [0.9160686051137825]}
##########eval dataset##########
acc--> [97.29979841310079]
F1--> {'F1': [0.7403426132680058], 'precision': [0.6334450618790963], 'recall': [0.8906580442528875]}
save model!
step 51/334, epoch 42/501 --> loss:0.8449691236019135
step 101/334, epoch 42/501 --> loss:0.8363441729545593
step 151/334, epoch 42/501 --> loss:0.8231409811973571
step 201/334, epoch 42/501 --> loss:0.847812350988388
step 251/334, epoch 42/501 --> loss:0.8225955414772034
step 301/334, epoch 42/501 --> loss:0.8429278671741486
step 51/334, epoch 43/501 --> loss:0.8376124262809753
step 101/334, epoch 43/501 --> loss:0.844657974243164
step 151/334, epoch 43/501 --> loss:0.8398816931247711
step 201/334, epoch 43/501 --> loss:0.824865289926529
step 251/334, epoch 43/501 --> loss:0.8390770590305329
step 301/334, epoch 43/501 --> loss:0.8287910640239715
step 51/334, epoch 44/501 --> loss:0.8342808103561401
step 101/334, epoch 44/501 --> loss:0.8318338179588318
step 151/334, epoch 44/501 --> loss:0.8260351121425629
step 201/334, epoch 44/501 --> loss:0.8451466107368469
step 251/334, epoch 44/501 --> loss:0.8339442491531373
step 301/334, epoch 44/501 --> loss:0.8300696086883544
step 51/334, epoch 45/501 --> loss:0.8276792311668396
step 101/334, epoch 45/501 --> loss:0.8503927767276764
step 151/334, epoch 45/501 --> loss:0.8330237472057342
step 201/334, epoch 45/501 --> loss:0.8365937757492066
step 251/334, epoch 45/501 --> loss:0.8430784046649933
step 301/334, epoch 45/501 --> loss:0.8198465514183044
step 51/334, epoch 46/501 --> loss:0.8306119823455811
step 101/334, epoch 46/501 --> loss:0.8415241694450378
step 151/334, epoch 46/501 --> loss:0.8363557720184326
step 201/334, epoch 46/501 --> loss:0.8121755361557007
step 251/334, epoch 46/501 --> loss:0.8371732759475708
step 301/334, epoch 46/501 --> loss:0.8389957594871521
step 51/334, epoch 47/501 --> loss:0.8280082488059998
step 101/334, epoch 47/501 --> loss:0.841713547706604
step 151/334, epoch 47/501 --> loss:0.8255197858810425
step 201/334, epoch 47/501 --> loss:0.8322619736194611
step 251/334, epoch 47/501 --> loss:0.8341029453277587
step 301/334, epoch 47/501 --> loss:0.8464166188240051
step 51/334, epoch 48/501 --> loss:0.8243927812576294
step 101/334, epoch 48/501 --> loss:0.8382572114467621
step 151/334, epoch 48/501 --> loss:0.8355188739299774
step 201/334, epoch 48/501 --> loss:0.8506840527057647
step 251/334, epoch 48/501 --> loss:0.8303981387615204
step 301/334, epoch 48/501 --> loss:0.8304444003105164
step 51/334, epoch 49/501 --> loss:0.8339001333713532
step 101/334, epoch 49/501 --> loss:0.8377049589157104
step 151/334, epoch 49/501 --> loss:0.8384338331222534
step 201/334, epoch 49/501 --> loss:0.8256434655189514
step 251/334, epoch 49/501 --> loss:0.820109840631485
step 301/334, epoch 49/501 --> loss:0.8432017028331756
step 51/334, epoch 50/501 --> loss:0.8312862288951873
step 101/334, epoch 50/501 --> loss:0.8502678120136261
step 151/334, epoch 50/501 --> loss:0.8284278619289398
step 201/334, epoch 50/501 --> loss:0.8351910579204559
step 251/334, epoch 50/501 --> loss:0.8263605058193206
step 301/334, epoch 50/501 --> loss:0.8314672410488129
step 51/334, epoch 51/501 --> loss:0.8310195350646973
step 101/334, epoch 51/501 --> loss:0.8231167769432068
step 151/334, epoch 51/501 --> loss:0.834038325548172
step 201/334, epoch 51/501 --> loss:0.8432829570770264
step 251/334, epoch 51/501 --> loss:0.8178342807292939
step 301/334, epoch 51/501 --> loss:0.8404497504234314

##########train dataset##########
acc--> [97.76210205627093]
F1--> {'F1': [0.7735082188009085], 'precision': [0.6747321501940484], 'recall': [0.9061777719374344]}
##########eval dataset##########
acc--> [97.62010282313912]
F1--> {'F1': [0.7611615096348788], 'precision': [0.6721068193085982], 'recall': [0.8774334701922354]}
save model!
step 51/334, epoch 52/501 --> loss:0.816846969127655
step 101/334, epoch 52/501 --> loss:0.834262261390686
step 151/334, epoch 52/501 --> loss:0.8350663888454437
step 201/334, epoch 52/501 --> loss:0.8436736047267914
step 251/334, epoch 52/501 --> loss:0.8310426712036133
step 301/334, epoch 52/501 --> loss:0.8247970461845398
step 51/334, epoch 53/501 --> loss:0.814037401676178
step 101/334, epoch 53/501 --> loss:0.833621336221695
step 151/334, epoch 53/501 --> loss:0.8374245500564576
step 201/334, epoch 53/501 --> loss:0.835535020828247
step 251/334, epoch 53/501 --> loss:0.8360072672367096
step 301/334, epoch 53/501 --> loss:0.8435515820980072
step 51/334, epoch 54/501 --> loss:0.8438720214366913
step 101/334, epoch 54/501 --> loss:0.8222814619541168
step 151/334, epoch 54/501 --> loss:0.8307806086540223
step 201/334, epoch 54/501 --> loss:0.8334721529483795
step 251/334, epoch 54/501 --> loss:0.8395175397396087
step 301/334, epoch 54/501 --> loss:0.8246060025691986
step 51/334, epoch 55/501 --> loss:0.8374376833438874
step 101/334, epoch 55/501 --> loss:0.831000235080719
step 151/334, epoch 55/501 --> loss:0.8414888226985932
step 201/334, epoch 55/501 --> loss:0.8390986740589141
step 251/334, epoch 55/501 --> loss:0.8319798135757446
step 301/334, epoch 55/501 --> loss:0.8244165730476379
step 51/334, epoch 56/501 --> loss:0.8248638987541199
step 101/334, epoch 56/501 --> loss:0.8465535056591034
step 151/334, epoch 56/501 --> loss:0.8359343981742859
step 201/334, epoch 56/501 --> loss:0.8270095098018646
step 251/334, epoch 56/501 --> loss:0.8207921183109284
step 301/334, epoch 56/501 --> loss:0.8564045929908752
step 51/334, epoch 57/501 --> loss:0.8399622797966003
step 101/334, epoch 57/501 --> loss:0.8260351026058197
step 151/334, epoch 57/501 --> loss:0.8269507157802581
step 201/334, epoch 57/501 --> loss:0.8396565115451813
step 251/334, epoch 57/501 --> loss:0.839872133731842
step 301/334, epoch 57/501 --> loss:0.8297924530506134
step 51/334, epoch 58/501 --> loss:0.8304080545902253
step 101/334, epoch 58/501 --> loss:0.8449629521369935
step 151/334, epoch 58/501 --> loss:0.8383943819999695
step 201/334, epoch 58/501 --> loss:0.8361351776123047
step 251/334, epoch 58/501 --> loss:0.8275800883769989
step 301/334, epoch 58/501 --> loss:0.832538321018219
step 51/334, epoch 59/501 --> loss:0.828573293685913
step 101/334, epoch 59/501 --> loss:0.8205893623828888
step 151/334, epoch 59/501 --> loss:0.8447737443447113
step 201/334, epoch 59/501 --> loss:0.808808696269989
step 251/334, epoch 59/501 --> loss:0.8422972416877746
step 301/334, epoch 59/501 --> loss:0.8446450364589692
step 51/334, epoch 60/501 --> loss:0.8400772833824157
step 101/334, epoch 60/501 --> loss:0.8318901145458222
step 151/334, epoch 60/501 --> loss:0.8199991583824158
step 201/334, epoch 60/501 --> loss:0.8288657009601593
step 251/334, epoch 60/501 --> loss:0.8514828872680664
step 301/334, epoch 60/501 --> loss:0.8193017411231994
step 51/334, epoch 61/501 --> loss:0.8238514888286591
step 101/334, epoch 61/501 --> loss:0.8313104999065399
step 151/334, epoch 61/501 --> loss:0.8369589471817016
step 201/334, epoch 61/501 --> loss:0.8362722730636597
step 251/334, epoch 61/501 --> loss:0.8226748740673065
step 301/334, epoch 61/501 --> loss:0.8341135454177856

##########train dataset##########
acc--> [96.57448738936043]
F1--> {'F1': [0.7014790936179754], 'precision': [0.5545374735272314], 'recall': [0.9543852220730115]}
##########eval dataset##########
acc--> [96.4349915244905]
F1--> {'F1': [0.6927687597528439], 'precision': [0.5519881911556578], 'recall': [0.9299611225430094]}
step 51/334, epoch 62/501 --> loss:0.8260176455974579
step 101/334, epoch 62/501 --> loss:0.8265541565418243
step 151/334, epoch 62/501 --> loss:0.8291455471515655
step 201/334, epoch 62/501 --> loss:0.834148782491684
step 251/334, epoch 62/501 --> loss:0.8303519141674042
step 301/334, epoch 62/501 --> loss:0.8343268477916718
step 51/334, epoch 63/501 --> loss:0.8395588803291321
step 101/334, epoch 63/501 --> loss:0.8395429575443267
step 151/334, epoch 63/501 --> loss:0.8269000327587128
step 201/334, epoch 63/501 --> loss:0.8409964740276337
step 251/334, epoch 63/501 --> loss:0.8238219666481018
step 301/334, epoch 63/501 --> loss:0.8239176905155182
step 51/334, epoch 64/501 --> loss:0.8263173639774323
step 101/334, epoch 64/501 --> loss:0.8344249868392944
step 151/334, epoch 64/501 --> loss:0.8295764636993408
step 201/334, epoch 64/501 --> loss:0.8291427719593049
step 251/334, epoch 64/501 --> loss:0.8431324481964111
step 301/334, epoch 64/501 --> loss:0.8293343162536622
step 51/334, epoch 65/501 --> loss:0.8429391980171204
step 101/334, epoch 65/501 --> loss:0.8301527416706085
step 151/334, epoch 65/501 --> loss:0.8256462442874909
step 201/334, epoch 65/501 --> loss:0.8250502026081086
step 251/334, epoch 65/501 --> loss:0.8349522435665131
step 301/334, epoch 65/501 --> loss:0.8359300899505615
step 51/334, epoch 66/501 --> loss:0.8447025394439698
step 101/334, epoch 66/501 --> loss:0.8170258820056915
step 151/334, epoch 66/501 --> loss:0.8389716482162476
step 201/334, epoch 66/501 --> loss:0.8303902065753936
step 251/334, epoch 66/501 --> loss:0.8298220777511597
step 301/334, epoch 66/501 --> loss:0.8341405522823334
step 51/334, epoch 67/501 --> loss:0.8398639762401581
step 101/334, epoch 67/501 --> loss:0.8351794195175171
step 151/334, epoch 67/501 --> loss:0.8269976830482483
step 201/334, epoch 67/501 --> loss:0.8293988060951233
step 251/334, epoch 67/501 --> loss:0.8209140503406525
step 301/334, epoch 67/501 --> loss:0.8292872881889344
step 51/334, epoch 68/501 --> loss:0.8374169075489044
step 101/334, epoch 68/501 --> loss:0.8327030467987061
step 151/334, epoch 68/501 --> loss:0.8228599488735199
step 201/334, epoch 68/501 --> loss:0.8162426686286927
step 251/334, epoch 68/501 --> loss:0.8240916991233825
step 301/334, epoch 68/501 --> loss:0.8454537785053253
step 51/334, epoch 69/501 --> loss:0.8317645108699798
step 101/334, epoch 69/501 --> loss:0.8295092129707337
step 151/334, epoch 69/501 --> loss:0.8339203596115112
step 201/334, epoch 69/501 --> loss:0.8288126027584076
step 251/334, epoch 69/501 --> loss:0.8264401566982269
step 301/334, epoch 69/501 --> loss:0.829692405462265
step 51/334, epoch 70/501 --> loss:0.8295040166378022
step 101/334, epoch 70/501 --> loss:0.829210535287857
step 151/334, epoch 70/501 --> loss:0.8282596778869629
step 201/334, epoch 70/501 --> loss:0.8198809480667114
step 251/334, epoch 70/501 --> loss:0.8350255036354065
step 301/334, epoch 70/501 --> loss:0.8422070264816284
step 51/334, epoch 71/501 --> loss:0.8253307521343232
step 101/334, epoch 71/501 --> loss:0.8192236137390136
step 151/334, epoch 71/501 --> loss:0.8261407256126404
step 201/334, epoch 71/501 --> loss:0.8192432188987732
step 251/334, epoch 71/501 --> loss:0.8331509673595429
step 301/334, epoch 71/501 --> loss:0.8480380845069885

##########train dataset##########
acc--> [97.82221350367423]
F1--> {'F1': [0.77838639225121], 'precision': [0.6817636803371805], 'recall': [0.9069324897888852]}
##########eval dataset##########
acc--> [97.64160129549803]
F1--> {'F1': [0.7646239447909898], 'precision': [0.6723246064433851], 'recall': [0.8863116268813092]}
save model!
step 51/334, epoch 72/501 --> loss:0.8327957928180695
step 101/334, epoch 72/501 --> loss:0.8184836304187775
step 151/334, epoch 72/501 --> loss:0.8365425205230713
step 201/334, epoch 72/501 --> loss:0.8311649191379548
step 251/334, epoch 72/501 --> loss:0.8262515509128571
step 301/334, epoch 72/501 --> loss:0.8348449003696442
step 51/334, epoch 73/501 --> loss:0.8249333417415619
step 101/334, epoch 73/501 --> loss:0.8390912508964539
step 151/334, epoch 73/501 --> loss:0.8316262245178223
step 201/334, epoch 73/501 --> loss:0.8260087811946869
step 251/334, epoch 73/501 --> loss:0.8254227697849273
step 301/334, epoch 73/501 --> loss:0.830157732963562
step 51/334, epoch 74/501 --> loss:0.8208457815647126
step 101/334, epoch 74/501 --> loss:0.8382308828830719
step 151/334, epoch 74/501 --> loss:0.8324953317642212
step 201/334, epoch 74/501 --> loss:0.833154423236847
step 251/334, epoch 74/501 --> loss:0.8206551694869995
step 301/334, epoch 74/501 --> loss:0.8349796068668366
step 51/334, epoch 75/501 --> loss:0.8393314254283905
step 101/334, epoch 75/501 --> loss:0.8324186158180237
step 151/334, epoch 75/501 --> loss:0.8250031995773316
step 201/334, epoch 75/501 --> loss:0.8247122991085053
step 251/334, epoch 75/501 --> loss:0.8314801585674286
step 301/334, epoch 75/501 --> loss:0.8295296156406402
step 51/334, epoch 76/501 --> loss:0.8340757834911346
step 101/334, epoch 76/501 --> loss:0.8347392058372498
step 151/334, epoch 76/501 --> loss:0.8420213806629181
step 201/334, epoch 76/501 --> loss:0.8234894037246704
step 251/334, epoch 76/501 --> loss:0.8297136497497558
step 301/334, epoch 76/501 --> loss:0.8157304465770722
step 51/334, epoch 77/501 --> loss:0.8275187742710114
step 101/334, epoch 77/501 --> loss:0.8439977109432221
step 151/334, epoch 77/501 --> loss:0.8180882060527801
step 201/334, epoch 77/501 --> loss:0.8420514869689941
step 251/334, epoch 77/501 --> loss:0.8249960589408875
step 301/334, epoch 77/501 --> loss:0.8316197574138642
step 51/334, epoch 78/501 --> loss:0.8303300845623016
step 101/334, epoch 78/501 --> loss:0.81375652551651
step 151/334, epoch 78/501 --> loss:0.8261086547374725
step 201/334, epoch 78/501 --> loss:0.8306159770488739
step 251/334, epoch 78/501 --> loss:0.8349161052703857
step 301/334, epoch 78/501 --> loss:0.8363436806201935
step 51/334, epoch 79/501 --> loss:0.8367696154117584
step 101/334, epoch 79/501 --> loss:0.8378137063980102
step 151/334, epoch 79/501 --> loss:0.8244309830665588
step 201/334, epoch 79/501 --> loss:0.8273161780834198
step 251/334, epoch 79/501 --> loss:0.8299574685096741
step 301/334, epoch 79/501 --> loss:0.8279518604278564
step 51/334, epoch 80/501 --> loss:0.8276056671142578
step 101/334, epoch 80/501 --> loss:0.8263713836669921
step 151/334, epoch 80/501 --> loss:0.8328581821918487
step 201/334, epoch 80/501 --> loss:0.8320328438282013
step 251/334, epoch 80/501 --> loss:0.8240367829799652
step 301/334, epoch 80/501 --> loss:0.8322777950763702
step 51/334, epoch 81/501 --> loss:0.8191737699508667
step 101/334, epoch 81/501 --> loss:0.824092413187027
step 151/334, epoch 81/501 --> loss:0.8511634731292724
step 201/334, epoch 81/501 --> loss:0.832134917974472
step 251/334, epoch 81/501 --> loss:0.8268965518474579
step 301/334, epoch 81/501 --> loss:0.8226379656791687

##########train dataset##########
acc--> [97.09352794370885]
F1--> {'F1': [0.7355504094300475], 'precision': [0.5967497472720905], 'recall': [0.9585058485878626]}
##########eval dataset##########
acc--> [96.88064628593752]
F1--> {'F1': [0.7192705969920479], 'precision': [0.5885744565070018], 'recall': [0.9245938901193372]}
step 51/334, epoch 82/501 --> loss:0.833383092880249
step 101/334, epoch 82/501 --> loss:0.814563375711441
step 151/334, epoch 82/501 --> loss:0.8356532156467438
step 201/334, epoch 82/501 --> loss:0.8210547614097595
step 251/334, epoch 82/501 --> loss:0.8326123487949372
step 301/334, epoch 82/501 --> loss:0.8469204974174499
step 51/334, epoch 83/501 --> loss:0.8409403860569
step 101/334, epoch 83/501 --> loss:0.8352660298347473
step 151/334, epoch 83/501 --> loss:0.8346468198299408
step 201/334, epoch 83/501 --> loss:0.8233265960216523
step 251/334, epoch 83/501 --> loss:0.8297026681900025
step 301/334, epoch 83/501 --> loss:0.8231312417984009
step 51/334, epoch 84/501 --> loss:0.8271600258350372
step 101/334, epoch 84/501 --> loss:0.8317743384838104
step 151/334, epoch 84/501 --> loss:0.8386344122886658
step 201/334, epoch 84/501 --> loss:0.8418077194690704
step 251/334, epoch 84/501 --> loss:0.8125344395637513
step 301/334, epoch 84/501 --> loss:0.8265909242630005
step 51/334, epoch 85/501 --> loss:0.8349635863304138
step 101/334, epoch 85/501 --> loss:0.833913733959198
step 151/334, epoch 85/501 --> loss:0.8275175833702088
step 201/334, epoch 85/501 --> loss:0.8221428751945495
step 251/334, epoch 85/501 --> loss:0.8340135192871094
step 301/334, epoch 85/501 --> loss:0.8182483172416687
step 51/334, epoch 86/501 --> loss:0.8191827952861785
step 101/334, epoch 86/501 --> loss:0.8311883354187012
step 151/334, epoch 86/501 --> loss:0.8316076862812042
step 201/334, epoch 86/501 --> loss:0.8466634738445282
step 251/334, epoch 86/501 --> loss:0.8356466567516327
step 301/334, epoch 86/501 --> loss:0.8130004775524139
step 51/334, epoch 87/501 --> loss:0.8336637449264527
step 101/334, epoch 87/501 --> loss:0.830610226392746
step 151/334, epoch 87/501 --> loss:0.8209538793563843
step 201/334, epoch 87/501 --> loss:0.8307795226573944
step 251/334, epoch 87/501 --> loss:0.8193155694007873
step 301/334, epoch 87/501 --> loss:0.8312640380859375
step 51/334, epoch 88/501 --> loss:0.8420548903942108
step 101/334, epoch 88/501 --> loss:0.8286996531486511
step 151/334, epoch 88/501 --> loss:0.8334002017974853
step 201/334, epoch 88/501 --> loss:0.822765017747879
step 251/334, epoch 88/501 --> loss:0.8266640365123749
step 301/334, epoch 88/501 --> loss:0.8119795942306518
step 51/334, epoch 89/501 --> loss:0.8253012931346894
step 101/334, epoch 89/501 --> loss:0.8272857940196991
step 151/334, epoch 89/501 --> loss:0.8228235065937042
step 201/334, epoch 89/501 --> loss:0.8416236698627472
step 251/334, epoch 89/501 --> loss:0.8313515758514405
step 301/334, epoch 89/501 --> loss:0.8321942389011383
step 51/334, epoch 90/501 --> loss:0.8235609245300293
step 101/334, epoch 90/501 --> loss:0.8218819057941437
step 151/334, epoch 90/501 --> loss:0.8243637657165528
step 201/334, epoch 90/501 --> loss:0.8358222663402557
step 251/334, epoch 90/501 --> loss:0.829784209728241
step 301/334, epoch 90/501 --> loss:0.8354622054100037
step 51/334, epoch 91/501 --> loss:0.822491352558136
step 101/334, epoch 91/501 --> loss:0.8369269263744354
step 151/334, epoch 91/501 --> loss:0.8302868378162384
step 201/334, epoch 91/501 --> loss:0.8365662956237793
step 251/334, epoch 91/501 --> loss:0.8288851642608642
step 301/334, epoch 91/501 --> loss:0.8261160349845886

##########train dataset##########
acc--> [96.49163136376475]
F1--> {'F1': [0.6993928151940706], 'precision': [0.5475456261954762], 'recall': [0.9677987954490311]}
##########eval dataset##########
acc--> [96.3568813427371]
F1--> {'F1': [0.6905628870630978], 'precision': [0.5455628461710182], 'recall': [0.9405575225406457]}
step 51/334, epoch 92/501 --> loss:0.8322055912017823
step 101/334, epoch 92/501 --> loss:0.8305331897735596
step 151/334, epoch 92/501 --> loss:0.8303616058826446
step 201/334, epoch 92/501 --> loss:0.8383844566345214
step 251/334, epoch 92/501 --> loss:0.8167048597335815
step 301/334, epoch 92/501 --> loss:0.8302289927005768
step 51/334, epoch 93/501 --> loss:0.8318673658370972
step 101/334, epoch 93/501 --> loss:0.8358640730381012
step 151/334, epoch 93/501 --> loss:0.8341360247135162
step 201/334, epoch 93/501 --> loss:0.8191729390621185
step 251/334, epoch 93/501 --> loss:0.8280289685726165
step 301/334, epoch 93/501 --> loss:0.8258241260051727
step 51/334, epoch 94/501 --> loss:0.8131760454177857
step 101/334, epoch 94/501 --> loss:0.8370683526992798
step 151/334, epoch 94/501 --> loss:0.8215365099906922
step 201/334, epoch 94/501 --> loss:0.8319309115409851
step 251/334, epoch 94/501 --> loss:0.8216253137588501
step 301/334, epoch 94/501 --> loss:0.8281527876853942
step 51/334, epoch 95/501 --> loss:0.8373288261890411
step 101/334, epoch 95/501 --> loss:0.8244713568687438
step 151/334, epoch 95/501 --> loss:0.8396857416629792
step 201/334, epoch 95/501 --> loss:0.8306860554218293
step 251/334, epoch 95/501 --> loss:0.8272768270969391
step 301/334, epoch 95/501 --> loss:0.8094928526878357
step 51/334, epoch 96/501 --> loss:0.8282561600208282
step 101/334, epoch 96/501 --> loss:0.823811674118042
step 151/334, epoch 96/501 --> loss:0.8256263196468353
step 201/334, epoch 96/501 --> loss:0.8352716827392578
step 251/334, epoch 96/501 --> loss:0.826483359336853
step 301/334, epoch 96/501 --> loss:0.826949999332428
step 51/334, epoch 97/501 --> loss:0.8378401160240173
step 101/334, epoch 97/501 --> loss:0.8325042963027954
step 151/334, epoch 97/501 --> loss:0.8278736472129822
step 201/334, epoch 97/501 --> loss:0.8182242321968078
step 251/334, epoch 97/501 --> loss:0.8306388831138611
step 301/334, epoch 97/501 --> loss:0.8247057616710662
step 51/334, epoch 98/501 --> loss:0.8350417757034302
step 101/334, epoch 98/501 --> loss:0.8352938878536225
step 151/334, epoch 98/501 --> loss:0.8258085381984711
step 201/334, epoch 98/501 --> loss:0.8273581290245056
step 251/334, epoch 98/501 --> loss:0.828422964811325
step 301/334, epoch 98/501 --> loss:0.8273251020908355
step 51/334, epoch 99/501 --> loss:0.8340293872356415
step 101/334, epoch 99/501 --> loss:0.8450966680049896
step 151/334, epoch 99/501 --> loss:0.8242492270469666
step 201/334, epoch 99/501 --> loss:0.8090206909179688
step 251/334, epoch 99/501 --> loss:0.8186599659919739
step 301/334, epoch 99/501 --> loss:0.8347134459018707
step 51/334, epoch 100/501 --> loss:0.8246328461170197
step 101/334, epoch 100/501 --> loss:0.8159737002849579
step 151/334, epoch 100/501 --> loss:0.8304989552497863
step 201/334, epoch 100/501 --> loss:0.8309496092796326
step 251/334, epoch 100/501 --> loss:0.8295555102825165
step 301/334, epoch 100/501 --> loss:0.825523647069931
step 51/334, epoch 101/501 --> loss:0.8342434287071228
step 101/334, epoch 101/501 --> loss:0.8205272257328033
step 151/334, epoch 101/501 --> loss:0.8389317989349365
step 201/334, epoch 101/501 --> loss:0.8210155427455902
step 251/334, epoch 101/501 --> loss:0.8221576368808746
step 301/334, epoch 101/501 --> loss:0.8187272214889526

##########train dataset##########
acc--> [98.33740903770413]
F1--> {'F1': [0.8301165095942316], 'precision': [0.729325985201806], 'recall': [0.9632453220341164]}
##########eval dataset##########
acc--> [98.01980433292351]
F1--> {'F1': [0.799296611820516], 'precision': [0.7111990359162866], 'recall': [0.912318473291691]}
save model!
step 51/334, epoch 102/501 --> loss:0.8237376987934113
step 101/334, epoch 102/501 --> loss:0.8217720377445221
step 151/334, epoch 102/501 --> loss:0.8316675221920014
step 201/334, epoch 102/501 --> loss:0.8342690348625184
step 251/334, epoch 102/501 --> loss:0.8279525518417359
step 301/334, epoch 102/501 --> loss:0.8335757780075074
step 51/334, epoch 103/501 --> loss:0.8274002242088317
step 101/334, epoch 103/501 --> loss:0.8437734150886536
step 151/334, epoch 103/501 --> loss:0.8321431827545166
step 201/334, epoch 103/501 --> loss:0.820749145746231
step 251/334, epoch 103/501 --> loss:0.8265454137325287
step 301/334, epoch 103/501 --> loss:0.8098640871047974
step 51/334, epoch 104/501 --> loss:0.8424108958244324
step 101/334, epoch 104/501 --> loss:0.8262846624851227
step 151/334, epoch 104/501 --> loss:0.8177137470245361
step 201/334, epoch 104/501 --> loss:0.8282734680175782
step 251/334, epoch 104/501 --> loss:0.8165025293827057
step 301/334, epoch 104/501 --> loss:0.8298489165306091
step 51/334, epoch 105/501 --> loss:0.8356494176387786
step 101/334, epoch 105/501 --> loss:0.809128131866455
step 151/334, epoch 105/501 --> loss:0.8258615279197693
step 201/334, epoch 105/501 --> loss:0.822934650182724
step 251/334, epoch 105/501 --> loss:0.8344897282123566
step 301/334, epoch 105/501 --> loss:0.8276966261863709
step 51/334, epoch 106/501 --> loss:0.8335535550117492
step 101/334, epoch 106/501 --> loss:0.8212327361106873
step 151/334, epoch 106/501 --> loss:0.8327497100830078
step 201/334, epoch 106/501 --> loss:0.8155489099025727
step 251/334, epoch 106/501 --> loss:0.8361575710773468
step 301/334, epoch 106/501 --> loss:0.8255149173736572
step 51/334, epoch 107/501 --> loss:0.8259913837909698
step 101/334, epoch 107/501 --> loss:0.8295457315444946
step 151/334, epoch 107/501 --> loss:0.8195177721977234
step 201/334, epoch 107/501 --> loss:0.8283659517765045
step 251/334, epoch 107/501 --> loss:0.8305740928649903
step 301/334, epoch 107/501 --> loss:0.8263127470016479
step 51/334, epoch 108/501 --> loss:0.8223278069496155
step 101/334, epoch 108/501 --> loss:0.8231615269184113
step 151/334, epoch 108/501 --> loss:0.8305104506015778
step 201/334, epoch 108/501 --> loss:0.8139804184436799
step 251/334, epoch 108/501 --> loss:0.8357644760608673
step 301/334, epoch 108/501 --> loss:0.8294460499286651
step 51/334, epoch 109/501 --> loss:0.8195780479907989
step 101/334, epoch 109/501 --> loss:0.8266420114040375
step 151/334, epoch 109/501 --> loss:0.8337202608585358
step 201/334, epoch 109/501 --> loss:0.8110208666324615
step 251/334, epoch 109/501 --> loss:0.8362418723106384
step 301/334, epoch 109/501 --> loss:0.837824375629425
step 51/334, epoch 110/501 --> loss:0.8268206584453582
step 101/334, epoch 110/501 --> loss:0.8219313752651215
step 151/334, epoch 110/501 --> loss:0.8303637039661408
step 201/334, epoch 110/501 --> loss:0.8282404613494873
step 251/334, epoch 110/501 --> loss:0.8232692384719849
step 301/334, epoch 110/501 --> loss:0.8225350594520568
step 51/334, epoch 111/501 --> loss:0.8216030514240265
step 101/334, epoch 111/501 --> loss:0.8191630470752717
step 151/334, epoch 111/501 --> loss:0.8341432440280915
step 201/334, epoch 111/501 --> loss:0.8202862608432769
step 251/334, epoch 111/501 --> loss:0.8297559344768524
step 301/334, epoch 111/501 --> loss:0.8261828720569611

##########train dataset##########
acc--> [98.22629364322343]
F1--> {'F1': [0.8208266522794547], 'precision': [0.7149999423582638], 'recall': [0.9634356634228608]}
##########eval dataset##########
acc--> [97.93038822951041]
F1--> {'F1': [0.7920392852790518], 'precision': [0.7000450640723149], 'recall': [0.9118828122838307]}
step 51/334, epoch 112/501 --> loss:0.8275768208503723
step 101/334, epoch 112/501 --> loss:0.8389793229103089
step 151/334, epoch 112/501 --> loss:0.8253314745426178
step 201/334, epoch 112/501 --> loss:0.8296322667598724
step 251/334, epoch 112/501 --> loss:0.8270711398124695
step 301/334, epoch 112/501 --> loss:0.8104948103427887
step 51/334, epoch 113/501 --> loss:0.8350839853286743
step 101/334, epoch 113/501 --> loss:0.8227234816551209
step 151/334, epoch 113/501 --> loss:0.8222891652584076
step 201/334, epoch 113/501 --> loss:0.8412578690052033
step 251/334, epoch 113/501 --> loss:0.8236107015609742
step 301/334, epoch 113/501 --> loss:0.8229556429386139
step 51/334, epoch 114/501 --> loss:0.8209882354736329
step 101/334, epoch 114/501 --> loss:0.8282472908496856
step 151/334, epoch 114/501 --> loss:0.8238709056377411
step 201/334, epoch 114/501 --> loss:0.836912395954132
step 251/334, epoch 114/501 --> loss:0.82105593085289
step 301/334, epoch 114/501 --> loss:0.8183487343788147
step 51/334, epoch 115/501 --> loss:0.8263857340812684
step 101/334, epoch 115/501 --> loss:0.8210896408557892
step 151/334, epoch 115/501 --> loss:0.8233590042591095
step 201/334, epoch 115/501 --> loss:0.8362264084815979
step 251/334, epoch 115/501 --> loss:0.8206125283241272
step 301/334, epoch 115/501 --> loss:0.8400733768939972
step 51/334, epoch 116/501 --> loss:0.8232034456729889
step 101/334, epoch 116/501 --> loss:0.8257414555549621
step 151/334, epoch 116/501 --> loss:0.8332559001445771
step 201/334, epoch 116/501 --> loss:0.8166295564174653
step 251/334, epoch 116/501 --> loss:0.8236912322044373
step 301/334, epoch 116/501 --> loss:0.8361207866668701
step 51/334, epoch 117/501 --> loss:0.8140227735042572
step 101/334, epoch 117/501 --> loss:0.8224262988567352
step 151/334, epoch 117/501 --> loss:0.8350504231452942
step 201/334, epoch 117/501 --> loss:0.8383074271678924
step 251/334, epoch 117/501 --> loss:0.831330988407135
step 301/334, epoch 117/501 --> loss:0.8292271542549133
step 51/334, epoch 118/501 --> loss:0.8250473856925964
step 101/334, epoch 118/501 --> loss:0.8253997611999512
step 151/334, epoch 118/501 --> loss:0.8189394295215606
step 201/334, epoch 118/501 --> loss:0.8329589438438415
step 251/334, epoch 118/501 --> loss:0.8334578955173493
step 301/334, epoch 118/501 --> loss:0.8313381779193878
step 51/334, epoch 119/501 --> loss:0.8259499454498291
step 101/334, epoch 119/501 --> loss:0.8457617533206939
step 151/334, epoch 119/501 --> loss:0.8157772517204285
step 201/334, epoch 119/501 --> loss:0.8135006308555603
step 251/334, epoch 119/501 --> loss:0.8197893238067627
step 301/334, epoch 119/501 --> loss:0.835607385635376
step 51/334, epoch 120/501 --> loss:0.8302370965480804
step 101/334, epoch 120/501 --> loss:0.829088294506073
step 151/334, epoch 120/501 --> loss:0.8263158845901489
step 201/334, epoch 120/501 --> loss:0.8296335232257843
step 251/334, epoch 120/501 --> loss:0.8132869505882263
step 301/334, epoch 120/501 --> loss:0.835737372636795
step 51/334, epoch 121/501 --> loss:0.8263934922218322
step 101/334, epoch 121/501 --> loss:0.8210870754718781
step 151/334, epoch 121/501 --> loss:0.8238889157772065
step 201/334, epoch 121/501 --> loss:0.8248575508594513
step 251/334, epoch 121/501 --> loss:0.8373240673542023
step 301/334, epoch 121/501 --> loss:0.8245483469963074

##########train dataset##########
acc--> [98.56913437501548]
F1--> {'F1': [0.8510949136143933], 'precision': [0.7583555674902341], 'recall': [0.9696895244328069]}
##########eval dataset##########
acc--> [98.1427397106302]
F1--> {'F1': [0.8090244338975899], 'precision': [0.7280923141988487], 'recall': [0.9102113627666124]}
save model!
step 51/334, epoch 122/501 --> loss:0.8297151827812195
step 101/334, epoch 122/501 --> loss:0.8189658617973328
step 151/334, epoch 122/501 --> loss:0.839275484085083
step 201/334, epoch 122/501 --> loss:0.8236424434185028
step 251/334, epoch 122/501 --> loss:0.8325831723213196
step 301/334, epoch 122/501 --> loss:0.8247070193290711
step 51/334, epoch 123/501 --> loss:0.817903459072113
step 101/334, epoch 123/501 --> loss:0.8255722177028656
step 151/334, epoch 123/501 --> loss:0.8308100092411042
step 201/334, epoch 123/501 --> loss:0.8203038346767425
step 251/334, epoch 123/501 --> loss:0.8278352916240692
step 301/334, epoch 123/501 --> loss:0.8264576518535613
step 51/334, epoch 124/501 --> loss:0.8194530618190765
step 101/334, epoch 124/501 --> loss:0.8252291750907897
step 151/334, epoch 124/501 --> loss:0.8286715245246887
step 201/334, epoch 124/501 --> loss:0.8291614186763764
step 251/334, epoch 124/501 --> loss:0.8364340925216674
step 301/334, epoch 124/501 --> loss:0.82746919631958
step 51/334, epoch 125/501 --> loss:0.8386230981349945
step 101/334, epoch 125/501 --> loss:0.8143413019180298
step 151/334, epoch 125/501 --> loss:0.8218481802940368
step 201/334, epoch 125/501 --> loss:0.8273393332958221
step 251/334, epoch 125/501 --> loss:0.828086748123169
step 301/334, epoch 125/501 --> loss:0.8363381826877594
step 51/334, epoch 126/501 --> loss:0.8313952505588531
step 101/334, epoch 126/501 --> loss:0.8327615761756897
step 151/334, epoch 126/501 --> loss:0.8192861199378967
step 201/334, epoch 126/501 --> loss:0.831716183423996
step 251/334, epoch 126/501 --> loss:0.8347203755378723
step 301/334, epoch 126/501 --> loss:0.8016379380226135
step 51/334, epoch 127/501 --> loss:0.8329057657718658
step 101/334, epoch 127/501 --> loss:0.8320180833339691
step 151/334, epoch 127/501 --> loss:0.8105143058300018
step 201/334, epoch 127/501 --> loss:0.8256869065761566
step 251/334, epoch 127/501 --> loss:0.8388669681549072
step 301/334, epoch 127/501 --> loss:0.818790911436081
step 51/334, epoch 128/501 --> loss:0.819340876340866
step 101/334, epoch 128/501 --> loss:0.8200510823726654
step 151/334, epoch 128/501 --> loss:0.8237598872184754
step 201/334, epoch 128/501 --> loss:0.8181949210166931
step 251/334, epoch 128/501 --> loss:0.8406757235527038
step 301/334, epoch 128/501 --> loss:0.8441270077228546
step 51/334, epoch 129/501 --> loss:0.8332417488098145
step 101/334, epoch 129/501 --> loss:0.8205499398708344
step 151/334, epoch 129/501 --> loss:0.8319608747959137
step 201/334, epoch 129/501 --> loss:0.8198155772686004
step 251/334, epoch 129/501 --> loss:0.8371988511085511
step 301/334, epoch 129/501 --> loss:0.817553025484085
step 51/334, epoch 130/501 --> loss:0.8208276772499085
step 101/334, epoch 130/501 --> loss:0.8235617125034332
step 151/334, epoch 130/501 --> loss:0.8251988351345062
step 201/334, epoch 130/501 --> loss:0.8384914231300354
step 251/334, epoch 130/501 --> loss:0.8328116452693939
step 301/334, epoch 130/501 --> loss:0.828966578245163
step 51/334, epoch 131/501 --> loss:0.8355322980880737
step 101/334, epoch 131/501 --> loss:0.8253174352645875
step 151/334, epoch 131/501 --> loss:0.8169853794574737
step 201/334, epoch 131/501 --> loss:0.8189705049991608
step 251/334, epoch 131/501 --> loss:0.8269418954849244
step 301/334, epoch 131/501 --> loss:0.8310725319385529

##########train dataset##########
acc--> [98.2934649266836]
F1--> {'F1': [0.8281377190239412], 'precision': [0.7197389836627587], 'recall': [0.9749910679956455]}
##########eval dataset##########
acc--> [97.89255831393224]
F1--> {'F1': [0.7903683082571599], 'precision': [0.6932147146425586], 'recall': [0.9192058089767492]}
step 51/334, epoch 132/501 --> loss:0.81707803606987
step 101/334, epoch 132/501 --> loss:0.8412294888496399
step 151/334, epoch 132/501 --> loss:0.8236312580108642
step 201/334, epoch 132/501 --> loss:0.8171120464801789
step 251/334, epoch 132/501 --> loss:0.8328563642501831
step 301/334, epoch 132/501 --> loss:0.8126121926307678
step 51/334, epoch 133/501 --> loss:0.8160708606243133
step 101/334, epoch 133/501 --> loss:0.8178263485431672
step 151/334, epoch 133/501 --> loss:0.8104052031040192
step 201/334, epoch 133/501 --> loss:0.8439616191387177
step 251/334, epoch 133/501 --> loss:0.8291623961925506
step 301/334, epoch 133/501 --> loss:0.8302435863018036
step 51/334, epoch 134/501 --> loss:0.8187056899070739
step 101/334, epoch 134/501 --> loss:0.8240956377983093
step 151/334, epoch 134/501 --> loss:0.8292907118797302
step 201/334, epoch 134/501 --> loss:0.8255241882801055
step 251/334, epoch 134/501 --> loss:0.8166955471038818
step 301/334, epoch 134/501 --> loss:0.838929648399353
step 51/334, epoch 135/501 --> loss:0.8247255480289459
step 101/334, epoch 135/501 --> loss:0.8301202666759491
step 151/334, epoch 135/501 --> loss:0.8142929995059967
step 201/334, epoch 135/501 --> loss:0.8159763729572296
step 251/334, epoch 135/501 --> loss:0.8166064715385437
step 301/334, epoch 135/501 --> loss:0.8370880138874054
step 51/334, epoch 136/501 --> loss:0.82612788438797
step 101/334, epoch 136/501 --> loss:0.8159459388256073
step 151/334, epoch 136/501 --> loss:0.8351988744735718
step 201/334, epoch 136/501 --> loss:0.8310216033458709
step 251/334, epoch 136/501 --> loss:0.8296723866462707
step 301/334, epoch 136/501 --> loss:0.8305306768417359
step 51/334, epoch 137/501 --> loss:0.8303665661811829
step 101/334, epoch 137/501 --> loss:0.8200213134288787
step 151/334, epoch 137/501 --> loss:0.818156578540802
step 201/334, epoch 137/501 --> loss:0.8202771186828614
step 251/334, epoch 137/501 --> loss:0.8285636985301972
step 301/334, epoch 137/501 --> loss:0.8329872071743012
step 51/334, epoch 138/501 --> loss:0.8153110110759735
step 101/334, epoch 138/501 --> loss:0.8224937498569489
step 151/334, epoch 138/501 --> loss:0.825247323513031
step 201/334, epoch 138/501 --> loss:0.8282403874397278
step 251/334, epoch 138/501 --> loss:0.8255557835102081
step 301/334, epoch 138/501 --> loss:0.8303107404708863
step 51/334, epoch 139/501 --> loss:0.8240428733825683
step 101/334, epoch 139/501 --> loss:0.8273875999450684
step 151/334, epoch 139/501 --> loss:0.8180046427249908
step 201/334, epoch 139/501 --> loss:0.8299278509616852
step 251/334, epoch 139/501 --> loss:0.8267561781406403
step 301/334, epoch 139/501 --> loss:0.8281878292560577
step 51/334, epoch 140/501 --> loss:0.8352658498287201
step 101/334, epoch 140/501 --> loss:0.8177673757076264
step 151/334, epoch 140/501 --> loss:0.8174579668045044
step 201/334, epoch 140/501 --> loss:0.8256824457645416
step 251/334, epoch 140/501 --> loss:0.8262957215309144
step 301/334, epoch 140/501 --> loss:0.8291300010681152
step 51/334, epoch 141/501 --> loss:0.8203440964221954
step 101/334, epoch 141/501 --> loss:0.8224835121631622
step 151/334, epoch 141/501 --> loss:0.814371589422226
step 201/334, epoch 141/501 --> loss:0.8315278267860413
step 251/334, epoch 141/501 --> loss:0.8321851289272308
step 301/334, epoch 141/501 --> loss:0.8335139179229736

##########train dataset##########
acc--> [98.4778599833806]
F1--> {'F1': [0.8439434580545472], 'precision': [0.7433700224952551], 'recall': [0.9760019259340169]}
##########eval dataset##########
acc--> [98.04570022008312]
F1--> {'F1': [0.8010195798417515], 'precision': [0.715270291689334], 'recall': [0.9101420214718483]}
step 51/334, epoch 142/501 --> loss:0.8371556198596954
step 101/334, epoch 142/501 --> loss:0.8238872599601745
step 151/334, epoch 142/501 --> loss:0.824307769536972
step 201/334, epoch 142/501 --> loss:0.8145277261734009
step 251/334, epoch 142/501 --> loss:0.8312894988059998
step 301/334, epoch 142/501 --> loss:0.8308034920692444
step 51/334, epoch 143/501 --> loss:0.8289479112625122
step 101/334, epoch 143/501 --> loss:0.8215362870693207
step 151/334, epoch 143/501 --> loss:0.8249100303649902
step 201/334, epoch 143/501 --> loss:0.8182736074924469
step 251/334, epoch 143/501 --> loss:0.8302060282230377
step 301/334, epoch 143/501 --> loss:0.8285111248493194
step 51/334, epoch 144/501 --> loss:0.8223171019554139
step 101/334, epoch 144/501 --> loss:0.8290495538711548
step 151/334, epoch 144/501 --> loss:0.8376591241359711
step 201/334, epoch 144/501 --> loss:0.8163630664348602
step 251/334, epoch 144/501 --> loss:0.826280415058136
step 301/334, epoch 144/501 --> loss:0.8188886058330536
step 51/334, epoch 145/501 --> loss:0.8146896064281464
step 101/334, epoch 145/501 --> loss:0.8232839584350586
step 151/334, epoch 145/501 --> loss:0.8204045641422272
step 201/334, epoch 145/501 --> loss:0.833328595161438
step 251/334, epoch 145/501 --> loss:0.8411486411094665
step 301/334, epoch 145/501 --> loss:0.8131708931922913
step 51/334, epoch 146/501 --> loss:0.8115961134433747
step 101/334, epoch 146/501 --> loss:0.8331103038787842
step 151/334, epoch 146/501 --> loss:0.818841552734375
step 201/334, epoch 146/501 --> loss:0.8257521986961365
step 251/334, epoch 146/501 --> loss:0.8326995992660522
step 301/334, epoch 146/501 --> loss:0.8143085503578186
step 51/334, epoch 147/501 --> loss:0.8226073348522186
step 101/334, epoch 147/501 --> loss:0.8218860363960266
step 151/334, epoch 147/501 --> loss:0.8374259102344513
step 201/334, epoch 147/501 --> loss:0.8121070504188538
step 251/334, epoch 147/501 --> loss:0.8250807464122772
step 301/334, epoch 147/501 --> loss:0.8200994110107422
step 51/334, epoch 148/501 --> loss:0.8329758572578431
step 101/334, epoch 148/501 --> loss:0.8230525422096252
step 151/334, epoch 148/501 --> loss:0.8252213311195373
step 201/334, epoch 148/501 --> loss:0.8224032235145569
step 251/334, epoch 148/501 --> loss:0.8275886845588684
step 301/334, epoch 148/501 --> loss:0.8187290012836457
step 51/334, epoch 149/501 --> loss:0.8310356426239014
step 101/334, epoch 149/501 --> loss:0.828332827091217
step 151/334, epoch 149/501 --> loss:0.8309865534305573
step 201/334, epoch 149/501 --> loss:0.8128745687007904
step 251/334, epoch 149/501 --> loss:0.8246108257770538
step 301/334, epoch 149/501 --> loss:0.8232162809371948
step 51/334, epoch 150/501 --> loss:0.8218977105617523
step 101/334, epoch 150/501 --> loss:0.8399772918224335
step 151/334, epoch 150/501 --> loss:0.829383111000061
step 201/334, epoch 150/501 --> loss:0.8128037190437317
step 251/334, epoch 150/501 --> loss:0.8207952439785003
step 301/334, epoch 150/501 --> loss:0.8295126521587372
step 51/334, epoch 151/501 --> loss:0.820870635509491
step 101/334, epoch 151/501 --> loss:0.8281636166572571
step 151/334, epoch 151/501 --> loss:0.8305892145633698
step 201/334, epoch 151/501 --> loss:0.8202282571792603
step 251/334, epoch 151/501 --> loss:0.8193304932117462
step 301/334, epoch 151/501 --> loss:0.8234095096588134

##########train dataset##########
acc--> [98.95280252291178]
F1--> {'F1': [0.8869974872092721], 'precision': [0.8138427969533882], 'recall': [0.9746145230744335]}
##########eval dataset##########
acc--> [98.37568922200718]
F1--> {'F1': [0.8262491754270854], 'precision': [0.7683525356810353], 'recall': [0.8935936983111711]}
save model!
step 51/334, epoch 152/501 --> loss:0.8112797069549561
step 101/334, epoch 152/501 --> loss:0.8268790125846863
step 151/334, epoch 152/501 --> loss:0.8276045072078705
step 201/334, epoch 152/501 --> loss:0.8408696568012237
step 251/334, epoch 152/501 --> loss:0.8258017528057099
step 301/334, epoch 152/501 --> loss:0.8150142538547516
step 51/334, epoch 153/501 --> loss:0.823452136516571
step 101/334, epoch 153/501 --> loss:0.8217887270450592
step 151/334, epoch 153/501 --> loss:0.8169004714488983
step 201/334, epoch 153/501 --> loss:0.8211036682128906
step 251/334, epoch 153/501 --> loss:0.8348046875
step 301/334, epoch 153/501 --> loss:0.8138464868068696
step 51/334, epoch 154/501 --> loss:0.8334039342403412
step 101/334, epoch 154/501 --> loss:0.8194615983963013
step 151/334, epoch 154/501 --> loss:0.8226087176799775
step 201/334, epoch 154/501 --> loss:0.8263331735134125
step 251/334, epoch 154/501 --> loss:0.8243101894855499
step 301/334, epoch 154/501 --> loss:0.8203145730495452
step 51/334, epoch 155/501 --> loss:0.8211989974975586
step 101/334, epoch 155/501 --> loss:0.818510160446167
step 151/334, epoch 155/501 --> loss:0.824916307926178
step 201/334, epoch 155/501 --> loss:0.8374579405784607
step 251/334, epoch 155/501 --> loss:0.8152315127849579
step 301/334, epoch 155/501 --> loss:0.8233033776283264
step 51/334, epoch 156/501 --> loss:0.8244016420841217
step 101/334, epoch 156/501 --> loss:0.8262270820140839
step 151/334, epoch 156/501 --> loss:0.8201699697971344
step 201/334, epoch 156/501 --> loss:0.8224139726161956
step 251/334, epoch 156/501 --> loss:0.8250391721725464
step 301/334, epoch 156/501 --> loss:0.8291430783271789
step 51/334, epoch 157/501 --> loss:0.8412922835350036
step 101/334, epoch 157/501 --> loss:0.8276181602478028
step 151/334, epoch 157/501 --> loss:0.8152763938903809
step 201/334, epoch 157/501 --> loss:0.8203034174442291
step 251/334, epoch 157/501 --> loss:0.8284457767009735
step 301/334, epoch 157/501 --> loss:0.8207520747184753
step 51/334, epoch 158/501 --> loss:0.8358950185775756
step 101/334, epoch 158/501 --> loss:0.8271165227890015
step 151/334, epoch 158/501 --> loss:0.8257279074192048
step 201/334, epoch 158/501 --> loss:0.8248098385334015
step 251/334, epoch 158/501 --> loss:0.8300806081295014
step 301/334, epoch 158/501 --> loss:0.8107690787315369
step 51/334, epoch 159/501 --> loss:0.8150865268707276
step 101/334, epoch 159/501 --> loss:0.8391587650775909
step 151/334, epoch 159/501 --> loss:0.8092010772228241
step 201/334, epoch 159/501 --> loss:0.8161396145820617
step 251/334, epoch 159/501 --> loss:0.8383691740036011
step 301/334, epoch 159/501 --> loss:0.8266182363033294
step 51/334, epoch 160/501 --> loss:0.8337941896915436
step 101/334, epoch 160/501 --> loss:0.8146102213859558
step 151/334, epoch 160/501 --> loss:0.8162626564502716
step 201/334, epoch 160/501 --> loss:0.8136905980110168
step 251/334, epoch 160/501 --> loss:0.8374401545524597
step 301/334, epoch 160/501 --> loss:0.8314372253417969
step 51/334, epoch 161/501 --> loss:0.8269429385662079
step 101/334, epoch 161/501 --> loss:0.8161918163299561
step 151/334, epoch 161/501 --> loss:0.8147451055049896
step 201/334, epoch 161/501 --> loss:0.8217967534065247
step 251/334, epoch 161/501 --> loss:0.8039645504951477
step 301/334, epoch 161/501 --> loss:0.837956508398056

##########train dataset##########
acc--> [98.96002799149335]
F1--> {'F1': [0.8885746914655623], 'precision': [0.81047998502921], 'recall': [0.9833361066008214]}
##########eval dataset##########
acc--> [98.38392669863974]
F1--> {'F1': [0.8284736270835144], 'precision': [0.7653074780298312], 'recall': [0.9030167015212415]}
save model!
step 51/334, epoch 162/501 --> loss:0.818034896850586
step 101/334, epoch 162/501 --> loss:0.8377610003948212
step 151/334, epoch 162/501 --> loss:0.8337423026561737
step 201/334, epoch 162/501 --> loss:0.8102079224586487
step 251/334, epoch 162/501 --> loss:0.8132535493373871
step 301/334, epoch 162/501 --> loss:0.8282415437698364
step 51/334, epoch 163/501 --> loss:0.8339299595355988
step 101/334, epoch 163/501 --> loss:0.8128067982196808
step 151/334, epoch 163/501 --> loss:0.8155043303966523
step 201/334, epoch 163/501 --> loss:0.8221265840530395
step 251/334, epoch 163/501 --> loss:0.8299819660186768
step 301/334, epoch 163/501 --> loss:0.8253388488292694
step 51/334, epoch 164/501 --> loss:0.8253355216979981
step 101/334, epoch 164/501 --> loss:0.8300601768493653
step 151/334, epoch 164/501 --> loss:0.828649971485138
step 201/334, epoch 164/501 --> loss:0.8133700501918792
step 251/334, epoch 164/501 --> loss:0.8125884056091308
step 301/334, epoch 164/501 --> loss:0.827727519273758
step 51/334, epoch 165/501 --> loss:0.8194527637958526
step 101/334, epoch 165/501 --> loss:0.8222406888008118
step 151/334, epoch 165/501 --> loss:0.8140278708934784
step 201/334, epoch 165/501 --> loss:0.8378295087814331
step 251/334, epoch 165/501 --> loss:0.820875096321106
step 301/334, epoch 165/501 --> loss:0.8368149137496949
step 51/334, epoch 166/501 --> loss:0.8003971052169799
step 101/334, epoch 166/501 --> loss:0.8219550180435181
step 151/334, epoch 166/501 --> loss:0.8313901352882386
step 201/334, epoch 166/501 --> loss:0.8131763100624084
step 251/334, epoch 166/501 --> loss:0.8284172129631042
step 301/334, epoch 166/501 --> loss:0.8399192917346955
step 51/334, epoch 167/501 --> loss:0.815717066526413
step 101/334, epoch 167/501 --> loss:0.8324477708339691
step 151/334, epoch 167/501 --> loss:0.818840833902359
step 201/334, epoch 167/501 --> loss:0.8076835334300995
step 251/334, epoch 167/501 --> loss:0.8425704538822174
step 301/334, epoch 167/501 --> loss:0.830875952243805
step 51/334, epoch 168/501 --> loss:0.8262950325012207
step 101/334, epoch 168/501 --> loss:0.8169026052951813
step 151/334, epoch 168/501 --> loss:0.828876029253006
step 201/334, epoch 168/501 --> loss:0.8141566097736359
step 251/334, epoch 168/501 --> loss:0.810911500453949
step 301/334, epoch 168/501 --> loss:0.8383672213554383
step 51/334, epoch 169/501 --> loss:0.829448949098587
step 101/334, epoch 169/501 --> loss:0.8205854535102844
step 151/334, epoch 169/501 --> loss:0.8151800644397735
step 201/334, epoch 169/501 --> loss:0.8214437103271485
step 251/334, epoch 169/501 --> loss:0.8274343132972717
step 301/334, epoch 169/501 --> loss:0.8255160081386567
step 51/334, epoch 170/501 --> loss:0.8321297311782837
step 101/334, epoch 170/501 --> loss:0.8247495198249817
step 151/334, epoch 170/501 --> loss:0.8333950209617614
step 201/334, epoch 170/501 --> loss:0.8177872085571289
step 251/334, epoch 170/501 --> loss:0.8228122818470002
step 301/334, epoch 170/501 --> loss:0.8041130423545837
step 51/334, epoch 171/501 --> loss:0.8154277515411377
step 101/334, epoch 171/501 --> loss:0.8187760758399963
step 151/334, epoch 171/501 --> loss:0.8309310173988342
step 201/334, epoch 171/501 --> loss:0.8236358118057251
step 251/334, epoch 171/501 --> loss:0.8175920391082764
step 301/334, epoch 171/501 --> loss:0.8202943313121795

##########train dataset##########
acc--> [99.12884415067339]
F1--> {'F1': [0.9051371270128314], 'precision': [0.8368503114307192], 'recall': [0.9855702776555927]}
##########eval dataset##########
acc--> [98.48595099394267]
F1--> {'F1': [0.8357752162741731], 'precision': [0.7866846698340032], 'recall': [0.8914115323979959]}
save model!
step 51/334, epoch 172/501 --> loss:0.8197733902931214
step 101/334, epoch 172/501 --> loss:0.8045268142223359
step 151/334, epoch 172/501 --> loss:0.8396594595909118
step 201/334, epoch 172/501 --> loss:0.8305252027511597
step 251/334, epoch 172/501 --> loss:0.8193766856193543
step 301/334, epoch 172/501 --> loss:0.828711131811142
step 51/334, epoch 173/501 --> loss:0.824002412557602
step 101/334, epoch 173/501 --> loss:0.8147617352008819
step 151/334, epoch 173/501 --> loss:0.826694895029068
step 201/334, epoch 173/501 --> loss:0.8234748995304108
step 251/334, epoch 173/501 --> loss:0.8363889503479004
step 301/334, epoch 173/501 --> loss:0.8176141345500946
step 51/334, epoch 174/501 --> loss:0.8354227697849274
step 101/334, epoch 174/501 --> loss:0.8210815870761872
step 151/334, epoch 174/501 --> loss:0.8256104743480682
step 201/334, epoch 174/501 --> loss:0.8245132803916931
step 251/334, epoch 174/501 --> loss:0.8212884879112243
step 301/334, epoch 174/501 --> loss:0.8389435255527496
step 51/334, epoch 175/501 --> loss:0.8233526599407196
step 101/334, epoch 175/501 --> loss:0.8351291573047638
step 151/334, epoch 175/501 --> loss:0.8118958389759063
step 201/334, epoch 175/501 --> loss:0.8127998948097229
step 251/334, epoch 175/501 --> loss:0.8303795719146728
step 301/334, epoch 175/501 --> loss:0.8244324064254761
step 51/334, epoch 176/501 --> loss:0.8215670680999756
step 101/334, epoch 176/501 --> loss:0.820112054347992
step 151/334, epoch 176/501 --> loss:0.8236912107467651
step 201/334, epoch 176/501 --> loss:0.8137777018547058
step 251/334, epoch 176/501 --> loss:0.8210515570640564
step 301/334, epoch 176/501 --> loss:0.8364718389511109
step 51/334, epoch 177/501 --> loss:0.8235151624679565
step 101/334, epoch 177/501 --> loss:0.8093700313568115
step 151/334, epoch 177/501 --> loss:0.8326997089385987
step 201/334, epoch 177/501 --> loss:0.8101939225196838
step 251/334, epoch 177/501 --> loss:0.8388795149326325
step 301/334, epoch 177/501 --> loss:0.8181471192836761
step 51/334, epoch 178/501 --> loss:0.8176486873626709
step 101/334, epoch 178/501 --> loss:0.8124817669391632
step 151/334, epoch 178/501 --> loss:0.8185537445545197
step 201/334, epoch 178/501 --> loss:0.8307725381851196
step 251/334, epoch 178/501 --> loss:0.8350450432300568
step 301/334, epoch 178/501 --> loss:0.8175683891773224
step 51/334, epoch 179/501 --> loss:0.8287678372859955
step 101/334, epoch 179/501 --> loss:0.817941769361496
step 151/334, epoch 179/501 --> loss:0.8257493925094604
step 201/334, epoch 179/501 --> loss:0.8214895784854889
step 251/334, epoch 179/501 --> loss:0.8159852159023285
step 301/334, epoch 179/501 --> loss:0.8202064061164855
step 51/334, epoch 180/501 --> loss:0.8140755271911622
step 101/334, epoch 180/501 --> loss:0.8231505262851715
step 151/334, epoch 180/501 --> loss:0.8246188163757324
step 201/334, epoch 180/501 --> loss:0.8229824483394623
step 251/334, epoch 180/501 --> loss:0.8157459831237793
step 301/334, epoch 180/501 --> loss:0.8207980394363403
step 51/334, epoch 181/501 --> loss:0.8256153905391693
step 101/334, epoch 181/501 --> loss:0.8230041825771331
step 151/334, epoch 181/501 --> loss:0.815448077917099
step 201/334, epoch 181/501 --> loss:0.827166565656662
step 251/334, epoch 181/501 --> loss:0.8040935754776001
step 301/334, epoch 181/501 --> loss:0.8359875559806824

##########train dataset##########
acc--> [99.27285748293019]
F1--> {'F1': [0.9191578106168705], 'precision': [0.8652209346820576], 'recall': [0.9802778238098218]}
##########eval dataset##########
acc--> [98.60485474059347]
F1--> {'F1': [0.8440661513754711], 'precision': [0.8164185512530368], 'recall': [0.8736626318976849]}
save model!
step 51/334, epoch 182/501 --> loss:0.7950694227218628
step 101/334, epoch 182/501 --> loss:0.8193279778957367
step 151/334, epoch 182/501 --> loss:0.8291609954833984
step 201/334, epoch 182/501 --> loss:0.8290666317939759
step 251/334, epoch 182/501 --> loss:0.821354683637619
step 301/334, epoch 182/501 --> loss:0.8356288075447083
step 51/334, epoch 183/501 --> loss:0.8227675974369049
step 101/334, epoch 183/501 --> loss:0.8309439289569854
step 151/334, epoch 183/501 --> loss:0.8203220081329345
step 201/334, epoch 183/501 --> loss:0.8242536962032319
step 251/334, epoch 183/501 --> loss:0.8260666036605835
step 301/334, epoch 183/501 --> loss:0.8250736021995544
step 51/334, epoch 184/501 --> loss:0.8419668602943421
step 101/334, epoch 184/501 --> loss:0.8277633714675904
step 151/334, epoch 184/501 --> loss:0.8181399869918823
step 201/334, epoch 184/501 --> loss:0.8277505576610565
step 251/334, epoch 184/501 --> loss:0.7972935104370117
step 301/334, epoch 184/501 --> loss:0.8236167216300965
step 51/334, epoch 185/501 --> loss:0.8178893518447876
step 101/334, epoch 185/501 --> loss:0.8031516754627228
step 151/334, epoch 185/501 --> loss:0.8310271978378296
step 201/334, epoch 185/501 --> loss:0.8389783549308777
step 251/334, epoch 185/501 --> loss:0.8311045455932617
step 301/334, epoch 185/501 --> loss:0.8188790690898895
step 51/334, epoch 186/501 --> loss:0.8164441156387329
step 101/334, epoch 186/501 --> loss:0.8194293987751007
step 151/334, epoch 186/501 --> loss:0.8201816356182099
step 201/334, epoch 186/501 --> loss:0.832983672618866
step 251/334, epoch 186/501 --> loss:0.8288358998298645
step 301/334, epoch 186/501 --> loss:0.8166334116458893
step 51/334, epoch 187/501 --> loss:0.8300774812698364
step 101/334, epoch 187/501 --> loss:0.8080538213253021
step 151/334, epoch 187/501 --> loss:0.8343388772010804
step 201/334, epoch 187/501 --> loss:0.8138085806369781
step 251/334, epoch 187/501 --> loss:0.8160706198215485
step 301/334, epoch 187/501 --> loss:0.835820643901825
step 51/334, epoch 188/501 --> loss:0.8085274839401245
step 101/334, epoch 188/501 --> loss:0.8153824639320374
step 151/334, epoch 188/501 --> loss:0.8270606207847595
step 201/334, epoch 188/501 --> loss:0.8238028502464294
step 251/334, epoch 188/501 --> loss:0.8334047412872314
step 301/334, epoch 188/501 --> loss:0.8226416397094727
step 51/334, epoch 189/501 --> loss:0.8317961537837982
step 101/334, epoch 189/501 --> loss:0.8319904458522797
step 151/334, epoch 189/501 --> loss:0.8271484744548797
step 201/334, epoch 189/501 --> loss:0.8143419897556305
step 251/334, epoch 189/501 --> loss:0.7995439124107361
step 301/334, epoch 189/501 --> loss:0.8142582058906556
step 51/334, epoch 190/501 --> loss:0.8189492583274841
step 101/334, epoch 190/501 --> loss:0.822802642583847
step 151/334, epoch 190/501 --> loss:0.8207061684131622
step 201/334, epoch 190/501 --> loss:0.824870343208313
step 251/334, epoch 190/501 --> loss:0.8261824750900268
step 301/334, epoch 190/501 --> loss:0.82955482006073
step 51/334, epoch 191/501 --> loss:0.8116592836380004
step 101/334, epoch 191/501 --> loss:0.8209037208557128
step 151/334, epoch 191/501 --> loss:0.8206100451946259
step 201/334, epoch 191/501 --> loss:0.8261282205581665
step 251/334, epoch 191/501 --> loss:0.8284857928752899
step 301/334, epoch 191/501 --> loss:0.8300098061561585

##########train dataset##########
acc--> [99.22928754693953]
F1--> {'F1': [0.9152248450451312], 'precision': [0.8535112301602391], 'recall': [0.9865700786993221]}
##########eval dataset##########
acc--> [98.57943410530666]
F1--> {'F1': [0.8440603366636893], 'precision': [0.8030122829251182], 'recall': [0.889542097268556]}
step 51/334, epoch 192/501 --> loss:0.8181220257282257
step 101/334, epoch 192/501 --> loss:0.8095832729339599
step 151/334, epoch 192/501 --> loss:0.8305007803440094
step 201/334, epoch 192/501 --> loss:0.8308338248729705
step 251/334, epoch 192/501 --> loss:0.8243359875679016
step 301/334, epoch 192/501 --> loss:0.8138252103328705
step 51/334, epoch 193/501 --> loss:0.8206229841709137
step 101/334, epoch 193/501 --> loss:0.8284027743339538
step 151/334, epoch 193/501 --> loss:0.8139159142971039
step 201/334, epoch 193/501 --> loss:0.8164497184753418
step 251/334, epoch 193/501 --> loss:0.8217556416988373
step 301/334, epoch 193/501 --> loss:0.8295692098140717
step 51/334, epoch 194/501 --> loss:0.8222951269149781
step 101/334, epoch 194/501 --> loss:0.8362310194969177
step 151/334, epoch 194/501 --> loss:0.8085335838794708
step 201/334, epoch 194/501 --> loss:0.7994002783298493
step 251/334, epoch 194/501 --> loss:0.8362310683727264
step 301/334, epoch 194/501 --> loss:0.8255952179431916
step 51/334, epoch 195/501 --> loss:0.8212135970592499
step 101/334, epoch 195/501 --> loss:0.8244835007190704
step 151/334, epoch 195/501 --> loss:0.8187422287464142
step 201/334, epoch 195/501 --> loss:0.8292358779907226
step 251/334, epoch 195/501 --> loss:0.827407306432724
step 301/334, epoch 195/501 --> loss:0.8176574373245239
step 51/334, epoch 196/501 --> loss:0.822372522354126
step 101/334, epoch 196/501 --> loss:0.8180605137348175
step 151/334, epoch 196/501 --> loss:0.8352690351009369
step 201/334, epoch 196/501 --> loss:0.8199047744274139
step 251/334, epoch 196/501 --> loss:0.8317681455612183
step 301/334, epoch 196/501 --> loss:0.8121915209293366
step 51/334, epoch 197/501 --> loss:0.8291956615447998
step 101/334, epoch 197/501 --> loss:0.8133888244628906
step 151/334, epoch 197/501 --> loss:0.8322830510139465
step 201/334, epoch 197/501 --> loss:0.8115740728378296
step 251/334, epoch 197/501 --> loss:0.8121386146545411
step 301/334, epoch 197/501 --> loss:0.8334669232368469
step 51/334, epoch 198/501 --> loss:0.81591726064682
step 101/334, epoch 198/501 --> loss:0.819192361831665
step 151/334, epoch 198/501 --> loss:0.8237458765506744
step 201/334, epoch 198/501 --> loss:0.8236589479446411
step 251/334, epoch 198/501 --> loss:0.8225571441650391
step 301/334, epoch 198/501 --> loss:0.818039972782135
step 51/334, epoch 199/501 --> loss:0.8111515843868256
step 101/334, epoch 199/501 --> loss:0.8248036682605744
step 151/334, epoch 199/501 --> loss:0.8213233506679535
step 201/334, epoch 199/501 --> loss:0.8109344756603241
step 251/334, epoch 199/501 --> loss:0.8409042596817017
step 301/334, epoch 199/501 --> loss:0.8118492352962494
step 51/334, epoch 200/501 --> loss:0.8218437874317169
step 101/334, epoch 200/501 --> loss:0.8172404360771179
step 151/334, epoch 200/501 --> loss:0.8212929058074951
step 201/334, epoch 200/501 --> loss:0.8157869696617126
step 251/334, epoch 200/501 --> loss:0.8126900660991668
step 301/334, epoch 200/501 --> loss:0.840494750738144
step 51/334, epoch 201/501 --> loss:0.8240666103363037
step 101/334, epoch 201/501 --> loss:0.8129871606826782
step 151/334, epoch 201/501 --> loss:0.823647426366806
step 201/334, epoch 201/501 --> loss:0.8143977355957032
step 251/334, epoch 201/501 --> loss:0.8362714421749115
step 301/334, epoch 201/501 --> loss:0.823597537279129

##########train dataset##########
acc--> [99.11553576325747]
F1--> {'F1': [0.9037966586382516], 'precision': [0.8348113160801036], 'recall': [0.9852221550586444]}
##########eval dataset##########
acc--> [98.39718435692318]
F1--> {'F1': [0.8272335520129743], 'precision': [0.7743744375554539], 'recall': [0.8878491812926024]}
step 51/334, epoch 202/501 --> loss:0.8339428138732911
step 101/334, epoch 202/501 --> loss:0.8209000456333161
step 151/334, epoch 202/501 --> loss:0.8088174211978912
step 201/334, epoch 202/501 --> loss:0.8168619775772095
step 251/334, epoch 202/501 --> loss:0.8183241879940033
step 301/334, epoch 202/501 --> loss:0.8275160825252533
step 51/334, epoch 203/501 --> loss:0.8260106754302978
step 101/334, epoch 203/501 --> loss:0.8257808995246887
step 151/334, epoch 203/501 --> loss:0.8272904145717621
step 201/334, epoch 203/501 --> loss:0.805724766254425
step 251/334, epoch 203/501 --> loss:0.8246108639240265
step 301/334, epoch 203/501 --> loss:0.8136464560031891
step 51/334, epoch 204/501 --> loss:0.8195646131038665
step 101/334, epoch 204/501 --> loss:0.844091169834137
step 151/334, epoch 204/501 --> loss:0.8188705241680145
step 201/334, epoch 204/501 --> loss:0.8272349548339843
step 251/334, epoch 204/501 --> loss:0.8118207132816315
step 301/334, epoch 204/501 --> loss:0.8167540264129639
step 51/334, epoch 205/501 --> loss:0.8257285451889038
step 101/334, epoch 205/501 --> loss:0.8187500011920928
step 151/334, epoch 205/501 --> loss:0.831419427394867
step 201/334, epoch 205/501 --> loss:0.8171827495098114
step 251/334, epoch 205/501 --> loss:0.8140315055847168
step 301/334, epoch 205/501 --> loss:0.826246761083603
step 51/334, epoch 206/501 --> loss:0.8144287717342377
step 101/334, epoch 206/501 --> loss:0.8208397686481476
step 151/334, epoch 206/501 --> loss:0.8229098594188691
step 201/334, epoch 206/501 --> loss:0.8159423840045928
step 251/334, epoch 206/501 --> loss:0.8204263389110565
step 301/334, epoch 206/501 --> loss:0.8267513108253479
step 51/334, epoch 207/501 --> loss:0.8173352456092835
step 101/334, epoch 207/501 --> loss:0.8019744002819061
step 151/334, epoch 207/501 --> loss:0.8209340417385101
step 201/334, epoch 207/501 --> loss:0.8386546051502228
step 251/334, epoch 207/501 --> loss:0.8187116730213165
step 301/334, epoch 207/501 --> loss:0.8203593671321869
step 51/334, epoch 208/501 --> loss:0.8170670878887176
step 101/334, epoch 208/501 --> loss:0.8143575131893158
step 151/334, epoch 208/501 --> loss:0.8289518666267395
step 201/334, epoch 208/501 --> loss:0.8264523458480835
step 251/334, epoch 208/501 --> loss:0.8273540353775024
step 301/334, epoch 208/501 --> loss:0.8168133616447448
step 51/334, epoch 209/501 --> loss:0.8259955787658692
step 101/334, epoch 209/501 --> loss:0.8074501311779022
step 151/334, epoch 209/501 --> loss:0.8191883099079132
step 201/334, epoch 209/501 --> loss:0.823821347951889
step 251/334, epoch 209/501 --> loss:0.8147224032878876
step 301/334, epoch 209/501 --> loss:0.8366317582130433
step 51/334, epoch 210/501 --> loss:0.8167891347408295
step 101/334, epoch 210/501 --> loss:0.8301150858402252
step 151/334, epoch 210/501 --> loss:0.7986151707172394
step 201/334, epoch 210/501 --> loss:0.8282780182361603
step 251/334, epoch 210/501 --> loss:0.8213826131820678
step 301/334, epoch 210/501 --> loss:0.8283318936824798
step 51/334, epoch 211/501 --> loss:0.8221972143650055
step 101/334, epoch 211/501 --> loss:0.8174916613101959
step 151/334, epoch 211/501 --> loss:0.8298033690452575
step 201/334, epoch 211/501 --> loss:0.833660591840744
step 251/334, epoch 211/501 --> loss:0.8173360407352448
step 301/334, epoch 211/501 --> loss:0.8168637037277222

##########train dataset##########
acc--> [99.2184564957306]
F1--> {'F1': [0.9140026452895088], 'precision': [0.8526365958585302], 'recall': [0.9848985882645207]}
##########eval dataset##########
acc--> [98.48440509037884]
F1--> {'F1': [0.8334836793327202], 'precision': [0.7935760528928184], 'recall': [0.8776286759752682]}
step 51/334, epoch 212/501 --> loss:0.8177955102920532
step 101/334, epoch 212/501 --> loss:0.8248698079586029
step 151/334, epoch 212/501 --> loss:0.8003094828128815
step 201/334, epoch 212/501 --> loss:0.8315074253082275
step 251/334, epoch 212/501 --> loss:0.8189624929428101
step 301/334, epoch 212/501 --> loss:0.828452775478363
step 51/334, epoch 213/501 --> loss:0.8202330803871155
step 101/334, epoch 213/501 --> loss:0.8255219113826752
step 151/334, epoch 213/501 --> loss:0.8292340636253357
step 201/334, epoch 213/501 --> loss:0.8210943317413331
step 251/334, epoch 213/501 --> loss:0.8202865970134735
step 301/334, epoch 213/501 --> loss:0.8133961343765259
step 51/334, epoch 214/501 --> loss:0.8387804448604583
step 101/334, epoch 214/501 --> loss:0.8094964349269866
step 151/334, epoch 214/501 --> loss:0.8227064800262451
step 201/334, epoch 214/501 --> loss:0.8321299660205841
step 251/334, epoch 214/501 --> loss:0.8181930339336395
step 301/334, epoch 214/501 --> loss:0.8246550834178925
step 51/334, epoch 215/501 --> loss:0.8275744807720185
step 101/334, epoch 215/501 --> loss:0.8147497427463531
step 151/334, epoch 215/501 --> loss:0.8265757715702057
step 201/334, epoch 215/501 --> loss:0.8234363973140717
step 251/334, epoch 215/501 --> loss:0.8191611158847809
step 301/334, epoch 215/501 --> loss:0.8272806477546691
step 51/334, epoch 216/501 --> loss:0.8113183891773224
step 101/334, epoch 216/501 --> loss:0.8293521440029145
step 151/334, epoch 216/501 --> loss:0.8171746349334716
step 201/334, epoch 216/501 --> loss:0.8327365040779113
step 251/334, epoch 216/501 --> loss:0.8296359562873841
step 301/334, epoch 216/501 --> loss:0.8197883296012879
step 51/334, epoch 217/501 --> loss:0.8188981127738952
step 101/334, epoch 217/501 --> loss:0.8270923316478729
step 151/334, epoch 217/501 --> loss:0.8308651101589203
step 201/334, epoch 217/501 --> loss:0.8079950606822968
step 251/334, epoch 217/501 --> loss:0.820889595746994
step 301/334, epoch 217/501 --> loss:0.8180103433132172
step 51/334, epoch 218/501 --> loss:0.8138786923885345
step 101/334, epoch 218/501 --> loss:0.8166241240501404
step 151/334, epoch 218/501 --> loss:0.8096317517757415
step 201/334, epoch 218/501 --> loss:0.8239201557636261
step 251/334, epoch 218/501 --> loss:0.8298842930793762
step 301/334, epoch 218/501 --> loss:0.8302092909812927
step 51/334, epoch 219/501 --> loss:0.8189219129085541
step 101/334, epoch 219/501 --> loss:0.8305557286739349
step 151/334, epoch 219/501 --> loss:0.8123981094360352
step 201/334, epoch 219/501 --> loss:0.8116790926456452
step 251/334, epoch 219/501 --> loss:0.8241532135009766
step 301/334, epoch 219/501 --> loss:0.8208151376247406
step 51/334, epoch 220/501 --> loss:0.826462858915329
step 101/334, epoch 220/501 --> loss:0.8243562424182892
step 151/334, epoch 220/501 --> loss:0.8104591584205627
step 201/334, epoch 220/501 --> loss:0.8134827899932862
step 251/334, epoch 220/501 --> loss:0.8228121590614319
step 301/334, epoch 220/501 --> loss:0.8280329370498657
step 51/334, epoch 221/501 --> loss:0.8194582617282867
step 101/334, epoch 221/501 --> loss:0.8246788060665131
step 151/334, epoch 221/501 --> loss:0.8242904067039489
step 201/334, epoch 221/501 --> loss:0.825746750831604
step 251/334, epoch 221/501 --> loss:0.823318578004837
step 301/334, epoch 221/501 --> loss:0.81293985247612

##########train dataset##########
acc--> [99.38362616476854]
F1--> {'F1': [0.9312632405633003], 'precision': [0.8789804326170376], 'recall': [0.9901703528289063]}
##########eval dataset##########
acc--> [98.62324271614436]
F1--> {'F1': [0.8473274888263499], 'precision': [0.8136131173478774], 'recall': [0.8839676131353663]}
save model!
step 51/334, epoch 222/501 --> loss:0.8137854671478272
step 101/334, epoch 222/501 --> loss:0.8149026143550873
step 151/334, epoch 222/501 --> loss:0.8282129561901093
step 201/334, epoch 222/501 --> loss:0.8289689028263092
step 251/334, epoch 222/501 --> loss:0.8196855771541596
step 301/334, epoch 222/501 --> loss:0.8203305196762085
step 51/334, epoch 223/501 --> loss:0.8187488627433777
step 101/334, epoch 223/501 --> loss:0.8173112738132476
step 151/334, epoch 223/501 --> loss:0.8319547832012176
step 201/334, epoch 223/501 --> loss:0.818272374868393
step 251/334, epoch 223/501 --> loss:0.8290134060382843
step 301/334, epoch 223/501 --> loss:0.8186822640895843
step 51/334, epoch 224/501 --> loss:0.820292352437973
step 101/334, epoch 224/501 --> loss:0.8287925064563751
step 151/334, epoch 224/501 --> loss:0.8221899569034576
step 201/334, epoch 224/501 --> loss:0.8268125879764557
step 251/334, epoch 224/501 --> loss:0.8179180264472962
step 301/334, epoch 224/501 --> loss:0.820534690618515
step 51/334, epoch 225/501 --> loss:0.8121435499191284
step 101/334, epoch 225/501 --> loss:0.8215467095375061
step 151/334, epoch 225/501 --> loss:0.8117687714099884
step 201/334, epoch 225/501 --> loss:0.8327567195892334
step 251/334, epoch 225/501 --> loss:0.8332296764850616
step 301/334, epoch 225/501 --> loss:0.8185818946361542
step 51/334, epoch 226/501 --> loss:0.8169269406795502
step 101/334, epoch 226/501 --> loss:0.821370599269867
step 151/334, epoch 226/501 --> loss:0.8255786406993866
step 201/334, epoch 226/501 --> loss:0.8267322087287903
step 251/334, epoch 226/501 --> loss:0.8051109862327576
step 301/334, epoch 226/501 --> loss:0.821066768169403
step 51/334, epoch 227/501 --> loss:0.8318974649906159
step 101/334, epoch 227/501 --> loss:0.8158531713485718
step 151/334, epoch 227/501 --> loss:0.8052964460849762
step 201/334, epoch 227/501 --> loss:0.8184429204463959
step 251/334, epoch 227/501 --> loss:0.8348757266998291
step 301/334, epoch 227/501 --> loss:0.8175769412517547
step 51/334, epoch 228/501 --> loss:0.8147358810901641
step 101/334, epoch 228/501 --> loss:0.8304931104183197
step 151/334, epoch 228/501 --> loss:0.827001645565033
step 201/334, epoch 228/501 --> loss:0.8203788602352142
step 251/334, epoch 228/501 --> loss:0.8175317919254304
step 301/334, epoch 228/501 --> loss:0.816645964384079
step 51/334, epoch 229/501 --> loss:0.8270958721637726
step 101/334, epoch 229/501 --> loss:0.826011620759964
step 151/334, epoch 229/501 --> loss:0.8215151250362396
step 201/334, epoch 229/501 --> loss:0.8338264906406403
step 251/334, epoch 229/501 --> loss:0.8057347416877747
step 301/334, epoch 229/501 --> loss:0.8245772659778595
step 51/334, epoch 230/501 --> loss:0.825754873752594
step 101/334, epoch 230/501 --> loss:0.8285154497623444
step 151/334, epoch 230/501 --> loss:0.8165719318389892
step 201/334, epoch 230/501 --> loss:0.8177791273593903
step 251/334, epoch 230/501 --> loss:0.8210255384445191
step 301/334, epoch 230/501 --> loss:0.8295238101482392
step 51/334, epoch 231/501 --> loss:0.838225508928299
step 101/334, epoch 231/501 --> loss:0.8247523248195648
step 151/334, epoch 231/501 --> loss:0.8234367644786835
step 201/334, epoch 231/501 --> loss:0.80227783203125
step 251/334, epoch 231/501 --> loss:0.8263399970531463
step 301/334, epoch 231/501 --> loss:0.8157069170475006

##########train dataset##########
acc--> [99.50976691801789]
F1--> {'F1': [0.9445336236850669], 'precision': [0.9031736272403759], 'recall': [0.9898744621882423]}
##########eval dataset##########
acc--> [98.73092264223148]
F1--> {'F1': [0.855060473521755], 'precision': [0.8442719367928466], 'recall': [0.8661385608934462]}
save model!
step 51/334, epoch 232/501 --> loss:0.8326533961296082
step 101/334, epoch 232/501 --> loss:0.8202032911777496
step 151/334, epoch 232/501 --> loss:0.812467702627182
step 201/334, epoch 232/501 --> loss:0.8287126755714417
step 251/334, epoch 232/501 --> loss:0.8169528698921203
step 301/334, epoch 232/501 --> loss:0.8139898979663849
step 51/334, epoch 233/501 --> loss:0.8142155909538269
step 101/334, epoch 233/501 --> loss:0.8293443667888641
step 151/334, epoch 233/501 --> loss:0.8224894917011261
step 201/334, epoch 233/501 --> loss:0.8175332176685334
step 251/334, epoch 233/501 --> loss:0.8075758016109467
step 301/334, epoch 233/501 --> loss:0.8357524788379669
step 51/334, epoch 234/501 --> loss:0.8302357935905457
step 101/334, epoch 234/501 --> loss:0.826978224515915
step 151/334, epoch 234/501 --> loss:0.8112861430644989
step 201/334, epoch 234/501 --> loss:0.8190915262699128
step 251/334, epoch 234/501 --> loss:0.822871298789978
step 301/334, epoch 234/501 --> loss:0.8330448257923126
step 51/334, epoch 235/501 --> loss:0.8256044232845307
step 101/334, epoch 235/501 --> loss:0.8183128309249877
step 151/334, epoch 235/501 --> loss:0.8348474729061127
step 201/334, epoch 235/501 --> loss:0.8229617309570313
step 251/334, epoch 235/501 --> loss:0.8101722908020019
step 301/334, epoch 235/501 --> loss:0.8217496144771576
step 51/334, epoch 236/501 --> loss:0.8099021518230438
step 101/334, epoch 236/501 --> loss:0.8239068973064423
step 151/334, epoch 236/501 --> loss:0.8385182821750641
step 201/334, epoch 236/501 --> loss:0.8150042903423309
step 251/334, epoch 236/501 --> loss:0.8148183941841125
step 301/334, epoch 236/501 --> loss:0.828479984998703
step 51/334, epoch 237/501 --> loss:0.8382411825656891
step 101/334, epoch 237/501 --> loss:0.8218550956249238
step 151/334, epoch 237/501 --> loss:0.8175990331172943
step 201/334, epoch 237/501 --> loss:0.8144643938541413
step 251/334, epoch 237/501 --> loss:0.8250220417976379
step 301/334, epoch 237/501 --> loss:0.8177722680568695
step 51/334, epoch 238/501 --> loss:0.8250447225570678
step 101/334, epoch 238/501 --> loss:0.8054657065868378
step 151/334, epoch 238/501 --> loss:0.8351809322834015
step 201/334, epoch 238/501 --> loss:0.8106289875507354
step 251/334, epoch 238/501 --> loss:0.8173196578025818
step 301/334, epoch 238/501 --> loss:0.8239754819869995
step 51/334, epoch 239/501 --> loss:0.8130531120300293
step 101/334, epoch 239/501 --> loss:0.8348716425895691
step 151/334, epoch 239/501 --> loss:0.8220027577877045
step 201/334, epoch 239/501 --> loss:0.833866446018219
step 251/334, epoch 239/501 --> loss:0.8096152508258819
step 301/334, epoch 239/501 --> loss:0.8051299059391022
step 51/334, epoch 240/501 --> loss:0.8200901973247529
step 101/334, epoch 240/501 --> loss:0.8263405179977417
step 151/334, epoch 240/501 --> loss:0.8094713187217712
step 201/334, epoch 240/501 --> loss:0.8241973829269409
step 251/334, epoch 240/501 --> loss:0.8207238829135894
step 301/334, epoch 240/501 --> loss:0.8350175666809082
step 51/334, epoch 241/501 --> loss:0.8361128902435303
step 101/334, epoch 241/501 --> loss:0.827345825433731
step 151/334, epoch 241/501 --> loss:0.8351256847381592
step 201/334, epoch 241/501 --> loss:0.8060397946834564
step 251/334, epoch 241/501 --> loss:0.8163931357860565
step 301/334, epoch 241/501 --> loss:0.8140665197372436

##########train dataset##########
acc--> [99.21033449462332]
F1--> {'F1': [0.9135398945188187], 'precision': [0.8485617121287486], 'recall': [0.9893062870378061]}
##########eval dataset##########
acc--> [98.42506201769287]
F1--> {'F1': [0.8304476667268508], 'precision': [0.776548143345035], 'recall': [0.8923989894998248]}
step 51/334, epoch 242/501 --> loss:0.8322893583774567
step 101/334, epoch 242/501 --> loss:0.8206889843940735
step 151/334, epoch 242/501 --> loss:0.8177501320838928
step 201/334, epoch 242/501 --> loss:0.821743780374527
step 251/334, epoch 242/501 --> loss:0.8137701296806336
step 301/334, epoch 242/501 --> loss:0.8297795820236206
step 51/334, epoch 243/501 --> loss:0.8086892449855805
step 101/334, epoch 243/501 --> loss:0.8344708847999572
step 151/334, epoch 243/501 --> loss:0.8098472416400909
step 201/334, epoch 243/501 --> loss:0.8299731278419494
step 251/334, epoch 243/501 --> loss:0.8359199023246765
step 301/334, epoch 243/501 --> loss:0.8122573590278626
step 51/334, epoch 244/501 --> loss:0.8269383347034455
step 101/334, epoch 244/501 --> loss:0.8136446678638458
step 151/334, epoch 244/501 --> loss:0.8193048620223999
step 201/334, epoch 244/501 --> loss:0.8230430674552918
step 251/334, epoch 244/501 --> loss:0.8165196454524994
step 301/334, epoch 244/501 --> loss:0.8225074422359466
step 51/334, epoch 245/501 --> loss:0.8322420477867126
step 101/334, epoch 245/501 --> loss:0.8132745254039765
step 151/334, epoch 245/501 --> loss:0.8217567598819733
step 201/334, epoch 245/501 --> loss:0.8135111010074616
step 251/334, epoch 245/501 --> loss:0.8226808726787567
step 301/334, epoch 245/501 --> loss:0.8164639854431153
step 51/334, epoch 246/501 --> loss:0.8186722254753113
step 101/334, epoch 246/501 --> loss:0.8132022106647492
step 151/334, epoch 246/501 --> loss:0.8159471070766449
step 201/334, epoch 246/501 --> loss:0.8173909223079682
step 251/334, epoch 246/501 --> loss:0.8254369223117828
step 301/334, epoch 246/501 --> loss:0.8320528137683868
step 51/334, epoch 247/501 --> loss:0.8238736772537232
step 101/334, epoch 247/501 --> loss:0.8278280675411225
step 151/334, epoch 247/501 --> loss:0.8169190156459808
step 201/334, epoch 247/501 --> loss:0.8185834968090058
step 251/334, epoch 247/501 --> loss:0.8259338045120239
step 301/334, epoch 247/501 --> loss:0.8154268872737884
step 51/334, epoch 248/501 --> loss:0.8276996386051177
step 101/334, epoch 248/501 --> loss:0.8268324601650238
step 151/334, epoch 248/501 --> loss:0.7998986756801605
step 201/334, epoch 248/501 --> loss:0.8227954912185669
step 251/334, epoch 248/501 --> loss:0.822745121717453
step 301/334, epoch 248/501 --> loss:0.8278755378723145
step 51/334, epoch 249/501 --> loss:0.817241051197052
step 101/334, epoch 249/501 --> loss:0.8414685785770416
step 151/334, epoch 249/501 --> loss:0.8316934192180634
step 201/334, epoch 249/501 --> loss:0.8278915977478027
step 251/334, epoch 249/501 --> loss:0.8199947416782379
step 301/334, epoch 249/501 --> loss:0.8112273526191711
step 51/334, epoch 250/501 --> loss:0.8193450474739075
step 101/334, epoch 250/501 --> loss:0.829952734708786
step 151/334, epoch 250/501 --> loss:0.8314370548725128
step 201/334, epoch 250/501 --> loss:0.8197655963897705
step 251/334, epoch 250/501 --> loss:0.8136487770080566
step 301/334, epoch 250/501 --> loss:0.8193380343914032
step 51/334, epoch 251/501 --> loss:0.8151818645000458
step 101/334, epoch 251/501 --> loss:0.8199280619621276
step 151/334, epoch 251/501 --> loss:0.8250576853752136
step 201/334, epoch 251/501 --> loss:0.821223440170288
step 251/334, epoch 251/501 --> loss:0.827615796327591
step 301/334, epoch 251/501 --> loss:0.818595461845398

##########train dataset##########
acc--> [99.47482303237202]
F1--> {'F1': [0.9408826069504795], 'precision': [0.8955255011900352], 'recall': [0.9910904492640205]}
##########eval dataset##########
acc--> [98.62026438207626]
F1--> {'F1': [0.8464197218179761], 'precision': [0.8155749071088415], 'recall': [0.8797001120257578]}
step 51/334, epoch 252/501 --> loss:0.8240024173259735
step 101/334, epoch 252/501 --> loss:0.8156551551818848
step 151/334, epoch 252/501 --> loss:0.8199167478084565
step 201/334, epoch 252/501 --> loss:0.8363865494728089
step 251/334, epoch 252/501 --> loss:0.8132745957374573
step 301/334, epoch 252/501 --> loss:0.8193054413795471
step 51/334, epoch 253/501 --> loss:0.8324628937244415
step 101/334, epoch 253/501 --> loss:0.8177265906333924
step 151/334, epoch 253/501 --> loss:0.8159571015834808
step 201/334, epoch 253/501 --> loss:0.8277643239498138
step 251/334, epoch 253/501 --> loss:0.8248690128326416
step 301/334, epoch 253/501 --> loss:0.8095523095130921
step 51/334, epoch 254/501 --> loss:0.8160297489166259
step 101/334, epoch 254/501 --> loss:0.8329658472537994
step 151/334, epoch 254/501 --> loss:0.8266208350658417
step 201/334, epoch 254/501 --> loss:0.8119880127906799
step 251/334, epoch 254/501 --> loss:0.8190919387340546
step 301/334, epoch 254/501 --> loss:0.8192671358585357
step 51/334, epoch 255/501 --> loss:0.7985277152061463
step 101/334, epoch 255/501 --> loss:0.8162370562553406
step 151/334, epoch 255/501 --> loss:0.8274179029464722
step 201/334, epoch 255/501 --> loss:0.8337418591976166
step 251/334, epoch 255/501 --> loss:0.819008229970932
step 301/334, epoch 255/501 --> loss:0.8227991580963134
step 51/334, epoch 256/501 --> loss:0.8145908164978027
step 101/334, epoch 256/501 --> loss:0.8248702299594879
step 151/334, epoch 256/501 --> loss:0.8264888012409211
step 201/334, epoch 256/501 --> loss:0.8308062207698822
step 251/334, epoch 256/501 --> loss:0.8088798272609711
step 301/334, epoch 256/501 --> loss:0.8183900475502014
step 51/334, epoch 257/501 --> loss:0.8188939702510833
step 101/334, epoch 257/501 --> loss:0.8150954556465149
step 151/334, epoch 257/501 --> loss:0.8289709413051605
step 201/334, epoch 257/501 --> loss:0.8287079310417176
step 251/334, epoch 257/501 --> loss:0.8179820394515991
step 301/334, epoch 257/501 --> loss:0.8208669340610504
step 51/334, epoch 258/501 --> loss:0.8185979962348938
step 101/334, epoch 258/501 --> loss:0.8065202617645264
step 151/334, epoch 258/501 --> loss:0.8184169912338257
step 201/334, epoch 258/501 --> loss:0.8269080281257629
step 251/334, epoch 258/501 --> loss:0.8258350229263306
step 301/334, epoch 258/501 --> loss:0.821271207332611
step 51/334, epoch 259/501 --> loss:0.818349928855896
step 101/334, epoch 259/501 --> loss:0.8251248443126679
step 151/334, epoch 259/501 --> loss:0.8224614775180816
step 201/334, epoch 259/501 --> loss:0.8216583502292633
step 251/334, epoch 259/501 --> loss:0.8153460276126862
step 301/334, epoch 259/501 --> loss:0.8179887700080871
step 51/334, epoch 260/501 --> loss:0.8179990303516388
step 101/334, epoch 260/501 --> loss:0.8121331524848938
step 151/334, epoch 260/501 --> loss:0.8190739142894745
step 201/334, epoch 260/501 --> loss:0.8256860446929931
step 251/334, epoch 260/501 --> loss:0.8235714519023896
step 301/334, epoch 260/501 --> loss:0.8172012090682983
step 51/334, epoch 261/501 --> loss:0.8236393225193024
step 101/334, epoch 261/501 --> loss:0.8170299518108368
step 151/334, epoch 261/501 --> loss:0.8082599687576294
step 201/334, epoch 261/501 --> loss:0.8227032721042633
step 251/334, epoch 261/501 --> loss:0.8301907205581665
step 301/334, epoch 261/501 --> loss:0.8135851442813873

##########train dataset##########
acc--> [99.6489528774695]
F1--> {'F1': [0.9597372881844525], 'precision': [0.9293004002225033], 'recall': [0.9922461321720888]}
##########eval dataset##########
acc--> [98.8216777277469]
F1--> {'F1': [0.8635101019566338], 'precision': [0.8646082554738881], 'recall': [0.8624247090972653]}
save model!
step 51/334, epoch 262/501 --> loss:0.8080612552165986
step 101/334, epoch 262/501 --> loss:0.829216126203537
step 151/334, epoch 262/501 --> loss:0.8206154143810273
step 201/334, epoch 262/501 --> loss:0.8190646255016327
step 251/334, epoch 262/501 --> loss:0.8209813714027405
step 301/334, epoch 262/501 --> loss:0.8226326930522919
step 51/334, epoch 263/501 --> loss:0.8158602988719941
step 101/334, epoch 263/501 --> loss:0.8152188313007355
step 151/334, epoch 263/501 --> loss:0.8397744762897491
step 201/334, epoch 263/501 --> loss:0.8142485344409942
step 251/334, epoch 263/501 --> loss:0.8094155311584472
step 301/334, epoch 263/501 --> loss:0.8350675082206727
step 51/334, epoch 264/501 --> loss:0.8183226215839386
step 101/334, epoch 264/501 --> loss:0.8323625791072845
step 151/334, epoch 264/501 --> loss:0.8207047283649445
step 201/334, epoch 264/501 --> loss:0.8290898621082305
step 251/334, epoch 264/501 --> loss:0.8219525015354157
step 301/334, epoch 264/501 --> loss:0.7956224381923676
step 51/334, epoch 265/501 --> loss:0.8194629693031311
step 101/334, epoch 265/501 --> loss:0.8152299475669861
step 151/334, epoch 265/501 --> loss:0.8236232852935791
step 201/334, epoch 265/501 --> loss:0.8132527196407318
step 251/334, epoch 265/501 --> loss:0.8255909669399262
step 301/334, epoch 265/501 --> loss:0.8317029988765716
step 51/334, epoch 266/501 --> loss:0.825057077407837
step 101/334, epoch 266/501 --> loss:0.8178453207015991
step 151/334, epoch 266/501 --> loss:0.8260807263851165
step 201/334, epoch 266/501 --> loss:0.8044329369068146
step 251/334, epoch 266/501 --> loss:0.8173080611228943
step 301/334, epoch 266/501 --> loss:0.8285085082054138
step 51/334, epoch 267/501 --> loss:0.8192144775390625
step 101/334, epoch 267/501 --> loss:0.8257566094398499
step 151/334, epoch 267/501 --> loss:0.8141670334339142
step 201/334, epoch 267/501 --> loss:0.8062595629692078
step 251/334, epoch 267/501 --> loss:0.818529132604599
step 301/334, epoch 267/501 --> loss:0.8263425779342651
step 51/334, epoch 268/501 --> loss:0.8257954668998718
step 101/334, epoch 268/501 --> loss:0.8167858612537384
step 151/334, epoch 268/501 --> loss:0.8116088664531708
step 201/334, epoch 268/501 --> loss:0.8321200394630432
step 251/334, epoch 268/501 --> loss:0.8228972256183624
step 301/334, epoch 268/501 --> loss:0.8064626932144165
step 51/334, epoch 269/501 --> loss:0.8243660271167755
step 101/334, epoch 269/501 --> loss:0.8217362058162689
step 151/334, epoch 269/501 --> loss:0.826519523859024
step 201/334, epoch 269/501 --> loss:0.8242272186279297
step 251/334, epoch 269/501 --> loss:0.8227641177177429
step 301/334, epoch 269/501 --> loss:0.817190613746643
step 51/334, epoch 270/501 --> loss:0.8294836175441742
step 101/334, epoch 270/501 --> loss:0.8306722915172577
step 151/334, epoch 270/501 --> loss:0.8195351719856262
step 201/334, epoch 270/501 --> loss:0.8184658086299896
step 251/334, epoch 270/501 --> loss:0.8119551122188569
step 301/334, epoch 270/501 --> loss:0.8077101969718933
step 51/334, epoch 271/501 --> loss:0.8162886607646942
step 101/334, epoch 271/501 --> loss:0.8356128680706024
step 151/334, epoch 271/501 --> loss:0.8274443876743317
step 201/334, epoch 271/501 --> loss:0.8110648047924042
step 251/334, epoch 271/501 --> loss:0.8108708465099335
step 301/334, epoch 271/501 --> loss:0.8237896645069123

##########train dataset##########
acc--> [99.50721863730982]
F1--> {'F1': [0.9444906830637254], 'precision': [0.8995205626322524], 'recall': [0.9942048983664529]}
##########eval dataset##########
acc--> [98.62059946134615]
F1--> {'F1': [0.8468137638942064], 'precision': [0.8142006986335982], 'recall': [0.8821593342480791]}
step 51/334, epoch 272/501 --> loss:0.8162878894805908
step 101/334, epoch 272/501 --> loss:0.8134671592712402
step 151/334, epoch 272/501 --> loss:0.8267191445827484
step 201/334, epoch 272/501 --> loss:0.8242830550670623
step 251/334, epoch 272/501 --> loss:0.8142487156391144
step 301/334, epoch 272/501 --> loss:0.831960735321045
step 51/334, epoch 273/501 --> loss:0.8289154243469238
step 101/334, epoch 273/501 --> loss:0.8172918426990509
step 151/334, epoch 273/501 --> loss:0.8302384197711945
step 201/334, epoch 273/501 --> loss:0.8092011821269989
step 251/334, epoch 273/501 --> loss:0.8141710793972016
step 301/334, epoch 273/501 --> loss:0.8102608811855316
step 51/334, epoch 274/501 --> loss:0.8173574471473694
step 101/334, epoch 274/501 --> loss:0.8164814817905426
step 151/334, epoch 274/501 --> loss:0.8169811367988586
step 201/334, epoch 274/501 --> loss:0.820397402048111
step 251/334, epoch 274/501 --> loss:0.8227321195602417
step 301/334, epoch 274/501 --> loss:0.8279848837852478
step 51/334, epoch 275/501 --> loss:0.8202036988735198
step 101/334, epoch 275/501 --> loss:0.8227848684787751
step 151/334, epoch 275/501 --> loss:0.829657974243164
step 201/334, epoch 275/501 --> loss:0.8242945384979248
step 251/334, epoch 275/501 --> loss:0.8120267748832702
step 301/334, epoch 275/501 --> loss:0.8258091115951538
step 51/334, epoch 276/501 --> loss:0.8284342050552368
step 101/334, epoch 276/501 --> loss:0.8268273282051086
step 151/334, epoch 276/501 --> loss:0.8155945801734924
step 201/334, epoch 276/501 --> loss:0.8183367621898651
step 251/334, epoch 276/501 --> loss:0.8206191170215607
step 301/334, epoch 276/501 --> loss:0.8190426516532898
step 51/334, epoch 277/501 --> loss:0.8068753576278687
step 101/334, epoch 277/501 --> loss:0.8332779228687286
step 151/334, epoch 277/501 --> loss:0.8094721961021424
step 201/334, epoch 277/501 --> loss:0.8239420068264007
step 251/334, epoch 277/501 --> loss:0.8216655743122101
step 301/334, epoch 277/501 --> loss:0.8250480592250824
step 51/334, epoch 278/501 --> loss:0.8333933281898499
step 101/334, epoch 278/501 --> loss:0.8164753973484039
step 151/334, epoch 278/501 --> loss:0.8275432312488555
step 201/334, epoch 278/501 --> loss:0.8111311411857605
step 251/334, epoch 278/501 --> loss:0.8309057438373566
step 301/334, epoch 278/501 --> loss:0.8051536703109741
step 51/334, epoch 279/501 --> loss:0.8281172525882721
step 101/334, epoch 279/501 --> loss:0.8108455061912536
step 151/334, epoch 279/501 --> loss:0.8155651450157165
step 201/334, epoch 279/501 --> loss:0.8330047440528869
step 251/334, epoch 279/501 --> loss:0.8227131736278533
step 301/334, epoch 279/501 --> loss:0.8052091872692109
step 51/334, epoch 280/501 --> loss:0.8119177448749543
step 101/334, epoch 280/501 --> loss:0.825620414018631
step 151/334, epoch 280/501 --> loss:0.8281125938892364
step 201/334, epoch 280/501 --> loss:0.8263888347148896
step 251/334, epoch 280/501 --> loss:0.8197578990459442
step 301/334, epoch 280/501 --> loss:0.8184862875938416
step 51/334, epoch 281/501 --> loss:0.8109858798980712
step 101/334, epoch 281/501 --> loss:0.8298548293113709
step 151/334, epoch 281/501 --> loss:0.8174931907653809
step 201/334, epoch 281/501 --> loss:0.8166786134243011
step 251/334, epoch 281/501 --> loss:0.821684228181839
step 301/334, epoch 281/501 --> loss:0.830075273513794

##########train dataset##########
acc--> [99.66644412978366]
F1--> {'F1': [0.9617250952610551], 'precision': [0.9316373689376367], 'recall': [0.9938317451307139]}
##########eval dataset##########
acc--> [98.8033491586801]
F1--> {'F1': [0.8614587947828144], 'precision': [0.8621073386073375], 'recall': [0.8608212109602167]}
step 51/334, epoch 282/501 --> loss:0.8201559937000275
step 101/334, epoch 282/501 --> loss:0.8204895985126496
step 151/334, epoch 282/501 --> loss:0.8149879848957062
step 201/334, epoch 282/501 --> loss:0.8148887658119202
step 251/334, epoch 282/501 --> loss:0.8209352660179138
step 301/334, epoch 282/501 --> loss:0.8243952882289887
step 51/334, epoch 283/501 --> loss:0.8229251646995545
step 101/334, epoch 283/501 --> loss:0.8327385890483856
step 151/334, epoch 283/501 --> loss:0.8171125745773316
step 201/334, epoch 283/501 --> loss:0.8107384502887726
step 251/334, epoch 283/501 --> loss:0.8142855548858643
step 301/334, epoch 283/501 --> loss:0.8183060133457184
step 51/334, epoch 284/501 --> loss:0.8329463601112366
step 101/334, epoch 284/501 --> loss:0.8146047294139862
step 151/334, epoch 284/501 --> loss:0.80029296875
step 201/334, epoch 284/501 --> loss:0.8313603103160858
step 251/334, epoch 284/501 --> loss:0.8215268409252167
step 301/334, epoch 284/501 --> loss:0.8290649306774139
step 51/334, epoch 285/501 --> loss:0.8183611786365509
step 101/334, epoch 285/501 --> loss:0.8260515940189361
step 151/334, epoch 285/501 --> loss:0.7957506728172302
step 201/334, epoch 285/501 --> loss:0.8348747932910919
step 251/334, epoch 285/501 --> loss:0.8203889298439025
step 301/334, epoch 285/501 --> loss:0.8205943930149079
step 51/334, epoch 286/501 --> loss:0.8068365621566772
step 101/334, epoch 286/501 --> loss:0.8217596733570098
step 151/334, epoch 286/501 --> loss:0.8222839987277984
step 201/334, epoch 286/501 --> loss:0.819054811000824
step 251/334, epoch 286/501 --> loss:0.8180905628204346
step 301/334, epoch 286/501 --> loss:0.8299441075325013
step 51/334, epoch 287/501 --> loss:0.8277847397327424
step 101/334, epoch 287/501 --> loss:0.818309977054596
step 151/334, epoch 287/501 --> loss:0.8145172607898712
step 201/334, epoch 287/501 --> loss:0.8004882669448853
step 251/334, epoch 287/501 --> loss:0.8295099115371705
step 301/334, epoch 287/501 --> loss:0.8196915066242219
step 51/334, epoch 288/501 --> loss:0.8223565483093261
step 101/334, epoch 288/501 --> loss:0.818786336183548
step 151/334, epoch 288/501 --> loss:0.8183322143554688
step 201/334, epoch 288/501 --> loss:0.8237035167217255
step 251/334, epoch 288/501 --> loss:0.8156786680221557
step 301/334, epoch 288/501 --> loss:0.8178329503536225
step 51/334, epoch 289/501 --> loss:0.8266101479530334
step 101/334, epoch 289/501 --> loss:0.8052966034412384
step 151/334, epoch 289/501 --> loss:0.8186021995544434
step 201/334, epoch 289/501 --> loss:0.8194544184207916
step 251/334, epoch 289/501 --> loss:0.8302267456054687
step 301/334, epoch 289/501 --> loss:0.8220516729354859
step 51/334, epoch 290/501 --> loss:0.8092349600791932
step 101/334, epoch 290/501 --> loss:0.8226145422458648
step 151/334, epoch 290/501 --> loss:0.8203258967399597
step 201/334, epoch 290/501 --> loss:0.8364784705638886
step 251/334, epoch 290/501 --> loss:0.8114793467521667
step 301/334, epoch 290/501 --> loss:0.822836571931839
step 51/334, epoch 291/501 --> loss:0.8219615161418915
step 101/334, epoch 291/501 --> loss:0.8262351441383362
step 151/334, epoch 291/501 --> loss:0.8132750725746155
step 201/334, epoch 291/501 --> loss:0.8124641537666321
step 251/334, epoch 291/501 --> loss:0.8154334557056427
step 301/334, epoch 291/501 --> loss:0.8279972624778748

##########train dataset##########
acc--> [99.59741541630132]
F1--> {'F1': [0.9541975965288265], 'precision': [0.9170427090105189], 'recall': [0.9945011960093737]}
##########eval dataset##########
acc--> [98.67702694389104]
F1--> {'F1': [0.8516961259006619], 'precision': [0.8260767523884233], 'recall': [0.8789660826849707]}
step 51/334, epoch 292/501 --> loss:0.8017171013355255
step 101/334, epoch 292/501 --> loss:0.8225878763198853
step 151/334, epoch 292/501 --> loss:0.820156625509262
step 201/334, epoch 292/501 --> loss:0.8181518495082856
step 251/334, epoch 292/501 --> loss:0.8392917156219483
step 301/334, epoch 292/501 --> loss:0.8222959446907043
step 51/334, epoch 293/501 --> loss:0.8328946626186371
step 101/334, epoch 293/501 --> loss:0.8090125155448914
step 151/334, epoch 293/501 --> loss:0.8091821956634522
step 201/334, epoch 293/501 --> loss:0.8253275549411774
step 251/334, epoch 293/501 --> loss:0.827900698184967
step 301/334, epoch 293/501 --> loss:0.8195756185054779
step 51/334, epoch 294/501 --> loss:0.8130847299098969
step 101/334, epoch 294/501 --> loss:0.8231309950351715
step 151/334, epoch 294/501 --> loss:0.8262929201126099
step 201/334, epoch 294/501 --> loss:0.8168330192565918
step 251/334, epoch 294/501 --> loss:0.8238106429576874
step 301/334, epoch 294/501 --> loss:0.8117261004447937
step 51/334, epoch 295/501 --> loss:0.8265061855316163
step 101/334, epoch 295/501 --> loss:0.8228419780731201
step 151/334, epoch 295/501 --> loss:0.8055728054046631
step 201/334, epoch 295/501 --> loss:0.8260559356212616
step 251/334, epoch 295/501 --> loss:0.8260774159431458
step 301/334, epoch 295/501 --> loss:0.8145930695533753
step 51/334, epoch 296/501 --> loss:0.8132321095466614
step 101/334, epoch 296/501 --> loss:0.8105724370479583
step 151/334, epoch 296/501 --> loss:0.8146086812019349
step 201/334, epoch 296/501 --> loss:0.8248248612880706
step 251/334, epoch 296/501 --> loss:0.8149659585952759
step 301/334, epoch 296/501 --> loss:0.8375183820724488
step 51/334, epoch 297/501 --> loss:0.813689843416214
step 101/334, epoch 297/501 --> loss:0.8137194085121154
step 151/334, epoch 297/501 --> loss:0.821456093788147
step 201/334, epoch 297/501 --> loss:0.8245191502571106
step 251/334, epoch 297/501 --> loss:0.813393634557724
step 301/334, epoch 297/501 --> loss:0.8265541756153106
step 51/334, epoch 298/501 --> loss:0.8133794558048248
step 101/334, epoch 298/501 --> loss:0.8191574358940125
step 151/334, epoch 298/501 --> loss:0.8150088119506836
step 201/334, epoch 298/501 --> loss:0.8095058131217957
step 251/334, epoch 298/501 --> loss:0.8252785062789917
step 301/334, epoch 298/501 --> loss:0.8197912442684173
step 51/334, epoch 299/501 --> loss:0.8056377601623536
step 101/334, epoch 299/501 --> loss:0.8346216785907745
step 151/334, epoch 299/501 --> loss:0.8206354939937591
step 201/334, epoch 299/501 --> loss:0.812937684059143
step 251/334, epoch 299/501 --> loss:0.8262110245227814
step 301/334, epoch 299/501 --> loss:0.8200178289413452
step 51/334, epoch 300/501 --> loss:0.8256779217720032
step 101/334, epoch 300/501 --> loss:0.8112883365154266
step 151/334, epoch 300/501 --> loss:0.8168699955940246
step 201/334, epoch 300/501 --> loss:0.8283688414096833
step 251/334, epoch 300/501 --> loss:0.8194095766544343
step 301/334, epoch 300/501 --> loss:0.8172124302387238
step 51/334, epoch 301/501 --> loss:0.8214774608612061
step 101/334, epoch 301/501 --> loss:0.8206012058258056
step 151/334, epoch 301/501 --> loss:0.8264508855342865
step 201/334, epoch 301/501 --> loss:0.8232100582122803
step 251/334, epoch 301/501 --> loss:0.82093705534935
step 301/334, epoch 301/501 --> loss:0.8228228962421418

##########train dataset##########
acc--> [99.5997285503163]
F1--> {'F1': [0.9543622607785978], 'precision': [0.9190336846005667], 'recall': [0.9925263532258676]}
##########eval dataset##########
acc--> [98.73869621429718]
F1--> {'F1': [0.8572480220012642], 'precision': [0.839052036769289], 'recall': [0.876261154449354]}
step 51/334, epoch 302/501 --> loss:0.8054878401756287
step 101/334, epoch 302/501 --> loss:0.8137803328037262
step 151/334, epoch 302/501 --> loss:0.8261453855037689
step 201/334, epoch 302/501 --> loss:0.8282814884185791
step 251/334, epoch 302/501 --> loss:0.8156705701351166
step 301/334, epoch 302/501 --> loss:0.8317802846431732
step 51/334, epoch 303/501 --> loss:0.8132825994491577
step 101/334, epoch 303/501 --> loss:0.8185236704349518
step 151/334, epoch 303/501 --> loss:0.8113707053661346
step 201/334, epoch 303/501 --> loss:0.8283409070968628
step 251/334, epoch 303/501 --> loss:0.8252218961715698
step 301/334, epoch 303/501 --> loss:0.8177941262722015
step 51/334, epoch 304/501 --> loss:0.8297089970111847
step 101/334, epoch 304/501 --> loss:0.835368857383728
step 151/334, epoch 304/501 --> loss:0.8104954993724823
step 201/334, epoch 304/501 --> loss:0.8161246943473816
step 251/334, epoch 304/501 --> loss:0.8075214016437531
step 301/334, epoch 304/501 --> loss:0.8229886710643768
step 51/334, epoch 305/501 --> loss:0.8255250477790832
step 101/334, epoch 305/501 --> loss:0.8300247812271118
step 151/334, epoch 305/501 --> loss:0.8172041451931
step 201/334, epoch 305/501 --> loss:0.8055922698974609
step 251/334, epoch 305/501 --> loss:0.820902841091156
step 301/334, epoch 305/501 --> loss:0.8111550891399384
step 51/334, epoch 306/501 --> loss:0.8266947460174561
step 101/334, epoch 306/501 --> loss:0.8160481584072113
step 151/334, epoch 306/501 --> loss:0.8281733059883117
step 201/334, epoch 306/501 --> loss:0.826076889038086
step 251/334, epoch 306/501 --> loss:0.820472742319107
step 301/334, epoch 306/501 --> loss:0.8127621221542358
step 51/334, epoch 307/501 --> loss:0.8313906109333038
step 101/334, epoch 307/501 --> loss:0.8146528935432434
step 151/334, epoch 307/501 --> loss:0.8025293850898743
step 201/334, epoch 307/501 --> loss:0.8282751226425171
step 251/334, epoch 307/501 --> loss:0.820281810760498
step 301/334, epoch 307/501 --> loss:0.814925285577774
step 51/334, epoch 308/501 --> loss:0.8303235185146332
step 101/334, epoch 308/501 --> loss:0.8162147796154022
step 151/334, epoch 308/501 --> loss:0.8116866278648377
step 201/334, epoch 308/501 --> loss:0.8234003758430481
step 251/334, epoch 308/501 --> loss:0.8161456286907196
step 301/334, epoch 308/501 --> loss:0.8257819247245789
step 51/334, epoch 309/501 --> loss:0.8244109404087067
step 101/334, epoch 309/501 --> loss:0.8090044987201691
step 151/334, epoch 309/501 --> loss:0.8428405833244323
step 201/334, epoch 309/501 --> loss:0.8099045073986053
step 251/334, epoch 309/501 --> loss:0.827933201789856
step 301/334, epoch 309/501 --> loss:0.8116474831104279
step 51/334, epoch 310/501 --> loss:0.8191565287113189
step 101/334, epoch 310/501 --> loss:0.8234143114089966
step 151/334, epoch 310/501 --> loss:0.8169354164600372
step 201/334, epoch 310/501 --> loss:0.8229705607891082
step 251/334, epoch 310/501 --> loss:0.8394116830825805
step 301/334, epoch 310/501 --> loss:0.8142666006088257
step 51/334, epoch 311/501 --> loss:0.816750408411026
step 101/334, epoch 311/501 --> loss:0.8019189047813415
step 151/334, epoch 311/501 --> loss:0.8127812922000885
step 201/334, epoch 311/501 --> loss:0.8318237864971161
step 251/334, epoch 311/501 --> loss:0.8339624869823455
step 301/334, epoch 311/501 --> loss:0.8335725343227387

##########train dataset##########
acc--> [99.70828479356055]
F1--> {'F1': [0.9663972676085912], 'precision': [0.9395405227322253], 'recall': [0.9948451807500449]}
##########eval dataset##########
acc--> [98.78749830724234]
F1--> {'F1': [0.8608217547465211], 'precision': [0.8541689441891189], 'recall': [0.8675891684251131]}
step 51/334, epoch 312/501 --> loss:0.8070918798446656
step 101/334, epoch 312/501 --> loss:0.8219760620594024
step 151/334, epoch 312/501 --> loss:0.8273360025882721
step 201/334, epoch 312/501 --> loss:0.8133984994888306
step 251/334, epoch 312/501 --> loss:0.8154439520835877
step 301/334, epoch 312/501 --> loss:0.8308232450485229
step 51/334, epoch 313/501 --> loss:0.8263658905029296
step 101/334, epoch 313/501 --> loss:0.8116580581665039
step 151/334, epoch 313/501 --> loss:0.8171147155761719
step 201/334, epoch 313/501 --> loss:0.8353224503993988
step 251/334, epoch 313/501 --> loss:0.8090895760059357
step 301/334, epoch 313/501 --> loss:0.825399204492569
step 51/334, epoch 314/501 --> loss:0.814404239654541
step 101/334, epoch 314/501 --> loss:0.8224662506580352
step 151/334, epoch 314/501 --> loss:0.8184693264961242
step 201/334, epoch 314/501 --> loss:0.8166209626197815
step 251/334, epoch 314/501 --> loss:0.81762371301651
step 301/334, epoch 314/501 --> loss:0.822065862417221
step 51/334, epoch 315/501 --> loss:0.8145401477813721
step 101/334, epoch 315/501 --> loss:0.8091234755516052
step 151/334, epoch 315/501 --> loss:0.8172741973400116
step 201/334, epoch 315/501 --> loss:0.8309650754928589
step 251/334, epoch 315/501 --> loss:0.8219192481040954
step 301/334, epoch 315/501 --> loss:0.831839029788971
step 51/334, epoch 316/501 --> loss:0.8147990989685059
step 101/334, epoch 316/501 --> loss:0.8287912487983704
step 151/334, epoch 316/501 --> loss:0.8255375564098358
step 201/334, epoch 316/501 --> loss:0.816974937915802
step 251/334, epoch 316/501 --> loss:0.8367526543140411
step 301/334, epoch 316/501 --> loss:0.8070533001422882
step 51/334, epoch 317/501 --> loss:0.8200631415843964
step 101/334, epoch 317/501 --> loss:0.8238815605640412
step 151/334, epoch 317/501 --> loss:0.8142075431346893
step 201/334, epoch 317/501 --> loss:0.8268785321712494
step 251/334, epoch 317/501 --> loss:0.8153366577625275
step 301/334, epoch 317/501 --> loss:0.8275860607624054
step 51/334, epoch 318/501 --> loss:0.8250422883033752
step 101/334, epoch 318/501 --> loss:0.8094913673400879
step 151/334, epoch 318/501 --> loss:0.8188086438179016
step 201/334, epoch 318/501 --> loss:0.8165737020969391
step 251/334, epoch 318/501 --> loss:0.8324935960769654
step 301/334, epoch 318/501 --> loss:0.8177113437652588
step 51/334, epoch 319/501 --> loss:0.8252408969402313
step 101/334, epoch 319/501 --> loss:0.8146613240242004
step 151/334, epoch 319/501 --> loss:0.8107426071166992
step 201/334, epoch 319/501 --> loss:0.8223467290401458
step 251/334, epoch 319/501 --> loss:0.8238113284111023
step 301/334, epoch 319/501 --> loss:0.8269212317466735
step 51/334, epoch 320/501 --> loss:0.8052419745922088
step 101/334, epoch 320/501 --> loss:0.8297828137874603
step 151/334, epoch 320/501 --> loss:0.8333727252483368
step 201/334, epoch 320/501 --> loss:0.8206037271022797
step 251/334, epoch 320/501 --> loss:0.8060760402679443
step 301/334, epoch 320/501 --> loss:0.8212407851219177
step 51/334, epoch 321/501 --> loss:0.8139128792285919
step 101/334, epoch 321/501 --> loss:0.8333067405223846
step 151/334, epoch 321/501 --> loss:0.8152135014533997
step 201/334, epoch 321/501 --> loss:0.8190459048748017
step 251/334, epoch 321/501 --> loss:0.828894715309143
step 301/334, epoch 321/501 --> loss:0.8158517551422119

##########train dataset##########
acc--> [99.71141264507179]
F1--> {'F1': [0.9667505938471889], 'precision': [0.9400688145811621], 'recall': [0.9950018087851877]}
##########eval dataset##########
acc--> [98.82691751314239]
F1--> {'F1': [0.8652955292650797], 'precision': [0.8589273902848256], 'recall': [0.8717689504803209]}
save model!
step 51/334, epoch 322/501 --> loss:0.8257584619522095
step 101/334, epoch 322/501 --> loss:0.8126308262348175
step 151/334, epoch 322/501 --> loss:0.8219577467441559
step 201/334, epoch 322/501 --> loss:0.8157398843765259
step 251/334, epoch 322/501 --> loss:0.8255776059627533
step 301/334, epoch 322/501 --> loss:0.820931648015976
step 51/334, epoch 323/501 --> loss:0.8203710234165191
step 101/334, epoch 323/501 --> loss:0.8229103600978851
step 151/334, epoch 323/501 --> loss:0.8265153133869171
step 201/334, epoch 323/501 --> loss:0.814948914051056
step 251/334, epoch 323/501 --> loss:0.819064998626709
step 301/334, epoch 323/501 --> loss:0.808028861284256
step 51/334, epoch 324/501 --> loss:0.8230611932277679
step 101/334, epoch 324/501 --> loss:0.8106003046035767
step 151/334, epoch 324/501 --> loss:0.8318933534622193
step 201/334, epoch 324/501 --> loss:0.8038971650600434
step 251/334, epoch 324/501 --> loss:0.8151940488815308
step 301/334, epoch 324/501 --> loss:0.8317273831367493
step 51/334, epoch 325/501 --> loss:0.819023288488388
step 101/334, epoch 325/501 --> loss:0.8159419918060302
step 151/334, epoch 325/501 --> loss:0.8291249048709869
step 201/334, epoch 325/501 --> loss:0.8200387871265411
step 251/334, epoch 325/501 --> loss:0.8153096556663513
step 301/334, epoch 325/501 --> loss:0.8243066847324372
step 51/334, epoch 326/501 --> loss:0.8129190707206726
step 101/334, epoch 326/501 --> loss:0.8319087088108063
step 151/334, epoch 326/501 --> loss:0.8109457457065582
step 201/334, epoch 326/501 --> loss:0.8222906351089477
step 251/334, epoch 326/501 --> loss:0.8138858687877655
step 301/334, epoch 326/501 --> loss:0.823060473203659
step 51/334, epoch 327/501 --> loss:0.818048529624939
step 101/334, epoch 327/501 --> loss:0.8283017575740814
step 151/334, epoch 327/501 --> loss:0.8147710478305816
step 201/334, epoch 327/501 --> loss:0.8181903970241546
step 251/334, epoch 327/501 --> loss:0.8157891559600831
step 301/334, epoch 327/501 --> loss:0.8213374507427216
step 51/334, epoch 328/501 --> loss:0.826613495349884
step 101/334, epoch 328/501 --> loss:0.8380709743499756
step 151/334, epoch 328/501 --> loss:0.806090567111969
step 201/334, epoch 328/501 --> loss:0.8132577002048492
step 251/334, epoch 328/501 --> loss:0.8224815285205841
step 301/334, epoch 328/501 --> loss:0.8111540853977204
step 51/334, epoch 329/501 --> loss:0.8173919451236725
step 101/334, epoch 329/501 --> loss:0.8212409543991089
step 151/334, epoch 329/501 --> loss:0.8363291943073272
step 201/334, epoch 329/501 --> loss:0.8277695679664612
step 251/334, epoch 329/501 --> loss:0.8057659637928009
step 301/334, epoch 329/501 --> loss:0.8181441950798035
step 51/334, epoch 330/501 --> loss:0.8307945775985718
step 101/334, epoch 330/501 --> loss:0.8025036978721619
step 151/334, epoch 330/501 --> loss:0.8212106263637543
step 201/334, epoch 330/501 --> loss:0.8185674500465393
step 251/334, epoch 330/501 --> loss:0.8275261521339417
step 301/334, epoch 330/501 --> loss:0.834755744934082
step 51/334, epoch 331/501 --> loss:0.820724869966507
step 101/334, epoch 331/501 --> loss:0.8202439022064208
step 151/334, epoch 331/501 --> loss:0.8102151012420654
step 201/334, epoch 331/501 --> loss:0.8238001680374145
step 251/334, epoch 331/501 --> loss:0.830027312040329
step 301/334, epoch 331/501 --> loss:0.8121799838542938

##########train dataset##########
acc--> [99.69830164356611]
F1--> {'F1': [0.9653243116373436], 'precision': [0.9365416387881037], 'recall': [0.9959428658365249]}
##########eval dataset##########
acc--> [98.76942471881517]
F1--> {'F1': [0.8604592628323435], 'precision': [0.8437406858966479], 'recall': [0.877864189281538]}
step 51/334, epoch 332/501 --> loss:0.8216100990772247
step 101/334, epoch 332/501 --> loss:0.8264189004898072
step 151/334, epoch 332/501 --> loss:0.8206404423713685
step 201/334, epoch 332/501 --> loss:0.8094963788986206
step 251/334, epoch 332/501 --> loss:0.8190205645561218
step 301/334, epoch 332/501 --> loss:0.8288250851631165
step 51/334, epoch 333/501 --> loss:0.8191201794147491
step 101/334, epoch 333/501 --> loss:0.8310093426704407
step 151/334, epoch 333/501 --> loss:0.8114386343955994
step 201/334, epoch 333/501 --> loss:0.8205163884162903
step 251/334, epoch 333/501 --> loss:0.8200349092483521
step 301/334, epoch 333/501 --> loss:0.822158727645874
step 51/334, epoch 334/501 --> loss:0.8323547744750976
step 101/334, epoch 334/501 --> loss:0.8216092884540558
step 151/334, epoch 334/501 --> loss:0.8128539443016052
step 201/334, epoch 334/501 --> loss:0.823570146560669
step 251/334, epoch 334/501 --> loss:0.8256000351905822
step 301/334, epoch 334/501 --> loss:0.8145367050170899
step 51/334, epoch 335/501 --> loss:0.8229397523403168
step 101/334, epoch 335/501 --> loss:0.8243375015258789
step 151/334, epoch 335/501 --> loss:0.8184631490707397
step 201/334, epoch 335/501 --> loss:0.8129487729072571
step 251/334, epoch 335/501 --> loss:0.8097568261623382
step 301/334, epoch 335/501 --> loss:0.8292377293109894
step 51/334, epoch 336/501 --> loss:0.8258483815193176
step 101/334, epoch 336/501 --> loss:0.8091633093357086
step 151/334, epoch 336/501 --> loss:0.8307517158985138
step 201/334, epoch 336/501 --> loss:0.8045025599002839
step 251/334, epoch 336/501 --> loss:0.8275330686569213
step 301/334, epoch 336/501 --> loss:0.8154712104797364
step 51/334, epoch 337/501 --> loss:0.8276148664951325
step 101/334, epoch 337/501 --> loss:0.824743822813034
step 151/334, epoch 337/501 --> loss:0.8222952735424042
step 201/334, epoch 337/501 --> loss:0.8243974483013153
step 251/334, epoch 337/501 --> loss:0.8102620697021484
step 301/334, epoch 337/501 --> loss:0.8170590138435364
step 51/334, epoch 338/501 --> loss:0.8059272539615631
step 101/334, epoch 338/501 --> loss:0.8179990673065185
step 151/334, epoch 338/501 --> loss:0.8237720263004303
step 201/334, epoch 338/501 --> loss:0.8193625986576081
step 251/334, epoch 338/501 --> loss:0.8209388279914855
step 301/334, epoch 338/501 --> loss:0.8376545083522796
step 51/334, epoch 339/501 --> loss:0.8013786375522614
step 101/334, epoch 339/501 --> loss:0.8190223002433776
step 151/334, epoch 339/501 --> loss:0.8315432250499726
step 201/334, epoch 339/501 --> loss:0.8303827166557312
step 251/334, epoch 339/501 --> loss:0.8331074357032776
step 301/334, epoch 339/501 --> loss:0.8063051652908325
step 51/334, epoch 340/501 --> loss:0.8037578773498535
step 101/334, epoch 340/501 --> loss:0.8162594401836395
step 151/334, epoch 340/501 --> loss:0.8295513725280762
step 201/334, epoch 340/501 --> loss:0.8180476367473603
step 251/334, epoch 340/501 --> loss:0.8239708459377288
step 301/334, epoch 340/501 --> loss:0.8345653486251831
step 51/334, epoch 341/501 --> loss:0.8173603892326355
step 101/334, epoch 341/501 --> loss:0.8112750470638275
step 151/334, epoch 341/501 --> loss:0.8221289050579071
step 201/334, epoch 341/501 --> loss:0.8191539251804352
step 251/334, epoch 341/501 --> loss:0.8226436853408814
step 301/334, epoch 341/501 --> loss:0.8263806831836701

##########train dataset##########
acc--> [98.62552323857795]
F1--> {'F1': [0.8575077745653776], 'precision': [0.7618000925249615], 'recall': [0.9807319026609463]}
##########eval dataset##########
acc--> [97.70793898107054]
F1--> {'F1': [0.7708394728058822], 'precision': [0.6787014496070106], 'recall': [0.8919369201146712]}
step 51/334, epoch 342/501 --> loss:0.8143319928646088
step 101/334, epoch 342/501 --> loss:0.82691277384758
step 151/334, epoch 342/501 --> loss:0.8240968537330627
step 201/334, epoch 342/501 --> loss:0.8137729585170745
step 251/334, epoch 342/501 --> loss:0.8205699050426483
step 301/334, epoch 342/501 --> loss:0.8274262022972106
step 51/334, epoch 343/501 --> loss:0.818328115940094
step 101/334, epoch 343/501 --> loss:0.832215609550476
step 151/334, epoch 343/501 --> loss:0.813543996810913
step 201/334, epoch 343/501 --> loss:0.82260990858078
step 251/334, epoch 343/501 --> loss:0.8174855232238769
step 301/334, epoch 343/501 --> loss:0.8175040376186371
step 51/334, epoch 344/501 --> loss:0.8320993101596832
step 101/334, epoch 344/501 --> loss:0.8356051981449127
step 151/334, epoch 344/501 --> loss:0.8084819757938385
step 201/334, epoch 344/501 --> loss:0.8166639840602875
step 251/334, epoch 344/501 --> loss:0.8223929965496063
step 301/334, epoch 344/501 --> loss:0.8083204889297485
step 51/334, epoch 345/501 --> loss:0.812972526550293
step 101/334, epoch 345/501 --> loss:0.813794949054718
step 151/334, epoch 345/501 --> loss:0.8281092071533203
step 201/334, epoch 345/501 --> loss:0.8230540776252746
step 251/334, epoch 345/501 --> loss:0.8112893092632294
step 301/334, epoch 345/501 --> loss:0.8368250107765198
step 51/334, epoch 346/501 --> loss:0.8157427144050599
step 101/334, epoch 346/501 --> loss:0.8172111618518829
step 151/334, epoch 346/501 --> loss:0.8180148279666901
step 201/334, epoch 346/501 --> loss:0.822936143875122
step 251/334, epoch 346/501 --> loss:0.8163419401645661
step 301/334, epoch 346/501 --> loss:0.8289533352851868
step 51/334, epoch 347/501 --> loss:0.8190732038021088
step 101/334, epoch 347/501 --> loss:0.8262995517253876
step 151/334, epoch 347/501 --> loss:0.8291074800491333
step 201/334, epoch 347/501 --> loss:0.8192746734619141
step 251/334, epoch 347/501 --> loss:0.8034146928787231
step 301/334, epoch 347/501 --> loss:0.8174722254276275
step 51/334, epoch 348/501 --> loss:0.815727356672287
step 101/334, epoch 348/501 --> loss:0.8417524039745331
step 151/334, epoch 348/501 --> loss:0.8285905182361603
step 201/334, epoch 348/501 --> loss:0.8065189135074615
step 251/334, epoch 348/501 --> loss:0.820006731748581
step 301/334, epoch 348/501 --> loss:0.8294063866138458
step 51/334, epoch 349/501 --> loss:0.8259695398807526
step 101/334, epoch 349/501 --> loss:0.8293661630153656
step 151/334, epoch 349/501 --> loss:0.8180924820899963
step 201/334, epoch 349/501 --> loss:0.8301594364643097
step 251/334, epoch 349/501 --> loss:0.8017533254623413
step 301/334, epoch 349/501 --> loss:0.8215856313705444
step 51/334, epoch 350/501 --> loss:0.8303341090679168
step 101/334, epoch 350/501 --> loss:0.8264403438568115
step 151/334, epoch 350/501 --> loss:0.8089575839042663
step 201/334, epoch 350/501 --> loss:0.824048285484314
step 251/334, epoch 350/501 --> loss:0.8146944260597229
step 301/334, epoch 350/501 --> loss:0.8144193649291992
step 51/334, epoch 351/501 --> loss:0.8140751349925995
step 101/334, epoch 351/501 --> loss:0.8214616131782532
step 151/334, epoch 351/501 --> loss:0.8103920555114746
step 201/334, epoch 351/501 --> loss:0.8215307986736298
step 251/334, epoch 351/501 --> loss:0.8250213849544525
step 301/334, epoch 351/501 --> loss:0.8257494437694549

##########train dataset##########
acc--> [99.74597205729248]
F1--> {'F1': [0.97063055772594], 'precision': [0.946934934796369], 'recall': [0.995553025508259]}
##########eval dataset##########
acc--> [98.8544554222221]
F1--> {'F1': [0.8660128145881699], 'precision': [0.8756749929532565], 'recall': [0.8565713154354703]}
save model!
step 51/334, epoch 352/501 --> loss:0.8059985756874084
step 101/334, epoch 352/501 --> loss:0.8129852962493896
step 151/334, epoch 352/501 --> loss:0.8270082581043243
step 201/334, epoch 352/501 --> loss:0.8206218826770783
step 251/334, epoch 352/501 --> loss:0.8265632510185241
step 301/334, epoch 352/501 --> loss:0.827162253856659
step 51/334, epoch 353/501 --> loss:0.8211082291603088
step 101/334, epoch 353/501 --> loss:0.8017650401592255
step 151/334, epoch 353/501 --> loss:0.8324310088157654
step 201/334, epoch 353/501 --> loss:0.8094626605510712
step 251/334, epoch 353/501 --> loss:0.8306681275367737
step 301/334, epoch 353/501 --> loss:0.8214626252651215
step 51/334, epoch 354/501 --> loss:0.813735362291336
step 101/334, epoch 354/501 --> loss:0.8158375918865204
step 151/334, epoch 354/501 --> loss:0.808566552400589
step 201/334, epoch 354/501 --> loss:0.8236554825305938
step 251/334, epoch 354/501 --> loss:0.8273447132110596
step 301/334, epoch 354/501 --> loss:0.8252326691150665
step 51/334, epoch 355/501 --> loss:0.8109438252449036
step 101/334, epoch 355/501 --> loss:0.8141696584224701
step 151/334, epoch 355/501 --> loss:0.8208427906036377
step 201/334, epoch 355/501 --> loss:0.8315029740333557
step 251/334, epoch 355/501 --> loss:0.8079206848144531
step 301/334, epoch 355/501 --> loss:0.8236395251750946
step 51/334, epoch 356/501 --> loss:0.8242032909393311
step 101/334, epoch 356/501 --> loss:0.8127806270122528
step 151/334, epoch 356/501 --> loss:0.8222900021076203
step 201/334, epoch 356/501 --> loss:0.8253883755207062
step 251/334, epoch 356/501 --> loss:0.8115707755088806
step 301/334, epoch 356/501 --> loss:0.8190817153453827
step 51/334, epoch 357/501 --> loss:0.8134700179100036
step 101/334, epoch 357/501 --> loss:0.813756846189499
step 151/334, epoch 357/501 --> loss:0.831411076784134
step 201/334, epoch 357/501 --> loss:0.8118353617191315
step 251/334, epoch 357/501 --> loss:0.8202739369869232
step 301/334, epoch 357/501 --> loss:0.8259904623031616
step 51/334, epoch 358/501 --> loss:0.8270374643802643
step 101/334, epoch 358/501 --> loss:0.8131284761428833
step 151/334, epoch 358/501 --> loss:0.8152501475811005
step 201/334, epoch 358/501 --> loss:0.8326171576976776
step 251/334, epoch 358/501 --> loss:0.8159955430030823
step 301/334, epoch 358/501 --> loss:0.829920928478241
step 51/334, epoch 359/501 --> loss:0.827637425661087
step 101/334, epoch 359/501 --> loss:0.8099706745147706
step 151/334, epoch 359/501 --> loss:0.8201774430274963
step 201/334, epoch 359/501 --> loss:0.8174898648262023
step 251/334, epoch 359/501 --> loss:0.8230756294727325
step 301/334, epoch 359/501 --> loss:0.8162726235389709
step 51/334, epoch 360/501 --> loss:0.8095897126197815
step 101/334, epoch 360/501 --> loss:0.8243805921077728
step 151/334, epoch 360/501 --> loss:0.8204960036277771
step 201/334, epoch 360/501 --> loss:0.8207408714294434
step 251/334, epoch 360/501 --> loss:0.8273047375679016
step 301/334, epoch 360/501 --> loss:0.8193786275386811
step 51/334, epoch 361/501 --> loss:0.8257572937011719
step 101/334, epoch 361/501 --> loss:0.8226214492321015
step 151/334, epoch 361/501 --> loss:0.8186198055744172
step 201/334, epoch 361/501 --> loss:0.8293550527095794
step 251/334, epoch 361/501 --> loss:0.804983662366867
step 301/334, epoch 361/501 --> loss:0.816874623298645

##########train dataset##########
acc--> [99.74570601541343]
F1--> {'F1': [0.970588689825156], 'precision': [0.9472347571489459], 'recall': [0.9951338131837967]}
##########eval dataset##########
acc--> [98.79833364944932]
F1--> {'F1': [0.8621850297787024], 'precision': [0.8547921019713273], 'recall': [0.8697171276691076]}
step 51/334, epoch 362/501 --> loss:0.8178633749485016
step 101/334, epoch 362/501 --> loss:0.8433716750144958
step 151/334, epoch 362/501 --> loss:0.820784330368042
step 201/334, epoch 362/501 --> loss:0.8115484666824341
step 251/334, epoch 362/501 --> loss:0.8221788072586059
step 301/334, epoch 362/501 --> loss:0.8061107385158539
step 51/334, epoch 363/501 --> loss:0.8334242868423462
step 101/334, epoch 363/501 --> loss:0.8235796868801117
step 151/334, epoch 363/501 --> loss:0.8201719808578491
step 201/334, epoch 363/501 --> loss:0.8160035622119903
step 251/334, epoch 363/501 --> loss:0.8131339144706726
step 301/334, epoch 363/501 --> loss:0.8175575387477875
step 51/334, epoch 364/501 --> loss:0.8149931848049163
step 101/334, epoch 364/501 --> loss:0.8269725942611694
step 151/334, epoch 364/501 --> loss:0.8231501626968384
step 201/334, epoch 364/501 --> loss:0.8026726806163788
step 251/334, epoch 364/501 --> loss:0.8165709257125855
step 301/334, epoch 364/501 --> loss:0.8193217229843139
step 51/334, epoch 365/501 --> loss:0.8051993453502655
step 101/334, epoch 365/501 --> loss:0.8177285301685333
step 151/334, epoch 365/501 --> loss:0.8151360309123993
step 201/334, epoch 365/501 --> loss:0.8244534420967102
step 251/334, epoch 365/501 --> loss:0.821384003162384
step 301/334, epoch 365/501 --> loss:0.8259690654277801
step 51/334, epoch 366/501 --> loss:0.822076416015625
step 101/334, epoch 366/501 --> loss:0.8106419336795807
step 151/334, epoch 366/501 --> loss:0.8262954986095429
step 201/334, epoch 366/501 --> loss:0.8350329530239106
step 251/334, epoch 366/501 --> loss:0.8207534539699555
step 301/334, epoch 366/501 --> loss:0.8086177241802216
step 51/334, epoch 367/501 --> loss:0.8221886777877807
step 101/334, epoch 367/501 --> loss:0.8128637671470642
step 151/334, epoch 367/501 --> loss:0.8292990553379059
step 201/334, epoch 367/501 --> loss:0.8208513939380646
step 251/334, epoch 367/501 --> loss:0.8082516896724701
step 301/334, epoch 367/501 --> loss:0.8097291862964631
step 51/334, epoch 368/501 --> loss:0.8121109831333161
step 101/334, epoch 368/501 --> loss:0.8244858014583588
step 151/334, epoch 368/501 --> loss:0.8070138382911682
step 201/334, epoch 368/501 --> loss:0.8297315120697022
step 251/334, epoch 368/501 --> loss:0.8253216671943665
step 301/334, epoch 368/501 --> loss:0.8180895960330963
step 51/334, epoch 369/501 --> loss:0.8350206923484802
step 101/334, epoch 369/501 --> loss:0.8347960889339447
step 151/334, epoch 369/501 --> loss:0.8141627395153046
step 201/334, epoch 369/501 --> loss:0.8066468596458435
step 251/334, epoch 369/501 --> loss:0.8215622842311859
step 301/334, epoch 369/501 --> loss:0.8149420070648193
step 51/334, epoch 370/501 --> loss:0.8253022789955139
step 101/334, epoch 370/501 --> loss:0.8213702249526977
step 151/334, epoch 370/501 --> loss:0.8183220529556274
step 201/334, epoch 370/501 --> loss:0.8209328532218934
step 251/334, epoch 370/501 --> loss:0.8175474023818969
step 301/334, epoch 370/501 --> loss:0.8274742233753204
step 51/334, epoch 371/501 --> loss:0.82294970870018
step 101/334, epoch 371/501 --> loss:0.8202489960193634
step 151/334, epoch 371/501 --> loss:0.8138701343536376
step 201/334, epoch 371/501 --> loss:0.8208198165893554
step 251/334, epoch 371/501 --> loss:0.8242857027053833
step 301/334, epoch 371/501 --> loss:0.8156063365936279

##########train dataset##########
acc--> [99.55922152873306]
F1--> {'F1': [0.950064245318621], 'precision': [0.9095322925710565], 'recall': [0.9943881172157041]}
##########eval dataset##########
acc--> [98.55887211943308]
F1--> {'F1': [0.8409698427599769], 'precision': [0.8038953518980186], 'recall': [0.881640278364556]}
step 51/334, epoch 372/501 --> loss:0.8181599473953247
step 101/334, epoch 372/501 --> loss:0.8233902990818024
step 151/334, epoch 372/501 --> loss:0.8083359313011169
step 201/334, epoch 372/501 --> loss:0.8278296256065368
step 251/334, epoch 372/501 --> loss:0.8182121598720551
step 301/334, epoch 372/501 --> loss:0.8191397869586945
step 51/334, epoch 373/501 --> loss:0.8230881273746491
step 101/334, epoch 373/501 --> loss:0.8171802926063537
step 151/334, epoch 373/501 --> loss:0.8239082968235016
step 201/334, epoch 373/501 --> loss:0.8063111436367035
step 251/334, epoch 373/501 --> loss:0.8231209969520569
step 301/334, epoch 373/501 --> loss:0.8251220107078552
step 51/334, epoch 374/501 --> loss:0.8374806749820709
step 101/334, epoch 374/501 --> loss:0.807744653224945
step 151/334, epoch 374/501 --> loss:0.8197088825702668
step 201/334, epoch 374/501 --> loss:0.8125515878200531
step 251/334, epoch 374/501 --> loss:0.8162076222896576
step 301/334, epoch 374/501 --> loss:0.8250090897083282
step 51/334, epoch 375/501 --> loss:0.8355727887153626
step 101/334, epoch 375/501 --> loss:0.8174612879753113
step 151/334, epoch 375/501 --> loss:0.8167127931118011
step 201/334, epoch 375/501 --> loss:0.8124695861339569
step 251/334, epoch 375/501 --> loss:0.8114859187602996
step 301/334, epoch 375/501 --> loss:0.8321934318542481
step 51/334, epoch 376/501 --> loss:0.8097537660598755
step 101/334, epoch 376/501 --> loss:0.814505467414856
step 151/334, epoch 376/501 --> loss:0.8201801872253418
step 201/334, epoch 376/501 --> loss:0.8303784203529357
step 251/334, epoch 376/501 --> loss:0.8126770043373108
step 301/334, epoch 376/501 --> loss:0.8207955265045166
step 51/334, epoch 377/501 --> loss:0.8204527974128724
step 101/334, epoch 377/501 --> loss:0.8322063529491425
step 151/334, epoch 377/501 --> loss:0.8178016459941864
step 201/334, epoch 377/501 --> loss:0.810342024564743
step 251/334, epoch 377/501 --> loss:0.8254814291000366
step 301/334, epoch 377/501 --> loss:0.8225995802879333
step 51/334, epoch 378/501 --> loss:0.8112952518463135
step 101/334, epoch 378/501 --> loss:0.808534231185913
step 151/334, epoch 378/501 --> loss:0.82714146733284
step 201/334, epoch 378/501 --> loss:0.8298427784442901
step 251/334, epoch 378/501 --> loss:0.816896288394928
step 301/334, epoch 378/501 --> loss:0.8210815584659577
step 51/334, epoch 379/501 --> loss:0.8170132267475129
step 101/334, epoch 379/501 --> loss:0.8160101199150085
step 151/334, epoch 379/501 --> loss:0.8176896798610688
step 201/334, epoch 379/501 --> loss:0.8325191283226013
step 251/334, epoch 379/501 --> loss:0.8287413382530212
step 301/334, epoch 379/501 --> loss:0.8141814815998077
step 51/334, epoch 380/501 --> loss:0.8210716247558594
step 101/334, epoch 380/501 --> loss:0.8178780508041382
step 151/334, epoch 380/501 --> loss:0.8225025975704193
step 201/334, epoch 380/501 --> loss:0.8138718771934509
step 251/334, epoch 380/501 --> loss:0.826622486114502
step 301/334, epoch 380/501 --> loss:0.8258381605148315
step 51/334, epoch 381/501 --> loss:0.8223786997795105
step 101/334, epoch 381/501 --> loss:0.818326723575592
step 151/334, epoch 381/501 --> loss:0.8113176202774048
step 201/334, epoch 381/501 --> loss:0.8256502330303193
step 251/334, epoch 381/501 --> loss:0.8124015390872955
step 301/334, epoch 381/501 --> loss:0.8227822005748748

##########train dataset##########
acc--> [91.53482003027557]
F1--> {'F1': [0.48129608960257214], 'precision': [0.32450339531599065], 'recall': [0.931288386671591]}
##########eval dataset##########
acc--> [91.53462207222874]
F1--> {'F1': [0.46915336634033444], 'precision': [0.3217947657172101], 'recall': [0.8655073852326431]}
step 51/334, epoch 382/501 --> loss:0.8337541556358338
step 101/334, epoch 382/501 --> loss:0.8228198993206024
step 151/334, epoch 382/501 --> loss:0.8135945868492126
step 201/334, epoch 382/501 --> loss:0.8181968569755554
step 251/334, epoch 382/501 --> loss:0.8118561887741089
step 301/334, epoch 382/501 --> loss:0.8239153718948364
step 51/334, epoch 383/501 --> loss:0.8289691531658172
step 101/334, epoch 383/501 --> loss:0.8145154392719269
step 151/334, epoch 383/501 --> loss:0.8093504846096039
step 201/334, epoch 383/501 --> loss:0.8126480185985565
step 251/334, epoch 383/501 --> loss:0.8331083965301513
step 301/334, epoch 383/501 --> loss:0.8151569402217865
step 51/334, epoch 384/501 --> loss:0.8231254041194915
step 101/334, epoch 384/501 --> loss:0.820090742111206
step 151/334, epoch 384/501 --> loss:0.8234846949577331
step 201/334, epoch 384/501 --> loss:0.8230739140510559
step 251/334, epoch 384/501 --> loss:0.8096140098571777
step 301/334, epoch 384/501 --> loss:0.8210931277275085
step 51/334, epoch 385/501 --> loss:0.8132226049900055
step 101/334, epoch 385/501 --> loss:0.8218075489997864
step 151/334, epoch 385/501 --> loss:0.816633267402649
step 201/334, epoch 385/501 --> loss:0.8071128654479981
step 251/334, epoch 385/501 --> loss:0.8249991989135742
step 301/334, epoch 385/501 --> loss:0.8278762936592102
step 51/334, epoch 386/501 --> loss:0.8068673706054688
step 101/334, epoch 386/501 --> loss:0.818762502670288
step 151/334, epoch 386/501 --> loss:0.7995239543914795
step 201/334, epoch 386/501 --> loss:0.8317464876174927
step 251/334, epoch 386/501 --> loss:0.8468026793003083
step 301/334, epoch 386/501 --> loss:0.8131266641616821
step 51/334, epoch 387/501 --> loss:0.7940854263305664
step 101/334, epoch 387/501 --> loss:0.8338418924808502
step 151/334, epoch 387/501 --> loss:0.8202109408378601
step 201/334, epoch 387/501 --> loss:0.820702669620514
step 251/334, epoch 387/501 --> loss:0.8245345258712768
step 301/334, epoch 387/501 --> loss:0.8205454742908478
step 51/334, epoch 388/501 --> loss:0.8158078372478486
step 101/334, epoch 388/501 --> loss:0.8315544176101685
step 151/334, epoch 388/501 --> loss:0.8307688450813293
step 201/334, epoch 388/501 --> loss:0.8024366009235382
step 251/334, epoch 388/501 --> loss:0.8109122788906098
step 301/334, epoch 388/501 --> loss:0.8216254663467407
step 51/334, epoch 389/501 --> loss:0.8133608496189118
step 101/334, epoch 389/501 --> loss:0.8340712118148804
step 151/334, epoch 389/501 --> loss:0.8103097653388978
step 201/334, epoch 389/501 --> loss:0.8418911457061767
step 251/334, epoch 389/501 --> loss:0.8145094513893127
step 301/334, epoch 389/501 --> loss:0.8164116871356965
step 51/334, epoch 390/501 --> loss:0.8275754070281982
step 101/334, epoch 390/501 --> loss:0.820297269821167
step 151/334, epoch 390/501 --> loss:0.8181644725799561
step 201/334, epoch 390/501 --> loss:0.8136047482490539
step 251/334, epoch 390/501 --> loss:0.8222777581214905
step 301/334, epoch 390/501 --> loss:0.8263377511501312
step 51/334, epoch 391/501 --> loss:0.808576887845993
step 101/334, epoch 391/501 --> loss:0.8210975039005279
step 151/334, epoch 391/501 --> loss:0.8131457543373108
step 201/334, epoch 391/501 --> loss:0.8158453869819641
step 251/334, epoch 391/501 --> loss:0.8368742692470551
step 301/334, epoch 391/501 --> loss:0.8210987591743469

##########train dataset##########
acc--> [99.78245669730451]
F1--> {'F1': [0.9747619062164808], 'precision': [0.9540726786798032], 'recall': [0.9963787652535143]}
##########eval dataset##########
acc--> [98.83689313005135]
F1--> {'F1': [0.8656130448050796], 'precision': [0.864523028224809], 'recall': [0.8667158387549783]}
step 51/334, epoch 392/501 --> loss:0.8277200102806092
step 101/334, epoch 392/501 --> loss:0.8219355475902558
step 151/334, epoch 392/501 --> loss:0.8241646659374237
step 201/334, epoch 392/501 --> loss:0.8194836580753326
step 251/334, epoch 392/501 --> loss:0.8105190205574035
step 301/334, epoch 392/501 --> loss:0.8067470574378968
step 51/334, epoch 393/501 --> loss:0.8207817697525024
step 101/334, epoch 393/501 --> loss:0.7949975645542144
step 151/334, epoch 393/501 --> loss:0.8194633960723877
step 201/334, epoch 393/501 --> loss:0.817079895734787
step 251/334, epoch 393/501 --> loss:0.8342577600479126
step 301/334, epoch 393/501 --> loss:0.8201960742473602
step 51/334, epoch 394/501 --> loss:0.8257608270645141
step 101/334, epoch 394/501 --> loss:0.8248495805263519
step 151/334, epoch 394/501 --> loss:0.8147673356533051
step 201/334, epoch 394/501 --> loss:0.8053084862232208
step 251/334, epoch 394/501 --> loss:0.8099913322925567
step 301/334, epoch 394/501 --> loss:0.8269619500637054
step 51/334, epoch 395/501 --> loss:0.8117109155654907
step 101/334, epoch 395/501 --> loss:0.815448511838913
step 151/334, epoch 395/501 --> loss:0.8130269026756287
step 201/334, epoch 395/501 --> loss:0.8272501647472381
step 251/334, epoch 395/501 --> loss:0.8295648503303528
step 301/334, epoch 395/501 --> loss:0.8215153694152832
step 51/334, epoch 396/501 --> loss:0.8173887348175048
step 101/334, epoch 396/501 --> loss:0.8181904292106629
step 151/334, epoch 396/501 --> loss:0.8277698874473571
step 201/334, epoch 396/501 --> loss:0.8214530944824219
step 251/334, epoch 396/501 --> loss:0.8118553614616394
step 301/334, epoch 396/501 --> loss:0.8207701826095581
step 51/334, epoch 397/501 --> loss:0.820075089931488
step 101/334, epoch 397/501 --> loss:0.8264522576332092
step 151/334, epoch 397/501 --> loss:0.8284915566444397
step 201/334, epoch 397/501 --> loss:0.8145732736587524
step 251/334, epoch 397/501 --> loss:0.8045895421504974
step 301/334, epoch 397/501 --> loss:0.8215634524822235
step 51/334, epoch 398/501 --> loss:0.8108975625038147
step 101/334, epoch 398/501 --> loss:0.8235848772525788
step 151/334, epoch 398/501 --> loss:0.8293143475055694
step 201/334, epoch 398/501 --> loss:0.8258154881000519
step 251/334, epoch 398/501 --> loss:0.8209445309638977
step 301/334, epoch 398/501 --> loss:0.8140643501281738
step 51/334, epoch 399/501 --> loss:0.8085484719276428
step 101/334, epoch 399/501 --> loss:0.820688648223877
step 151/334, epoch 399/501 --> loss:0.8149157798290253
step 201/334, epoch 399/501 --> loss:0.8294848799705505
step 251/334, epoch 399/501 --> loss:0.8210301780700684
step 301/334, epoch 399/501 --> loss:0.8111401855945587
step 51/334, epoch 400/501 --> loss:0.8143319690227508
step 101/334, epoch 400/501 --> loss:0.8130100381374359
step 151/334, epoch 400/501 --> loss:0.8150447881221772
step 201/334, epoch 400/501 --> loss:0.8297326505184174
step 251/334, epoch 400/501 --> loss:0.8221528923511505
step 301/334, epoch 400/501 --> loss:0.8236958074569702
step 51/334, epoch 401/501 --> loss:0.8346972715854645
step 101/334, epoch 401/501 --> loss:0.8114643239974976
step 151/334, epoch 401/501 --> loss:0.8237529468536376
step 201/334, epoch 401/501 --> loss:0.8138373231887818
step 251/334, epoch 401/501 --> loss:0.8222234880924225
step 301/334, epoch 401/501 --> loss:0.8277290391921998

##########train dataset##########
acc--> [99.04143709254687]
F1--> {'F1': [0.8971041760868249], 'precision': [0.8195259576578615], 'recall': [0.990917676806026]}
##########eval dataset##########
acc--> [98.16211757175327]
F1--> {'F1': [0.805669173890665], 'precision': [0.7418657471511684], 'recall': [0.8814918663728674]}
step 51/334, epoch 402/501 --> loss:0.812304185628891
step 101/334, epoch 402/501 --> loss:0.8315290069580078
step 151/334, epoch 402/501 --> loss:0.8179102277755738
step 201/334, epoch 402/501 --> loss:0.8154271614551544
step 251/334, epoch 402/501 --> loss:0.8154581558704376
step 301/334, epoch 402/501 --> loss:0.8251903331279755
step 51/334, epoch 403/501 --> loss:0.8103165006637574
step 101/334, epoch 403/501 --> loss:0.8146952521800995
step 151/334, epoch 403/501 --> loss:0.8190852057933807
step 201/334, epoch 403/501 --> loss:0.8307854223251343
step 251/334, epoch 403/501 --> loss:0.8138775181770325
step 301/334, epoch 403/501 --> loss:0.820428980588913
step 51/334, epoch 404/501 --> loss:0.8170892763137817
step 101/334, epoch 404/501 --> loss:0.8260237085819244
step 151/334, epoch 404/501 --> loss:0.8156382381916046
step 201/334, epoch 404/501 --> loss:0.8215007805824279
step 251/334, epoch 404/501 --> loss:0.8251259911060334
step 301/334, epoch 404/501 --> loss:0.8302456843852997
step 51/334, epoch 405/501 --> loss:0.8078687071800232
step 101/334, epoch 405/501 --> loss:0.8218409955501557
step 151/334, epoch 405/501 --> loss:0.8327686858177185
step 201/334, epoch 405/501 --> loss:0.8163083302974701
step 251/334, epoch 405/501 --> loss:0.8184894752502442
step 301/334, epoch 405/501 --> loss:0.8185239195823669
step 51/334, epoch 406/501 --> loss:0.8210521113872528
step 101/334, epoch 406/501 --> loss:0.8294861960411072
step 151/334, epoch 406/501 --> loss:0.8122512185573578
step 201/334, epoch 406/501 --> loss:0.82805619597435
step 251/334, epoch 406/501 --> loss:0.8263597238063812
step 301/334, epoch 406/501 --> loss:0.8084388303756714
step 51/334, epoch 407/501 --> loss:0.832998538017273
step 101/334, epoch 407/501 --> loss:0.8109564328193665
step 151/334, epoch 407/501 --> loss:0.8220963132381439
step 201/334, epoch 407/501 --> loss:0.8218743097782135
step 251/334, epoch 407/501 --> loss:0.8046473944187165
step 301/334, epoch 407/501 --> loss:0.8346670079231262
step 51/334, epoch 408/501 --> loss:0.8194236469268799
step 101/334, epoch 408/501 --> loss:0.8257662451267243
step 151/334, epoch 408/501 --> loss:0.8228556191921235
step 201/334, epoch 408/501 --> loss:0.8048261308670044
step 251/334, epoch 408/501 --> loss:0.8258892130851746
step 301/334, epoch 408/501 --> loss:0.8061919772624969
step 51/334, epoch 409/501 --> loss:0.8136662924289704
step 101/334, epoch 409/501 --> loss:0.8145518600940704
step 151/334, epoch 409/501 --> loss:0.8238860535621643
step 201/334, epoch 409/501 --> loss:0.8116440904140473
step 251/334, epoch 409/501 --> loss:0.8180705010890961
step 301/334, epoch 409/501 --> loss:0.8311305284500122
step 51/334, epoch 410/501 --> loss:0.8104740107059478
step 101/334, epoch 410/501 --> loss:0.8308857226371765
step 151/334, epoch 410/501 --> loss:0.8254282808303833
step 201/334, epoch 410/501 --> loss:0.816290442943573
step 251/334, epoch 410/501 --> loss:0.8188104677200317
step 301/334, epoch 410/501 --> loss:0.8121275424957275
step 51/334, epoch 411/501 --> loss:0.8148146712779999
step 101/334, epoch 411/501 --> loss:0.8172124457359314
step 151/334, epoch 411/501 --> loss:0.8316501724720001
step 201/334, epoch 411/501 --> loss:0.828994071483612
step 251/334, epoch 411/501 --> loss:0.8186237800121308
step 301/334, epoch 411/501 --> loss:0.8093081665039062

##########train dataset##########
acc--> [99.81893218318737]
F1--> {'F1': [0.978907196032845], 'precision': [0.9618813810378518], 'recall': [0.9965569644082655]}
##########eval dataset##########
acc--> [98.87928332764506]
F1--> {'F1': [0.8687255445091744], 'precision': [0.8797293242355204], 'recall': [0.8580033907725455]}
save model!
step 51/334, epoch 412/501 --> loss:0.816373634338379
step 101/334, epoch 412/501 --> loss:0.8158389365673065
step 151/334, epoch 412/501 --> loss:0.8312747836112976
step 201/334, epoch 412/501 --> loss:0.8319130718708039
step 251/334, epoch 412/501 --> loss:0.8067178428173065
step 301/334, epoch 412/501 --> loss:0.8115309917926788
step 51/334, epoch 413/501 --> loss:0.8156907546520233
step 101/334, epoch 413/501 --> loss:0.8209256863594055
step 151/334, epoch 413/501 --> loss:0.822157084941864
step 201/334, epoch 413/501 --> loss:0.805302768945694
step 251/334, epoch 413/501 --> loss:0.8296189844608307
step 301/334, epoch 413/501 --> loss:0.8211677360534668
step 51/334, epoch 414/501 --> loss:0.8218720996379852
step 101/334, epoch 414/501 --> loss:0.8117707467079163
step 151/334, epoch 414/501 --> loss:0.8214171373844147
step 201/334, epoch 414/501 --> loss:0.8197324752807618
step 251/334, epoch 414/501 --> loss:0.8119913363456726
step 301/334, epoch 414/501 --> loss:0.8285222733020783
step 51/334, epoch 415/501 --> loss:0.8206235218048096
step 101/334, epoch 415/501 --> loss:0.8188152718544006
step 151/334, epoch 415/501 --> loss:0.8085140633583069
step 201/334, epoch 415/501 --> loss:0.8169623529911041
step 251/334, epoch 415/501 --> loss:0.8268962943553925
step 301/334, epoch 415/501 --> loss:0.8263750314712525
step 51/334, epoch 416/501 --> loss:0.8241471493244171
step 101/334, epoch 416/501 --> loss:0.8048834526538848
step 151/334, epoch 416/501 --> loss:0.8275666856765747
step 201/334, epoch 416/501 --> loss:0.8356221318244934
step 251/334, epoch 416/501 --> loss:0.8149025785923004
step 301/334, epoch 416/501 --> loss:0.812141261100769
step 51/334, epoch 417/501 --> loss:0.8212050437927246
step 101/334, epoch 417/501 --> loss:0.8210371220111847
step 151/334, epoch 417/501 --> loss:0.8226200330257416
step 201/334, epoch 417/501 --> loss:0.8178742003440856
step 251/334, epoch 417/501 --> loss:0.81182244181633
step 301/334, epoch 417/501 --> loss:0.8226955485343933
step 51/334, epoch 418/501 --> loss:0.8126206660270691
step 101/334, epoch 418/501 --> loss:0.8077508544921875
step 151/334, epoch 418/501 --> loss:0.8316422688961029
step 201/334, epoch 418/501 --> loss:0.8153971791267395
step 251/334, epoch 418/501 --> loss:0.8264305114746093
step 301/334, epoch 418/501 --> loss:0.8123593699932098
step 51/334, epoch 419/501 --> loss:0.8223510777950287
step 101/334, epoch 419/501 --> loss:0.8098846697807311
step 151/334, epoch 419/501 --> loss:0.8136047029495239
step 201/334, epoch 419/501 --> loss:0.8205128860473633
step 251/334, epoch 419/501 --> loss:0.8086337697505951
step 301/334, epoch 419/501 --> loss:0.8336675143241883
step 51/334, epoch 420/501 --> loss:0.8013410985469818
step 101/334, epoch 420/501 --> loss:0.821348934173584
step 151/334, epoch 420/501 --> loss:0.8205029273033142
step 201/334, epoch 420/501 --> loss:0.8304994237422944
step 251/334, epoch 420/501 --> loss:0.8231331586837769
step 301/334, epoch 420/501 --> loss:0.8171202123165131
step 51/334, epoch 421/501 --> loss:0.8186995768547058
step 101/334, epoch 421/501 --> loss:0.823519822359085
step 151/334, epoch 421/501 --> loss:0.8165589499473572
step 201/334, epoch 421/501 --> loss:0.8156218671798706
step 251/334, epoch 421/501 --> loss:0.8204515039920807
step 301/334, epoch 421/501 --> loss:0.8146918261051178

##########train dataset##########
acc--> [99.81078472215833]
F1--> {'F1': [0.9779786369551379], 'precision': [0.9601143265473692], 'recall': [0.9965307127627044]}
##########eval dataset##########
acc--> [98.85419777162814]
F1--> {'F1': [0.8665321734729132], 'precision': [0.8725426445253083], 'recall': [0.8606138048157443]}
step 51/334, epoch 422/501 --> loss:0.8103288292884827
step 101/334, epoch 422/501 --> loss:0.8183541762828827
step 151/334, epoch 422/501 --> loss:0.815316435098648
step 201/334, epoch 422/501 --> loss:0.8219468879699707
step 251/334, epoch 422/501 --> loss:0.8279990661144256
step 301/334, epoch 422/501 --> loss:0.817931627035141
step 51/334, epoch 423/501 --> loss:0.8162997889518738
step 101/334, epoch 423/501 --> loss:0.8123406136035919
step 151/334, epoch 423/501 --> loss:0.8208865582942962
step 201/334, epoch 423/501 --> loss:0.8211750566959382
step 251/334, epoch 423/501 --> loss:0.8253700459003448
step 301/334, epoch 423/501 --> loss:0.8126206254959106
step 51/334, epoch 424/501 --> loss:0.8165585505962372
step 101/334, epoch 424/501 --> loss:0.8233524513244629
step 151/334, epoch 424/501 --> loss:0.8143735408782959
step 201/334, epoch 424/501 --> loss:0.821827597618103
step 251/334, epoch 424/501 --> loss:0.8259482097625732
step 301/334, epoch 424/501 --> loss:0.8089691805839538
step 51/334, epoch 425/501 --> loss:0.7974245941638947
step 101/334, epoch 425/501 --> loss:0.8136195945739746
step 151/334, epoch 425/501 --> loss:0.8091667485237122
step 201/334, epoch 425/501 --> loss:0.8341758799552917
step 251/334, epoch 425/501 --> loss:0.8208174109458923
step 301/334, epoch 425/501 --> loss:0.8187092673778534
step 51/334, epoch 426/501 --> loss:0.8142924582958222
step 101/334, epoch 426/501 --> loss:0.8140597033500672
step 151/334, epoch 426/501 --> loss:0.8313992297649384
step 201/334, epoch 426/501 --> loss:0.8193921780586243
step 251/334, epoch 426/501 --> loss:0.823069384098053
step 301/334, epoch 426/501 --> loss:0.8181610536575318
step 51/334, epoch 427/501 --> loss:0.8246504461765289
step 101/334, epoch 427/501 --> loss:0.8167467319965362
step 151/334, epoch 427/501 --> loss:0.8196514451503754
step 201/334, epoch 427/501 --> loss:0.8257302558422088
step 251/334, epoch 427/501 --> loss:0.8125882542133331
step 301/334, epoch 427/501 --> loss:0.8250927007198334
step 51/334, epoch 428/501 --> loss:0.8170382153987884
step 101/334, epoch 428/501 --> loss:0.8229432225227356
step 151/334, epoch 428/501 --> loss:0.8139018642902375
step 201/334, epoch 428/501 --> loss:0.8294996356964112
step 251/334, epoch 428/501 --> loss:0.8183203041553497
step 301/334, epoch 428/501 --> loss:0.8137560355663299
step 51/334, epoch 429/501 --> loss:0.8133275139331818
step 101/334, epoch 429/501 --> loss:0.8303408813476563
step 151/334, epoch 429/501 --> loss:0.8251666712760926
step 201/334, epoch 429/501 --> loss:0.8185417997837067
step 251/334, epoch 429/501 --> loss:0.814931401014328
step 301/334, epoch 429/501 --> loss:0.8161874878406524
step 51/334, epoch 430/501 --> loss:0.8132324445247651
step 101/334, epoch 430/501 --> loss:0.8311988222599029
step 151/334, epoch 430/501 --> loss:0.8117655420303345
step 201/334, epoch 430/501 --> loss:0.8200918316841126
step 251/334, epoch 430/501 --> loss:0.8273326337337494
step 301/334, epoch 430/501 --> loss:0.8162724208831788
step 51/334, epoch 431/501 --> loss:0.8159329068660736
step 101/334, epoch 431/501 --> loss:0.8251205229759216
step 151/334, epoch 431/501 --> loss:0.8240591132640839
step 201/334, epoch 431/501 --> loss:0.8228072309494019
step 251/334, epoch 431/501 --> loss:0.81966921210289
step 301/334, epoch 431/501 --> loss:0.8082720935344696

##########train dataset##########
acc--> [99.8235126805715]
F1--> {'F1': [0.9794234364148631], 'precision': [0.9631675110252237], 'recall': [0.9962478461942567]}
##########eval dataset##########
acc--> [98.88502573194535]
F1--> {'F1': [0.8694188043071337], 'precision': [0.8802886283554056], 'recall': [0.8588239036880939]}
save model!
step 51/334, epoch 432/501 --> loss:0.8247213184833526
step 101/334, epoch 432/501 --> loss:0.8224265897274017
step 151/334, epoch 432/501 --> loss:0.8298403632640838
step 201/334, epoch 432/501 --> loss:0.796597591638565
step 251/334, epoch 432/501 --> loss:0.8235089266300202
step 301/334, epoch 432/501 --> loss:0.8222295451164245
step 51/334, epoch 433/501 --> loss:0.8154697966575623
step 101/334, epoch 433/501 --> loss:0.8244986629486084
step 151/334, epoch 433/501 --> loss:0.8344685888290405
step 201/334, epoch 433/501 --> loss:0.8116475558280944
step 251/334, epoch 433/501 --> loss:0.8109014570713043
step 301/334, epoch 433/501 --> loss:0.8132579183578491
step 51/334, epoch 434/501 --> loss:0.8242150628566742
step 101/334, epoch 434/501 --> loss:0.8218758630752564
step 151/334, epoch 434/501 --> loss:0.8199032294750214
step 201/334, epoch 434/501 --> loss:0.8209943091869354
step 251/334, epoch 434/501 --> loss:0.8184759819507599
step 301/334, epoch 434/501 --> loss:0.8156926476955414
step 51/334, epoch 435/501 --> loss:0.8200849282741547
step 101/334, epoch 435/501 --> loss:0.8287300229072571
step 151/334, epoch 435/501 --> loss:0.8296066343784332
step 201/334, epoch 435/501 --> loss:0.8215852439403534
step 251/334, epoch 435/501 --> loss:0.8031862688064575
step 301/334, epoch 435/501 --> loss:0.806825100183487
step 51/334, epoch 436/501 --> loss:0.829729413986206
step 101/334, epoch 436/501 --> loss:0.8406360554695129
step 151/334, epoch 436/501 --> loss:0.8170615947246551
step 201/334, epoch 436/501 --> loss:0.8074395716190338
step 251/334, epoch 436/501 --> loss:0.8103632402420043
step 301/334, epoch 436/501 --> loss:0.8151310241222381
step 51/334, epoch 437/501 --> loss:0.8246145462989807
step 101/334, epoch 437/501 --> loss:0.8364033472537994
step 151/334, epoch 437/501 --> loss:0.8130797636508942
step 201/334, epoch 437/501 --> loss:0.8154224741458893
step 251/334, epoch 437/501 --> loss:0.8226464116573333
step 301/334, epoch 437/501 --> loss:0.8007812380790711
step 51/334, epoch 438/501 --> loss:0.8260099577903748
step 101/334, epoch 438/501 --> loss:0.8153688597679138
step 151/334, epoch 438/501 --> loss:0.8245240521430969
step 201/334, epoch 438/501 --> loss:0.8182598888874054
step 251/334, epoch 438/501 --> loss:0.8237772691249847
step 301/334, epoch 438/501 --> loss:0.8121393394470214
step 51/334, epoch 439/501 --> loss:0.8225611674785615
step 101/334, epoch 439/501 --> loss:0.8126154744625091
step 151/334, epoch 439/501 --> loss:0.8330460262298583
step 201/334, epoch 439/501 --> loss:0.8175381696224213
step 251/334, epoch 439/501 --> loss:0.8282645893096924
step 301/334, epoch 439/501 --> loss:0.8059514582157135
step 51/334, epoch 440/501 --> loss:0.8145672416687012
step 101/334, epoch 440/501 --> loss:0.8231108963489533
step 151/334, epoch 440/501 --> loss:0.8162211740016937
step 201/334, epoch 440/501 --> loss:0.8185651755332947
step 251/334, epoch 440/501 --> loss:0.8279967343807221
step 301/334, epoch 440/501 --> loss:0.8158381712436676
step 51/334, epoch 441/501 --> loss:0.8074895000457764
step 101/334, epoch 441/501 --> loss:0.8416838955879211
step 151/334, epoch 441/501 --> loss:0.8307777655124664
step 201/334, epoch 441/501 --> loss:0.8223679113388062
step 251/334, epoch 441/501 --> loss:0.8120851671695709
step 301/334, epoch 441/501 --> loss:0.8115996646881104

##########train dataset##########
acc--> [99.71467981098634]
F1--> {'F1': [0.9671271165913623], 'precision': [0.9404307565939661], 'recall': [0.9953940232932849]}
##########eval dataset##########
acc--> [98.7344676741086]
F1--> {'F1': [0.8571023514788182], 'precision': [0.837046574880661], 'recall': [0.8781532915171688]}
step 51/334, epoch 442/501 --> loss:0.8385588300228118
step 101/334, epoch 442/501 --> loss:0.8145764970779419
step 151/334, epoch 442/501 --> loss:0.8280151820182801
step 201/334, epoch 442/501 --> loss:0.8198150181770325
step 251/334, epoch 442/501 --> loss:0.8136899292469024
step 301/334, epoch 442/501 --> loss:0.8076153516769409
step 51/334, epoch 443/501 --> loss:0.835399284362793
step 101/334, epoch 443/501 --> loss:0.8230301523208619
step 151/334, epoch 443/501 --> loss:0.8133987200260162
step 201/334, epoch 443/501 --> loss:0.8290260314941407
step 251/334, epoch 443/501 --> loss:0.7970401561260223
step 301/334, epoch 443/501 --> loss:0.8077228105068207
step 51/334, epoch 444/501 --> loss:0.8065019369125366
step 101/334, epoch 444/501 --> loss:0.8221321702003479
step 151/334, epoch 444/501 --> loss:0.8236156606674194
step 201/334, epoch 444/501 --> loss:0.8191658210754394
step 251/334, epoch 444/501 --> loss:0.8192220044136047
step 301/334, epoch 444/501 --> loss:0.8225557839870453
step 51/334, epoch 445/501 --> loss:0.8276154971122742
step 101/334, epoch 445/501 --> loss:0.8200429999828338
step 151/334, epoch 445/501 --> loss:0.8070378673076629
step 201/334, epoch 445/501 --> loss:0.8168603682518005
step 251/334, epoch 445/501 --> loss:0.8334535050392151
step 301/334, epoch 445/501 --> loss:0.8146586775779724
step 51/334, epoch 446/501 --> loss:0.8143939769268036
step 101/334, epoch 446/501 --> loss:0.8231112146377564
step 151/334, epoch 446/501 --> loss:0.8225649452209473
step 201/334, epoch 446/501 --> loss:0.816413186788559
step 251/334, epoch 446/501 --> loss:0.8198706781864167
step 301/334, epoch 446/501 --> loss:0.8246912598609925
step 51/334, epoch 447/501 --> loss:0.808499448299408
step 101/334, epoch 447/501 --> loss:0.8190794491767883
step 151/334, epoch 447/501 --> loss:0.8150769150257111
step 201/334, epoch 447/501 --> loss:0.8226095032691956
step 251/334, epoch 447/501 --> loss:0.8277278864383697
step 301/334, epoch 447/501 --> loss:0.8238198804855347
step 51/334, epoch 448/501 --> loss:0.8307483088970185
step 101/334, epoch 448/501 --> loss:0.8022298765182495
step 151/334, epoch 448/501 --> loss:0.8212582504749298
step 201/334, epoch 448/501 --> loss:0.8155732476711273
step 251/334, epoch 448/501 --> loss:0.8285171139240265
step 301/334, epoch 448/501 --> loss:0.806634076833725
step 51/334, epoch 449/501 --> loss:0.8194498491287231
step 101/334, epoch 449/501 --> loss:0.8119617331027985
step 151/334, epoch 449/501 --> loss:0.819481589794159
step 201/334, epoch 449/501 --> loss:0.8314924216270447
step 251/334, epoch 449/501 --> loss:0.8140500462055207
step 301/334, epoch 449/501 --> loss:0.8325144815444946
step 51/334, epoch 450/501 --> loss:0.824514924287796
step 101/334, epoch 450/501 --> loss:0.836979684829712
step 151/334, epoch 450/501 --> loss:0.8125645446777344
step 201/334, epoch 450/501 --> loss:0.8187668395042419
step 251/334, epoch 450/501 --> loss:0.8060854542255401
step 301/334, epoch 450/501 --> loss:0.8082202458381653
step 51/334, epoch 451/501 --> loss:0.8162967121601105
step 101/334, epoch 451/501 --> loss:0.8119537091255188
step 151/334, epoch 451/501 --> loss:0.8138701617717743
step 201/334, epoch 451/501 --> loss:0.8281382596492768
step 251/334, epoch 451/501 --> loss:0.815820986032486
step 301/334, epoch 451/501 --> loss:0.8223139452934265

##########train dataset##########
acc--> [99.79210400518497]
F1--> {'F1': [0.9758575553690935], 'precision': [0.9560316093389588], 'recall': [0.9965336296122113]}
##########eval dataset##########
acc--> [98.7923409369295]
F1--> {'F1': [0.859793392736726], 'precision': [0.8628572806143231], 'recall': [0.8567611159950804]}
step 51/334, epoch 452/501 --> loss:0.8207246804237366
step 101/334, epoch 452/501 --> loss:0.8289823031425476
step 151/334, epoch 452/501 --> loss:0.8252478945255279
step 201/334, epoch 452/501 --> loss:0.8021008884906768
step 251/334, epoch 452/501 --> loss:0.8345510506629944
step 301/334, epoch 452/501 --> loss:0.8033738994598388
step 51/334, epoch 453/501 --> loss:0.8233544862270356
step 101/334, epoch 453/501 --> loss:0.8083818507194519
step 151/334, epoch 453/501 --> loss:0.8343567156791687
step 201/334, epoch 453/501 --> loss:0.820619637966156
step 251/334, epoch 453/501 --> loss:0.8141405153274536
step 301/334, epoch 453/501 --> loss:0.8214344477653504
step 51/334, epoch 454/501 --> loss:0.833999434709549
step 101/334, epoch 454/501 --> loss:0.7957971966266633
step 151/334, epoch 454/501 --> loss:0.810306202173233
step 201/334, epoch 454/501 --> loss:0.8155088651180268
step 251/334, epoch 454/501 --> loss:0.8419451379776001
step 301/334, epoch 454/501 --> loss:0.8111557447910309
step 51/334, epoch 455/501 --> loss:0.822692666053772
step 101/334, epoch 455/501 --> loss:0.812447692155838
step 151/334, epoch 455/501 --> loss:0.8332708847522735
step 201/334, epoch 455/501 --> loss:0.8270983135700226
step 251/334, epoch 455/501 --> loss:0.8061777055263519
step 301/334, epoch 455/501 --> loss:0.8137927293777466
step 51/334, epoch 456/501 --> loss:0.8153798794746399
step 101/334, epoch 456/501 --> loss:0.8227420997619629
step 151/334, epoch 456/501 --> loss:0.8269122886657715
step 201/334, epoch 456/501 --> loss:0.8328789901733399
step 251/334, epoch 456/501 --> loss:0.8102884256839752
step 301/334, epoch 456/501 --> loss:0.8184156489372253
step 51/334, epoch 457/501 --> loss:0.8105367243289947
step 101/334, epoch 457/501 --> loss:0.8229567766189575
step 151/334, epoch 457/501 --> loss:0.8177144885063171
step 201/334, epoch 457/501 --> loss:0.8266582429409027
step 251/334, epoch 457/501 --> loss:0.8207126688957215
step 301/334, epoch 457/501 --> loss:0.8083954751491547
step 51/334, epoch 458/501 --> loss:0.8311831200122833
step 101/334, epoch 458/501 --> loss:0.8184930002689361
step 151/334, epoch 458/501 --> loss:0.8170640599727631
step 201/334, epoch 458/501 --> loss:0.8179688918590545
step 251/334, epoch 458/501 --> loss:0.8190951132774353
step 301/334, epoch 458/501 --> loss:0.8210931479930877
step 51/334, epoch 459/501 --> loss:0.8267355859279633
step 101/334, epoch 459/501 --> loss:0.8061440300941467
step 151/334, epoch 459/501 --> loss:0.8226581072807312
step 201/334, epoch 459/501 --> loss:0.8198723328113556
step 251/334, epoch 459/501 --> loss:0.8251293516159057
step 301/334, epoch 459/501 --> loss:0.8123429298400879
step 51/334, epoch 460/501 --> loss:0.8129340529441833
step 101/334, epoch 460/501 --> loss:0.8276996719837189
step 151/334, epoch 460/501 --> loss:0.8312442088127137
step 201/334, epoch 460/501 --> loss:0.808800823688507
step 251/334, epoch 460/501 --> loss:0.8295160210132599
step 301/334, epoch 460/501 --> loss:0.805378749370575
step 51/334, epoch 461/501 --> loss:0.8194552659988403
step 101/334, epoch 461/501 --> loss:0.8306831836700439
step 151/334, epoch 461/501 --> loss:0.8091743063926696
step 201/334, epoch 461/501 --> loss:0.8153796696662903
step 251/334, epoch 461/501 --> loss:0.8159723973274231
step 301/334, epoch 461/501 --> loss:0.8273021125793457

##########train dataset##########
acc--> [99.83244453604391]
F1--> {'F1': [0.9804482225942195], 'precision': [0.9649548922300384], 'recall': [0.9964575201901973]}
##########eval dataset##########
acc--> [98.88621586409315]
F1--> {'F1': [0.8688008441716256], 'precision': [0.8849294167774597], 'recall': [0.8532593033920198]}
step 51/334, epoch 462/501 --> loss:0.8257500171661377
step 101/334, epoch 462/501 --> loss:0.8134655857086182
step 151/334, epoch 462/501 --> loss:0.8345263516902923
step 201/334, epoch 462/501 --> loss:0.8175813984870911
step 251/334, epoch 462/501 --> loss:0.8173825800418854
step 301/334, epoch 462/501 --> loss:0.8141094779968262
step 51/334, epoch 463/501 --> loss:0.8276570451259613
step 101/334, epoch 463/501 --> loss:0.8342859899997711
step 151/334, epoch 463/501 --> loss:0.8119417905807496
step 201/334, epoch 463/501 --> loss:0.8153173339366913
step 251/334, epoch 463/501 --> loss:0.8120111989974975
step 301/334, epoch 463/501 --> loss:0.8091185653209686
step 51/334, epoch 464/501 --> loss:0.8234595012664795
step 101/334, epoch 464/501 --> loss:0.8275399959087372
step 151/334, epoch 464/501 --> loss:0.8031659841537475
step 201/334, epoch 464/501 --> loss:0.8237098920345306
step 251/334, epoch 464/501 --> loss:0.8075136661529541
step 301/334, epoch 464/501 --> loss:0.8290666949748993
step 51/334, epoch 465/501 --> loss:0.8183715748786926
step 101/334, epoch 465/501 --> loss:0.8155841982364654
step 151/334, epoch 465/501 --> loss:0.8251104760169983
step 201/334, epoch 465/501 --> loss:0.8115004742145538
step 251/334, epoch 465/501 --> loss:0.831915842294693
step 301/334, epoch 465/501 --> loss:0.8136212277412415
step 51/334, epoch 466/501 --> loss:0.8160249519348145
step 101/334, epoch 466/501 --> loss:0.8066948878765107
step 151/334, epoch 466/501 --> loss:0.8225517570972443
step 201/334, epoch 466/501 --> loss:0.8234655702114105
step 251/334, epoch 466/501 --> loss:0.8213437068462371
step 301/334, epoch 466/501 --> loss:0.8205480599403381
step 51/334, epoch 467/501 --> loss:0.8152982103824615
step 101/334, epoch 467/501 --> loss:0.8244523334503174
step 151/334, epoch 467/501 --> loss:0.8235139667987823
step 201/334, epoch 467/501 --> loss:0.8303487765789032
step 251/334, epoch 467/501 --> loss:0.8115559208393097
step 301/334, epoch 467/501 --> loss:0.8183713293075562
step 51/334, epoch 468/501 --> loss:0.8263658964633942
step 101/334, epoch 468/501 --> loss:0.8286690175533294
step 151/334, epoch 468/501 --> loss:0.8206953096389771
step 201/334, epoch 468/501 --> loss:0.8169393455982208
step 251/334, epoch 468/501 --> loss:0.8024448013305664
step 301/334, epoch 468/501 --> loss:0.8175839579105377
step 51/334, epoch 469/501 --> loss:0.8116981625556946
step 101/334, epoch 469/501 --> loss:0.825604053735733
step 151/334, epoch 469/501 --> loss:0.8253090846538543
step 201/334, epoch 469/501 --> loss:0.8155418264865876
step 251/334, epoch 469/501 --> loss:0.8311910700798034
step 301/334, epoch 469/501 --> loss:0.8018561351299286
step 51/334, epoch 470/501 --> loss:0.8086312317848205
step 101/334, epoch 470/501 --> loss:0.813800357580185
step 151/334, epoch 470/501 --> loss:0.8192627477645874
step 201/334, epoch 470/501 --> loss:0.8199242305755615
step 251/334, epoch 470/501 --> loss:0.8110385525226593
step 301/334, epoch 470/501 --> loss:0.8311716103553772
step 51/334, epoch 471/501 --> loss:0.8211820828914642
step 101/334, epoch 471/501 --> loss:0.80809783577919
step 151/334, epoch 471/501 --> loss:0.8140041863918305
step 201/334, epoch 471/501 --> loss:0.8285738039016723
step 251/334, epoch 471/501 --> loss:0.8174111473560334
step 301/334, epoch 471/501 --> loss:0.8144521176815033

##########train dataset##########
acc--> [99.8224319211962]
F1--> {'F1': [0.9793128986427267], 'precision': [0.9623648893680757], 'recall': [0.9968789031933621]}
##########eval dataset##########
acc--> [98.87814792956128]
F1--> {'F1': [0.8678754106990404], 'precision': [0.883814093069247], 'recall': [0.8525110660353791]}
step 51/334, epoch 472/501 --> loss:0.8203988158702851
step 101/334, epoch 472/501 --> loss:0.834710191488266
step 151/334, epoch 472/501 --> loss:0.8028777587413788
step 201/334, epoch 472/501 --> loss:0.8200333511829376
step 251/334, epoch 472/501 --> loss:0.818535395860672
step 301/334, epoch 472/501 --> loss:0.8127657783031463
step 51/334, epoch 473/501 --> loss:0.818437567949295
step 101/334, epoch 473/501 --> loss:0.8128793334960938
step 151/334, epoch 473/501 --> loss:0.8247878229618073
step 201/334, epoch 473/501 --> loss:0.8206227707862854
step 251/334, epoch 473/501 --> loss:0.8253002691268921
step 301/334, epoch 473/501 --> loss:0.8169360911846161
step 51/334, epoch 474/501 --> loss:0.8079377138614654
step 101/334, epoch 474/501 --> loss:0.8268484163284302
step 151/334, epoch 474/501 --> loss:0.8262268900871277
step 201/334, epoch 474/501 --> loss:0.8301852464675903
step 251/334, epoch 474/501 --> loss:0.8099492073059082
step 301/334, epoch 474/501 --> loss:0.8087590086460114
step 51/334, epoch 475/501 --> loss:0.8217840099334717
step 101/334, epoch 475/501 --> loss:0.8133751964569091
step 151/334, epoch 475/501 --> loss:0.816961464881897
step 201/334, epoch 475/501 --> loss:0.8219898068904876
step 251/334, epoch 475/501 --> loss:0.8209577810764312
step 301/334, epoch 475/501 --> loss:0.8255130386352539
step 51/334, epoch 476/501 --> loss:0.8036789226531983
step 101/334, epoch 476/501 --> loss:0.8190970003604889
step 151/334, epoch 476/501 --> loss:0.8277127611637115
step 201/334, epoch 476/501 --> loss:0.8217771184444428
step 251/334, epoch 476/501 --> loss:0.822320145368576
step 301/334, epoch 476/501 --> loss:0.8210968470573425
step 51/334, epoch 477/501 --> loss:0.8162306690216065
step 101/334, epoch 477/501 --> loss:0.8068052864074707
step 151/334, epoch 477/501 --> loss:0.829998676776886
step 201/334, epoch 477/501 --> loss:0.8236120319366456
step 251/334, epoch 477/501 --> loss:0.8125947093963624
step 301/334, epoch 477/501 --> loss:0.8217398595809936
step 51/334, epoch 478/501 --> loss:0.8228030741214752
step 101/334, epoch 478/501 --> loss:0.8294713187217713
step 151/334, epoch 478/501 --> loss:0.8229913246631623
step 201/334, epoch 478/501 --> loss:0.8119741630554199
step 251/334, epoch 478/501 --> loss:0.8220977783203125
step 301/334, epoch 478/501 --> loss:0.8160428440570832
step 51/334, epoch 479/501 --> loss:0.825897285938263
step 101/334, epoch 479/501 --> loss:0.8262116742134095
step 151/334, epoch 479/501 --> loss:0.8002802658081055
step 201/334, epoch 479/501 --> loss:0.815126724243164
step 251/334, epoch 479/501 --> loss:0.8261188805103302
step 301/334, epoch 479/501 --> loss:0.8217505276203155
step 51/334, epoch 480/501 --> loss:0.8142189764976502
step 101/334, epoch 480/501 --> loss:0.8249739086627961
step 151/334, epoch 480/501 --> loss:0.8051263844966888
step 201/334, epoch 480/501 --> loss:0.8249134600162507
step 251/334, epoch 480/501 --> loss:0.8272517108917237
step 301/334, epoch 480/501 --> loss:0.8236564493179321
step 51/334, epoch 481/501 --> loss:0.8034705746173859
step 101/334, epoch 481/501 --> loss:0.8261326503753662
step 151/334, epoch 481/501 --> loss:0.8324402141571045
step 201/334, epoch 481/501 --> loss:0.8203152358531952
step 251/334, epoch 481/501 --> loss:0.8055022776126861
step 301/334, epoch 481/501 --> loss:0.8203229296207428

##########train dataset##########
acc--> [99.80953918845796]
F1--> {'F1': [0.97783860986782], 'precision': [0.959772467821992], 'recall': [0.9966083145263266]}
##########eval dataset##########
acc--> [98.82258684720024]
F1--> {'F1': [0.8646527851314494], 'precision': [0.8592002778881632], 'recall': [0.8701850655825485]}
step 51/334, epoch 482/501 --> loss:0.8303780424594879
step 101/334, epoch 482/501 --> loss:0.8215177977085113
step 151/334, epoch 482/501 --> loss:0.818562560081482
step 201/334, epoch 482/501 --> loss:0.811396758556366
step 251/334, epoch 482/501 --> loss:0.824786388874054
step 301/334, epoch 482/501 --> loss:0.8070853567123413
step 51/334, epoch 483/501 --> loss:0.828877249956131
step 101/334, epoch 483/501 --> loss:0.8218629038333893
step 151/334, epoch 483/501 --> loss:0.8181107497215271
step 201/334, epoch 483/501 --> loss:0.8158487010002137
step 251/334, epoch 483/501 --> loss:0.8233689272403717
step 301/334, epoch 483/501 --> loss:0.8112591254711151
step 51/334, epoch 484/501 --> loss:0.8266828417778015
step 101/334, epoch 484/501 --> loss:0.8156243658065796
step 151/334, epoch 484/501 --> loss:0.8123312950134277
step 201/334, epoch 484/501 --> loss:0.8351594841480255
step 251/334, epoch 484/501 --> loss:0.8109875130653381
step 301/334, epoch 484/501 --> loss:0.80484468460083
step 51/334, epoch 485/501 --> loss:0.8215017771720886
step 101/334, epoch 485/501 --> loss:0.8218867897987365
step 151/334, epoch 485/501 --> loss:0.8222826743125915
step 201/334, epoch 485/501 --> loss:0.8068475127220154
step 251/334, epoch 485/501 --> loss:0.8102341294288635
step 301/334, epoch 485/501 --> loss:0.824675339460373
step 51/334, epoch 486/501 --> loss:0.8056971085071564
step 101/334, epoch 486/501 --> loss:0.8172563004493714
step 151/334, epoch 486/501 --> loss:0.832049150466919
step 201/334, epoch 486/501 --> loss:0.8192846930027008
step 251/334, epoch 486/501 --> loss:0.8140408170223236
step 301/334, epoch 486/501 --> loss:0.826341700553894
step 51/334, epoch 487/501 --> loss:0.8166985714435577
step 101/334, epoch 487/501 --> loss:0.8307570588588714
step 151/334, epoch 487/501 --> loss:0.8253700506687164
step 201/334, epoch 487/501 --> loss:0.816106745004654
step 251/334, epoch 487/501 --> loss:0.8147459590435028
step 301/334, epoch 487/501 --> loss:0.8130875694751739
step 51/334, epoch 488/501 --> loss:0.8179076540470124
step 101/334, epoch 488/501 --> loss:0.8285138356685638
step 151/334, epoch 488/501 --> loss:0.8117794752120971
step 201/334, epoch 488/501 --> loss:0.8278766596317291
step 251/334, epoch 488/501 --> loss:0.8119580912590026
step 301/334, epoch 488/501 --> loss:0.8252489447593689
step 51/334, epoch 489/501 --> loss:0.812599401473999
step 101/334, epoch 489/501 --> loss:0.8092326843738555
step 151/334, epoch 489/501 --> loss:0.8229382121562958
step 201/334, epoch 489/501 --> loss:0.8221661674976349
step 251/334, epoch 489/501 --> loss:0.8182276868820191
step 301/334, epoch 489/501 --> loss:0.8269074523448944
step 51/334, epoch 490/501 --> loss:0.7982562291622162
step 101/334, epoch 490/501 --> loss:0.8191288936138154
step 151/334, epoch 490/501 --> loss:0.8130419433116913
step 201/334, epoch 490/501 --> loss:0.820141258239746
step 251/334, epoch 490/501 --> loss:0.827630239725113
step 301/334, epoch 490/501 --> loss:0.8340469944477081
step 51/334, epoch 491/501 --> loss:0.8256607532501221
step 101/334, epoch 491/501 --> loss:0.8124763691425323
step 151/334, epoch 491/501 --> loss:0.8048057222366333
step 201/334, epoch 491/501 --> loss:0.8282375645637512
step 251/334, epoch 491/501 --> loss:0.809710590839386
step 301/334, epoch 491/501 --> loss:0.8450374674797058

##########train dataset##########
acc--> [99.83961565198351]
F1--> {'F1': [0.9812778036913186], 'precision': [0.9661209292620655], 'recall': [0.99692815046643]}
##########eval dataset##########
acc--> [98.87896159814689]
F1--> {'F1': [0.8695747424064059], 'precision': [0.8745328814723209], 'recall': [0.8646823937034017]}
save model!
step 51/334, epoch 492/501 --> loss:0.8182938241958618
step 101/334, epoch 492/501 --> loss:0.8186849343776703
step 151/334, epoch 492/501 --> loss:0.8283579289913178
step 201/334, epoch 492/501 --> loss:0.8160972738265991
step 251/334, epoch 492/501 --> loss:0.8052683961391449
step 301/334, epoch 492/501 --> loss:0.8305350017547607
step 51/334, epoch 493/501 --> loss:0.8201342415809632
step 101/334, epoch 493/501 --> loss:0.8258204507827759
step 151/334, epoch 493/501 --> loss:0.8165240824222565
step 201/334, epoch 493/501 --> loss:0.8215908169746399
step 251/334, epoch 493/501 --> loss:0.814450991153717
step 301/334, epoch 493/501 --> loss:0.8293037605285645
step 51/334, epoch 494/501 --> loss:0.8313317048549652
step 101/334, epoch 494/501 --> loss:0.8322411334514618
step 151/334, epoch 494/501 --> loss:0.8162174606323243
step 201/334, epoch 494/501 --> loss:0.8004556715488433
step 251/334, epoch 494/501 --> loss:0.8038665676116943
step 301/334, epoch 494/501 --> loss:0.8228240931034088
step 51/334, epoch 495/501 --> loss:0.8225871980190277
step 101/334, epoch 495/501 --> loss:0.8260138797760009
step 151/334, epoch 495/501 --> loss:0.8124923431873321
step 201/334, epoch 495/501 --> loss:0.823344440460205
step 251/334, epoch 495/501 --> loss:0.8170799970626831
step 301/334, epoch 495/501 --> loss:0.8188733804225922
step 51/334, epoch 496/501 --> loss:0.8119522440433502
step 101/334, epoch 496/501 --> loss:0.8222071814537049
step 151/334, epoch 496/501 --> loss:0.8253337740898132
step 201/334, epoch 496/501 --> loss:0.8228455591201782
step 251/334, epoch 496/501 --> loss:0.8213052821159362
step 301/334, epoch 496/501 --> loss:0.8103781151771545
step 51/334, epoch 497/501 --> loss:0.831663146018982
step 101/334, epoch 497/501 --> loss:0.8035884082317353
step 151/334, epoch 497/501 --> loss:0.8131043946743012
step 201/334, epoch 497/501 --> loss:0.8205067694187165
step 251/334, epoch 497/501 --> loss:0.8229668891429901
step 301/334, epoch 497/501 --> loss:0.8178329527378082
step 51/334, epoch 498/501 --> loss:0.8274205219745636
step 101/334, epoch 498/501 --> loss:0.8170337295532226
step 151/334, epoch 498/501 --> loss:0.8059632790088653
step 201/334, epoch 498/501 --> loss:0.8130478775501251
step 251/334, epoch 498/501 --> loss:0.8269644451141357
step 301/334, epoch 498/501 --> loss:0.8267781281471253
step 51/334, epoch 499/501 --> loss:0.811040266752243
step 101/334, epoch 499/501 --> loss:0.8253534090518951
step 151/334, epoch 499/501 --> loss:0.8218585073947906
step 201/334, epoch 499/501 --> loss:0.8104392611980438
step 251/334, epoch 499/501 --> loss:0.8316807079315186
step 301/334, epoch 499/501 --> loss:0.8085085451602936
step 51/334, epoch 500/501 --> loss:0.8114898347854614
step 101/334, epoch 500/501 --> loss:0.8254647552967072
step 151/334, epoch 500/501 --> loss:0.8165100824832916
step 201/334, epoch 500/501 --> loss:0.8263352704048157
step 251/334, epoch 500/501 --> loss:0.8250888299942016
step 301/334, epoch 500/501 --> loss:0.8094564616680145
step 51/334, epoch 501/501 --> loss:0.8345795845985413
step 101/334, epoch 501/501 --> loss:0.8194326221942901
step 151/334, epoch 501/501 --> loss:0.8183689427375793
step 201/334, epoch 501/501 --> loss:0.8173388826847077
step 251/334, epoch 501/501 --> loss:0.8201200413703919
step 301/334, epoch 501/501 --> loss:0.8110088193416596

##########train dataset##########
acc--> [99.84647180866632]
F1--> {'F1': [0.9820623704419742], 'precision': [0.9677078740287993], 'recall': [0.9968594349187472]}
##########eval dataset##########
acc--> [98.87602731832543]
F1--> {'F1': [0.86842384841669], 'precision': [0.8788784085390441], 'recall': [0.8582248504979165]}
