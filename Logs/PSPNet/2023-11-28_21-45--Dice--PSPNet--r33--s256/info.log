##########Config##########
{'device': 'cuda:1', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 256, 'num_parallel_workers': 4, 'batch_size': 16, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/PSPNet', 'log_path': 'Logs/PSPNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (features): Sequential(
    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (5): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (6): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (7): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (pyramid_pooling): PyramidPoolingModule(
    (pooling_layers): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (final_conv): Conv2d(4096, 2, kernel_size=(1, 1), stride=(1, 1))
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.9008147144317626
step 101/334, epoch 1/501 --> loss:0.857256019115448
step 151/334, epoch 1/501 --> loss:0.8551158010959625
step 201/334, epoch 1/501 --> loss:0.8484608268737793
step 251/334, epoch 1/501 --> loss:0.8539995205402374
step 301/334, epoch 1/501 --> loss:0.8695649433135987

##########train dataset##########
acc--> [88.68816706139017]
F1--> {'F1': [0.4120043317339963], 'precision': [0.26450110981494956], 'recall': [0.9314674392454767]}
##########eval dataset##########
acc--> [89.2924491501796]
F1--> {'F1': [0.4215717236238441], 'precision': [0.2719310364588145], 'recall': [0.937462785297838]}
save model!
step 51/334, epoch 2/501 --> loss:0.8488805544376373
step 101/334, epoch 2/501 --> loss:0.8517064487934113
step 151/334, epoch 2/501 --> loss:0.830250700712204
step 201/334, epoch 2/501 --> loss:0.8548429811000824
step 251/334, epoch 2/501 --> loss:0.8602019619941711
step 301/334, epoch 2/501 --> loss:0.8610231685638428
step 51/334, epoch 3/501 --> loss:0.8548612916469573
step 101/334, epoch 3/501 --> loss:0.8645593845844268
step 151/334, epoch 3/501 --> loss:0.8434787666797638
step 201/334, epoch 3/501 --> loss:0.8615717434883118
step 251/334, epoch 3/501 --> loss:0.8752111291885376
step 301/334, epoch 3/501 --> loss:0.8367443668842316
step 51/334, epoch 4/501 --> loss:0.8548975646495819
step 101/334, epoch 4/501 --> loss:0.8537178587913513
step 151/334, epoch 4/501 --> loss:0.8488571131229401
step 201/334, epoch 4/501 --> loss:0.836463565826416
step 251/334, epoch 4/501 --> loss:0.8632108807563782
step 301/334, epoch 4/501 --> loss:0.8506083452701568
step 51/334, epoch 5/501 --> loss:0.8581469571590423
step 101/334, epoch 5/501 --> loss:0.851674748659134
step 151/334, epoch 5/501 --> loss:0.8566821599006653
step 201/334, epoch 5/501 --> loss:0.8586683547496796
step 251/334, epoch 5/501 --> loss:0.8398205161094665
step 301/334, epoch 5/501 --> loss:0.8669865930080414
step 51/334, epoch 6/501 --> loss:0.8556986141204834
step 101/334, epoch 6/501 --> loss:0.843511825799942
step 151/334, epoch 6/501 --> loss:0.8466885328292847
step 201/334, epoch 6/501 --> loss:0.8425500297546387
step 251/334, epoch 6/501 --> loss:0.8564494061470032
step 301/334, epoch 6/501 --> loss:0.8500411152839661
step 51/334, epoch 7/501 --> loss:0.8357071447372436
step 101/334, epoch 7/501 --> loss:0.8520339703559876
step 151/334, epoch 7/501 --> loss:0.8523947119712829
step 201/334, epoch 7/501 --> loss:0.8441797602176666
step 251/334, epoch 7/501 --> loss:0.8486472487449646
step 301/334, epoch 7/501 --> loss:0.854830836057663
step 51/334, epoch 8/501 --> loss:0.8466801190376282
step 101/334, epoch 8/501 --> loss:0.8612469935417175
step 151/334, epoch 8/501 --> loss:0.8408519613742829
step 201/334, epoch 8/501 --> loss:0.8537917530536652
step 251/334, epoch 8/501 --> loss:0.8552384507656098
step 301/334, epoch 8/501 --> loss:0.8333562958240509
step 51/334, epoch 9/501 --> loss:0.8373198628425598
step 101/334, epoch 9/501 --> loss:0.8471557438373566
step 151/334, epoch 9/501 --> loss:0.8556852889060974
step 201/334, epoch 9/501 --> loss:0.8400496470928193
step 251/334, epoch 9/501 --> loss:0.8561626744270324
step 301/334, epoch 9/501 --> loss:0.8317132484912872
step 51/334, epoch 10/501 --> loss:0.8527853906154632
step 101/334, epoch 10/501 --> loss:0.857079017162323
step 151/334, epoch 10/501 --> loss:0.86284334897995
step 201/334, epoch 10/501 --> loss:0.8532831108570099
step 251/334, epoch 10/501 --> loss:0.8290026772022248
step 301/334, epoch 10/501 --> loss:0.8364324736595153
step 51/334, epoch 11/501 --> loss:0.8620152378082275
step 101/334, epoch 11/501 --> loss:0.8580037045478821
step 151/334, epoch 11/501 --> loss:0.8378715479373932
step 201/334, epoch 11/501 --> loss:0.8536975944042205
step 251/334, epoch 11/501 --> loss:0.8315932166576385
step 301/334, epoch 11/501 --> loss:0.8421204602718353

##########train dataset##########
acc--> [84.89795136996793]
F1--> {'F1': [0.3490425909150023], 'precision': [0.21371711488423065], 'recall': [0.9516303909328024]}
##########eval dataset##########
acc--> [85.72504505814373]
F1--> {'F1': [0.35570885554779663], 'precision': [0.21899728806979663], 'recall': [0.9467357305211515]}
step 51/334, epoch 12/501 --> loss:0.8478497278690338
step 101/334, epoch 12/501 --> loss:0.8424014365673065
step 151/334, epoch 12/501 --> loss:0.8337663590908051
step 201/334, epoch 12/501 --> loss:0.8544928884506225
step 251/334, epoch 12/501 --> loss:0.8231405448913575
step 301/334, epoch 12/501 --> loss:0.8648890674114227
step 51/334, epoch 13/501 --> loss:0.8429345381259918
step 101/334, epoch 13/501 --> loss:0.8538942432403565
step 151/334, epoch 13/501 --> loss:0.8725616145133972
step 201/334, epoch 13/501 --> loss:0.8344377815723419
step 251/334, epoch 13/501 --> loss:0.8539775431156158
step 301/334, epoch 13/501 --> loss:0.8278171062469483
step 51/334, epoch 14/501 --> loss:0.8532454907894135
step 101/334, epoch 14/501 --> loss:0.8504398512840271
step 151/334, epoch 14/501 --> loss:0.8388451087474823
step 201/334, epoch 14/501 --> loss:0.8291523718833923
step 251/334, epoch 14/501 --> loss:0.8505462312698364
step 301/334, epoch 14/501 --> loss:0.8464376294612884
step 51/334, epoch 15/501 --> loss:0.8499093794822693
step 101/334, epoch 15/501 --> loss:0.8356872260570526
step 151/334, epoch 15/501 --> loss:0.8401194357872009
step 201/334, epoch 15/501 --> loss:0.8529669427871704
step 251/334, epoch 15/501 --> loss:0.827987265586853
step 301/334, epoch 15/501 --> loss:0.852161774635315
step 51/334, epoch 16/501 --> loss:0.8232067334651947
step 101/334, epoch 16/501 --> loss:0.8457992029190063
step 151/334, epoch 16/501 --> loss:0.8556989169120789
step 201/334, epoch 16/501 --> loss:0.8521525096893311
step 251/334, epoch 16/501 --> loss:0.8645107936859131
step 301/334, epoch 16/501 --> loss:0.8584131681919098
step 51/334, epoch 17/501 --> loss:0.8407267987728119
step 101/334, epoch 17/501 --> loss:0.8297742712497711
step 151/334, epoch 17/501 --> loss:0.8460289943218231
step 201/334, epoch 17/501 --> loss:0.8641998386383056
step 251/334, epoch 17/501 --> loss:0.8612417900562286
step 301/334, epoch 17/501 --> loss:0.8330133044719696
step 51/334, epoch 18/501 --> loss:0.8314404535293579
step 101/334, epoch 18/501 --> loss:0.8572966027259826
step 151/334, epoch 18/501 --> loss:0.8320330607891083
step 201/334, epoch 18/501 --> loss:0.8669065856933593
step 251/334, epoch 18/501 --> loss:0.844567289352417
step 301/334, epoch 18/501 --> loss:0.8618668580055237
step 51/334, epoch 19/501 --> loss:0.8565305280685425
step 101/334, epoch 19/501 --> loss:0.8312429094314575
step 151/334, epoch 19/501 --> loss:0.8423561465740204
step 201/334, epoch 19/501 --> loss:0.8310766613483429
step 251/334, epoch 19/501 --> loss:0.8747631967067718
step 301/334, epoch 19/501 --> loss:0.849389272928238
step 51/334, epoch 20/501 --> loss:0.8404674780368805
step 101/334, epoch 20/501 --> loss:0.8455686593055725
step 151/334, epoch 20/501 --> loss:0.8328568053245544
step 201/334, epoch 20/501 --> loss:0.8472353255748749
step 251/334, epoch 20/501 --> loss:0.8464807319641113
step 301/334, epoch 20/501 --> loss:0.8522873985767364
step 51/334, epoch 21/501 --> loss:0.8336997175216675
step 101/334, epoch 21/501 --> loss:0.8591417109966278
step 151/334, epoch 21/501 --> loss:0.8525818812847138
step 201/334, epoch 21/501 --> loss:0.8564124202728272
step 251/334, epoch 21/501 --> loss:0.8635189390182495
step 301/334, epoch 21/501 --> loss:0.82401083111763

##########train dataset##########
acc--> [93.1596499236702]
F1--> {'F1': [0.5367006297881542], 'precision': [0.37698937018302303], 'recall': [0.9312295604412986]}
##########eval dataset##########
acc--> [93.34942341267387]
F1--> {'F1': [0.5380276429265908], 'precision': [0.3784297445193496], 'recall': [0.9304452133039912]}
save model!
step 51/334, epoch 22/501 --> loss:0.8422061955928802
step 101/334, epoch 22/501 --> loss:0.8445873928070068
step 151/334, epoch 22/501 --> loss:0.8348151028156281
step 201/334, epoch 22/501 --> loss:0.8486329424381256
step 251/334, epoch 22/501 --> loss:0.8373826479911805
step 301/334, epoch 22/501 --> loss:0.8482958340644836
step 51/334, epoch 23/501 --> loss:0.8298346483707428
step 101/334, epoch 23/501 --> loss:0.83841956615448
step 151/334, epoch 23/501 --> loss:0.8600684583187104
step 201/334, epoch 23/501 --> loss:0.8351043748855591
step 251/334, epoch 23/501 --> loss:0.8574678385257721
step 301/334, epoch 23/501 --> loss:0.8309531843662262
step 51/334, epoch 24/501 --> loss:0.8245560705661774
step 101/334, epoch 24/501 --> loss:0.8649906718730926
step 151/334, epoch 24/501 --> loss:0.8499389362335205
step 201/334, epoch 24/501 --> loss:0.8200263321399689
step 251/334, epoch 24/501 --> loss:0.8663112008571625
step 301/334, epoch 24/501 --> loss:0.8362305355072022
step 51/334, epoch 25/501 --> loss:0.8572763812541961
step 101/334, epoch 25/501 --> loss:0.8322311389446259
step 151/334, epoch 25/501 --> loss:0.8639808785915375
step 201/334, epoch 25/501 --> loss:0.8352965974807739
step 251/334, epoch 25/501 --> loss:0.8503651463985443
step 301/334, epoch 25/501 --> loss:0.8394752550125122
step 51/334, epoch 26/501 --> loss:0.8446188616752625
step 101/334, epoch 26/501 --> loss:0.8362865626811982
step 151/334, epoch 26/501 --> loss:0.8443412387371063
step 201/334, epoch 26/501 --> loss:0.8592932045459747
step 251/334, epoch 26/501 --> loss:0.8501533305644989
step 301/334, epoch 26/501 --> loss:0.8270659554004669
step 51/334, epoch 27/501 --> loss:0.8383886456489563
step 101/334, epoch 27/501 --> loss:0.8424342381954193
step 151/334, epoch 27/501 --> loss:0.8156441640853882
step 201/334, epoch 27/501 --> loss:0.8569804847240448
step 251/334, epoch 27/501 --> loss:0.8493253290653229
step 301/334, epoch 27/501 --> loss:0.8701896071434021
step 51/334, epoch 28/501 --> loss:0.8432859063148499
step 101/334, epoch 28/501 --> loss:0.8725121784210205
step 151/334, epoch 28/501 --> loss:0.8282142567634583
step 201/334, epoch 28/501 --> loss:0.8418948030471802
step 251/334, epoch 28/501 --> loss:0.8343538546562195
step 301/334, epoch 28/501 --> loss:0.8282091295719147
step 51/334, epoch 29/501 --> loss:0.8363655972480774
step 101/334, epoch 29/501 --> loss:0.8477634251117706
step 151/334, epoch 29/501 --> loss:0.850670474767685
step 201/334, epoch 29/501 --> loss:0.8396191334724427
step 251/334, epoch 29/501 --> loss:0.846156747341156
step 301/334, epoch 29/501 --> loss:0.8522040367126464
step 51/334, epoch 30/501 --> loss:0.8301736414432526
step 101/334, epoch 30/501 --> loss:0.8472121465206146
step 151/334, epoch 30/501 --> loss:0.8299316227436065
step 201/334, epoch 30/501 --> loss:0.8530226123332977
step 251/334, epoch 30/501 --> loss:0.8269770216941833
step 301/334, epoch 30/501 --> loss:0.8476371014118195
step 51/334, epoch 31/501 --> loss:0.8577527451515198
step 101/334, epoch 31/501 --> loss:0.820056426525116
step 151/334, epoch 31/501 --> loss:0.8546291279792786
step 201/334, epoch 31/501 --> loss:0.8280145788192749
step 251/334, epoch 31/501 --> loss:0.8493318319320678
step 301/334, epoch 31/501 --> loss:0.8365143954753875

##########train dataset##########
acc--> [93.23358954138644]
F1--> {'F1': [0.5467500420640361], 'precision': [0.38234546946521325], 'recall': [0.9592181734558994]}
##########eval dataset##########
acc--> [93.2695637433381]
F1--> {'F1': [0.5420595945727794], 'precision': [0.3781146059417275], 'recall': [0.9570268330705374]}
save model!
step 51/334, epoch 32/501 --> loss:0.8406682169437408
step 101/334, epoch 32/501 --> loss:0.8542010068893433
step 151/334, epoch 32/501 --> loss:0.8232749724388122
step 201/334, epoch 32/501 --> loss:0.8513152086734772
step 251/334, epoch 32/501 --> loss:0.8320281946659088
step 301/334, epoch 32/501 --> loss:0.8252420961856842
step 51/334, epoch 33/501 --> loss:0.8755015206336975
step 101/334, epoch 33/501 --> loss:0.8457646489143371
step 151/334, epoch 33/501 --> loss:0.8211144757270813
step 201/334, epoch 33/501 --> loss:0.8324414503574371
step 251/334, epoch 33/501 --> loss:0.8541776204109192
step 301/334, epoch 33/501 --> loss:0.8313354754447937
step 51/334, epoch 34/501 --> loss:0.8412495064735412
step 101/334, epoch 34/501 --> loss:0.8211664927005767
step 151/334, epoch 34/501 --> loss:0.8572848761081695
step 201/334, epoch 34/501 --> loss:0.8570129013061524
step 251/334, epoch 34/501 --> loss:0.8557098722457885
step 301/334, epoch 34/501 --> loss:0.8204639208316803
step 51/334, epoch 35/501 --> loss:0.8524485599994659
step 101/334, epoch 35/501 --> loss:0.8432936954498291
step 151/334, epoch 35/501 --> loss:0.8423191404342651
step 201/334, epoch 35/501 --> loss:0.8245688271522522
step 251/334, epoch 35/501 --> loss:0.8475072050094604
step 301/334, epoch 35/501 --> loss:0.8535647821426392
step 51/334, epoch 36/501 --> loss:0.8453478837013244
step 101/334, epoch 36/501 --> loss:0.8503879261016846
step 151/334, epoch 36/501 --> loss:0.8531061637401581
step 201/334, epoch 36/501 --> loss:0.8285246133804322
step 251/334, epoch 36/501 --> loss:0.8484564995765687
step 301/334, epoch 36/501 --> loss:0.8310890507698059
step 51/334, epoch 37/501 --> loss:0.8461241900920868
step 101/334, epoch 37/501 --> loss:0.8591845571994782
step 151/334, epoch 37/501 --> loss:0.835186585187912
step 201/334, epoch 37/501 --> loss:0.8265403270721435
step 251/334, epoch 37/501 --> loss:0.8563717389106751
step 301/334, epoch 37/501 --> loss:0.8241680562496185
step 51/334, epoch 38/501 --> loss:0.8373048841953278
step 101/334, epoch 38/501 --> loss:0.8524590456485748
step 151/334, epoch 38/501 --> loss:0.8366095697879792
step 201/334, epoch 38/501 --> loss:0.8353508853912354
step 251/334, epoch 38/501 --> loss:0.8370341753959656
step 301/334, epoch 38/501 --> loss:0.865461288690567
step 51/334, epoch 39/501 --> loss:0.831026062965393
step 101/334, epoch 39/501 --> loss:0.8593962252140045
step 151/334, epoch 39/501 --> loss:0.8519233644008637
step 201/334, epoch 39/501 --> loss:0.8301672565937043
step 251/334, epoch 39/501 --> loss:0.8567447507381439
step 301/334, epoch 39/501 --> loss:0.8410768508911133
step 51/334, epoch 40/501 --> loss:0.8260482704639435
step 101/334, epoch 40/501 --> loss:0.8574768590927124
step 151/334, epoch 40/501 --> loss:0.8413019204139709
step 201/334, epoch 40/501 --> loss:0.8437082254886628
step 251/334, epoch 40/501 --> loss:0.8422365486621857
step 301/334, epoch 40/501 --> loss:0.8543501114845276
step 51/334, epoch 41/501 --> loss:0.8291898739337921
step 101/334, epoch 41/501 --> loss:0.8354052627086639
step 151/334, epoch 41/501 --> loss:0.8391240191459656
step 201/334, epoch 41/501 --> loss:0.8434781384468079
step 251/334, epoch 41/501 --> loss:0.844495815038681
step 301/334, epoch 41/501 --> loss:0.8630088829994201

##########train dataset##########
acc--> [96.0285514313735]
F1--> {'F1': [0.6632950074068094], 'precision': [0.5187821367284383], 'recall': [0.919424236474311]}
##########eval dataset##########
acc--> [96.0627237955665]
F1--> {'F1': [0.6586854043405429], 'precision': [0.5152596028541714], 'recall': [0.912774915706688]}
save model!
step 51/334, epoch 42/501 --> loss:0.8348013830184936
step 101/334, epoch 42/501 --> loss:0.8486989235877991
step 151/334, epoch 42/501 --> loss:0.8354810547828674
step 201/334, epoch 42/501 --> loss:0.8377764868736267
step 251/334, epoch 42/501 --> loss:0.8342941200733185
step 301/334, epoch 42/501 --> loss:0.8371205985546112
step 51/334, epoch 43/501 --> loss:0.8332931220531463
step 101/334, epoch 43/501 --> loss:0.8562006866931915
step 151/334, epoch 43/501 --> loss:0.8457553589344025
step 201/334, epoch 43/501 --> loss:0.8714260268211365
step 251/334, epoch 43/501 --> loss:0.8376724123954773
step 301/334, epoch 43/501 --> loss:0.8269189047813416
step 51/334, epoch 44/501 --> loss:0.8398877573013306
step 101/334, epoch 44/501 --> loss:0.833652696609497
step 151/334, epoch 44/501 --> loss:0.8508403253555298
step 201/334, epoch 44/501 --> loss:0.8394860339164734
step 251/334, epoch 44/501 --> loss:0.850814163684845
step 301/334, epoch 44/501 --> loss:0.8256763195991517
step 51/334, epoch 45/501 --> loss:0.8345126390457154
step 101/334, epoch 45/501 --> loss:0.8649916005134582
step 151/334, epoch 45/501 --> loss:0.8163585555553436
step 201/334, epoch 45/501 --> loss:0.8208392906188965
step 251/334, epoch 45/501 --> loss:0.8563192188739777
step 301/334, epoch 45/501 --> loss:0.8357059824466705
step 51/334, epoch 46/501 --> loss:0.8159546232223511
step 101/334, epoch 46/501 --> loss:0.8422404336929321
step 151/334, epoch 46/501 --> loss:0.843257474899292
step 201/334, epoch 46/501 --> loss:0.8300200283527375
step 251/334, epoch 46/501 --> loss:0.8425614583492279
step 301/334, epoch 46/501 --> loss:0.8607797598838807
step 51/334, epoch 47/501 --> loss:0.8457719731330872
step 101/334, epoch 47/501 --> loss:0.8274224483966828
step 151/334, epoch 47/501 --> loss:0.8408848035335541
step 201/334, epoch 47/501 --> loss:0.8203327429294586
step 251/334, epoch 47/501 --> loss:0.8507683980464935
step 301/334, epoch 47/501 --> loss:0.8356826186180115
step 51/334, epoch 48/501 --> loss:0.8195037722587586
step 101/334, epoch 48/501 --> loss:0.8477542865276336
step 151/334, epoch 48/501 --> loss:0.8435804760456085
step 201/334, epoch 48/501 --> loss:0.8403511607646942
step 251/334, epoch 48/501 --> loss:0.8474958992004394
step 301/334, epoch 48/501 --> loss:0.8263961148262023
step 51/334, epoch 49/501 --> loss:0.8543975305557251
step 101/334, epoch 49/501 --> loss:0.8276987612247467
step 151/334, epoch 49/501 --> loss:0.8535393476486206
step 201/334, epoch 49/501 --> loss:0.8272198486328125
step 251/334, epoch 49/501 --> loss:0.8055254673957825
step 301/334, epoch 49/501 --> loss:0.8632050240039826
step 51/334, epoch 50/501 --> loss:0.8391119337081909
step 101/334, epoch 50/501 --> loss:0.8467730951309204
step 151/334, epoch 50/501 --> loss:0.8323870444297791
step 201/334, epoch 50/501 --> loss:0.8408113515377045
step 251/334, epoch 50/501 --> loss:0.835345401763916
step 301/334, epoch 50/501 --> loss:0.8314150929450989
step 51/334, epoch 51/501 --> loss:0.832532809972763
step 101/334, epoch 51/501 --> loss:0.8193654656410218
step 151/334, epoch 51/501 --> loss:0.8281370997428894
step 201/334, epoch 51/501 --> loss:0.8337700855731964
step 251/334, epoch 51/501 --> loss:0.8378515481948853
step 301/334, epoch 51/501 --> loss:0.8523040127754211

##########train dataset##########
acc--> [96.21459423132136]
F1--> {'F1': [0.6832587047079294], 'precision': [0.5304873670198932], 'recall': [0.9596281748543952]}
##########eval dataset##########
acc--> [96.17890018385731]
F1--> {'F1': [0.6735242644278767], 'precision': [0.5226207566854723], 'recall': [0.9469684201226841]}
save model!
step 51/334, epoch 52/501 --> loss:0.8401455652713775
step 101/334, epoch 52/501 --> loss:0.8394751286506653
step 151/334, epoch 52/501 --> loss:0.8320222735404968
step 201/334, epoch 52/501 --> loss:0.8468544542789459
step 251/334, epoch 52/501 --> loss:0.8177381861209869
step 301/334, epoch 52/501 --> loss:0.8559556448459625
step 51/334, epoch 53/501 --> loss:0.8662783408164978
step 101/334, epoch 53/501 --> loss:0.8152578282356262
step 151/334, epoch 53/501 --> loss:0.8605841779708863
step 201/334, epoch 53/501 --> loss:0.8288208258152008
step 251/334, epoch 53/501 --> loss:0.8429288685321807
step 301/334, epoch 53/501 --> loss:0.8118728482723236
step 51/334, epoch 54/501 --> loss:0.818588992357254
step 101/334, epoch 54/501 --> loss:0.8386700236797333
step 151/334, epoch 54/501 --> loss:0.8312858724594117
step 201/334, epoch 54/501 --> loss:0.8448404347896576
step 251/334, epoch 54/501 --> loss:0.8595124399662017
step 301/334, epoch 54/501 --> loss:0.8521263360977173
step 51/334, epoch 55/501 --> loss:0.842109968662262
step 101/334, epoch 55/501 --> loss:0.8350850081443787
step 151/334, epoch 55/501 --> loss:0.8390706384181976
step 201/334, epoch 55/501 --> loss:0.8378412199020385
step 251/334, epoch 55/501 --> loss:0.8366857767105103
step 301/334, epoch 55/501 --> loss:0.8519249880313873
step 51/334, epoch 56/501 --> loss:0.8533417904376983
step 101/334, epoch 56/501 --> loss:0.8542521011829376
step 151/334, epoch 56/501 --> loss:0.8352815425395965
step 201/334, epoch 56/501 --> loss:0.8402261292934418
step 251/334, epoch 56/501 --> loss:0.8467273890972138
step 301/334, epoch 56/501 --> loss:0.8312166166305542
step 51/334, epoch 57/501 --> loss:0.8487967741489411
step 101/334, epoch 57/501 --> loss:0.8222762906551361
step 151/334, epoch 57/501 --> loss:0.8493056821823121
step 201/334, epoch 57/501 --> loss:0.8352121019363403
step 251/334, epoch 57/501 --> loss:0.8604111933708191
step 301/334, epoch 57/501 --> loss:0.8155081033706665
step 51/334, epoch 58/501 --> loss:0.8406700420379639
step 101/334, epoch 58/501 --> loss:0.8390236175060273
step 151/334, epoch 58/501 --> loss:0.8394361138343811
step 201/334, epoch 58/501 --> loss:0.8271187448501587
step 251/334, epoch 58/501 --> loss:0.8472123146057129
step 301/334, epoch 58/501 --> loss:0.8174829518795014
step 51/334, epoch 59/501 --> loss:0.8130034279823303
step 101/334, epoch 59/501 --> loss:0.8339335548877717
step 151/334, epoch 59/501 --> loss:0.836672500371933
step 201/334, epoch 59/501 --> loss:0.8420053589344024
step 251/334, epoch 59/501 --> loss:0.839401308298111
step 301/334, epoch 59/501 --> loss:0.8426725220680237
step 51/334, epoch 60/501 --> loss:0.8355555164813996
step 101/334, epoch 60/501 --> loss:0.848270925283432
step 151/334, epoch 60/501 --> loss:0.8266786170005799
step 201/334, epoch 60/501 --> loss:0.8119291114807129
step 251/334, epoch 60/501 --> loss:0.8592934727668762
step 301/334, epoch 60/501 --> loss:0.8583690392971038
step 51/334, epoch 61/501 --> loss:0.8391785800457001
step 101/334, epoch 61/501 --> loss:0.8576408982276916
step 151/334, epoch 61/501 --> loss:0.8327448809146881
step 201/334, epoch 61/501 --> loss:0.8365802204608918
step 251/334, epoch 61/501 --> loss:0.853206148147583
step 301/334, epoch 61/501 --> loss:0.828283965587616

##########train dataset##########
acc--> [96.60827213339999]
F1--> {'F1': [0.7025771871801914], 'precision': [0.5603545716291478], 'recall': [0.9415672703501743]}
##########eval dataset##########
acc--> [96.72001382288116]
F1--> {'F1': [0.7010465987154496], 'precision': [0.5647868307246176], 'recall': [0.9239764103083836]}
save model!
step 51/334, epoch 62/501 --> loss:0.8376385414600372
step 101/334, epoch 62/501 --> loss:0.8323654043674469
step 151/334, epoch 62/501 --> loss:0.8257790982723237
step 201/334, epoch 62/501 --> loss:0.8345550179481507
step 251/334, epoch 62/501 --> loss:0.8467032361030579
step 301/334, epoch 62/501 --> loss:0.8282707583904266
step 51/334, epoch 63/501 --> loss:0.8478415179252624
step 101/334, epoch 63/501 --> loss:0.8309550559520722
step 151/334, epoch 63/501 --> loss:0.8516342413425445
step 201/334, epoch 63/501 --> loss:0.8307722175121307
step 251/334, epoch 63/501 --> loss:0.8450025713443756
step 301/334, epoch 63/501 --> loss:0.8154850137233735
step 51/334, epoch 64/501 --> loss:0.8487964475154877
step 101/334, epoch 64/501 --> loss:0.819171130657196
step 151/334, epoch 64/501 --> loss:0.8372822058200836
step 201/334, epoch 64/501 --> loss:0.8239616250991821
step 251/334, epoch 64/501 --> loss:0.8259139287471772
step 301/334, epoch 64/501 --> loss:0.8531756567955017
step 51/334, epoch 65/501 --> loss:0.8405781531333923
step 101/334, epoch 65/501 --> loss:0.8479620325565338
step 151/334, epoch 65/501 --> loss:0.8298267817497254
step 201/334, epoch 65/501 --> loss:0.8330362391471863
step 251/334, epoch 65/501 --> loss:0.8282795202732086
step 301/334, epoch 65/501 --> loss:0.8593913555145264
step 51/334, epoch 66/501 --> loss:0.8400926852226257
step 101/334, epoch 66/501 --> loss:0.858421436548233
step 151/334, epoch 66/501 --> loss:0.8471250140666962
step 201/334, epoch 66/501 --> loss:0.8412739634513855
step 251/334, epoch 66/501 --> loss:0.8029729497432708
step 301/334, epoch 66/501 --> loss:0.8319105553627014
step 51/334, epoch 67/501 --> loss:0.8431254029273987
step 101/334, epoch 67/501 --> loss:0.836275827884674
step 151/334, epoch 67/501 --> loss:0.8194459545612335
step 201/334, epoch 67/501 --> loss:0.8499384462833405
step 251/334, epoch 67/501 --> loss:0.8301100254058837
step 301/334, epoch 67/501 --> loss:0.8481905591487885
step 51/334, epoch 68/501 --> loss:0.8253524279594422
step 101/334, epoch 68/501 --> loss:0.8257350611686707
step 151/334, epoch 68/501 --> loss:0.8480474472045898
step 201/334, epoch 68/501 --> loss:0.8391832482814788
step 251/334, epoch 68/501 --> loss:0.8497788369655609
step 301/334, epoch 68/501 --> loss:0.849808691740036
step 51/334, epoch 69/501 --> loss:0.846562284231186
step 101/334, epoch 69/501 --> loss:0.8453442788124085
step 151/334, epoch 69/501 --> loss:0.8264551544189453
step 201/334, epoch 69/501 --> loss:0.8263865327835083
step 251/334, epoch 69/501 --> loss:0.8392369604110718
step 301/334, epoch 69/501 --> loss:0.84164959192276
step 51/334, epoch 70/501 --> loss:0.8366115498542785
step 101/334, epoch 70/501 --> loss:0.8389484775066376
step 151/334, epoch 70/501 --> loss:0.831763869524002
step 201/334, epoch 70/501 --> loss:0.8261755776405334
step 251/334, epoch 70/501 --> loss:0.8519221663475036
step 301/334, epoch 70/501 --> loss:0.8098933529853821
step 51/334, epoch 71/501 --> loss:0.8344031572341919
step 101/334, epoch 71/501 --> loss:0.8467780435085297
step 151/334, epoch 71/501 --> loss:0.8242663598060608
step 201/334, epoch 71/501 --> loss:0.8567606782913209
step 251/334, epoch 71/501 --> loss:0.8509849035739898
step 301/334, epoch 71/501 --> loss:0.8330952298641204

##########train dataset##########
acc--> [96.81009751023886]
F1--> {'F1': [0.7194058013383128], 'precision': [0.5748396597039231], 'recall': [0.961133239023849]}
##########eval dataset##########
acc--> [96.82392310594928]
F1--> {'F1': [0.7098967325925032], 'precision': [0.5726670762503753], 'recall': [0.9336395235541154]}
save model!
step 51/334, epoch 72/501 --> loss:0.8346405458450318
step 101/334, epoch 72/501 --> loss:0.840541318655014
step 151/334, epoch 72/501 --> loss:0.8174358546733856
step 201/334, epoch 72/501 --> loss:0.847026469707489
step 251/334, epoch 72/501 --> loss:0.8529842102527618
step 301/334, epoch 72/501 --> loss:0.8265861523151398
step 51/334, epoch 73/501 --> loss:0.8357081842422486
step 101/334, epoch 73/501 --> loss:0.8173899924755097
step 151/334, epoch 73/501 --> loss:0.843082423210144
step 201/334, epoch 73/501 --> loss:0.8497799301147461
step 251/334, epoch 73/501 --> loss:0.842436546087265
step 301/334, epoch 73/501 --> loss:0.8328561604022979
step 51/334, epoch 74/501 --> loss:0.8232926487922668
step 101/334, epoch 74/501 --> loss:0.8291455125808715
step 151/334, epoch 74/501 --> loss:0.8537690377235413
step 201/334, epoch 74/501 --> loss:0.8171952617168426
step 251/334, epoch 74/501 --> loss:0.8317774212360383
step 301/334, epoch 74/501 --> loss:0.8536455452442169
step 51/334, epoch 75/501 --> loss:0.842765417098999
step 101/334, epoch 75/501 --> loss:0.8306618404388427
step 151/334, epoch 75/501 --> loss:0.8438859987258911
step 201/334, epoch 75/501 --> loss:0.8457545113563537
step 251/334, epoch 75/501 --> loss:0.8256500601768494
step 301/334, epoch 75/501 --> loss:0.8392175984382629
step 51/334, epoch 76/501 --> loss:0.8348634469509125
step 101/334, epoch 76/501 --> loss:0.8612895607948303
step 151/334, epoch 76/501 --> loss:0.8418761348724365
step 201/334, epoch 76/501 --> loss:0.8396411216259003
step 251/334, epoch 76/501 --> loss:0.8246064853668212
step 301/334, epoch 76/501 --> loss:0.8284099876880646
step 51/334, epoch 77/501 --> loss:0.8306874644756317
step 101/334, epoch 77/501 --> loss:0.852532753944397
step 151/334, epoch 77/501 --> loss:0.8360422348976135
step 201/334, epoch 77/501 --> loss:0.8175479638576507
step 251/334, epoch 77/501 --> loss:0.8424272012710571
step 301/334, epoch 77/501 --> loss:0.8470685148239135
step 51/334, epoch 78/501 --> loss:0.8579586923122406
step 101/334, epoch 78/501 --> loss:0.8390926551818848
step 151/334, epoch 78/501 --> loss:0.8320435035228729
step 201/334, epoch 78/501 --> loss:0.8306719851493836
step 251/334, epoch 78/501 --> loss:0.8097030806541443
step 301/334, epoch 78/501 --> loss:0.8449420046806335
step 51/334, epoch 79/501 --> loss:0.860662761926651
step 101/334, epoch 79/501 --> loss:0.8225178134441375
step 151/334, epoch 79/501 --> loss:0.8286141383647919
step 201/334, epoch 79/501 --> loss:0.8328914332389832
step 251/334, epoch 79/501 --> loss:0.8228980112075805
step 301/334, epoch 79/501 --> loss:0.8464946556091308
step 51/334, epoch 80/501 --> loss:0.8359005868434906
step 101/334, epoch 80/501 --> loss:0.8392607283592224
step 151/334, epoch 80/501 --> loss:0.8268443191051483
step 201/334, epoch 80/501 --> loss:0.8687097001075744
step 251/334, epoch 80/501 --> loss:0.8229938971996308
step 301/334, epoch 80/501 --> loss:0.8305398941040039
step 51/334, epoch 81/501 --> loss:0.8265124773979187
step 101/334, epoch 81/501 --> loss:0.8390527367591858
step 151/334, epoch 81/501 --> loss:0.841026121377945
step 201/334, epoch 81/501 --> loss:0.8279904270172119
step 251/334, epoch 81/501 --> loss:0.8380564296245575
step 301/334, epoch 81/501 --> loss:0.8340700280666351

##########train dataset##########
acc--> [96.70298819153676]
F1--> {'F1': [0.7148667058966738], 'precision': [0.5655185554503831], 'recall': [0.9714230773361799]}
##########eval dataset##########
acc--> [96.53485515924244]
F1--> {'F1': [0.6951106705533512], 'precision': [0.5483931888557023], 'recall': [0.9490265444135669]}
step 51/334, epoch 82/501 --> loss:0.8247768652439117
step 101/334, epoch 82/501 --> loss:0.844198123216629
step 151/334, epoch 82/501 --> loss:0.852246241569519
step 201/334, epoch 82/501 --> loss:0.8476497852802276
step 251/334, epoch 82/501 --> loss:0.8221386659145355
step 301/334, epoch 82/501 --> loss:0.8262728297710419
step 51/334, epoch 83/501 --> loss:0.8208303678035737
step 101/334, epoch 83/501 --> loss:0.848784692287445
step 151/334, epoch 83/501 --> loss:0.8474834930896759
step 201/334, epoch 83/501 --> loss:0.8150687718391418
step 251/334, epoch 83/501 --> loss:0.8455692970752716
step 301/334, epoch 83/501 --> loss:0.8309610664844513
step 51/334, epoch 84/501 --> loss:0.840180195569992
step 101/334, epoch 84/501 --> loss:0.8574568498134613
step 151/334, epoch 84/501 --> loss:0.8472884750366211
step 201/334, epoch 84/501 --> loss:0.8266819095611573
step 251/334, epoch 84/501 --> loss:0.8216550779342652
step 301/334, epoch 84/501 --> loss:0.812737009525299
step 51/334, epoch 85/501 --> loss:0.8529782938957214
step 101/334, epoch 85/501 --> loss:0.853548856973648
step 151/334, epoch 85/501 --> loss:0.841347405910492
step 201/334, epoch 85/501 --> loss:0.816154055595398
step 251/334, epoch 85/501 --> loss:0.8298244106769562
step 301/334, epoch 85/501 --> loss:0.8241055989265442
step 51/334, epoch 86/501 --> loss:0.8366562604904175
step 101/334, epoch 86/501 --> loss:0.8316116058826446
step 151/334, epoch 86/501 --> loss:0.8343135356903076
step 201/334, epoch 86/501 --> loss:0.8605753755569459
step 251/334, epoch 86/501 --> loss:0.8407559621334076
step 301/334, epoch 86/501 --> loss:0.8052781200408936
step 51/334, epoch 87/501 --> loss:0.8221328616142273
step 101/334, epoch 87/501 --> loss:0.815363564491272
step 151/334, epoch 87/501 --> loss:0.8460878801345825
step 201/334, epoch 87/501 --> loss:0.8289408087730408
step 251/334, epoch 87/501 --> loss:0.8549774539470673
step 301/334, epoch 87/501 --> loss:0.8471799421310425
step 51/334, epoch 88/501 --> loss:0.8266904449462891
step 101/334, epoch 88/501 --> loss:0.8465523111820221
step 151/334, epoch 88/501 --> loss:0.8292398071289062
step 201/334, epoch 88/501 --> loss:0.8507278823852539
step 251/334, epoch 88/501 --> loss:0.8192730164527893
step 301/334, epoch 88/501 --> loss:0.8267560803890228
step 51/334, epoch 89/501 --> loss:0.8293594133853912
step 101/334, epoch 89/501 --> loss:0.8483400511741638
step 151/334, epoch 89/501 --> loss:0.8265417551994324
step 201/334, epoch 89/501 --> loss:0.8351821434497834
step 251/334, epoch 89/501 --> loss:0.8457109582424164
step 301/334, epoch 89/501 --> loss:0.8185611462593079
step 51/334, epoch 90/501 --> loss:0.8353773486614228
step 101/334, epoch 90/501 --> loss:0.8428280615806579
step 151/334, epoch 90/501 --> loss:0.8251643514633179
step 201/334, epoch 90/501 --> loss:0.8274935603141784
step 251/334, epoch 90/501 --> loss:0.8376971507072448
step 301/334, epoch 90/501 --> loss:0.8430530405044556
step 51/334, epoch 91/501 --> loss:0.8303502762317657
step 101/334, epoch 91/501 --> loss:0.8367396593093872
step 151/334, epoch 91/501 --> loss:0.8400718069076538
step 201/334, epoch 91/501 --> loss:0.829726232290268
step 251/334, epoch 91/501 --> loss:0.8344682836532593
step 301/334, epoch 91/501 --> loss:0.8428216421604157

##########train dataset##########
acc--> [97.10125946399366]
F1--> {'F1': [0.7384204786318634], 'precision': [0.5993061339364809], 'recall': [0.9616584146426831]}
##########eval dataset##########
acc--> [96.74976111917859]
F1--> {'F1': [0.7049383985216884], 'precision': [0.5665421292579799], 'recall': [0.9328231053863669]}
step 51/334, epoch 92/501 --> loss:0.8086401903629303
step 101/334, epoch 92/501 --> loss:0.8426042950153351
step 151/334, epoch 92/501 --> loss:0.8357553029060364
step 201/334, epoch 92/501 --> loss:0.833872252702713
step 251/334, epoch 92/501 --> loss:0.8592977821826935
step 301/334, epoch 92/501 --> loss:0.8140744197368622
step 51/334, epoch 93/501 --> loss:0.8489019346237182
step 101/334, epoch 93/501 --> loss:0.8466673958301544
step 151/334, epoch 93/501 --> loss:0.8313871002197266
step 201/334, epoch 93/501 --> loss:0.8340353834629058
step 251/334, epoch 93/501 --> loss:0.811822988986969
step 301/334, epoch 93/501 --> loss:0.8629512691497803
step 51/334, epoch 94/501 --> loss:0.840542596578598
step 101/334, epoch 94/501 --> loss:0.8496430599689484
step 151/334, epoch 94/501 --> loss:0.8315116453170777
step 201/334, epoch 94/501 --> loss:0.8350914180278778
step 251/334, epoch 94/501 --> loss:0.8270242941379548
step 301/334, epoch 94/501 --> loss:0.8329846930503845
step 51/334, epoch 95/501 --> loss:0.8251405966281891
step 101/334, epoch 95/501 --> loss:0.8455988776683807
step 151/334, epoch 95/501 --> loss:0.8523119175434113
step 201/334, epoch 95/501 --> loss:0.8314665496349335
step 251/334, epoch 95/501 --> loss:0.8343525552749633
step 301/334, epoch 95/501 --> loss:0.8511846280097961
step 51/334, epoch 96/501 --> loss:0.8250902807712555
step 101/334, epoch 96/501 --> loss:0.8290826773643494
step 151/334, epoch 96/501 --> loss:0.8159219300746918
step 201/334, epoch 96/501 --> loss:0.8505412602424621
step 251/334, epoch 96/501 --> loss:0.8503715217113494
step 301/334, epoch 96/501 --> loss:0.8250302147865295
step 51/334, epoch 97/501 --> loss:0.8587481939792633
step 101/334, epoch 97/501 --> loss:0.8070120072364807
step 151/334, epoch 97/501 --> loss:0.8456492090225219
step 201/334, epoch 97/501 --> loss:0.8487730121612549
step 251/334, epoch 97/501 --> loss:0.81432448387146
step 301/334, epoch 97/501 --> loss:0.8304366683959961
step 51/334, epoch 98/501 --> loss:0.8371786856651307
step 101/334, epoch 98/501 --> loss:0.8366762387752533
step 151/334, epoch 98/501 --> loss:0.8550380158424378
step 201/334, epoch 98/501 --> loss:0.8376782345771789
step 251/334, epoch 98/501 --> loss:0.8221385097503662
step 301/334, epoch 98/501 --> loss:0.8213427364826202
step 51/334, epoch 99/501 --> loss:0.8489614999294282
step 101/334, epoch 99/501 --> loss:0.8271578681468964
step 151/334, epoch 99/501 --> loss:0.8143267822265625
step 201/334, epoch 99/501 --> loss:0.8255740809440613
step 251/334, epoch 99/501 --> loss:0.8376146328449249
step 301/334, epoch 99/501 --> loss:0.8484920716285705
step 51/334, epoch 100/501 --> loss:0.8441496574878693
step 101/334, epoch 100/501 --> loss:0.8344562304019928
step 151/334, epoch 100/501 --> loss:0.8251706695556641
step 201/334, epoch 100/501 --> loss:0.8441428089141846
step 251/334, epoch 100/501 --> loss:0.8463383483886718
step 301/334, epoch 100/501 --> loss:0.8004084086418152
step 51/334, epoch 101/501 --> loss:0.8384131610393524
step 101/334, epoch 101/501 --> loss:0.8326128840446472
step 151/334, epoch 101/501 --> loss:0.85262540102005
step 201/334, epoch 101/501 --> loss:0.8216480946540833
step 251/334, epoch 101/501 --> loss:0.8422873318195343
step 301/334, epoch 101/501 --> loss:0.8300924038887024

##########train dataset##########
acc--> [97.66638676772179]
F1--> {'F1': [0.7785430774205118], 'precision': [0.6528780297332325], 'recall': [0.9641292479311911]}
##########eval dataset##########
acc--> [97.38220121394697]
F1--> {'F1': [0.7463889480843963], 'precision': [0.6253663067560106], 'recall': [0.9255077356047556]}
save model!
step 51/334, epoch 102/501 --> loss:0.836520414352417
step 101/334, epoch 102/501 --> loss:0.8465974652767181
step 151/334, epoch 102/501 --> loss:0.8440106153488159
step 201/334, epoch 102/501 --> loss:0.8346954894065857
step 251/334, epoch 102/501 --> loss:0.8056358075141907
step 301/334, epoch 102/501 --> loss:0.84568763256073
step 51/334, epoch 103/501 --> loss:0.8400968527793884
step 101/334, epoch 103/501 --> loss:0.8397199046611786
step 151/334, epoch 103/501 --> loss:0.8415969443321228
step 201/334, epoch 103/501 --> loss:0.8338821351528167
step 251/334, epoch 103/501 --> loss:0.8181031584739685
step 301/334, epoch 103/501 --> loss:0.811575379371643
step 51/334, epoch 104/501 --> loss:0.8478693985939025
step 101/334, epoch 104/501 --> loss:0.8247024428844452
step 151/334, epoch 104/501 --> loss:0.8100296127796173
step 201/334, epoch 104/501 --> loss:0.8507795023918152
step 251/334, epoch 104/501 --> loss:0.840525072813034
step 301/334, epoch 104/501 --> loss:0.8335429048538208
step 51/334, epoch 105/501 --> loss:0.8417860472202301
step 101/334, epoch 105/501 --> loss:0.8372697579860687
step 151/334, epoch 105/501 --> loss:0.8087929809093475
step 201/334, epoch 105/501 --> loss:0.8493996286392211
step 251/334, epoch 105/501 --> loss:0.8423370611667633
step 301/334, epoch 105/501 --> loss:0.8293882524967193
step 51/334, epoch 106/501 --> loss:0.848606083393097
step 101/334, epoch 106/501 --> loss:0.8433771228790283
step 151/334, epoch 106/501 --> loss:0.8357786095142364
step 201/334, epoch 106/501 --> loss:0.841425951719284
step 251/334, epoch 106/501 --> loss:0.8309234285354614
step 301/334, epoch 106/501 --> loss:0.8246332395076752
step 51/334, epoch 107/501 --> loss:0.8257947993278504
step 101/334, epoch 107/501 --> loss:0.8304532885551452
step 151/334, epoch 107/501 --> loss:0.8146012091636657
step 201/334, epoch 107/501 --> loss:0.845250540971756
step 251/334, epoch 107/501 --> loss:0.8229914700984955
step 301/334, epoch 107/501 --> loss:0.8550273251533508
step 51/334, epoch 108/501 --> loss:0.8019995868206025
step 101/334, epoch 108/501 --> loss:0.840131151676178
step 151/334, epoch 108/501 --> loss:0.8507953250408172
step 201/334, epoch 108/501 --> loss:0.8542748689651489
step 251/334, epoch 108/501 --> loss:0.8307034730911255
step 301/334, epoch 108/501 --> loss:0.8203679466247559
step 51/334, epoch 109/501 --> loss:0.8342630290985107
step 101/334, epoch 109/501 --> loss:0.8376613461971283
step 151/334, epoch 109/501 --> loss:0.8405954504013061
step 201/334, epoch 109/501 --> loss:0.8160707855224609
step 251/334, epoch 109/501 --> loss:0.8127198481559753
step 301/334, epoch 109/501 --> loss:0.8596424341201783
step 51/334, epoch 110/501 --> loss:0.8189657783508301
step 101/334, epoch 110/501 --> loss:0.8220053887367249
step 151/334, epoch 110/501 --> loss:0.8351611840724945
step 201/334, epoch 110/501 --> loss:0.8163384890556336
step 251/334, epoch 110/501 --> loss:0.8286456739902497
step 301/334, epoch 110/501 --> loss:0.8486580312252044
step 51/334, epoch 111/501 --> loss:0.8489113318920135
step 101/334, epoch 111/501 --> loss:0.8131000328063965
step 151/334, epoch 111/501 --> loss:0.8578150248527527
step 201/334, epoch 111/501 --> loss:0.8240788280963898
step 251/334, epoch 111/501 --> loss:0.8382385396957397
step 301/334, epoch 111/501 --> loss:0.837717148065567

##########train dataset##########
acc--> [98.08580636709681]
F1--> {'F1': [0.8112563833857697], 'precision': [0.698775156863507], 'recall': [0.9669107759495204]}
##########eval dataset##########
acc--> [97.83690378316925]
F1--> {'F1': [0.780870570515686], 'precision': [0.6750860311418179], 'recall': [0.9259814537873384]}
save model!
step 51/334, epoch 112/501 --> loss:0.8334558987617493
step 101/334, epoch 112/501 --> loss:0.8266702282428742
step 151/334, epoch 112/501 --> loss:0.8229523468017578
step 201/334, epoch 112/501 --> loss:0.8302063632011414
step 251/334, epoch 112/501 --> loss:0.8498464810848236
step 301/334, epoch 112/501 --> loss:0.8228784072399139
step 51/334, epoch 113/501 --> loss:0.856973739862442
step 101/334, epoch 113/501 --> loss:0.8107527697086334
step 151/334, epoch 113/501 --> loss:0.8186466455459595
step 201/334, epoch 113/501 --> loss:0.8489530658721924
step 251/334, epoch 113/501 --> loss:0.8358229911327362
step 301/334, epoch 113/501 --> loss:0.8313610601425171
step 51/334, epoch 114/501 --> loss:0.8554940366744995
step 101/334, epoch 114/501 --> loss:0.8539394247531891
step 151/334, epoch 114/501 --> loss:0.8374926686286926
step 201/334, epoch 114/501 --> loss:0.8184663212299347
step 251/334, epoch 114/501 --> loss:0.8013845372200012
step 301/334, epoch 114/501 --> loss:0.8259614372253418
step 51/334, epoch 115/501 --> loss:0.8537201869487763
step 101/334, epoch 115/501 --> loss:0.8271790874004364
step 151/334, epoch 115/501 --> loss:0.8544547438621521
step 201/334, epoch 115/501 --> loss:0.7999369430541993
step 251/334, epoch 115/501 --> loss:0.8137378764152526
step 301/334, epoch 115/501 --> loss:0.8529033017158508
step 51/334, epoch 116/501 --> loss:0.843661173582077
step 101/334, epoch 116/501 --> loss:0.8352694523334503
step 151/334, epoch 116/501 --> loss:0.8161391353607178
step 201/334, epoch 116/501 --> loss:0.8349936020374298
step 251/334, epoch 116/501 --> loss:0.842251433134079
step 301/334, epoch 116/501 --> loss:0.8433806467056274
step 51/334, epoch 117/501 --> loss:0.8329516029357911
step 101/334, epoch 117/501 --> loss:0.8115036392211914
step 151/334, epoch 117/501 --> loss:0.8297017359733582
step 201/334, epoch 117/501 --> loss:0.8496960783004761
step 251/334, epoch 117/501 --> loss:0.8260979628562928
step 301/334, epoch 117/501 --> loss:0.8289999055862427
step 51/334, epoch 118/501 --> loss:0.8469735431671143
step 101/334, epoch 118/501 --> loss:0.8304924774169922
step 151/334, epoch 118/501 --> loss:0.8419924306869507
step 201/334, epoch 118/501 --> loss:0.8129506897926331
step 251/334, epoch 118/501 --> loss:0.8354913580417633
step 301/334, epoch 118/501 --> loss:0.8287861406803131
step 51/334, epoch 119/501 --> loss:0.8391911745071411
step 101/334, epoch 119/501 --> loss:0.8205970132350922
step 151/334, epoch 119/501 --> loss:0.8593317174911499
step 201/334, epoch 119/501 --> loss:0.8152259778976441
step 251/334, epoch 119/501 --> loss:0.8415165424346924
step 301/334, epoch 119/501 --> loss:0.8442865777015686
step 51/334, epoch 120/501 --> loss:0.8333844888210297
step 101/334, epoch 120/501 --> loss:0.8422103667259216
step 151/334, epoch 120/501 --> loss:0.8215655255317688
step 201/334, epoch 120/501 --> loss:0.8333222579956054
step 251/334, epoch 120/501 --> loss:0.8346343719959259
step 301/334, epoch 120/501 --> loss:0.8434284174442291
step 51/334, epoch 121/501 --> loss:0.8111786413192749
step 101/334, epoch 121/501 --> loss:0.8316820669174194
step 151/334, epoch 121/501 --> loss:0.8514976596832275
step 201/334, epoch 121/501 --> loss:0.8394006180763245
step 251/334, epoch 121/501 --> loss:0.8233956861495971
step 301/334, epoch 121/501 --> loss:0.8368573474884033

##########train dataset##########
acc--> [97.72897497919902]
F1--> {'F1': [0.7861105868111395], 'precision': [0.6558688172900506], 'recall': [0.9809108647002236]}
##########eval dataset##########
acc--> [97.26653211698107]
F1--> {'F1': [0.7417902438656064], 'precision': [0.6112091152502245], 'recall': [0.9433416054925918]}
step 51/334, epoch 122/501 --> loss:0.8251392543315887
step 101/334, epoch 122/501 --> loss:0.8372133851051331
step 151/334, epoch 122/501 --> loss:0.8511746716499329
step 201/334, epoch 122/501 --> loss:0.8120399677753448
step 251/334, epoch 122/501 --> loss:0.83669393658638
step 301/334, epoch 122/501 --> loss:0.835269364118576
step 51/334, epoch 123/501 --> loss:0.8234962248802185
step 101/334, epoch 123/501 --> loss:0.8347497081756592
step 151/334, epoch 123/501 --> loss:0.8178836023807525
step 201/334, epoch 123/501 --> loss:0.8373898375034332
step 251/334, epoch 123/501 --> loss:0.8100997459888458
step 301/334, epoch 123/501 --> loss:0.8376273512840271
step 51/334, epoch 124/501 --> loss:0.8449615943431854
step 101/334, epoch 124/501 --> loss:0.8183230125904083
step 151/334, epoch 124/501 --> loss:0.8321659147739411
step 201/334, epoch 124/501 --> loss:0.8482234025001526
step 251/334, epoch 124/501 --> loss:0.8233438789844513
step 301/334, epoch 124/501 --> loss:0.8218319761753082
step 51/334, epoch 125/501 --> loss:0.8222149443626404
step 101/334, epoch 125/501 --> loss:0.83475945353508
step 151/334, epoch 125/501 --> loss:0.8389304041862488
step 201/334, epoch 125/501 --> loss:0.8226608788967132
step 251/334, epoch 125/501 --> loss:0.8119817042350769
step 301/334, epoch 125/501 --> loss:0.8433626461029052
step 51/334, epoch 126/501 --> loss:0.8237054824829102
step 101/334, epoch 126/501 --> loss:0.8223477256298065
step 151/334, epoch 126/501 --> loss:0.8573270630836487
step 201/334, epoch 126/501 --> loss:0.8419049406051635
step 251/334, epoch 126/501 --> loss:0.8142177593708039
step 301/334, epoch 126/501 --> loss:0.8292470264434815
step 51/334, epoch 127/501 --> loss:0.8369081163406372
step 101/334, epoch 127/501 --> loss:0.8383821952342987
step 151/334, epoch 127/501 --> loss:0.8320446002483368
step 201/334, epoch 127/501 --> loss:0.8342840909957886
step 251/334, epoch 127/501 --> loss:0.8288303518295288
step 301/334, epoch 127/501 --> loss:0.829211231470108
step 51/334, epoch 128/501 --> loss:0.8233314907550812
step 101/334, epoch 128/501 --> loss:0.8179962229728699
step 151/334, epoch 128/501 --> loss:0.8243757402896881
step 201/334, epoch 128/501 --> loss:0.8479016613960266
step 251/334, epoch 128/501 --> loss:0.8498796463012696
step 301/334, epoch 128/501 --> loss:0.8350864636898041
step 51/334, epoch 129/501 --> loss:0.8290410649776458
step 101/334, epoch 129/501 --> loss:0.8286411595344544
step 151/334, epoch 129/501 --> loss:0.843813591003418
step 201/334, epoch 129/501 --> loss:0.8127386116981506
step 251/334, epoch 129/501 --> loss:0.8511922478675842
step 301/334, epoch 129/501 --> loss:0.8298061156272888
step 51/334, epoch 130/501 --> loss:0.8565109956264496
step 101/334, epoch 130/501 --> loss:0.8555857527256012
step 151/334, epoch 130/501 --> loss:0.8181234836578369
step 201/334, epoch 130/501 --> loss:0.8154438090324402
step 251/334, epoch 130/501 --> loss:0.8274651622772217
step 301/334, epoch 130/501 --> loss:0.8244002270698547
step 51/334, epoch 131/501 --> loss:0.8284877896308899
step 101/334, epoch 131/501 --> loss:0.8123702228069305
step 151/334, epoch 131/501 --> loss:0.8331913900375366
step 201/334, epoch 131/501 --> loss:0.8265385913848877
step 251/334, epoch 131/501 --> loss:0.8222767734527587
step 301/334, epoch 131/501 --> loss:0.8466775870323181

##########train dataset##########
acc--> [98.08365314427579]
F1--> {'F1': [0.8136072700290724], 'precision': [0.6939964876281706], 'recall': [0.983048008756076]}
##########eval dataset##########
acc--> [97.68925731054206]
F1--> {'F1': [0.770929701973243], 'precision': [0.6562400692918716], 'recall': [0.9342115454760087]}
step 51/334, epoch 132/501 --> loss:0.8344530189037322
step 101/334, epoch 132/501 --> loss:0.8104333734512329
step 151/334, epoch 132/501 --> loss:0.8181874012947082
step 201/334, epoch 132/501 --> loss:0.8232601988315582
step 251/334, epoch 132/501 --> loss:0.8251927495002747
step 301/334, epoch 132/501 --> loss:0.8640120804309845
step 51/334, epoch 133/501 --> loss:0.8445892906188965
step 101/334, epoch 133/501 --> loss:0.8507684683799743
step 151/334, epoch 133/501 --> loss:0.8264727723598481
step 201/334, epoch 133/501 --> loss:0.835080955028534
step 251/334, epoch 133/501 --> loss:0.8403646075725555
step 301/334, epoch 133/501 --> loss:0.8171550726890564
step 51/334, epoch 134/501 --> loss:0.833048883676529
step 101/334, epoch 134/501 --> loss:0.8365063858032227
step 151/334, epoch 134/501 --> loss:0.8350859403610229
step 201/334, epoch 134/501 --> loss:0.820171183347702
step 251/334, epoch 134/501 --> loss:0.8580161690711975
step 301/334, epoch 134/501 --> loss:0.8183926033973694
step 51/334, epoch 135/501 --> loss:0.8445276761054993
step 101/334, epoch 135/501 --> loss:0.8076074194908142
step 151/334, epoch 135/501 --> loss:0.826664320230484
step 201/334, epoch 135/501 --> loss:0.8346827864646912
step 251/334, epoch 135/501 --> loss:0.825416818857193
step 301/334, epoch 135/501 --> loss:0.8334182739257813
step 51/334, epoch 136/501 --> loss:0.8173644912242889
step 101/334, epoch 136/501 --> loss:0.8232215690612793
step 151/334, epoch 136/501 --> loss:0.839952882528305
step 201/334, epoch 136/501 --> loss:0.845551677942276
step 251/334, epoch 136/501 --> loss:0.8288016951084137
step 301/334, epoch 136/501 --> loss:0.8292375838756562
step 51/334, epoch 137/501 --> loss:0.8295390033721923
step 101/334, epoch 137/501 --> loss:0.8067322528362274
step 151/334, epoch 137/501 --> loss:0.8244245707988739
step 201/334, epoch 137/501 --> loss:0.826247911453247
step 251/334, epoch 137/501 --> loss:0.833198003768921
step 301/334, epoch 137/501 --> loss:0.8751767265796662
step 51/334, epoch 138/501 --> loss:0.8297947680950165
step 101/334, epoch 138/501 --> loss:0.8381113088130951
step 151/334, epoch 138/501 --> loss:0.8220150983333587
step 201/334, epoch 138/501 --> loss:0.8435517704486847
step 251/334, epoch 138/501 --> loss:0.8286205101013183
step 301/334, epoch 138/501 --> loss:0.8341151130199432
step 51/334, epoch 139/501 --> loss:0.8278949248790741
step 101/334, epoch 139/501 --> loss:0.8491858541965485
step 151/334, epoch 139/501 --> loss:0.8176959133148194
step 201/334, epoch 139/501 --> loss:0.8217530393600464
step 251/334, epoch 139/501 --> loss:0.8305900382995606
step 301/334, epoch 139/501 --> loss:0.833504866361618
step 51/334, epoch 140/501 --> loss:0.861494414806366
step 101/334, epoch 140/501 --> loss:0.8151030743122101
step 151/334, epoch 140/501 --> loss:0.8374426686763763
step 201/334, epoch 140/501 --> loss:0.8474330592155457
step 251/334, epoch 140/501 --> loss:0.8386109972000122
step 301/334, epoch 140/501 --> loss:0.8045352613925933
step 51/334, epoch 141/501 --> loss:0.841583321094513
step 101/334, epoch 141/501 --> loss:0.828035101890564
step 151/334, epoch 141/501 --> loss:0.8302233695983887
step 201/334, epoch 141/501 --> loss:0.8215519797801971
step 251/334, epoch 141/501 --> loss:0.8532491827011108
step 301/334, epoch 141/501 --> loss:0.8308149671554566

##########train dataset##########
acc--> [97.90536303353646]
F1--> {'F1': [0.7998164565959296], 'precision': [0.6739425715599476], 'recall': [0.9835233629521017]}
##########eval dataset##########
acc--> [97.45375932774597]
F1--> {'F1': [0.7530669825411502], 'precision': [0.6314014116414273], 'recall': [0.9328264730511721]}
step 51/334, epoch 142/501 --> loss:0.8186039018630982
step 101/334, epoch 142/501 --> loss:0.8311523973941803
step 151/334, epoch 142/501 --> loss:0.8445349073410034
step 201/334, epoch 142/501 --> loss:0.8212103199958801
step 251/334, epoch 142/501 --> loss:0.8261782705783844
step 301/334, epoch 142/501 --> loss:0.8369423067569732
step 51/334, epoch 143/501 --> loss:0.8290019488334656
step 101/334, epoch 143/501 --> loss:0.8416373550891876
step 151/334, epoch 143/501 --> loss:0.8235034728050232
step 201/334, epoch 143/501 --> loss:0.8382959747314453
step 251/334, epoch 143/501 --> loss:0.835647474527359
step 301/334, epoch 143/501 --> loss:0.8235270214080811
step 51/334, epoch 144/501 --> loss:0.826281087398529
step 101/334, epoch 144/501 --> loss:0.798005610704422
step 151/334, epoch 144/501 --> loss:0.8350278830528259
step 201/334, epoch 144/501 --> loss:0.8352178311347962
step 251/334, epoch 144/501 --> loss:0.8504313993453979
step 301/334, epoch 144/501 --> loss:0.83443412899971
step 51/334, epoch 145/501 --> loss:0.8163320672512054
step 101/334, epoch 145/501 --> loss:0.8222653889656066
step 151/334, epoch 145/501 --> loss:0.8090277695655823
step 201/334, epoch 145/501 --> loss:0.8352908504009247
step 251/334, epoch 145/501 --> loss:0.8514117467403411
step 301/334, epoch 145/501 --> loss:0.8444019210338592
step 51/334, epoch 146/501 --> loss:0.8355862355232239
step 101/334, epoch 146/501 --> loss:0.8430138921737671
step 151/334, epoch 146/501 --> loss:0.8225972616672516
step 201/334, epoch 146/501 --> loss:0.8159103190898895
step 251/334, epoch 146/501 --> loss:0.8185401463508606
step 301/334, epoch 146/501 --> loss:0.848289145231247
step 51/334, epoch 147/501 --> loss:0.8125579750537872
step 101/334, epoch 147/501 --> loss:0.8275577914714813
step 151/334, epoch 147/501 --> loss:0.8468962168693542
step 201/334, epoch 147/501 --> loss:0.8552272534370422
step 251/334, epoch 147/501 --> loss:0.8207421684265137
step 301/334, epoch 147/501 --> loss:0.8377256846427917
step 51/334, epoch 148/501 --> loss:0.826078872680664
step 101/334, epoch 148/501 --> loss:0.8213488399982453
step 151/334, epoch 148/501 --> loss:0.8259266591072083
step 201/334, epoch 148/501 --> loss:0.8304018175601959
step 251/334, epoch 148/501 --> loss:0.8464682054519653
step 301/334, epoch 148/501 --> loss:0.847871196269989
step 51/334, epoch 149/501 --> loss:0.8048706197738648
step 101/334, epoch 149/501 --> loss:0.8289775359630585
step 151/334, epoch 149/501 --> loss:0.8508917891979217
step 201/334, epoch 149/501 --> loss:0.8153370237350464
step 251/334, epoch 149/501 --> loss:0.8424738419055938
step 301/334, epoch 149/501 --> loss:0.8357897722721099
step 51/334, epoch 150/501 --> loss:0.8416479206085206
step 101/334, epoch 150/501 --> loss:0.8276071715354919
step 151/334, epoch 150/501 --> loss:0.8521671295166016
step 201/334, epoch 150/501 --> loss:0.8134676802158356
step 251/334, epoch 150/501 --> loss:0.8363098895549774
step 301/334, epoch 150/501 --> loss:0.8380964112281799
step 51/334, epoch 151/501 --> loss:0.8448906397819519
step 101/334, epoch 151/501 --> loss:0.819458589553833
step 151/334, epoch 151/501 --> loss:0.8361900103092194
step 201/334, epoch 151/501 --> loss:0.8305611276626587
step 251/334, epoch 151/501 --> loss:0.8368746089935303
step 301/334, epoch 151/501 --> loss:0.8437055134773255

##########train dataset##########
acc--> [98.60289651979866]
F1--> {'F1': [0.8555891951180825], 'precision': [0.763609471561946], 'recall': [0.9727749120554579]}
##########eval dataset##########
acc--> [98.10167180792419]
F1--> {'F1': [0.7997314628984652], 'precision': [0.7129066490630547], 'recall': [0.9106507210396292]}
save model!
step 51/334, epoch 152/501 --> loss:0.8394428980350495
step 101/334, epoch 152/501 --> loss:0.8444914960861206
step 151/334, epoch 152/501 --> loss:0.8392169308662415
step 201/334, epoch 152/501 --> loss:0.8064807915687561
step 251/334, epoch 152/501 --> loss:0.822490826845169
step 301/334, epoch 152/501 --> loss:0.8347183072566986
step 51/334, epoch 153/501 --> loss:0.8125640034675599
step 101/334, epoch 153/501 --> loss:0.8415998721122742
step 151/334, epoch 153/501 --> loss:0.8527979671955108
step 201/334, epoch 153/501 --> loss:0.8204801166057587
step 251/334, epoch 153/501 --> loss:0.8491603374481201
step 301/334, epoch 153/501 --> loss:0.8292397296428681
step 51/334, epoch 154/501 --> loss:0.8464778184890747
step 101/334, epoch 154/501 --> loss:0.8310436916351318
step 151/334, epoch 154/501 --> loss:0.8296657478809357
step 201/334, epoch 154/501 --> loss:0.8204874348640442
step 251/334, epoch 154/501 --> loss:0.828024891614914
step 301/334, epoch 154/501 --> loss:0.8298934376239777
step 51/334, epoch 155/501 --> loss:0.8322032761573791
step 101/334, epoch 155/501 --> loss:0.8383585953712464
step 151/334, epoch 155/501 --> loss:0.8292962336540222
step 201/334, epoch 155/501 --> loss:0.8162381637096405
step 251/334, epoch 155/501 --> loss:0.8262356460094452
step 301/334, epoch 155/501 --> loss:0.8347203016281128
step 51/334, epoch 156/501 --> loss:0.8225917518138885
step 101/334, epoch 156/501 --> loss:0.8289402997493744
step 151/334, epoch 156/501 --> loss:0.8394669353961944
step 201/334, epoch 156/501 --> loss:0.8323748731613159
step 251/334, epoch 156/501 --> loss:0.8383350265026093
step 301/334, epoch 156/501 --> loss:0.8350121092796325
step 51/334, epoch 157/501 --> loss:0.7986148595809937
step 101/334, epoch 157/501 --> loss:0.8131238698959351
step 151/334, epoch 157/501 --> loss:0.8602337217330933
step 201/334, epoch 157/501 --> loss:0.8309907054901123
step 251/334, epoch 157/501 --> loss:0.8206649947166443
step 301/334, epoch 157/501 --> loss:0.8601372599601745
step 51/334, epoch 158/501 --> loss:0.8398443865776062
step 101/334, epoch 158/501 --> loss:0.8534053480625152
step 151/334, epoch 158/501 --> loss:0.8453006315231323
step 201/334, epoch 158/501 --> loss:0.8551258158683777
step 251/334, epoch 158/501 --> loss:0.8014225780963897
step 301/334, epoch 158/501 --> loss:0.837649062871933
step 51/334, epoch 159/501 --> loss:0.8448196303844452
step 101/334, epoch 159/501 --> loss:0.8426633095741272
step 151/334, epoch 159/501 --> loss:0.8456888747215271
step 201/334, epoch 159/501 --> loss:0.8510190296173096
step 251/334, epoch 159/501 --> loss:0.7983174002170563
step 301/334, epoch 159/501 --> loss:0.7978998708724976
step 51/334, epoch 160/501 --> loss:0.8265689957141876
step 101/334, epoch 160/501 --> loss:0.8230347275733948
step 151/334, epoch 160/501 --> loss:0.8367248833179474
step 201/334, epoch 160/501 --> loss:0.8200373637676239
step 251/334, epoch 160/501 --> loss:0.8350455439090729
step 301/334, epoch 160/501 --> loss:0.8440469706058502
step 51/334, epoch 161/501 --> loss:0.848666090965271
step 101/334, epoch 161/501 --> loss:0.823236962556839
step 151/334, epoch 161/501 --> loss:0.8264640951156617
step 201/334, epoch 161/501 --> loss:0.8327811861038208
step 251/334, epoch 161/501 --> loss:0.8174999690055847
step 301/334, epoch 161/501 --> loss:0.8281437289714814

##########train dataset##########
acc--> [98.44073054939506]
F1--> {'F1': [0.8434278021224522], 'precision': [0.7362591457544495], 'recall': [0.9871232801181009]}
##########eval dataset##########
acc--> [97.93700103892608]
F1--> {'F1': [0.7879212501644324], 'precision': [0.6886026578738219], 'recall': [0.9207317454511738]}
step 51/334, epoch 162/501 --> loss:0.8339287185668945
step 101/334, epoch 162/501 --> loss:0.818547477722168
step 151/334, epoch 162/501 --> loss:0.82720334649086
step 201/334, epoch 162/501 --> loss:0.836533694267273
step 251/334, epoch 162/501 --> loss:0.8492999172210693
step 301/334, epoch 162/501 --> loss:0.8126523196697235
step 51/334, epoch 163/501 --> loss:0.8378121399879456
step 101/334, epoch 163/501 --> loss:0.8330133986473084
step 151/334, epoch 163/501 --> loss:0.8455029737949371
step 201/334, epoch 163/501 --> loss:0.8287324118614197
step 251/334, epoch 163/501 --> loss:0.8363563323020935
step 301/334, epoch 163/501 --> loss:0.8145423984527588
step 51/334, epoch 164/501 --> loss:0.8310350513458252
step 101/334, epoch 164/501 --> loss:0.8250109720230102
step 151/334, epoch 164/501 --> loss:0.8299018406867981
step 201/334, epoch 164/501 --> loss:0.841337970495224
step 251/334, epoch 164/501 --> loss:0.8291818058490753
step 301/334, epoch 164/501 --> loss:0.8282453262805939
step 51/334, epoch 165/501 --> loss:0.8488599574565887
step 101/334, epoch 165/501 --> loss:0.8261658382415772
step 151/334, epoch 165/501 --> loss:0.8227619671821594
step 201/334, epoch 165/501 --> loss:0.822132271528244
step 251/334, epoch 165/501 --> loss:0.8052660191059112
step 301/334, epoch 165/501 --> loss:0.8423030638694763
step 51/334, epoch 166/501 --> loss:0.8155136203765869
step 101/334, epoch 166/501 --> loss:0.8155090618133545
step 151/334, epoch 166/501 --> loss:0.8342665588855743
step 201/334, epoch 166/501 --> loss:0.8307582294940948
step 251/334, epoch 166/501 --> loss:0.8521548438072205
step 301/334, epoch 166/501 --> loss:0.8414501905441284
step 51/334, epoch 167/501 --> loss:0.8165597999095917
step 101/334, epoch 167/501 --> loss:0.8341760611534119
step 151/334, epoch 167/501 --> loss:0.8269831323623658
step 201/334, epoch 167/501 --> loss:0.8422552573680878
step 251/334, epoch 167/501 --> loss:0.8166012871265411
step 301/334, epoch 167/501 --> loss:0.8573456203937531
step 51/334, epoch 168/501 --> loss:0.8386469626426697
step 101/334, epoch 168/501 --> loss:0.8238297379016877
step 151/334, epoch 168/501 --> loss:0.8509531760215759
step 201/334, epoch 168/501 --> loss:0.8302210831642151
step 251/334, epoch 168/501 --> loss:0.8018975234031678
step 301/334, epoch 168/501 --> loss:0.8499040102958679
step 51/334, epoch 169/501 --> loss:0.8338392102718353
step 101/334, epoch 169/501 --> loss:0.8404744350910187
step 151/334, epoch 169/501 --> loss:0.8271253716945648
step 201/334, epoch 169/501 --> loss:0.8324377799034118
step 251/334, epoch 169/501 --> loss:0.8369066274166107
step 301/334, epoch 169/501 --> loss:0.8345090758800506
step 51/334, epoch 170/501 --> loss:0.8125311219692231
step 101/334, epoch 170/501 --> loss:0.84126993060112
step 151/334, epoch 170/501 --> loss:0.8298975896835327
step 201/334, epoch 170/501 --> loss:0.8433703196048736
step 251/334, epoch 170/501 --> loss:0.8338606929779053
step 301/334, epoch 170/501 --> loss:0.8227055835723877
step 51/334, epoch 171/501 --> loss:0.8443732190132142
step 101/334, epoch 171/501 --> loss:0.8172203910350799
step 151/334, epoch 171/501 --> loss:0.8205429899692536
step 201/334, epoch 171/501 --> loss:0.8569429314136505
step 251/334, epoch 171/501 --> loss:0.8048056268692017
step 301/334, epoch 171/501 --> loss:0.8418722975254059

##########train dataset##########
acc--> [98.42910652174666]
F1--> {'F1': [0.8423592361789077], 'precision': [0.7349836471591908], 'recall': [0.9864893848762153]}
##########eval dataset##########
acc--> [97.87528571180756]
F1--> {'F1': [0.7827331147201857], 'precision': [0.6813711883040822], 'recall': [0.9195363848103572]}
step 51/334, epoch 172/501 --> loss:0.82657372713089
step 101/334, epoch 172/501 --> loss:0.8263526904582977
step 151/334, epoch 172/501 --> loss:0.8428288149833679
step 201/334, epoch 172/501 --> loss:0.8559325778484345
step 251/334, epoch 172/501 --> loss:0.8157099866867066
step 301/334, epoch 172/501 --> loss:0.8197787594795227
step 51/334, epoch 173/501 --> loss:0.8173946917057038
step 101/334, epoch 173/501 --> loss:0.8443681788444519
step 151/334, epoch 173/501 --> loss:0.8278021907806397
step 201/334, epoch 173/501 --> loss:0.8393090081214905
step 251/334, epoch 173/501 --> loss:0.8349819040298462
step 301/334, epoch 173/501 --> loss:0.829970954656601
step 51/334, epoch 174/501 --> loss:0.8311561632156372
step 101/334, epoch 174/501 --> loss:0.8501739513874054
step 151/334, epoch 174/501 --> loss:0.8171023309230805
step 201/334, epoch 174/501 --> loss:0.8447479104995728
step 251/334, epoch 174/501 --> loss:0.848443021774292
step 301/334, epoch 174/501 --> loss:0.8216251480579376
step 51/334, epoch 175/501 --> loss:0.8475957775115966
step 101/334, epoch 175/501 --> loss:0.8341225636005402
step 151/334, epoch 175/501 --> loss:0.7942755031585693
step 201/334, epoch 175/501 --> loss:0.8368282055854798
step 251/334, epoch 175/501 --> loss:0.8446827089786529
step 301/334, epoch 175/501 --> loss:0.8238437414169312
step 51/334, epoch 176/501 --> loss:0.8137158501148224
step 101/334, epoch 176/501 --> loss:0.8177077102661133
step 151/334, epoch 176/501 --> loss:0.8240877795219421
step 201/334, epoch 176/501 --> loss:0.83971306681633
step 251/334, epoch 176/501 --> loss:0.8376436901092529
step 301/334, epoch 176/501 --> loss:0.8459781527519226
step 51/334, epoch 177/501 --> loss:0.8351980984210968
step 101/334, epoch 177/501 --> loss:0.8186757481098175
step 151/334, epoch 177/501 --> loss:0.838245313167572
step 201/334, epoch 177/501 --> loss:0.8124909019470214
step 251/334, epoch 177/501 --> loss:0.8390222406387329
step 301/334, epoch 177/501 --> loss:0.8350429391860962
step 51/334, epoch 178/501 --> loss:0.8270586478710175
step 101/334, epoch 178/501 --> loss:0.8331009149551392
step 151/334, epoch 178/501 --> loss:0.8139606297016144
step 201/334, epoch 178/501 --> loss:0.8338448178768157
step 251/334, epoch 178/501 --> loss:0.8311098861694336
step 301/334, epoch 178/501 --> loss:0.8506212568283081
step 51/334, epoch 179/501 --> loss:0.8240441012382508
step 101/334, epoch 179/501 --> loss:0.8386507952213287
step 151/334, epoch 179/501 --> loss:0.8334264314174652
step 201/334, epoch 179/501 --> loss:0.8480293655395508
step 251/334, epoch 179/501 --> loss:0.7978978538513184
step 301/334, epoch 179/501 --> loss:0.8383086717128754
step 51/334, epoch 180/501 --> loss:0.8162489759922028
step 101/334, epoch 180/501 --> loss:0.8226201808452607
step 151/334, epoch 180/501 --> loss:0.825739631652832
step 201/334, epoch 180/501 --> loss:0.8496580374240875
step 251/334, epoch 180/501 --> loss:0.8423116099834442
step 301/334, epoch 180/501 --> loss:0.8279982030391693
step 51/334, epoch 181/501 --> loss:0.8303248775005341
step 101/334, epoch 181/501 --> loss:0.8509256505966186
step 151/334, epoch 181/501 --> loss:0.8314195990562439
step 201/334, epoch 181/501 --> loss:0.8232434844970703
step 251/334, epoch 181/501 --> loss:0.8267414486408233
step 301/334, epoch 181/501 --> loss:0.8414586281776428

##########train dataset##########
acc--> [97.99844994274727]
F1--> {'F1': [0.8075594670473389], 'precision': [0.683289325674645], 'recall': [0.9870946378426402]}
##########eval dataset##########
acc--> [97.47308512475938]
F1--> {'F1': [0.755870708891158], 'precision': [0.6321291338242945], 'recall': [0.9398645717638308]}
step 51/334, epoch 182/501 --> loss:0.8288593828678131
step 101/334, epoch 182/501 --> loss:0.8272553789615631
step 151/334, epoch 182/501 --> loss:0.8151641833782196
step 201/334, epoch 182/501 --> loss:0.8280349314212799
step 251/334, epoch 182/501 --> loss:0.8307878172397614
step 301/334, epoch 182/501 --> loss:0.8546317899227143
step 51/334, epoch 183/501 --> loss:0.8414523589611054
step 101/334, epoch 183/501 --> loss:0.8248107409477234
step 151/334, epoch 183/501 --> loss:0.8198340082168579
step 201/334, epoch 183/501 --> loss:0.8156698536872864
step 251/334, epoch 183/501 --> loss:0.833817048072815
step 301/334, epoch 183/501 --> loss:0.829394713640213
step 51/334, epoch 184/501 --> loss:0.8134773230552673
step 101/334, epoch 184/501 --> loss:0.8321578347682953
step 151/334, epoch 184/501 --> loss:0.83038112282753
step 201/334, epoch 184/501 --> loss:0.8544482469558716
step 251/334, epoch 184/501 --> loss:0.8319974994659424
step 301/334, epoch 184/501 --> loss:0.8260953938961029
step 51/334, epoch 185/501 --> loss:0.8166801965236664
step 101/334, epoch 185/501 --> loss:0.8321904444694519
step 151/334, epoch 185/501 --> loss:0.8173593056201934
step 201/334, epoch 185/501 --> loss:0.8263311040401459
step 251/334, epoch 185/501 --> loss:0.8346621632575989
step 301/334, epoch 185/501 --> loss:0.8464311361312866
step 51/334, epoch 186/501 --> loss:0.8430486238002777
step 101/334, epoch 186/501 --> loss:0.7921366894245148
step 151/334, epoch 186/501 --> loss:0.8434972953796387
step 201/334, epoch 186/501 --> loss:0.8263821411132812
step 251/334, epoch 186/501 --> loss:0.8380866312980652
step 301/334, epoch 186/501 --> loss:0.8231890535354615
step 51/334, epoch 187/501 --> loss:0.8372536075115203
step 101/334, epoch 187/501 --> loss:0.8407333159446716
step 151/334, epoch 187/501 --> loss:0.8236397635936737
step 201/334, epoch 187/501 --> loss:0.8277634620666504
step 251/334, epoch 187/501 --> loss:0.8235994684696197
step 301/334, epoch 187/501 --> loss:0.8310343050956726
step 51/334, epoch 188/501 --> loss:0.8185921955108643
step 101/334, epoch 188/501 --> loss:0.8485147333145142
step 151/334, epoch 188/501 --> loss:0.8230296897888184
step 201/334, epoch 188/501 --> loss:0.8240587341785431
step 251/334, epoch 188/501 --> loss:0.8380174517631531
step 301/334, epoch 188/501 --> loss:0.8225101029872894
step 51/334, epoch 189/501 --> loss:0.8327983164787293
step 101/334, epoch 189/501 --> loss:0.8240127325057983
step 151/334, epoch 189/501 --> loss:0.8388762843608856
step 201/334, epoch 189/501 --> loss:0.8410194993019104
step 251/334, epoch 189/501 --> loss:0.8297029101848602
step 301/334, epoch 189/501 --> loss:0.8187433540821075
step 51/334, epoch 190/501 --> loss:0.8085119080543518
step 101/334, epoch 190/501 --> loss:0.8464121961593628
step 151/334, epoch 190/501 --> loss:0.8104722809791565
step 201/334, epoch 190/501 --> loss:0.8496187138557434
step 251/334, epoch 190/501 --> loss:0.8295984101295472
step 301/334, epoch 190/501 --> loss:0.8488698613643646
step 51/334, epoch 191/501 --> loss:0.8389901864528656
step 101/334, epoch 191/501 --> loss:0.836610506772995
step 151/334, epoch 191/501 --> loss:0.855817997455597
step 201/334, epoch 191/501 --> loss:0.8144962263107299
step 251/334, epoch 191/501 --> loss:0.8056786632537842
step 301/334, epoch 191/501 --> loss:0.8295078098773956

##########train dataset##########
acc--> [98.38295426313006]
F1--> {'F1': [0.8387958031052534], 'precision': [0.728303588641267], 'recall': [0.9888234613847069]}
##########eval dataset##########
acc--> [97.79431934639719]
F1--> {'F1': [0.778776112553432], 'precision': [0.6684347946995595], 'recall': [0.9327628081498567]}
step 51/334, epoch 192/501 --> loss:0.8268703210353852
step 101/334, epoch 192/501 --> loss:0.825437124967575
step 151/334, epoch 192/501 --> loss:0.8359222710132599
step 201/334, epoch 192/501 --> loss:0.8345608055591583
step 251/334, epoch 192/501 --> loss:0.8336563575267791
step 301/334, epoch 192/501 --> loss:0.820798100233078
step 51/334, epoch 193/501 --> loss:0.8276097702980042
step 101/334, epoch 193/501 --> loss:0.8397483444213867
step 151/334, epoch 193/501 --> loss:0.8366444623470306
step 201/334, epoch 193/501 --> loss:0.8008720016479492
step 251/334, epoch 193/501 --> loss:0.8295969891548157
step 301/334, epoch 193/501 --> loss:0.825430006980896
step 51/334, epoch 194/501 --> loss:0.8324624478816987
step 101/334, epoch 194/501 --> loss:0.839899570941925
step 151/334, epoch 194/501 --> loss:0.8271990728378296
step 201/334, epoch 194/501 --> loss:0.8225702941417694
step 251/334, epoch 194/501 --> loss:0.8276288950443268
step 301/334, epoch 194/501 --> loss:0.819239376783371
step 51/334, epoch 195/501 --> loss:0.8502673280239105
step 101/334, epoch 195/501 --> loss:0.8284493970870972
step 151/334, epoch 195/501 --> loss:0.8208835482597351
step 201/334, epoch 195/501 --> loss:0.8305689096450806
step 251/334, epoch 195/501 --> loss:0.8272148883342743
step 301/334, epoch 195/501 --> loss:0.8367811274528504
step 51/334, epoch 196/501 --> loss:0.8447014415264129
step 101/334, epoch 196/501 --> loss:0.8084801232814789
step 151/334, epoch 196/501 --> loss:0.8232711172103881
step 201/334, epoch 196/501 --> loss:0.8324368500709534
step 251/334, epoch 196/501 --> loss:0.8272023725509644
step 301/334, epoch 196/501 --> loss:0.8450081944465637
step 51/334, epoch 197/501 --> loss:0.8161679768562317
step 101/334, epoch 197/501 --> loss:0.8232883107662201
step 151/334, epoch 197/501 --> loss:0.8602341282367706
step 201/334, epoch 197/501 --> loss:0.828381609916687
step 251/334, epoch 197/501 --> loss:0.8317972898483277
step 301/334, epoch 197/501 --> loss:0.8211418974399567
step 51/334, epoch 198/501 --> loss:0.8177003288269042
step 101/334, epoch 198/501 --> loss:0.8266340065002441
step 151/334, epoch 198/501 --> loss:0.8308225870132446
step 201/334, epoch 198/501 --> loss:0.831099568605423
step 251/334, epoch 198/501 --> loss:0.8443373703956604
step 301/334, epoch 198/501 --> loss:0.8160268831253051
step 51/334, epoch 199/501 --> loss:0.8465558278560639
step 101/334, epoch 199/501 --> loss:0.82591215133667
step 151/334, epoch 199/501 --> loss:0.8269094216823578
step 201/334, epoch 199/501 --> loss:0.827660710811615
step 251/334, epoch 199/501 --> loss:0.8319810688495636
step 301/334, epoch 199/501 --> loss:0.8097699391841888
step 51/334, epoch 200/501 --> loss:0.8372027575969696
step 101/334, epoch 200/501 --> loss:0.8275192058086396
step 151/334, epoch 200/501 --> loss:0.8345343148708344
step 201/334, epoch 200/501 --> loss:0.8337390351295472
step 251/334, epoch 200/501 --> loss:0.8319107377529145
step 301/334, epoch 200/501 --> loss:0.8030642580986023
step 51/334, epoch 201/501 --> loss:0.8545668268203735
step 101/334, epoch 201/501 --> loss:0.8055021512508392
step 151/334, epoch 201/501 --> loss:0.81714026927948
step 201/334, epoch 201/501 --> loss:0.8129339921474457
step 251/334, epoch 201/501 --> loss:0.8435598826408386
step 301/334, epoch 201/501 --> loss:0.8399957323074341

##########train dataset##########
acc--> [98.66619875335125]
F1--> {'F1': [0.8631211087206035], 'precision': [0.7660182606103555], 'recall': [0.9884286551839907]}
##########eval dataset##########
acc--> [98.07889175748127]
F1--> {'F1': [0.7982844050843964], 'precision': [0.708998199993117], 'recall': [0.9133116573306232]}
step 51/334, epoch 202/501 --> loss:0.8306470847129822
step 101/334, epoch 202/501 --> loss:0.8251479959487915
step 151/334, epoch 202/501 --> loss:0.8495527923107147
step 201/334, epoch 202/501 --> loss:0.8190910613536835
step 251/334, epoch 202/501 --> loss:0.8253737616539002
step 301/334, epoch 202/501 --> loss:0.8335891616344452
step 51/334, epoch 203/501 --> loss:0.8213811802864075
step 101/334, epoch 203/501 --> loss:0.8378620421886445
step 151/334, epoch 203/501 --> loss:0.828020886182785
step 201/334, epoch 203/501 --> loss:0.8475939440727234
step 251/334, epoch 203/501 --> loss:0.839811087846756
step 301/334, epoch 203/501 --> loss:0.8143817615509034
step 51/334, epoch 204/501 --> loss:0.8169273924827576
step 101/334, epoch 204/501 --> loss:0.8262150573730469
step 151/334, epoch 204/501 --> loss:0.8494056022167206
step 201/334, epoch 204/501 --> loss:0.8249154341220856
step 251/334, epoch 204/501 --> loss:0.8326075315475464
step 301/334, epoch 204/501 --> loss:0.8114013397693634
step 51/334, epoch 205/501 --> loss:0.847065509557724
step 101/334, epoch 205/501 --> loss:0.8160392761230468
step 151/334, epoch 205/501 --> loss:0.8219061505794525
step 201/334, epoch 205/501 --> loss:0.8371144735813141
step 251/334, epoch 205/501 --> loss:0.8178673171997071
step 301/334, epoch 205/501 --> loss:0.8457762253284454
step 51/334, epoch 206/501 --> loss:0.8403154337406158
step 101/334, epoch 206/501 --> loss:0.8350990521907806
step 151/334, epoch 206/501 --> loss:0.8339932632446289
step 201/334, epoch 206/501 --> loss:0.8422155857086182
step 251/334, epoch 206/501 --> loss:0.8334136605262756
step 301/334, epoch 206/501 --> loss:0.8112977802753448
step 51/334, epoch 207/501 --> loss:0.8256175768375397
step 101/334, epoch 207/501 --> loss:0.8247468829154968
step 151/334, epoch 207/501 --> loss:0.8088841736316681
step 201/334, epoch 207/501 --> loss:0.8191201412677764
step 251/334, epoch 207/501 --> loss:0.8151557326316834
step 301/334, epoch 207/501 --> loss:0.8590450084209442
step 51/334, epoch 208/501 --> loss:0.8425325632095337
step 101/334, epoch 208/501 --> loss:0.80447900056839
step 151/334, epoch 208/501 --> loss:0.8418004262447357
step 201/334, epoch 208/501 --> loss:0.8050419199466705
step 251/334, epoch 208/501 --> loss:0.8551523041725159
step 301/334, epoch 208/501 --> loss:0.8351624286174775
step 51/334, epoch 209/501 --> loss:0.8303163075447082
step 101/334, epoch 209/501 --> loss:0.8073797798156739
step 151/334, epoch 209/501 --> loss:0.8363674545288086
step 201/334, epoch 209/501 --> loss:0.8377864253520966
step 251/334, epoch 209/501 --> loss:0.8073812472820282
step 301/334, epoch 209/501 --> loss:0.8503998804092407
step 51/334, epoch 210/501 --> loss:0.8245327532291412
step 101/334, epoch 210/501 --> loss:0.816523312330246
step 151/334, epoch 210/501 --> loss:0.8194795632362366
step 201/334, epoch 210/501 --> loss:0.84533811211586
step 251/334, epoch 210/501 --> loss:0.8316364777088165
step 301/334, epoch 210/501 --> loss:0.8340090918540954
step 51/334, epoch 211/501 --> loss:0.8501004266738892
step 101/334, epoch 211/501 --> loss:0.8302340483665467
step 151/334, epoch 211/501 --> loss:0.8280676531791688
step 201/334, epoch 211/501 --> loss:0.8304013466835022
step 251/334, epoch 211/501 --> loss:0.8298384606838226
step 301/334, epoch 211/501 --> loss:0.8235415828227997

##########train dataset##########
acc--> [98.62374447685335]
F1--> {'F1': [0.8590944477522947], 'precision': [0.7610665775801841], 'recall': [0.9861214055954728]}
##########eval dataset##########
acc--> [98.06904963828939]
F1--> {'F1': [0.7965046326397895], 'precision': [0.7094449712888283], 'recall': [0.9079330155419207]}
step 51/334, epoch 212/501 --> loss:0.8411159908771515
step 101/334, epoch 212/501 --> loss:0.8137485539913177
step 151/334, epoch 212/501 --> loss:0.8102354788780213
step 201/334, epoch 212/501 --> loss:0.8246793568134307
step 251/334, epoch 212/501 --> loss:0.8408944129943847
step 301/334, epoch 212/501 --> loss:0.852482670545578
step 51/334, epoch 213/501 --> loss:0.8491135728359223
step 101/334, epoch 213/501 --> loss:0.829876093864441
step 151/334, epoch 213/501 --> loss:0.820207222700119
step 201/334, epoch 213/501 --> loss:0.7991825437545776
step 251/334, epoch 213/501 --> loss:0.8272572195529938
step 301/334, epoch 213/501 --> loss:0.8389845168590546
step 51/334, epoch 214/501 --> loss:0.8313950669765472
step 101/334, epoch 214/501 --> loss:0.8287120282649993
step 151/334, epoch 214/501 --> loss:0.8452908599376678
step 201/334, epoch 214/501 --> loss:0.8355470108985901
step 251/334, epoch 214/501 --> loss:0.814496386051178
step 301/334, epoch 214/501 --> loss:0.8380073046684265
step 51/334, epoch 215/501 --> loss:0.8275670719146728
step 101/334, epoch 215/501 --> loss:0.8486758160591126
step 151/334, epoch 215/501 --> loss:0.8409832882881164
step 201/334, epoch 215/501 --> loss:0.8199159860610962
step 251/334, epoch 215/501 --> loss:0.8196307420730591
step 301/334, epoch 215/501 --> loss:0.8242811667919159
step 51/334, epoch 216/501 --> loss:0.8336468744277954
step 101/334, epoch 216/501 --> loss:0.8259918928146363
step 151/334, epoch 216/501 --> loss:0.810161964893341
step 201/334, epoch 216/501 --> loss:0.8616768324375152
step 251/334, epoch 216/501 --> loss:0.8335670495033264
step 301/334, epoch 216/501 --> loss:0.8314067304134369
step 51/334, epoch 217/501 --> loss:0.8399205756187439
step 101/334, epoch 217/501 --> loss:0.8347676968574524
step 151/334, epoch 217/501 --> loss:0.8137604510784149
step 201/334, epoch 217/501 --> loss:0.8284621131420136
step 251/334, epoch 217/501 --> loss:0.8134364926815033
step 301/334, epoch 217/501 --> loss:0.8570649337768554
step 51/334, epoch 218/501 --> loss:0.8148939490318299
step 101/334, epoch 218/501 --> loss:0.8297680342197418
step 151/334, epoch 218/501 --> loss:0.8220761299133301
step 201/334, epoch 218/501 --> loss:0.8217568385601044
step 251/334, epoch 218/501 --> loss:0.8484987878799438
step 301/334, epoch 218/501 --> loss:0.8361409401893616
step 51/334, epoch 219/501 --> loss:0.8602574849128723
step 101/334, epoch 219/501 --> loss:0.8396803081035614
step 151/334, epoch 219/501 --> loss:0.8164630961418152
step 201/334, epoch 219/501 --> loss:0.8323301839828491
step 251/334, epoch 219/501 --> loss:0.8205758333206177
step 301/334, epoch 219/501 --> loss:0.8233988630771637
step 51/334, epoch 220/501 --> loss:0.8565695750713348
step 101/334, epoch 220/501 --> loss:0.8076694715023041
step 151/334, epoch 220/501 --> loss:0.8160349643230438
step 201/334, epoch 220/501 --> loss:0.8362953901290894
step 251/334, epoch 220/501 --> loss:0.8319568479061127
step 301/334, epoch 220/501 --> loss:0.8268222415447235
step 51/334, epoch 221/501 --> loss:0.835243878364563
step 101/334, epoch 221/501 --> loss:0.8381720161437989
step 151/334, epoch 221/501 --> loss:0.8415926325321198
step 201/334, epoch 221/501 --> loss:0.8368953037261962
step 251/334, epoch 221/501 --> loss:0.8243902051448821
step 301/334, epoch 221/501 --> loss:0.8272007524967193

##########train dataset##########
acc--> [97.4136363981696]
F1--> {'F1': [0.7641981697515161], 'precision': [0.6242451545189389], 'recall': [0.9850555229830829]}
##########eval dataset##########
acc--> [96.8155995232892]
F1--> {'F1': [0.7118095094800877], 'precision': [0.5709914249507624], 'recall': [0.9448382919509687]}
step 51/334, epoch 222/501 --> loss:0.8334599483013153
step 101/334, epoch 222/501 --> loss:0.8228877317905426
step 151/334, epoch 222/501 --> loss:0.8440339040756225
step 201/334, epoch 222/501 --> loss:0.8424448716640472
step 251/334, epoch 222/501 --> loss:0.8357328677177429
step 301/334, epoch 222/501 --> loss:0.8302917623519898
step 51/334, epoch 223/501 --> loss:0.8359600615501404
step 101/334, epoch 223/501 --> loss:0.8330447375774384
step 151/334, epoch 223/501 --> loss:0.7935470485687256
step 201/334, epoch 223/501 --> loss:0.831225973367691
step 251/334, epoch 223/501 --> loss:0.8471008670330048
step 301/334, epoch 223/501 --> loss:0.8464236974716186
step 51/334, epoch 224/501 --> loss:0.8269919133186341
step 101/334, epoch 224/501 --> loss:0.8294796478748322
step 151/334, epoch 224/501 --> loss:0.8301065075397491
step 201/334, epoch 224/501 --> loss:0.8450816011428833
step 251/334, epoch 224/501 --> loss:0.8142824780941009
step 301/334, epoch 224/501 --> loss:0.8225754868984222
step 51/334, epoch 225/501 --> loss:0.8186580228805542
step 101/334, epoch 225/501 --> loss:0.8094703686237336
step 151/334, epoch 225/501 --> loss:0.8333370363712311
step 201/334, epoch 225/501 --> loss:0.8150115311145782
step 251/334, epoch 225/501 --> loss:0.8459482300281524
step 301/334, epoch 225/501 --> loss:0.8502975392341614
step 51/334, epoch 226/501 --> loss:0.8436985397338868
step 101/334, epoch 226/501 --> loss:0.822488065958023
step 151/334, epoch 226/501 --> loss:0.8320162582397461
step 201/334, epoch 226/501 --> loss:0.8297665274143219
step 251/334, epoch 226/501 --> loss:0.8193473267555237
step 301/334, epoch 226/501 --> loss:0.8312009072303772
step 51/334, epoch 227/501 --> loss:0.8226693272590637
step 101/334, epoch 227/501 --> loss:0.8195920705795288
step 151/334, epoch 227/501 --> loss:0.8515130293369293
step 201/334, epoch 227/501 --> loss:0.8336483597755432
step 251/334, epoch 227/501 --> loss:0.8331548202037812
step 301/334, epoch 227/501 --> loss:0.82545853972435
step 51/334, epoch 228/501 --> loss:0.820321979522705
step 101/334, epoch 228/501 --> loss:0.831847733259201
step 151/334, epoch 228/501 --> loss:0.8295205986499786
step 201/334, epoch 228/501 --> loss:0.831146913766861
step 251/334, epoch 228/501 --> loss:0.8352386355400085
step 301/334, epoch 228/501 --> loss:0.8257035636901855
step 51/334, epoch 229/501 --> loss:0.8472412884235382
step 101/334, epoch 229/501 --> loss:0.8524070525169373
step 151/334, epoch 229/501 --> loss:0.8203150141239166
step 201/334, epoch 229/501 --> loss:0.8166646552085877
step 251/334, epoch 229/501 --> loss:0.8223675835132599
step 301/334, epoch 229/501 --> loss:0.8288279581069946
step 51/334, epoch 230/501 --> loss:0.8367710340023041
step 101/334, epoch 230/501 --> loss:0.8143103039264679
step 151/334, epoch 230/501 --> loss:0.8342668783664703
step 201/334, epoch 230/501 --> loss:0.8480805015563965
step 251/334, epoch 230/501 --> loss:0.8296314859390259
step 301/334, epoch 230/501 --> loss:0.8184960746765136
step 51/334, epoch 231/501 --> loss:0.8270923960208892
step 101/334, epoch 231/501 --> loss:0.823338451385498
step 151/334, epoch 231/501 --> loss:0.8132637512683868
step 201/334, epoch 231/501 --> loss:0.827473361492157
step 251/334, epoch 231/501 --> loss:0.8416762113571167
step 301/334, epoch 231/501 --> loss:0.8405981647968293

##########train dataset##########
acc--> [98.62425424742158]
F1--> {'F1': [0.85951766122094], 'precision': [0.7598966233218452], 'recall': [0.989212955989739]}
##########eval dataset##########
acc--> [97.89327319420721]
F1--> {'F1': [0.7870571960377387], 'precision': [0.6793291235646695], 'recall': [0.9354054628319088]}
step 51/334, epoch 232/501 --> loss:0.8525482296943665
step 101/334, epoch 232/501 --> loss:0.839487942457199
step 151/334, epoch 232/501 --> loss:0.8381262695789338
step 201/334, epoch 232/501 --> loss:0.8201071298122407
step 251/334, epoch 232/501 --> loss:0.8108912169933319
step 301/334, epoch 232/501 --> loss:0.830972249507904
step 51/334, epoch 233/501 --> loss:0.8345887446403504
step 101/334, epoch 233/501 --> loss:0.7917220270633698
step 151/334, epoch 233/501 --> loss:0.8551026546955108
step 201/334, epoch 233/501 --> loss:0.833124213218689
step 251/334, epoch 233/501 --> loss:0.8305632483959198
step 301/334, epoch 233/501 --> loss:0.8302155709266663
step 51/334, epoch 234/501 --> loss:0.8157242870330811
step 101/334, epoch 234/501 --> loss:0.8185678863525391
step 151/334, epoch 234/501 --> loss:0.8354200911521912
step 201/334, epoch 234/501 --> loss:0.8264724814891815
step 251/334, epoch 234/501 --> loss:0.8414297032356263
step 301/334, epoch 234/501 --> loss:0.8367909526824951
step 51/334, epoch 235/501 --> loss:0.8508092725276947
step 101/334, epoch 235/501 --> loss:0.8305700552463532
step 151/334, epoch 235/501 --> loss:0.8124964606761932
step 201/334, epoch 235/501 --> loss:0.8270736920833588
step 251/334, epoch 235/501 --> loss:0.8276542973518372
step 301/334, epoch 235/501 --> loss:0.827088726758957
step 51/334, epoch 236/501 --> loss:0.841322339773178
step 101/334, epoch 236/501 --> loss:0.8203786885738373
step 151/334, epoch 236/501 --> loss:0.8247899794578553
step 201/334, epoch 236/501 --> loss:0.8407046461105346
step 251/334, epoch 236/501 --> loss:0.8265711379051208
step 301/334, epoch 236/501 --> loss:0.8292298066616058
step 51/334, epoch 237/501 --> loss:0.8319681131839752
step 101/334, epoch 237/501 --> loss:0.8205681920051575
step 151/334, epoch 237/501 --> loss:0.8359431624412537
step 201/334, epoch 237/501 --> loss:0.8187204957008362
step 251/334, epoch 237/501 --> loss:0.8399554121494294
step 301/334, epoch 237/501 --> loss:0.8247033655643463
step 51/334, epoch 238/501 --> loss:0.8490720164775848
step 101/334, epoch 238/501 --> loss:0.8163123989105224
step 151/334, epoch 238/501 --> loss:0.8218906641006469
step 201/334, epoch 238/501 --> loss:0.8349122679233552
step 251/334, epoch 238/501 --> loss:0.8185903823375702
step 301/334, epoch 238/501 --> loss:0.8337451636791229
step 51/334, epoch 239/501 --> loss:0.8553856325149536
step 101/334, epoch 239/501 --> loss:0.8260654890537262
step 151/334, epoch 239/501 --> loss:0.8291561329364776
step 201/334, epoch 239/501 --> loss:0.8128388023376465
step 251/334, epoch 239/501 --> loss:0.8440367007255554
step 301/334, epoch 239/501 --> loss:0.8352144384384155
step 51/334, epoch 240/501 --> loss:0.8450753593444824
step 101/334, epoch 240/501 --> loss:0.835079391002655
step 151/334, epoch 240/501 --> loss:0.8320436143875122
step 201/334, epoch 240/501 --> loss:0.8119115459918976
step 251/334, epoch 240/501 --> loss:0.8157681083679199
step 301/334, epoch 240/501 --> loss:0.842086091041565
step 51/334, epoch 241/501 --> loss:0.835943020582199
step 101/334, epoch 241/501 --> loss:0.8311998426914216
step 151/334, epoch 241/501 --> loss:0.8338489222526551
step 201/334, epoch 241/501 --> loss:0.8362321472167968
step 251/334, epoch 241/501 --> loss:0.8344884598255158
step 301/334, epoch 241/501 --> loss:0.8072805964946747

##########train dataset##########
acc--> [98.41974871320124]
F1--> {'F1': [0.8414690106497082], 'precision': [0.7340385643579841], 'recall': [0.9857502662514753]}
##########eval dataset##########
acc--> [97.67733396194086]
F1--> {'F1': [0.7689852362709734], 'precision': [0.6561111756831104], 'recall': [0.928779502145397]}
step 51/334, epoch 242/501 --> loss:0.837864236831665
step 101/334, epoch 242/501 --> loss:0.8232470738887787
step 151/334, epoch 242/501 --> loss:0.8479128277301788
step 201/334, epoch 242/501 --> loss:0.8376158726215363
step 251/334, epoch 242/501 --> loss:0.8026229989528656
step 301/334, epoch 242/501 --> loss:0.8118331253528595
step 51/334, epoch 243/501 --> loss:0.8350255596637726
step 101/334, epoch 243/501 --> loss:0.82118173122406
step 151/334, epoch 243/501 --> loss:0.8310807716846466
step 201/334, epoch 243/501 --> loss:0.8255304014682769
step 251/334, epoch 243/501 --> loss:0.8441567897796631
step 301/334, epoch 243/501 --> loss:0.8304648005962372
step 51/334, epoch 244/501 --> loss:0.8245768761634826
step 101/334, epoch 244/501 --> loss:0.8218141102790832
step 151/334, epoch 244/501 --> loss:0.829523057937622
step 201/334, epoch 244/501 --> loss:0.8471345698833466
step 251/334, epoch 244/501 --> loss:0.8134068322181701
step 301/334, epoch 244/501 --> loss:0.8327188432216645
step 51/334, epoch 245/501 --> loss:0.8163528168201446
step 101/334, epoch 245/501 --> loss:0.8306031239032745
step 151/334, epoch 245/501 --> loss:0.8534401369094848
step 201/334, epoch 245/501 --> loss:0.8140177524089813
step 251/334, epoch 245/501 --> loss:0.8283649432659149
step 301/334, epoch 245/501 --> loss:0.8335070645809174
step 51/334, epoch 246/501 --> loss:0.8166111314296722
step 101/334, epoch 246/501 --> loss:0.8327177274227142
step 151/334, epoch 246/501 --> loss:0.8157088088989258
step 201/334, epoch 246/501 --> loss:0.8386605644226074
step 251/334, epoch 246/501 --> loss:0.8129014241695404
step 301/334, epoch 246/501 --> loss:0.8426344025135041
step 51/334, epoch 247/501 --> loss:0.832562198638916
step 101/334, epoch 247/501 --> loss:0.8250600850582123
step 151/334, epoch 247/501 --> loss:0.8208566761016846
step 201/334, epoch 247/501 --> loss:0.8153149402141571
step 251/334, epoch 247/501 --> loss:0.8470677733421326
step 301/334, epoch 247/501 --> loss:0.8235697257518768
step 51/334, epoch 248/501 --> loss:0.8412801587581634
step 101/334, epoch 248/501 --> loss:0.820306020975113
step 151/334, epoch 248/501 --> loss:0.8376369178295135
step 201/334, epoch 248/501 --> loss:0.7969632339477539
step 251/334, epoch 248/501 --> loss:0.828406594991684
step 301/334, epoch 248/501 --> loss:0.8409108114242554
step 51/334, epoch 249/501 --> loss:0.829263459444046
step 101/334, epoch 249/501 --> loss:0.8199393713474273
step 151/334, epoch 249/501 --> loss:0.8267348694801331
step 201/334, epoch 249/501 --> loss:0.8308179426193237
step 251/334, epoch 249/501 --> loss:0.8171522450447083
step 301/334, epoch 249/501 --> loss:0.8259138965606689
step 51/334, epoch 250/501 --> loss:0.8331760787963867
step 101/334, epoch 250/501 --> loss:0.8012605857849121
step 151/334, epoch 250/501 --> loss:0.8468484318256378
step 201/334, epoch 250/501 --> loss:0.82915012717247
step 251/334, epoch 250/501 --> loss:0.8443323099613189
step 301/334, epoch 250/501 --> loss:0.8218221473693847
step 51/334, epoch 251/501 --> loss:0.8137621688842773
step 101/334, epoch 251/501 --> loss:0.8281650006771087
step 151/334, epoch 251/501 --> loss:0.8374288582801819
step 201/334, epoch 251/501 --> loss:0.8407689023017884
step 251/334, epoch 251/501 --> loss:0.8208733654022217
step 301/334, epoch 251/501 --> loss:0.8371701550483703

##########train dataset##########
acc--> [98.92800999438386]
F1--> {'F1': [0.8870018127212154], 'precision': [0.8041290217607792], 'recall': [0.9889311724769322]}
##########eval dataset##########
acc--> [98.40927991757941]
F1--> {'F1': [0.8249526397871798], 'precision': [0.7610580791304792], 'recall': [0.9005708191830197]}
save model!
step 51/334, epoch 252/501 --> loss:0.812460458278656
step 101/334, epoch 252/501 --> loss:0.836858983039856
step 151/334, epoch 252/501 --> loss:0.8440955007076263
step 201/334, epoch 252/501 --> loss:0.823594468832016
step 251/334, epoch 252/501 --> loss:0.8220179057121277
step 301/334, epoch 252/501 --> loss:0.8341348946094513
step 51/334, epoch 253/501 --> loss:0.8144951629638671
step 101/334, epoch 253/501 --> loss:0.8249068117141723
step 151/334, epoch 253/501 --> loss:0.8273549234867096
step 201/334, epoch 253/501 --> loss:0.848753764629364
step 251/334, epoch 253/501 --> loss:0.8329924261569976
step 301/334, epoch 253/501 --> loss:0.8307432687282562
step 51/334, epoch 254/501 --> loss:0.8319021785259246
step 101/334, epoch 254/501 --> loss:0.8512969899177552
step 151/334, epoch 254/501 --> loss:0.8452750039100647
step 201/334, epoch 254/501 --> loss:0.821910730600357
step 251/334, epoch 254/501 --> loss:0.8093917846679688
step 301/334, epoch 254/501 --> loss:0.8292891776561737
step 51/334, epoch 255/501 --> loss:0.8147773087024689
step 101/334, epoch 255/501 --> loss:0.8381790840625762
step 151/334, epoch 255/501 --> loss:0.8338554215431213
step 201/334, epoch 255/501 --> loss:0.829203006029129
step 251/334, epoch 255/501 --> loss:0.8093432080745697
step 301/334, epoch 255/501 --> loss:0.8419092047214508
step 51/334, epoch 256/501 --> loss:0.8340705299377441
step 101/334, epoch 256/501 --> loss:0.811646134853363
step 151/334, epoch 256/501 --> loss:0.8258089637756347
step 201/334, epoch 256/501 --> loss:0.8179913914203644
step 251/334, epoch 256/501 --> loss:0.8517372155189514
step 301/334, epoch 256/501 --> loss:0.8261863231658936
step 51/334, epoch 257/501 --> loss:0.85475940823555
step 101/334, epoch 257/501 --> loss:0.8103008031845093
step 151/334, epoch 257/501 --> loss:0.8396330678462982
step 201/334, epoch 257/501 --> loss:0.8180027639865876
step 251/334, epoch 257/501 --> loss:0.8544449126720428
step 301/334, epoch 257/501 --> loss:0.8013930761814118
step 51/334, epoch 258/501 --> loss:0.8313871550559998
step 101/334, epoch 258/501 --> loss:0.8250966870784759
step 151/334, epoch 258/501 --> loss:0.8389969289302825
step 201/334, epoch 258/501 --> loss:0.8322397255897522
step 251/334, epoch 258/501 --> loss:0.8260078990459442
step 301/334, epoch 258/501 --> loss:0.8263608062267304
step 51/334, epoch 259/501 --> loss:0.8181989133358002
step 101/334, epoch 259/501 --> loss:0.8200029170513153
step 151/334, epoch 259/501 --> loss:0.8404617178440094
step 201/334, epoch 259/501 --> loss:0.8300927305221557
step 251/334, epoch 259/501 --> loss:0.8172757339477539
step 301/334, epoch 259/501 --> loss:0.8414646506309509
step 51/334, epoch 260/501 --> loss:0.8252355086803437
step 101/334, epoch 260/501 --> loss:0.8383599960803986
step 151/334, epoch 260/501 --> loss:0.8224829006195068
step 201/334, epoch 260/501 --> loss:0.8467231178283692
step 251/334, epoch 260/501 --> loss:0.8520166563987732
step 301/334, epoch 260/501 --> loss:0.814074820280075
step 51/334, epoch 261/501 --> loss:0.8523068130016327
step 101/334, epoch 261/501 --> loss:0.849134488105774
step 151/334, epoch 261/501 --> loss:0.804357557296753
step 201/334, epoch 261/501 --> loss:0.8379373133182526
step 251/334, epoch 261/501 --> loss:0.8037156903743744
step 301/334, epoch 261/501 --> loss:0.8340033829212189

##########train dataset##########
acc--> [98.90338538691292]
F1--> {'F1': [0.8849292110471152], 'precision': [0.7993095626891651], 'recall': [0.9911044219363583]}
##########eval dataset##########
acc--> [98.26708081929776]
F1--> {'F1': [0.8150409456357923], 'precision': [0.7332745723959564], 'recall': [0.9173437142922615]}
step 51/334, epoch 262/501 --> loss:0.8394857871532441
step 101/334, epoch 262/501 --> loss:0.8388299071788787
step 151/334, epoch 262/501 --> loss:0.8090181279182435
step 201/334, epoch 262/501 --> loss:0.8316304004192352
step 251/334, epoch 262/501 --> loss:0.8315730786323547
step 301/334, epoch 262/501 --> loss:0.8276676094532013
step 51/334, epoch 263/501 --> loss:0.8293567740917206
step 101/334, epoch 263/501 --> loss:0.8403292214870453
step 151/334, epoch 263/501 --> loss:0.8319265162944793
step 201/334, epoch 263/501 --> loss:0.8210643124580383
step 251/334, epoch 263/501 --> loss:0.8179355585575103
step 301/334, epoch 263/501 --> loss:0.8351995146274567
step 51/334, epoch 264/501 --> loss:0.8304731547832489
step 101/334, epoch 264/501 --> loss:0.8204891300201416
step 151/334, epoch 264/501 --> loss:0.853432297706604
step 201/334, epoch 264/501 --> loss:0.8144705474376679
step 251/334, epoch 264/501 --> loss:0.8260878705978394
step 301/334, epoch 264/501 --> loss:0.8232288312911987
step 51/334, epoch 265/501 --> loss:0.8450598788261413
step 101/334, epoch 265/501 --> loss:0.8362455248832703
step 151/334, epoch 265/501 --> loss:0.8408835172653198
step 201/334, epoch 265/501 --> loss:0.8401987898349762
step 251/334, epoch 265/501 --> loss:0.8072740602493286
step 301/334, epoch 265/501 --> loss:0.8142432332038879
step 51/334, epoch 266/501 --> loss:0.8221782100200653
step 101/334, epoch 266/501 --> loss:0.8423660945892334
step 151/334, epoch 266/501 --> loss:0.8292467570304871
step 201/334, epoch 266/501 --> loss:0.8352335476875306
step 251/334, epoch 266/501 --> loss:0.8085384130477905
step 301/334, epoch 266/501 --> loss:0.8357037341594696
step 51/334, epoch 267/501 --> loss:0.8195982539653778
step 101/334, epoch 267/501 --> loss:0.836796418428421
step 151/334, epoch 267/501 --> loss:0.8426351809501648
step 201/334, epoch 267/501 --> loss:0.8231185126304627
step 251/334, epoch 267/501 --> loss:0.8262666666507721
step 301/334, epoch 267/501 --> loss:0.8353421533107758
step 51/334, epoch 268/501 --> loss:0.8516208374500275
step 101/334, epoch 268/501 --> loss:0.8486992800235749
step 151/334, epoch 268/501 --> loss:0.8113903272151947
step 201/334, epoch 268/501 --> loss:0.8321626305580139
step 251/334, epoch 268/501 --> loss:0.8240252184867859
step 301/334, epoch 268/501 --> loss:0.8324257802963256
step 51/334, epoch 269/501 --> loss:0.8275766038894653
step 101/334, epoch 269/501 --> loss:0.8354171991348267
step 151/334, epoch 269/501 --> loss:0.8178978705406189
step 201/334, epoch 269/501 --> loss:0.8395690286159515
step 251/334, epoch 269/501 --> loss:0.8302387428283692
step 301/334, epoch 269/501 --> loss:0.8240935623645782
step 51/334, epoch 270/501 --> loss:0.8464133179187775
step 101/334, epoch 270/501 --> loss:0.8221482849121093
step 151/334, epoch 270/501 --> loss:0.8407642817497254
step 201/334, epoch 270/501 --> loss:0.8299264240264893
step 251/334, epoch 270/501 --> loss:0.8233832621574402
step 301/334, epoch 270/501 --> loss:0.8175421667098999
step 51/334, epoch 271/501 --> loss:0.8181491112709045
step 101/334, epoch 271/501 --> loss:0.8394961130619049
step 151/334, epoch 271/501 --> loss:0.8236092412471772
step 201/334, epoch 271/501 --> loss:0.8224727392196656
step 251/334, epoch 271/501 --> loss:0.8459302031993866
step 301/334, epoch 271/501 --> loss:0.8325733089447022

##########train dataset##########
acc--> [98.91916281460628]
F1--> {'F1': [0.8862338891229902], 'precision': [0.8024899998348879], 'recall': [0.9895047575754182]}
##########eval dataset##########
acc--> [98.41178366726328]
F1--> {'F1': [0.8266067539201942], 'precision': [0.7575341953448684], 'recall': [0.9095512586632624]}
save model!
step 51/334, epoch 272/501 --> loss:0.8184375667572021
step 101/334, epoch 272/501 --> loss:0.8255614686012268
step 151/334, epoch 272/501 --> loss:0.8193541324138641
step 201/334, epoch 272/501 --> loss:0.8324467813968659
step 251/334, epoch 272/501 --> loss:0.8317283689975739
step 301/334, epoch 272/501 --> loss:0.8357099759578704
step 51/334, epoch 273/501 --> loss:0.8253483724594116
step 101/334, epoch 273/501 --> loss:0.8377361834049225
step 151/334, epoch 273/501 --> loss:0.8258438611030579
step 201/334, epoch 273/501 --> loss:0.8333329629898071
step 251/334, epoch 273/501 --> loss:0.8503346848487854
step 301/334, epoch 273/501 --> loss:0.807569546699524
step 51/334, epoch 274/501 --> loss:0.8139993643760681
step 101/334, epoch 274/501 --> loss:0.8373118710517883
step 151/334, epoch 274/501 --> loss:0.831128500699997
step 201/334, epoch 274/501 --> loss:0.820788391828537
step 251/334, epoch 274/501 --> loss:0.8314149248600006
step 301/334, epoch 274/501 --> loss:0.8492646527290344
step 51/334, epoch 275/501 --> loss:0.828062870502472
step 101/334, epoch 275/501 --> loss:0.8181831645965576
step 151/334, epoch 275/501 --> loss:0.8263066685199738
step 201/334, epoch 275/501 --> loss:0.828498569726944
step 251/334, epoch 275/501 --> loss:0.8176969051361084
step 301/334, epoch 275/501 --> loss:0.8431418442726135
step 51/334, epoch 276/501 --> loss:0.84212282538414
step 101/334, epoch 276/501 --> loss:0.8309452247619629
step 151/334, epoch 276/501 --> loss:0.8241161680221558
step 201/334, epoch 276/501 --> loss:0.8165846002101899
step 251/334, epoch 276/501 --> loss:0.8491441607475281
step 301/334, epoch 276/501 --> loss:0.8302333641052246
step 51/334, epoch 277/501 --> loss:0.8341823375225067
step 101/334, epoch 277/501 --> loss:0.810772784948349
step 151/334, epoch 277/501 --> loss:0.8263490164279937
step 201/334, epoch 277/501 --> loss:0.8479960358142853
step 251/334, epoch 277/501 --> loss:0.8379438841342925
step 301/334, epoch 277/501 --> loss:0.811451040506363
step 51/334, epoch 278/501 --> loss:0.8473833167552948
step 101/334, epoch 278/501 --> loss:0.8288943827152252
step 151/334, epoch 278/501 --> loss:0.8297865998744964
step 201/334, epoch 278/501 --> loss:0.8091493618488311
step 251/334, epoch 278/501 --> loss:0.8545221161842346
step 301/334, epoch 278/501 --> loss:0.8300944077968597
step 51/334, epoch 279/501 --> loss:0.8256344103813171
step 101/334, epoch 279/501 --> loss:0.8463562965393067
step 151/334, epoch 279/501 --> loss:0.8213612473011017
step 201/334, epoch 279/501 --> loss:0.8228691244125366
step 251/334, epoch 279/501 --> loss:0.825523099899292
step 301/334, epoch 279/501 --> loss:0.8252789580821991
step 51/334, epoch 280/501 --> loss:0.8156426060199737
step 101/334, epoch 280/501 --> loss:0.8236489367485046
step 151/334, epoch 280/501 --> loss:0.8042083132266998
step 201/334, epoch 280/501 --> loss:0.8344799351692199
step 251/334, epoch 280/501 --> loss:0.8236172318458557
step 301/334, epoch 280/501 --> loss:0.8450068330764771
step 51/334, epoch 281/501 --> loss:0.8053967559337616
step 101/334, epoch 281/501 --> loss:0.8478201735019684
step 151/334, epoch 281/501 --> loss:0.8218765830993653
step 201/334, epoch 281/501 --> loss:0.8452707576751709
step 251/334, epoch 281/501 --> loss:0.851942456960678
step 301/334, epoch 281/501 --> loss:0.8122010743618011

##########train dataset##########
acc--> [98.81826171188158]
F1--> {'F1': [0.8769459568154979], 'precision': [0.7872416093269097], 'recall': [0.9897351060160949]}
##########eval dataset##########
acc--> [98.09859735549979]
F1--> {'F1': [0.8000019015982357], 'precision': [0.711496139138694], 'recall': [0.9136678279750093]}
step 51/334, epoch 282/501 --> loss:0.8258242917060852
step 101/334, epoch 282/501 --> loss:0.8420652878284455
step 151/334, epoch 282/501 --> loss:0.8018565011024475
step 201/334, epoch 282/501 --> loss:0.8275750124454498
step 251/334, epoch 282/501 --> loss:0.811087554693222
step 301/334, epoch 282/501 --> loss:0.848473870754242
step 51/334, epoch 283/501 --> loss:0.8380743265151978
step 101/334, epoch 283/501 --> loss:0.8315784764289856
step 151/334, epoch 283/501 --> loss:0.8182153940200806
step 201/334, epoch 283/501 --> loss:0.82769904255867
step 251/334, epoch 283/501 --> loss:0.8296797525882721
step 301/334, epoch 283/501 --> loss:0.826381789445877
step 51/334, epoch 284/501 --> loss:0.8186741852760315
step 101/334, epoch 284/501 --> loss:0.8473320543766022
step 151/334, epoch 284/501 --> loss:0.8080168068408966
step 201/334, epoch 284/501 --> loss:0.8255157279968262
step 251/334, epoch 284/501 --> loss:0.8435485768318176
step 301/334, epoch 284/501 --> loss:0.8353015911579132
step 51/334, epoch 285/501 --> loss:0.8220193088054657
step 101/334, epoch 285/501 --> loss:0.8235959649085999
step 151/334, epoch 285/501 --> loss:0.820518102645874
step 201/334, epoch 285/501 --> loss:0.8451261556148529
step 251/334, epoch 285/501 --> loss:0.841684981584549
step 301/334, epoch 285/501 --> loss:0.8249698424339295
step 51/334, epoch 286/501 --> loss:0.8267091238498687
step 101/334, epoch 286/501 --> loss:0.8211796200275421
step 151/334, epoch 286/501 --> loss:0.8405284094810486
step 201/334, epoch 286/501 --> loss:0.8354763710498809
step 251/334, epoch 286/501 --> loss:0.842158774137497
step 301/334, epoch 286/501 --> loss:0.8028336477279663
step 51/334, epoch 287/501 --> loss:0.834240779876709
step 101/334, epoch 287/501 --> loss:0.8238559234142303
step 151/334, epoch 287/501 --> loss:0.8249243402481079
step 201/334, epoch 287/501 --> loss:0.8171373140811921
step 251/334, epoch 287/501 --> loss:0.8343706703186036
step 301/334, epoch 287/501 --> loss:0.8360342729091644
step 51/334, epoch 288/501 --> loss:0.8403198373317718
step 101/334, epoch 288/501 --> loss:0.8421495151519776
step 151/334, epoch 288/501 --> loss:0.8420705497264862
step 201/334, epoch 288/501 --> loss:0.8191300785541534
step 251/334, epoch 288/501 --> loss:0.8378342473506928
step 301/334, epoch 288/501 --> loss:0.8145487058162689
step 51/334, epoch 289/501 --> loss:0.8427242004871368
step 101/334, epoch 289/501 --> loss:0.8315118408203125
step 151/334, epoch 289/501 --> loss:0.8090617156028748
step 201/334, epoch 289/501 --> loss:0.8135523843765259
step 251/334, epoch 289/501 --> loss:0.8502217853069305
step 301/334, epoch 289/501 --> loss:0.8327224695682526
step 51/334, epoch 290/501 --> loss:0.8388317096233368
step 101/334, epoch 290/501 --> loss:0.8220535624027252
step 151/334, epoch 290/501 --> loss:0.8215117764472961
step 201/334, epoch 290/501 --> loss:0.8315208387374878
step 251/334, epoch 290/501 --> loss:0.8224794864654541
step 301/334, epoch 290/501 --> loss:0.826792323589325
step 51/334, epoch 291/501 --> loss:0.8348901236057281
step 101/334, epoch 291/501 --> loss:0.8363040590286255
step 151/334, epoch 291/501 --> loss:0.8302841091156006
step 201/334, epoch 291/501 --> loss:0.8234225308895111
step 251/334, epoch 291/501 --> loss:0.8266829466819763
step 301/334, epoch 291/501 --> loss:0.8268790662288665

##########train dataset##########
acc--> [99.05766307256964]
F1--> {'F1': [0.8994648210684113], 'precision': [0.8235384747781288], 'recall': [0.9908251261329224]}
##########eval dataset##########
acc--> [98.53161455555922]
F1--> {'F1': [0.8362663514495816], 'precision': [0.7802609096230125], 'recall': [0.9009449507063663]}
save model!
step 51/334, epoch 292/501 --> loss:0.8349415695667267
step 101/334, epoch 292/501 --> loss:0.8168465256690979
step 151/334, epoch 292/501 --> loss:0.8522571301460267
step 201/334, epoch 292/501 --> loss:0.8098225343227387
step 251/334, epoch 292/501 --> loss:0.8111656427383422
step 301/334, epoch 292/501 --> loss:0.8561981523036957
step 51/334, epoch 293/501 --> loss:0.8212068748474121
step 101/334, epoch 293/501 --> loss:0.8180297863483429
step 151/334, epoch 293/501 --> loss:0.8361765134334564
step 201/334, epoch 293/501 --> loss:0.8163466131687165
step 251/334, epoch 293/501 --> loss:0.8277532362937927
step 301/334, epoch 293/501 --> loss:0.8470103096961975
step 51/334, epoch 294/501 --> loss:0.8258915770053864
step 101/334, epoch 294/501 --> loss:0.815637995004654
step 151/334, epoch 294/501 --> loss:0.8154777133464813
step 201/334, epoch 294/501 --> loss:0.8341616904735565
step 251/334, epoch 294/501 --> loss:0.8407892632484436
step 301/334, epoch 294/501 --> loss:0.8402323830127716
step 51/334, epoch 295/501 --> loss:0.8285140144824982
step 101/334, epoch 295/501 --> loss:0.8109031784534454
step 151/334, epoch 295/501 --> loss:0.8115790581703186
step 201/334, epoch 295/501 --> loss:0.8287901270389557
step 251/334, epoch 295/501 --> loss:0.8403486335277557
step 301/334, epoch 295/501 --> loss:0.8431674706935882
step 51/334, epoch 296/501 --> loss:0.84043297290802
step 101/334, epoch 296/501 --> loss:0.8360628151893615
step 151/334, epoch 296/501 --> loss:0.8135777354240418
step 201/334, epoch 296/501 --> loss:0.8228797733783721
step 251/334, epoch 296/501 --> loss:0.8351728260517121
step 301/334, epoch 296/501 --> loss:0.8256200480461121
step 51/334, epoch 297/501 --> loss:0.847839549779892
step 101/334, epoch 297/501 --> loss:0.8062984693050385
step 151/334, epoch 297/501 --> loss:0.8162821674346924
step 201/334, epoch 297/501 --> loss:0.8308111536502838
step 251/334, epoch 297/501 --> loss:0.8423433685302735
step 301/334, epoch 297/501 --> loss:0.8220518982410431
step 51/334, epoch 298/501 --> loss:0.8258959865570068
step 101/334, epoch 298/501 --> loss:0.8278399157524109
step 151/334, epoch 298/501 --> loss:0.8444318997859955
step 201/334, epoch 298/501 --> loss:0.8280291676521301
step 251/334, epoch 298/501 --> loss:0.8319029200077057
step 301/334, epoch 298/501 --> loss:0.8240794456005096
step 51/334, epoch 299/501 --> loss:0.8169250440597534
step 101/334, epoch 299/501 --> loss:0.8242918121814727
step 151/334, epoch 299/501 --> loss:0.825990926027298
step 201/334, epoch 299/501 --> loss:0.8373580014705658
step 251/334, epoch 299/501 --> loss:0.8231504380702972
step 301/334, epoch 299/501 --> loss:0.8689787089824677
step 51/334, epoch 300/501 --> loss:0.8171265685558319
step 101/334, epoch 300/501 --> loss:0.8192948651313782
step 151/334, epoch 300/501 --> loss:0.8219198775291443
step 201/334, epoch 300/501 --> loss:0.8479689228534698
step 251/334, epoch 300/501 --> loss:0.8223852789402009
step 301/334, epoch 300/501 --> loss:0.8215488648414612
step 51/334, epoch 301/501 --> loss:0.8168183600902558
step 101/334, epoch 301/501 --> loss:0.8215331602096557
step 151/334, epoch 301/501 --> loss:0.831516284942627
step 201/334, epoch 301/501 --> loss:0.8471887946128845
step 251/334, epoch 301/501 --> loss:0.8371502411365509
step 301/334, epoch 301/501 --> loss:0.820987981557846

##########train dataset##########
acc--> [99.02627184724184]
F1--> {'F1': [0.8965461707570584], 'precision': [0.8180502337356322], 'recall': [0.991717272501673]}
##########eval dataset##########
acc--> [98.42775199669208]
F1--> {'F1': [0.8266213531527697], 'precision': [0.7639556529715397], 'recall': [0.9004981738422242]}
step 51/334, epoch 302/501 --> loss:0.8378172421455383
step 101/334, epoch 302/501 --> loss:0.8254057741165162
step 151/334, epoch 302/501 --> loss:0.8269192397594451
step 201/334, epoch 302/501 --> loss:0.8239838993549347
step 251/334, epoch 302/501 --> loss:0.8369759523868561
step 301/334, epoch 302/501 --> loss:0.8112116384506226
step 51/334, epoch 303/501 --> loss:0.8305133962631226
step 101/334, epoch 303/501 --> loss:0.8470414507389069
step 151/334, epoch 303/501 --> loss:0.7990553283691406
step 201/334, epoch 303/501 --> loss:0.8266342437267303
step 251/334, epoch 303/501 --> loss:0.8270857632160187
step 301/334, epoch 303/501 --> loss:0.8355426025390625
step 51/334, epoch 304/501 --> loss:0.8265756833553314
step 101/334, epoch 304/501 --> loss:0.8274947881698609
step 151/334, epoch 304/501 --> loss:0.8218857181072236
step 201/334, epoch 304/501 --> loss:0.8356876349449158
step 251/334, epoch 304/501 --> loss:0.8259683907032013
step 301/334, epoch 304/501 --> loss:0.8229177105426788
step 51/334, epoch 305/501 --> loss:0.8314006567001343
step 101/334, epoch 305/501 --> loss:0.8106686127185821
step 151/334, epoch 305/501 --> loss:0.8202712142467499
step 201/334, epoch 305/501 --> loss:0.8216274034976959
step 251/334, epoch 305/501 --> loss:0.8219831037521362
step 301/334, epoch 305/501 --> loss:0.8312240612506866
step 51/334, epoch 306/501 --> loss:0.8389931309223175
step 101/334, epoch 306/501 --> loss:0.8486536681652069
step 151/334, epoch 306/501 --> loss:0.8376975750923157
step 201/334, epoch 306/501 --> loss:0.8152552223205567
step 251/334, epoch 306/501 --> loss:0.8223305177688599
step 301/334, epoch 306/501 --> loss:0.8105502939224243
step 51/334, epoch 307/501 --> loss:0.8189882123470307
step 101/334, epoch 307/501 --> loss:0.8266474175453186
step 151/334, epoch 307/501 --> loss:0.8237693548202515
step 201/334, epoch 307/501 --> loss:0.8441321861743927
step 251/334, epoch 307/501 --> loss:0.8526329612731933
step 301/334, epoch 307/501 --> loss:0.8059996497631073
step 51/334, epoch 308/501 --> loss:0.8526399087905884
step 101/334, epoch 308/501 --> loss:0.8410119557380676
step 151/334, epoch 308/501 --> loss:0.820851731300354
step 201/334, epoch 308/501 --> loss:0.8021335613727569
step 251/334, epoch 308/501 --> loss:0.8202443623542786
step 301/334, epoch 308/501 --> loss:0.8253362214565277
step 51/334, epoch 309/501 --> loss:0.8495625734329224
step 101/334, epoch 309/501 --> loss:0.8231723117828369
step 151/334, epoch 309/501 --> loss:0.8211754441261292
step 201/334, epoch 309/501 --> loss:0.8342743945121766
step 251/334, epoch 309/501 --> loss:0.8359522032737732
step 301/334, epoch 309/501 --> loss:0.8240490305423737
step 51/334, epoch 310/501 --> loss:0.8128702688217163
step 101/334, epoch 310/501 --> loss:0.8351870846748352
step 151/334, epoch 310/501 --> loss:0.8241076862812042
step 201/334, epoch 310/501 --> loss:0.8297952055931092
step 251/334, epoch 310/501 --> loss:0.8325564956665039
step 301/334, epoch 310/501 --> loss:0.8507050728797912
step 51/334, epoch 311/501 --> loss:0.8278238153457642
step 101/334, epoch 311/501 --> loss:0.8291664648056031
step 151/334, epoch 311/501 --> loss:0.8425464141368866
step 201/334, epoch 311/501 --> loss:0.8142006397247314
step 251/334, epoch 311/501 --> loss:0.8277966415882111
step 301/334, epoch 311/501 --> loss:0.8379560279846191

##########train dataset##########
acc--> [99.05662322070907]
F1--> {'F1': [0.899431776572101], 'precision': [0.822978834632868], 'recall': [0.9915561092756654]}
##########eval dataset##########
acc--> [98.50200142446745]
F1--> {'F1': [0.833281523488861], 'precision': [0.7761997656174819], 'recall': [0.8994368783336484]}
step 51/334, epoch 312/501 --> loss:0.8181743216514588
step 101/334, epoch 312/501 --> loss:0.8272737514972687
step 151/334, epoch 312/501 --> loss:0.8399286985397338
step 201/334, epoch 312/501 --> loss:0.8173115670680999
step 251/334, epoch 312/501 --> loss:0.8302362990379334
step 301/334, epoch 312/501 --> loss:0.8276936531066894
step 51/334, epoch 313/501 --> loss:0.8426980566978455
step 101/334, epoch 313/501 --> loss:0.8273618149757386
step 151/334, epoch 313/501 --> loss:0.8403179347515106
step 201/334, epoch 313/501 --> loss:0.8171489751338958
step 251/334, epoch 313/501 --> loss:0.8131300508975983
step 301/334, epoch 313/501 --> loss:0.8417928206920624
step 51/334, epoch 314/501 --> loss:0.8123977291584015
step 101/334, epoch 314/501 --> loss:0.8296988689899445
step 151/334, epoch 314/501 --> loss:0.8493998301029205
step 201/334, epoch 314/501 --> loss:0.8243786180019379
step 251/334, epoch 314/501 --> loss:0.828556227684021
step 301/334, epoch 314/501 --> loss:0.8277375173568725
step 51/334, epoch 315/501 --> loss:0.836071424484253
step 101/334, epoch 315/501 --> loss:0.8422309303283692
step 151/334, epoch 315/501 --> loss:0.8306750202178955
step 201/334, epoch 315/501 --> loss:0.8336825227737427
step 251/334, epoch 315/501 --> loss:0.8076554083824158
step 301/334, epoch 315/501 --> loss:0.8223635411262512
step 51/334, epoch 316/501 --> loss:0.8265322744846344
step 101/334, epoch 316/501 --> loss:0.8129872393608093
step 151/334, epoch 316/501 --> loss:0.8486758613586426
step 201/334, epoch 316/501 --> loss:0.8192298400402069
step 251/334, epoch 316/501 --> loss:0.8312538135051727
step 301/334, epoch 316/501 --> loss:0.8211574637889862
step 51/334, epoch 317/501 --> loss:0.8335217654705047
step 101/334, epoch 317/501 --> loss:0.8056473314762116
step 151/334, epoch 317/501 --> loss:0.8135029482841492
step 201/334, epoch 317/501 --> loss:0.8320632159709931
step 251/334, epoch 317/501 --> loss:0.8519800961017608
step 301/334, epoch 317/501 --> loss:0.8341187417507172
step 51/334, epoch 318/501 --> loss:0.8161680936813355
step 101/334, epoch 318/501 --> loss:0.8237298476696014
step 151/334, epoch 318/501 --> loss:0.8432210636138916
step 201/334, epoch 318/501 --> loss:0.8248924958705902
step 251/334, epoch 318/501 --> loss:0.845068588256836
step 301/334, epoch 318/501 --> loss:0.8252884268760681
step 51/334, epoch 319/501 --> loss:0.8150547587871552
step 101/334, epoch 319/501 --> loss:0.8083829939365387
step 151/334, epoch 319/501 --> loss:0.8321742725372314
step 201/334, epoch 319/501 --> loss:0.8352113592624665
step 251/334, epoch 319/501 --> loss:0.8425113642215729
step 301/334, epoch 319/501 --> loss:0.8411693322658539
step 51/334, epoch 320/501 --> loss:0.8302132809162139
step 101/334, epoch 320/501 --> loss:0.8341010773181915
step 151/334, epoch 320/501 --> loss:0.8168672370910645
step 201/334, epoch 320/501 --> loss:0.8400871014595032
step 251/334, epoch 320/501 --> loss:0.8363074123859405
step 301/334, epoch 320/501 --> loss:0.82476118683815
step 51/334, epoch 321/501 --> loss:0.8339018142223358
step 101/334, epoch 321/501 --> loss:0.8206956553459167
step 151/334, epoch 321/501 --> loss:0.8301284778118133
step 201/334, epoch 321/501 --> loss:0.8161510443687439
step 251/334, epoch 321/501 --> loss:0.847433809041977
step 301/334, epoch 321/501 --> loss:0.8252029025554657

##########train dataset##########
acc--> [98.87213519238767]
F1--> {'F1': [0.8815486024636984], 'precision': [0.7968076338463755], 'recall': [0.9864715002628994]}
##########eval dataset##########
acc--> [98.18574533269965]
F1--> {'F1': [0.8057937943921419], 'precision': [0.7266536807026149], 'recall': [0.9042916076976731]}
step 51/334, epoch 322/501 --> loss:0.816895169019699
step 101/334, epoch 322/501 --> loss:0.8427498114109039
step 151/334, epoch 322/501 --> loss:0.8289292430877686
step 201/334, epoch 322/501 --> loss:0.8565653038024902
step 251/334, epoch 322/501 --> loss:0.8188503491878509
step 301/334, epoch 322/501 --> loss:0.8134099698066711
step 51/334, epoch 323/501 --> loss:0.8213556456565857
step 101/334, epoch 323/501 --> loss:0.798196097612381
step 151/334, epoch 323/501 --> loss:0.8504256200790405
step 201/334, epoch 323/501 --> loss:0.8323748135566711
step 251/334, epoch 323/501 --> loss:0.8250853025913238
step 301/334, epoch 323/501 --> loss:0.8448981416225433
step 51/334, epoch 324/501 --> loss:0.846313933134079
step 101/334, epoch 324/501 --> loss:0.8181056082248688
step 151/334, epoch 324/501 --> loss:0.8287561011314392
step 201/334, epoch 324/501 --> loss:0.8267512011528015
step 251/334, epoch 324/501 --> loss:0.8358884239196778
step 301/334, epoch 324/501 --> loss:0.8309905970096588
step 51/334, epoch 325/501 --> loss:0.8589214444160461
step 101/334, epoch 325/501 --> loss:0.832048407793045
step 151/334, epoch 325/501 --> loss:0.8516259396076202
step 201/334, epoch 325/501 --> loss:0.8178174495697021
step 251/334, epoch 325/501 --> loss:0.8150936150550843
step 301/334, epoch 325/501 --> loss:0.8029865610599518
step 51/334, epoch 326/501 --> loss:0.83118945479393
step 101/334, epoch 326/501 --> loss:0.8383525371551513
step 151/334, epoch 326/501 --> loss:0.8101815712451935
step 201/334, epoch 326/501 --> loss:0.8498797905445099
step 251/334, epoch 326/501 --> loss:0.7967495155334473
step 301/334, epoch 326/501 --> loss:0.8408447802066803
step 51/334, epoch 327/501 --> loss:0.8110887479782104
step 101/334, epoch 327/501 --> loss:0.8334273588657379
step 151/334, epoch 327/501 --> loss:0.8186554145812989
step 201/334, epoch 327/501 --> loss:0.8201929926872253
step 251/334, epoch 327/501 --> loss:0.8281146955490112
step 301/334, epoch 327/501 --> loss:0.8378720211982728
step 51/334, epoch 328/501 --> loss:0.8298674130439758
step 101/334, epoch 328/501 --> loss:0.8206544935703277
step 151/334, epoch 328/501 --> loss:0.8360656774044037
step 201/334, epoch 328/501 --> loss:0.8372545707225799
step 251/334, epoch 328/501 --> loss:0.8249285519123077
step 301/334, epoch 328/501 --> loss:0.8211309123039245
step 51/334, epoch 329/501 --> loss:0.8366531813144684
step 101/334, epoch 329/501 --> loss:0.8222074365615845
step 151/334, epoch 329/501 --> loss:0.8323497378826141
step 201/334, epoch 329/501 --> loss:0.838143105506897
step 251/334, epoch 329/501 --> loss:0.80943274974823
step 301/334, epoch 329/501 --> loss:0.8274578487873078
step 51/334, epoch 330/501 --> loss:0.8192007672786713
step 101/334, epoch 330/501 --> loss:0.8060659027099609
step 151/334, epoch 330/501 --> loss:0.8430306398868561
step 201/334, epoch 330/501 --> loss:0.8128691947460175
step 251/334, epoch 330/501 --> loss:0.8505256855487824
step 301/334, epoch 330/501 --> loss:0.829858283996582
step 51/334, epoch 331/501 --> loss:0.8293156063556671
step 101/334, epoch 331/501 --> loss:0.8168587303161621
step 151/334, epoch 331/501 --> loss:0.8174247205257416
step 201/334, epoch 331/501 --> loss:0.8491123938560485
step 251/334, epoch 331/501 --> loss:0.819340169429779
step 301/334, epoch 331/501 --> loss:0.8361857914924622

##########train dataset##########
acc--> [98.7981117571762]
F1--> {'F1': [0.8750189561760593], 'precision': [0.7846583283612971], 'recall': [0.9889124810389556]}
##########eval dataset##########
acc--> [98.17882814848903]
F1--> {'F1': [0.8061217975231671], 'precision': [0.723761930917932], 'recall': [0.9096452325478235]}
step 51/334, epoch 332/501 --> loss:0.8238252484798432
step 101/334, epoch 332/501 --> loss:0.8457567965984345
step 151/334, epoch 332/501 --> loss:0.8437504994869233
step 201/334, epoch 332/501 --> loss:0.8515239119529724
step 251/334, epoch 332/501 --> loss:0.8359762573242188
step 301/334, epoch 332/501 --> loss:0.8061254560947418
step 51/334, epoch 333/501 --> loss:0.8348653674125671
step 101/334, epoch 333/501 --> loss:0.8103634536266326
step 151/334, epoch 333/501 --> loss:0.8263209092617035
step 201/334, epoch 333/501 --> loss:0.8122608006000519
step 251/334, epoch 333/501 --> loss:0.8477366864681244
step 301/334, epoch 333/501 --> loss:0.8220833790302277
step 51/334, epoch 334/501 --> loss:0.8450065755844116
step 101/334, epoch 334/501 --> loss:0.800020854473114
step 151/334, epoch 334/501 --> loss:0.8224581468105316
step 201/334, epoch 334/501 --> loss:0.8317353749275207
step 251/334, epoch 334/501 --> loss:0.8419141364097595
step 301/334, epoch 334/501 --> loss:0.8316234421730041
step 51/334, epoch 335/501 --> loss:0.841460931301117
step 101/334, epoch 335/501 --> loss:0.8012662053108215
step 151/334, epoch 335/501 --> loss:0.8344062507152558
step 201/334, epoch 335/501 --> loss:0.8383041894435883
step 251/334, epoch 335/501 --> loss:0.8367077696323395
step 301/334, epoch 335/501 --> loss:0.8225122570991517
step 51/334, epoch 336/501 --> loss:0.8142883789539337
step 101/334, epoch 336/501 --> loss:0.8323841440677643
step 151/334, epoch 336/501 --> loss:0.8537466275691986
step 201/334, epoch 336/501 --> loss:0.8424257814884186
step 251/334, epoch 336/501 --> loss:0.8044249737262725
step 301/334, epoch 336/501 --> loss:0.8123496317863464
step 51/334, epoch 337/501 --> loss:0.8258101534843445
step 101/334, epoch 337/501 --> loss:0.8369347035884858
step 151/334, epoch 337/501 --> loss:0.8088169872760773
step 201/334, epoch 337/501 --> loss:0.8403167223930359
step 251/334, epoch 337/501 --> loss:0.8197209191322327
step 301/334, epoch 337/501 --> loss:0.832099643945694
step 51/334, epoch 338/501 --> loss:0.833829859495163
step 101/334, epoch 338/501 --> loss:0.8107763791084289
step 151/334, epoch 338/501 --> loss:0.8341737687587738
step 201/334, epoch 338/501 --> loss:0.8219495153427124
step 251/334, epoch 338/501 --> loss:0.8149079537391662
step 301/334, epoch 338/501 --> loss:0.840041047334671
step 51/334, epoch 339/501 --> loss:0.8189530646800995
step 101/334, epoch 339/501 --> loss:0.8493532967567444
step 151/334, epoch 339/501 --> loss:0.8320022094249725
step 201/334, epoch 339/501 --> loss:0.8261538219451904
step 251/334, epoch 339/501 --> loss:0.8414306545257568
step 301/334, epoch 339/501 --> loss:0.8194882071018219
step 51/334, epoch 340/501 --> loss:0.8199982523918152
step 101/334, epoch 340/501 --> loss:0.8284302365779876
step 151/334, epoch 340/501 --> loss:0.8279894769191742
step 201/334, epoch 340/501 --> loss:0.8362148952484131
step 251/334, epoch 340/501 --> loss:0.8301099824905396
step 301/334, epoch 340/501 --> loss:0.8231919801235199
step 51/334, epoch 341/501 --> loss:0.8442494225502014
step 101/334, epoch 341/501 --> loss:0.8378906607627868
step 151/334, epoch 341/501 --> loss:0.8420157599449157
step 201/334, epoch 341/501 --> loss:0.8049135375022888
step 251/334, epoch 341/501 --> loss:0.8361634290218354
step 301/334, epoch 341/501 --> loss:0.8136613929271698

##########train dataset##########
acc--> [98.91019005161913]
F1--> {'F1': [0.885439631630512], 'precision': [0.8009205379174906], 'recall': [0.9899137504430879]}
##########eval dataset##########
acc--> [98.25444726284168]
F1--> {'F1': [0.8143418286026761], 'precision': [0.7306103419143606], 'recall': [0.9197621787172889]}
step 51/334, epoch 342/501 --> loss:0.8286583209037781
step 101/334, epoch 342/501 --> loss:0.8211853742599488
step 151/334, epoch 342/501 --> loss:0.8074026000499726
step 201/334, epoch 342/501 --> loss:0.8394989693164825
step 251/334, epoch 342/501 --> loss:0.8061661398410798
step 301/334, epoch 342/501 --> loss:0.8432572340965271
step 51/334, epoch 343/501 --> loss:0.8191992509365081
step 101/334, epoch 343/501 --> loss:0.8239493286609649
step 151/334, epoch 343/501 --> loss:0.8459598684310913
step 201/334, epoch 343/501 --> loss:0.8250203442573547
step 251/334, epoch 343/501 --> loss:0.8275377202033997
step 301/334, epoch 343/501 --> loss:0.8352639853954316
step 51/334, epoch 344/501 --> loss:0.8331473731994629
step 101/334, epoch 344/501 --> loss:0.846155891418457
step 151/334, epoch 344/501 --> loss:0.8248482871055604
step 201/334, epoch 344/501 --> loss:0.8084915459156037
step 251/334, epoch 344/501 --> loss:0.8218176317214966
step 301/334, epoch 344/501 --> loss:0.834009815454483
step 51/334, epoch 345/501 --> loss:0.8231436598300934
step 101/334, epoch 345/501 --> loss:0.8130808019638062
step 151/334, epoch 345/501 --> loss:0.8137837648391724
step 201/334, epoch 345/501 --> loss:0.836204137802124
step 251/334, epoch 345/501 --> loss:0.8348331570625305
step 301/334, epoch 345/501 --> loss:0.8383112847805023
step 51/334, epoch 346/501 --> loss:0.8184172093868256
step 101/334, epoch 346/501 --> loss:0.8276776266098023
step 151/334, epoch 346/501 --> loss:0.8252894616127014
step 201/334, epoch 346/501 --> loss:0.8537141847610473
step 251/334, epoch 346/501 --> loss:0.8462104189395905
step 301/334, epoch 346/501 --> loss:0.7983900249004364
step 51/334, epoch 347/501 --> loss:0.8360616219043732
step 101/334, epoch 347/501 --> loss:0.85660511136055
step 151/334, epoch 347/501 --> loss:0.8146149289608001
step 201/334, epoch 347/501 --> loss:0.8387111270427704
step 251/334, epoch 347/501 --> loss:0.8111071074008942
step 301/334, epoch 347/501 --> loss:0.8232626283168792
step 51/334, epoch 348/501 --> loss:0.8496336197853088
step 101/334, epoch 348/501 --> loss:0.8185142934322357
step 151/334, epoch 348/501 --> loss:0.8658328247070313
step 201/334, epoch 348/501 --> loss:0.8426511180400849
step 251/334, epoch 348/501 --> loss:0.7834332418441773
step 301/334, epoch 348/501 --> loss:0.8158868932723999
step 51/334, epoch 349/501 --> loss:0.8330446720123291
step 101/334, epoch 349/501 --> loss:0.8425145649909973
step 151/334, epoch 349/501 --> loss:0.8184443616867065
step 201/334, epoch 349/501 --> loss:0.8470857393741608
step 251/334, epoch 349/501 --> loss:0.8336233234405518
step 301/334, epoch 349/501 --> loss:0.8296459734439849
step 51/334, epoch 350/501 --> loss:0.8272951817512513
step 101/334, epoch 350/501 --> loss:0.8288501465320587
step 151/334, epoch 350/501 --> loss:0.8424275958538056
step 201/334, epoch 350/501 --> loss:0.8239319336414337
step 251/334, epoch 350/501 --> loss:0.8155477666854858
step 301/334, epoch 350/501 --> loss:0.8416740703582763
step 51/334, epoch 351/501 --> loss:0.8387309229373932
step 101/334, epoch 351/501 --> loss:0.8230057477951049
step 151/334, epoch 351/501 --> loss:0.8378866636753082
step 201/334, epoch 351/501 --> loss:0.8325534141063691
step 251/334, epoch 351/501 --> loss:0.8192552769184113
step 301/334, epoch 351/501 --> loss:0.832927188873291

##########train dataset##########
acc--> [99.07661669701892]
F1--> {'F1': [0.9014612319289104], 'precision': [0.8255446441106684], 'recall': [0.9927663462669616]}
##########eval dataset##########
acc--> [98.44646770913842]
F1--> {'F1': [0.8303221754189504], 'precision': [0.7612048846592993], 'recall': [0.9132566521388068]}
step 51/334, epoch 352/501 --> loss:0.8242843282222748
step 101/334, epoch 352/501 --> loss:0.8262324798107147
step 151/334, epoch 352/501 --> loss:0.8318326663970947
step 201/334, epoch 352/501 --> loss:0.8471777963638306
step 251/334, epoch 352/501 --> loss:0.8302015733718872
step 301/334, epoch 352/501 --> loss:0.8246505665779114
step 51/334, epoch 353/501 --> loss:0.819362860918045
step 101/334, epoch 353/501 --> loss:0.8423954403400421
step 151/334, epoch 353/501 --> loss:0.8287241840362549
step 201/334, epoch 353/501 --> loss:0.8369837498664856
step 251/334, epoch 353/501 --> loss:0.8071994507312774
step 301/334, epoch 353/501 --> loss:0.8270318651199341
step 51/334, epoch 354/501 --> loss:0.8095530796051026
step 101/334, epoch 354/501 --> loss:0.8327130341529846
step 151/334, epoch 354/501 --> loss:0.8240563380718231
step 201/334, epoch 354/501 --> loss:0.8212390744686127
step 251/334, epoch 354/501 --> loss:0.8299927639961243
step 301/334, epoch 354/501 --> loss:0.8359267389774323
step 51/334, epoch 355/501 --> loss:0.8219581568241119
step 101/334, epoch 355/501 --> loss:0.8306067955493927
step 151/334, epoch 355/501 --> loss:0.8393579649925232
step 201/334, epoch 355/501 --> loss:0.8155051577091217
step 251/334, epoch 355/501 --> loss:0.8468419754505158
step 301/334, epoch 355/501 --> loss:0.8107116317749024
step 51/334, epoch 356/501 --> loss:0.8372815012931824
step 101/334, epoch 356/501 --> loss:0.8401901292800903
step 151/334, epoch 356/501 --> loss:0.8267120468616486
step 201/334, epoch 356/501 --> loss:0.8292225325107574
step 251/334, epoch 356/501 --> loss:0.8192545747756959
step 301/334, epoch 356/501 --> loss:0.8234269440174102
step 51/334, epoch 357/501 --> loss:0.8430281043052673
step 101/334, epoch 357/501 --> loss:0.8377632200717926
step 151/334, epoch 357/501 --> loss:0.8281159424781799
step 201/334, epoch 357/501 --> loss:0.8406953096389771
step 251/334, epoch 357/501 --> loss:0.8382051849365234
step 301/334, epoch 357/501 --> loss:0.8065599238872528
step 51/334, epoch 358/501 --> loss:0.8232285988330841
step 101/334, epoch 358/501 --> loss:0.8385821509361268
step 151/334, epoch 358/501 --> loss:0.8386959409713746
step 201/334, epoch 358/501 --> loss:0.8140009045600891
step 251/334, epoch 358/501 --> loss:0.8269150876998901
step 301/334, epoch 358/501 --> loss:0.822351176738739
step 51/334, epoch 359/501 --> loss:0.83724156498909
step 101/334, epoch 359/501 --> loss:0.8266467773914337
step 151/334, epoch 359/501 --> loss:0.8225440764427185
step 201/334, epoch 359/501 --> loss:0.8311266851425171
step 251/334, epoch 359/501 --> loss:0.833118290901184
step 301/334, epoch 359/501 --> loss:0.8171934986114502
step 51/334, epoch 360/501 --> loss:0.8312353825569153
step 101/334, epoch 360/501 --> loss:0.8409822273254395
step 151/334, epoch 360/501 --> loss:0.833056298494339
step 201/334, epoch 360/501 --> loss:0.8322547447681427
step 251/334, epoch 360/501 --> loss:0.8376869976520538
step 301/334, epoch 360/501 --> loss:0.8204009056091308
step 51/334, epoch 361/501 --> loss:0.829476214647293
step 101/334, epoch 361/501 --> loss:0.842916738986969
step 151/334, epoch 361/501 --> loss:0.8198264145851135
step 201/334, epoch 361/501 --> loss:0.8297891569137573
step 251/334, epoch 361/501 --> loss:0.8204142224788665
step 301/334, epoch 361/501 --> loss:0.8364279079437256

##########train dataset##########
acc--> [98.53773428162506]
F1--> {'F1': [0.8520542891873378], 'precision': [0.7480214420380934], 'recall': [0.9897122459840372]}
##########eval dataset##########
acc--> [97.81646461519537]
F1--> {'F1': [0.7793866992692516], 'precision': [0.6725028731026054], 'recall': [0.9266795225919366]}
step 51/334, epoch 362/501 --> loss:0.838188955783844
step 101/334, epoch 362/501 --> loss:0.8416447770595551
step 151/334, epoch 362/501 --> loss:0.8254615950584412
step 201/334, epoch 362/501 --> loss:0.8372813403606415
step 251/334, epoch 362/501 --> loss:0.835259850025177
step 301/334, epoch 362/501 --> loss:0.8193940997123719
step 51/334, epoch 363/501 --> loss:0.803164826631546
step 101/334, epoch 363/501 --> loss:0.8357202911376953
step 151/334, epoch 363/501 --> loss:0.8188738858699799
step 201/334, epoch 363/501 --> loss:0.8704961466789246
step 251/334, epoch 363/501 --> loss:0.8265595090389252
step 301/334, epoch 363/501 --> loss:0.8311406874656677
step 51/334, epoch 364/501 --> loss:0.7986030781269073
step 101/334, epoch 364/501 --> loss:0.8363747406005859
step 151/334, epoch 364/501 --> loss:0.8358637475967408
step 201/334, epoch 364/501 --> loss:0.8372849559783936
step 251/334, epoch 364/501 --> loss:0.8229565846920014
step 301/334, epoch 364/501 --> loss:0.8371238601207733
step 51/334, epoch 365/501 --> loss:0.8334407389163971
step 101/334, epoch 365/501 --> loss:0.8221738040447235
step 151/334, epoch 365/501 --> loss:0.8186685049533844
step 201/334, epoch 365/501 --> loss:0.8237068700790405
step 251/334, epoch 365/501 --> loss:0.8379221320152282
step 301/334, epoch 365/501 --> loss:0.8272542369365692
step 51/334, epoch 366/501 --> loss:0.8301664865016938
step 101/334, epoch 366/501 --> loss:0.8248495995998383
step 151/334, epoch 366/501 --> loss:0.8377723109722137
step 201/334, epoch 366/501 --> loss:0.8285832428932189
step 251/334, epoch 366/501 --> loss:0.8071394002437592
step 301/334, epoch 366/501 --> loss:0.8388774406909942
step 51/334, epoch 367/501 --> loss:0.8238082301616668
step 101/334, epoch 367/501 --> loss:0.8215886807441711
step 151/334, epoch 367/501 --> loss:0.8298898649215698
step 201/334, epoch 367/501 --> loss:0.8427896404266357
step 251/334, epoch 367/501 --> loss:0.8325020945072175
step 301/334, epoch 367/501 --> loss:0.8270825660228729
step 51/334, epoch 368/501 --> loss:0.8008151602745056
step 101/334, epoch 368/501 --> loss:0.8151061296463012
step 151/334, epoch 368/501 --> loss:0.814369455575943
step 201/334, epoch 368/501 --> loss:0.8509784317016602
step 251/334, epoch 368/501 --> loss:0.8386356782913208
step 301/334, epoch 368/501 --> loss:0.8402754294872284
step 51/334, epoch 369/501 --> loss:0.818046088218689
step 101/334, epoch 369/501 --> loss:0.8232194972038269
step 151/334, epoch 369/501 --> loss:0.8282656836509704
step 201/334, epoch 369/501 --> loss:0.8554551482200623
step 251/334, epoch 369/501 --> loss:0.8047676610946656
step 301/334, epoch 369/501 --> loss:0.840419991016388
step 51/334, epoch 370/501 --> loss:0.8184666335582733
step 101/334, epoch 370/501 --> loss:0.8391217839717865
step 151/334, epoch 370/501 --> loss:0.8519765436649323
step 201/334, epoch 370/501 --> loss:0.8002454936504364
step 251/334, epoch 370/501 --> loss:0.8313822984695435
step 301/334, epoch 370/501 --> loss:0.8131378650665283
step 51/334, epoch 371/501 --> loss:0.8297590637207031
step 101/334, epoch 371/501 --> loss:0.8282797646522522
step 151/334, epoch 371/501 --> loss:0.8414598619937896
step 201/334, epoch 371/501 --> loss:0.8243685221672058
step 251/334, epoch 371/501 --> loss:0.8327457749843598
step 301/334, epoch 371/501 --> loss:0.8222242939472199

##########train dataset##########
acc--> [99.10867874837506]
F1--> {'F1': [0.9045109921449457], 'precision': [0.8310372185309357], 'recall': [0.9922487010116289]}
##########eval dataset##########
acc--> [98.39472065655443]
F1--> {'F1': [0.8254524497177111], 'precision': [0.7539378108984331], 'recall': [0.9119679590733919]}
step 51/334, epoch 372/501 --> loss:0.846851315498352
step 101/334, epoch 372/501 --> loss:0.8346335458755493
step 151/334, epoch 372/501 --> loss:0.8180227327346802
step 201/334, epoch 372/501 --> loss:0.837910383939743
step 251/334, epoch 372/501 --> loss:0.8179648542404174
step 301/334, epoch 372/501 --> loss:0.8124394071102142
step 51/334, epoch 373/501 --> loss:0.8387319600582123
step 101/334, epoch 373/501 --> loss:0.8271193027496337
step 151/334, epoch 373/501 --> loss:0.8190431761741638
step 201/334, epoch 373/501 --> loss:0.8134036993980408
step 251/334, epoch 373/501 --> loss:0.8307926726341247
step 301/334, epoch 373/501 --> loss:0.8368954586982728
step 51/334, epoch 374/501 --> loss:0.8270084500312805
step 101/334, epoch 374/501 --> loss:0.8256554043293
step 151/334, epoch 374/501 --> loss:0.8224661111831665
step 201/334, epoch 374/501 --> loss:0.8340785300731659
step 251/334, epoch 374/501 --> loss:0.8345573604106903
step 301/334, epoch 374/501 --> loss:0.8238289976119995
step 51/334, epoch 375/501 --> loss:0.8295035088062286
step 101/334, epoch 375/501 --> loss:0.7916185092926026
step 151/334, epoch 375/501 --> loss:0.8277824544906616
step 201/334, epoch 375/501 --> loss:0.8415086102485657
step 251/334, epoch 375/501 --> loss:0.8387554252147674
step 301/334, epoch 375/501 --> loss:0.8355029833316803
step 51/334, epoch 376/501 --> loss:0.8007245409488678
step 101/334, epoch 376/501 --> loss:0.8149836361408234
step 151/334, epoch 376/501 --> loss:0.8444980823993683
step 201/334, epoch 376/501 --> loss:0.8534276437759399
step 251/334, epoch 376/501 --> loss:0.8370533239841461
step 301/334, epoch 376/501 --> loss:0.8254133677482605
step 51/334, epoch 377/501 --> loss:0.844884524345398
step 101/334, epoch 377/501 --> loss:0.7998813283443451
step 151/334, epoch 377/501 --> loss:0.8422570741176605
step 201/334, epoch 377/501 --> loss:0.8187186229228973
step 251/334, epoch 377/501 --> loss:0.838815016746521
step 301/334, epoch 377/501 --> loss:0.8337042450904846
step 51/334, epoch 378/501 --> loss:0.8311812615394593
step 101/334, epoch 378/501 --> loss:0.8373136901855469
step 151/334, epoch 378/501 --> loss:0.8200777852535248
step 201/334, epoch 378/501 --> loss:0.8330592894554139
step 251/334, epoch 378/501 --> loss:0.8451655554771423
step 301/334, epoch 378/501 --> loss:0.8193393123149871
step 51/334, epoch 379/501 --> loss:0.8314534342288971
step 101/334, epoch 379/501 --> loss:0.8339647722244262
step 151/334, epoch 379/501 --> loss:0.8319983816146851
step 201/334, epoch 379/501 --> loss:0.8021958673000336
step 251/334, epoch 379/501 --> loss:0.8435767221450806
step 301/334, epoch 379/501 --> loss:0.8262935924530029
step 51/334, epoch 380/501 --> loss:0.8045015370845795
step 101/334, epoch 380/501 --> loss:0.8496397590637207
step 151/334, epoch 380/501 --> loss:0.8271634912490845
step 201/334, epoch 380/501 --> loss:0.8378476679325104
step 251/334, epoch 380/501 --> loss:0.8231455338001251
step 301/334, epoch 380/501 --> loss:0.8199240827560424
step 51/334, epoch 381/501 --> loss:0.8275299847126008
step 101/334, epoch 381/501 --> loss:0.8245317149162292
step 151/334, epoch 381/501 --> loss:0.815689150094986
step 201/334, epoch 381/501 --> loss:0.8220109832286835
step 251/334, epoch 381/501 --> loss:0.8379740738868713
step 301/334, epoch 381/501 --> loss:0.8238927161693573

##########train dataset##########
acc--> [99.01717006824668]
F1--> {'F1': [0.8958743791346825], 'precision': [0.8155355094350885], 'recall': [0.993783550458146]}
##########eval dataset##########
acc--> [98.38166190986944]
F1--> {'F1': [0.8245249800614728], 'precision': [0.7513524700052703], 'recall': [0.9134996050997455]}
step 51/334, epoch 382/501 --> loss:0.817705397605896
step 101/334, epoch 382/501 --> loss:0.8424013721942901
step 151/334, epoch 382/501 --> loss:0.8222001361846923
step 201/334, epoch 382/501 --> loss:0.8308965003490448
step 251/334, epoch 382/501 --> loss:0.8138819503784179
step 301/334, epoch 382/501 --> loss:0.831760185956955
step 51/334, epoch 383/501 --> loss:0.8254726338386535
step 101/334, epoch 383/501 --> loss:0.8297631883621216
step 151/334, epoch 383/501 --> loss:0.8299987030029297
step 201/334, epoch 383/501 --> loss:0.8451008200645447
step 251/334, epoch 383/501 --> loss:0.8211979234218597
step 301/334, epoch 383/501 --> loss:0.8243918132781982
step 51/334, epoch 384/501 --> loss:0.8380658853054047
step 101/334, epoch 384/501 --> loss:0.8252679431438446
step 151/334, epoch 384/501 --> loss:0.821904137134552
step 201/334, epoch 384/501 --> loss:0.8399165010452271
step 251/334, epoch 384/501 --> loss:0.8196895408630371
step 301/334, epoch 384/501 --> loss:0.8252281177043915
step 51/334, epoch 385/501 --> loss:0.8043282330036163
step 101/334, epoch 385/501 --> loss:0.8117810952663421
step 151/334, epoch 385/501 --> loss:0.854037138223648
step 201/334, epoch 385/501 --> loss:0.8231087493896484
step 251/334, epoch 385/501 --> loss:0.8239027416706085
step 301/334, epoch 385/501 --> loss:0.8388940536975861
step 51/334, epoch 386/501 --> loss:0.814493578672409
step 101/334, epoch 386/501 --> loss:0.8031760489940644
step 151/334, epoch 386/501 --> loss:0.8332605290412903
step 201/334, epoch 386/501 --> loss:0.8300255048274994
step 251/334, epoch 386/501 --> loss:0.8337973570823669
step 301/334, epoch 386/501 --> loss:0.839565908908844
step 51/334, epoch 387/501 --> loss:0.8057045388221741
step 101/334, epoch 387/501 --> loss:0.84207976937294
step 151/334, epoch 387/501 --> loss:0.8347698390483856
step 201/334, epoch 387/501 --> loss:0.8048851776123047
step 251/334, epoch 387/501 --> loss:0.841328604221344
step 301/334, epoch 387/501 --> loss:0.8303294348716735
step 51/334, epoch 388/501 --> loss:0.7960038900375366
step 101/334, epoch 388/501 --> loss:0.845809177160263
step 151/334, epoch 388/501 --> loss:0.8233057928085327
step 201/334, epoch 388/501 --> loss:0.8338919794559478
step 251/334, epoch 388/501 --> loss:0.8325292253494263
step 301/334, epoch 388/501 --> loss:0.8294624555110931
step 51/334, epoch 389/501 --> loss:0.8089791917800904
step 101/334, epoch 389/501 --> loss:0.8315053236484528
step 151/334, epoch 389/501 --> loss:0.850233496427536
step 201/334, epoch 389/501 --> loss:0.8458976888656616
step 251/334, epoch 389/501 --> loss:0.8001168835163116
step 301/334, epoch 389/501 --> loss:0.838347510099411
step 51/334, epoch 390/501 --> loss:0.8386745178699493
step 101/334, epoch 390/501 --> loss:0.8416515851020813
step 151/334, epoch 390/501 --> loss:0.8297018659114838
step 201/334, epoch 390/501 --> loss:0.8215638029575348
step 251/334, epoch 390/501 --> loss:0.8110345876216889
step 301/334, epoch 390/501 --> loss:0.8288505303859711
step 51/334, epoch 391/501 --> loss:0.8088959610462189
step 101/334, epoch 391/501 --> loss:0.8431127047538758
step 151/334, epoch 391/501 --> loss:0.8163706767559051
step 201/334, epoch 391/501 --> loss:0.8339242851734161
step 251/334, epoch 391/501 --> loss:0.8336230909824371
step 301/334, epoch 391/501 --> loss:0.8248208820819855

##########train dataset##########
acc--> [99.04217285567903]
F1--> {'F1': [0.8982745609384292], 'precision': [0.8193678082782082], 'recall': [0.9940107388355678]}
##########eval dataset##########
acc--> [98.47965924653657]
F1--> {'F1': [0.8306570229730438], 'precision': [0.7743001384909494], 'recall': [0.8958732475098992]}
step 51/334, epoch 392/501 --> loss:0.8349513781070709
step 101/334, epoch 392/501 --> loss:0.8397368621826172
step 151/334, epoch 392/501 --> loss:0.8284242832660675
step 201/334, epoch 392/501 --> loss:0.8230539762973785
step 251/334, epoch 392/501 --> loss:0.8328531038761139
step 301/334, epoch 392/501 --> loss:0.8292090284824372
step 51/334, epoch 393/501 --> loss:0.8255047452449799
step 101/334, epoch 393/501 --> loss:0.8268512046337128
step 151/334, epoch 393/501 --> loss:0.8144009435176849
step 201/334, epoch 393/501 --> loss:0.8322773575782776
step 251/334, epoch 393/501 --> loss:0.8430601716041565
step 301/334, epoch 393/501 --> loss:0.8445568764209748
step 51/334, epoch 394/501 --> loss:0.8155042326450348
step 101/334, epoch 394/501 --> loss:0.8424770629405975
step 151/334, epoch 394/501 --> loss:0.8230056846141816
step 201/334, epoch 394/501 --> loss:0.8226199209690094
step 251/334, epoch 394/501 --> loss:0.8218549990653992
step 301/334, epoch 394/501 --> loss:0.834196412563324
step 51/334, epoch 395/501 --> loss:0.8343559169769287
step 101/334, epoch 395/501 --> loss:0.8202760875225067
step 151/334, epoch 395/501 --> loss:0.8259156918525696
step 201/334, epoch 395/501 --> loss:0.8253380668163299
step 251/334, epoch 395/501 --> loss:0.8346230018138886
step 301/334, epoch 395/501 --> loss:0.8227930533885955
step 51/334, epoch 396/501 --> loss:0.8451342117786408
step 101/334, epoch 396/501 --> loss:0.8231854510307312
step 151/334, epoch 396/501 --> loss:0.8449278056621552
step 201/334, epoch 396/501 --> loss:0.8173434507846832
step 251/334, epoch 396/501 --> loss:0.8244490230083465
step 301/334, epoch 396/501 --> loss:0.8061993837356567
step 51/334, epoch 397/501 --> loss:0.8424887013435364
step 101/334, epoch 397/501 --> loss:0.8266941559314728
step 151/334, epoch 397/501 --> loss:0.8449880397319793
step 201/334, epoch 397/501 --> loss:0.8163242518901825
step 251/334, epoch 397/501 --> loss:0.8081895971298217
step 301/334, epoch 397/501 --> loss:0.8225323975086212
step 51/334, epoch 398/501 --> loss:0.8430591833591461
step 101/334, epoch 398/501 --> loss:0.8197230052947998
step 151/334, epoch 398/501 --> loss:0.840141693353653
step 201/334, epoch 398/501 --> loss:0.8451472926139831
step 251/334, epoch 398/501 --> loss:0.8242259430885315
step 301/334, epoch 398/501 --> loss:0.8171750736236573
step 51/334, epoch 399/501 --> loss:0.8309794700145722
step 101/334, epoch 399/501 --> loss:0.8265302133560181
step 151/334, epoch 399/501 --> loss:0.8232120645046234
step 201/334, epoch 399/501 --> loss:0.8341523325443267
step 251/334, epoch 399/501 --> loss:0.8397508692741394
step 301/334, epoch 399/501 --> loss:0.8175686931610108
step 51/334, epoch 400/501 --> loss:0.8331503033638
step 101/334, epoch 400/501 --> loss:0.8296034502983093
step 151/334, epoch 400/501 --> loss:0.8354999685287475
step 201/334, epoch 400/501 --> loss:0.8129065012931824
step 251/334, epoch 400/501 --> loss:0.8387331771850586
step 301/334, epoch 400/501 --> loss:0.8179378807544708
step 51/334, epoch 401/501 --> loss:0.8239874196052551
step 101/334, epoch 401/501 --> loss:0.8421118772029876
step 151/334, epoch 401/501 --> loss:0.8078148221969604
step 201/334, epoch 401/501 --> loss:0.8518447971343994
step 251/334, epoch 401/501 --> loss:0.811483291387558
step 301/334, epoch 401/501 --> loss:0.8367949891090393

##########train dataset##########
acc--> [99.19280090446173]
F1--> {'F1': [0.9126882605779341], 'precision': [0.8453824839398087], 'recall': [0.9916500371132676]}
##########eval dataset##########
acc--> [98.60018432296134]
F1--> {'F1': [0.8391904600529982], 'precision': [0.8040540371752746], 'recall': [0.8775489814804455]}
save model!
step 51/334, epoch 402/501 --> loss:0.8152631366252899
step 101/334, epoch 402/501 --> loss:0.8303861200809479
step 151/334, epoch 402/501 --> loss:0.8320647394657135
step 201/334, epoch 402/501 --> loss:0.8190815067291259
step 251/334, epoch 402/501 --> loss:0.8307640874385833
step 301/334, epoch 402/501 --> loss:0.8259873533248902
step 51/334, epoch 403/501 --> loss:0.8306050992012024
step 101/334, epoch 403/501 --> loss:0.8388263750076294
step 151/334, epoch 403/501 --> loss:0.8585988843441009
step 201/334, epoch 403/501 --> loss:0.8297908902168274
step 251/334, epoch 403/501 --> loss:0.8322333860397338
step 301/334, epoch 403/501 --> loss:0.8045628726482391
step 51/334, epoch 404/501 --> loss:0.8279999709129333
step 101/334, epoch 404/501 --> loss:0.807766627073288
step 151/334, epoch 404/501 --> loss:0.8218824243545533
step 201/334, epoch 404/501 --> loss:0.8525796210765839
step 251/334, epoch 404/501 --> loss:0.8128320217132569
step 301/334, epoch 404/501 --> loss:0.8546872103214264
step 51/334, epoch 405/501 --> loss:0.8308561766147613
step 101/334, epoch 405/501 --> loss:0.8581304240226746
step 151/334, epoch 405/501 --> loss:0.8025775957107544
step 201/334, epoch 405/501 --> loss:0.8361668419837952
step 251/334, epoch 405/501 --> loss:0.8145090198516846
step 301/334, epoch 405/501 --> loss:0.8398436498641968
step 51/334, epoch 406/501 --> loss:0.8088380122184753
step 101/334, epoch 406/501 --> loss:0.8515373718738556
step 151/334, epoch 406/501 --> loss:0.8282237637043
step 201/334, epoch 406/501 --> loss:0.81735875248909
step 251/334, epoch 406/501 --> loss:0.8371593487262726
step 301/334, epoch 406/501 --> loss:0.8399496984481811
step 51/334, epoch 407/501 --> loss:0.8190406692028046
step 101/334, epoch 407/501 --> loss:0.8368638217449188
step 151/334, epoch 407/501 --> loss:0.8146780669689179
step 201/334, epoch 407/501 --> loss:0.8466041684150696
step 251/334, epoch 407/501 --> loss:0.8445992696285248
step 301/334, epoch 407/501 --> loss:0.8170181620121002
step 51/334, epoch 408/501 --> loss:0.8236176490783691
step 101/334, epoch 408/501 --> loss:0.8267509758472442
step 151/334, epoch 408/501 --> loss:0.8354080295562745
step 201/334, epoch 408/501 --> loss:0.8208619439601899
step 251/334, epoch 408/501 --> loss:0.8404309558868408
step 301/334, epoch 408/501 --> loss:0.81434357047081
step 51/334, epoch 409/501 --> loss:0.8380056667327881
step 101/334, epoch 409/501 --> loss:0.8507478272914887
step 151/334, epoch 409/501 --> loss:0.8274043273925781
step 201/334, epoch 409/501 --> loss:0.8145051217079162
step 251/334, epoch 409/501 --> loss:0.8279827094078064
step 301/334, epoch 409/501 --> loss:0.8428657209873199
step 51/334, epoch 410/501 --> loss:0.85263631939888
step 101/334, epoch 410/501 --> loss:0.8200482404232026
step 151/334, epoch 410/501 --> loss:0.812574154138565
step 201/334, epoch 410/501 --> loss:0.8275557839870453
step 251/334, epoch 410/501 --> loss:0.8563342761993408
step 301/334, epoch 410/501 --> loss:0.8137832427024841
step 51/334, epoch 411/501 --> loss:0.8426364648342133
step 101/334, epoch 411/501 --> loss:0.8379910552501678
step 151/334, epoch 411/501 --> loss:0.8332579553127288
step 201/334, epoch 411/501 --> loss:0.8133529579639435
step 251/334, epoch 411/501 --> loss:0.8267754292488099
step 301/334, epoch 411/501 --> loss:0.8233913242816925

##########train dataset##########
acc--> [99.21925633776695]
F1--> {'F1': [0.9153362303238828], 'precision': [0.849666080620439], 'recall': [0.9920195628079435]}
##########eval dataset##########
acc--> [98.61825991385427]
F1--> {'F1': [0.8419702262948351], 'precision': [0.8034554604775175], 'recall': [0.8843744362154113]}
save model!
step 51/334, epoch 412/501 --> loss:0.8170309925079345
step 101/334, epoch 412/501 --> loss:0.8311800479888916
step 151/334, epoch 412/501 --> loss:0.820392690896988
step 201/334, epoch 412/501 --> loss:0.8442200088500976
step 251/334, epoch 412/501 --> loss:0.8318879592418671
step 301/334, epoch 412/501 --> loss:0.8228550982475281
step 51/334, epoch 413/501 --> loss:0.8330745315551757
step 101/334, epoch 413/501 --> loss:0.8426612210273743
step 151/334, epoch 413/501 --> loss:0.8312512826919556
step 201/334, epoch 413/501 --> loss:0.8075538611412049
step 251/334, epoch 413/501 --> loss:0.8304627287387848
step 301/334, epoch 413/501 --> loss:0.8121105802059173
step 51/334, epoch 414/501 --> loss:0.8228746938705445
step 101/334, epoch 414/501 --> loss:0.8265274083614349
step 151/334, epoch 414/501 --> loss:0.8346968114376068
step 201/334, epoch 414/501 --> loss:0.8242112100124359
step 251/334, epoch 414/501 --> loss:0.8371294677257538
step 301/334, epoch 414/501 --> loss:0.8368658173084259
step 51/334, epoch 415/501 --> loss:0.8355488061904908
step 101/334, epoch 415/501 --> loss:0.8066212129592896
step 151/334, epoch 415/501 --> loss:0.8339272284507752
step 201/334, epoch 415/501 --> loss:0.8413889598846436
step 251/334, epoch 415/501 --> loss:0.8144216632843018
step 301/334, epoch 415/501 --> loss:0.8430485129356384
step 51/334, epoch 416/501 --> loss:0.8174831950664521
step 101/334, epoch 416/501 --> loss:0.8430628859996796
step 151/334, epoch 416/501 --> loss:0.8204809427261353
step 201/334, epoch 416/501 --> loss:0.8426274573802948
step 251/334, epoch 416/501 --> loss:0.8403356111049652
step 301/334, epoch 416/501 --> loss:0.8039989721775055
step 51/334, epoch 417/501 --> loss:0.811181206703186
step 101/334, epoch 417/501 --> loss:0.816084874868393
step 151/334, epoch 417/501 --> loss:0.8107332623004914
step 201/334, epoch 417/501 --> loss:0.8432945704460144
step 251/334, epoch 417/501 --> loss:0.8298566854000091
step 301/334, epoch 417/501 --> loss:0.8500617218017578
step 51/334, epoch 418/501 --> loss:0.8342874443531036
step 101/334, epoch 418/501 --> loss:0.8303492963314056
step 151/334, epoch 418/501 --> loss:0.7871260964870452
step 201/334, epoch 418/501 --> loss:0.8319472074508667
step 251/334, epoch 418/501 --> loss:0.8418155932426452
step 301/334, epoch 418/501 --> loss:0.8290073132514953
step 51/334, epoch 419/501 --> loss:0.808559056520462
step 101/334, epoch 419/501 --> loss:0.7940052354335785
step 151/334, epoch 419/501 --> loss:0.8279526245594024
step 201/334, epoch 419/501 --> loss:0.8301838171482087
step 251/334, epoch 419/501 --> loss:0.8392064917087555
step 301/334, epoch 419/501 --> loss:0.8540252649784088
step 51/334, epoch 420/501 --> loss:0.8196507275104523
step 101/334, epoch 420/501 --> loss:0.8308270800113678
step 151/334, epoch 420/501 --> loss:0.8423602652549743
step 201/334, epoch 420/501 --> loss:0.8289883530139923
step 251/334, epoch 420/501 --> loss:0.8353796672821044
step 301/334, epoch 420/501 --> loss:0.8280508434772491
step 51/334, epoch 421/501 --> loss:0.8367494177818299
step 101/334, epoch 421/501 --> loss:0.824051628112793
step 151/334, epoch 421/501 --> loss:0.8221073234081269
step 201/334, epoch 421/501 --> loss:0.8390692138671875
step 251/334, epoch 421/501 --> loss:0.8102916836738586
step 301/334, epoch 421/501 --> loss:0.8348250257968902

##########train dataset##########
acc--> [99.14019870364432]
F1--> {'F1': [0.907686032937614], 'precision': [0.8354857245170891], 'recall': [0.9935573033761621]}
##########eval dataset##########
acc--> [98.4599523135409]
F1--> {'F1': [0.8301175165481689], 'precision': [0.7673960860514574], 'recall': [0.9040161006436185]}
step 51/334, epoch 422/501 --> loss:0.8610976541042328
step 101/334, epoch 422/501 --> loss:0.8068039226531982
step 151/334, epoch 422/501 --> loss:0.826712143421173
step 201/334, epoch 422/501 --> loss:0.8180113029479981
step 251/334, epoch 422/501 --> loss:0.8209718775749206
step 301/334, epoch 422/501 --> loss:0.8353360283374787
step 51/334, epoch 423/501 --> loss:0.8271892559528351
step 101/334, epoch 423/501 --> loss:0.7886300921440125
step 151/334, epoch 423/501 --> loss:0.8332688784599305
step 201/334, epoch 423/501 --> loss:0.8582038354873657
step 251/334, epoch 423/501 --> loss:0.804731616973877
step 301/334, epoch 423/501 --> loss:0.8404065120220184
step 51/334, epoch 424/501 --> loss:0.8237600302696229
step 101/334, epoch 424/501 --> loss:0.8151620495319366
step 151/334, epoch 424/501 --> loss:0.8354886150360108
step 201/334, epoch 424/501 --> loss:0.8263348388671875
step 251/334, epoch 424/501 --> loss:0.8278470194339752
step 301/334, epoch 424/501 --> loss:0.820461882352829
step 51/334, epoch 425/501 --> loss:0.8241671812534332
step 101/334, epoch 425/501 --> loss:0.8276382768154145
step 151/334, epoch 425/501 --> loss:0.8125163400173188
step 201/334, epoch 425/501 --> loss:0.8456005299091339
step 251/334, epoch 425/501 --> loss:0.7933251130580902
step 301/334, epoch 425/501 --> loss:0.8419288063049316
step 51/334, epoch 426/501 --> loss:0.8321825456619263
step 101/334, epoch 426/501 --> loss:0.8268801748752594
step 151/334, epoch 426/501 --> loss:0.803678412437439
step 201/334, epoch 426/501 --> loss:0.8477071952819825
step 251/334, epoch 426/501 --> loss:0.819164320230484
step 301/334, epoch 426/501 --> loss:0.829186214208603
step 51/334, epoch 427/501 --> loss:0.8302977097034454
step 101/334, epoch 427/501 --> loss:0.832602721452713
step 151/334, epoch 427/501 --> loss:0.8305626285076141
step 201/334, epoch 427/501 --> loss:0.826111809015274
step 251/334, epoch 427/501 --> loss:0.8308769476413727
step 301/334, epoch 427/501 --> loss:0.8225610721111297
step 51/334, epoch 428/501 --> loss:0.8230542755126953
step 101/334, epoch 428/501 --> loss:0.8509101843833924
step 151/334, epoch 428/501 --> loss:0.8196850669384003
step 201/334, epoch 428/501 --> loss:0.8332933449745178
step 251/334, epoch 428/501 --> loss:0.8222757816314697
step 301/334, epoch 428/501 --> loss:0.8077101647853852
step 51/334, epoch 429/501 --> loss:0.8452243947982788
step 101/334, epoch 429/501 --> loss:0.8177065229415894
step 151/334, epoch 429/501 --> loss:0.8161272418498993
step 201/334, epoch 429/501 --> loss:0.8285265874862671
step 251/334, epoch 429/501 --> loss:0.8465368914604187
step 301/334, epoch 429/501 --> loss:0.8159136021137238
step 51/334, epoch 430/501 --> loss:0.8364780521392823
step 101/334, epoch 430/501 --> loss:0.8309451997280121
step 151/334, epoch 430/501 --> loss:0.8345265173912049
step 201/334, epoch 430/501 --> loss:0.8153302443027496
step 251/334, epoch 430/501 --> loss:0.8256196546554565
step 301/334, epoch 430/501 --> loss:0.8129975807666778
step 51/334, epoch 431/501 --> loss:0.8268558120727539
step 101/334, epoch 431/501 --> loss:0.8367983591556549
step 151/334, epoch 431/501 --> loss:0.8373881351947784
step 201/334, epoch 431/501 --> loss:0.8050794053077698
step 251/334, epoch 431/501 --> loss:0.8308008921146393
step 301/334, epoch 431/501 --> loss:0.8195889830589295

##########train dataset##########
acc--> [99.14014950020001]
F1--> {'F1': [0.9077460331905212], 'precision': [0.8350443800049662], 'recall': [0.9943261400425772]}
##########eval dataset##########
acc--> [98.51824542668203]
F1--> {'F1': [0.8346548049558523], 'precision': [0.7792509509129759], 'recall': [0.8985515032198909]}
step 51/334, epoch 432/501 --> loss:0.8317304611206054
step 101/334, epoch 432/501 --> loss:0.8309697437286377
step 151/334, epoch 432/501 --> loss:0.837257182598114
step 201/334, epoch 432/501 --> loss:0.8374001801013946
step 251/334, epoch 432/501 --> loss:0.8057342863082886
step 301/334, epoch 432/501 --> loss:0.8195074701309204
step 51/334, epoch 433/501 --> loss:0.8266795492172241
step 101/334, epoch 433/501 --> loss:0.8098413968086242
step 151/334, epoch 433/501 --> loss:0.8245298433303833
step 201/334, epoch 433/501 --> loss:0.8169133007526398
step 251/334, epoch 433/501 --> loss:0.841330201625824
step 301/334, epoch 433/501 --> loss:0.8465869688987732
step 51/334, epoch 434/501 --> loss:0.8236922907829285
step 101/334, epoch 434/501 --> loss:0.8426765620708465
step 151/334, epoch 434/501 --> loss:0.7953155314922333
step 201/334, epoch 434/501 --> loss:0.8240613102912903
step 251/334, epoch 434/501 --> loss:0.84274729013443
step 301/334, epoch 434/501 --> loss:0.8416636371612549
step 51/334, epoch 435/501 --> loss:0.8118917667865753
step 101/334, epoch 435/501 --> loss:0.8303744649887085
step 151/334, epoch 435/501 --> loss:0.8360342824459076
step 201/334, epoch 435/501 --> loss:0.8197662222385407
step 251/334, epoch 435/501 --> loss:0.8409932827949524
step 301/334, epoch 435/501 --> loss:0.8164462435245514
step 51/334, epoch 436/501 --> loss:0.8249039709568023
step 101/334, epoch 436/501 --> loss:0.8398689532279968
step 151/334, epoch 436/501 --> loss:0.8367479956150055
step 201/334, epoch 436/501 --> loss:0.8086148774623871
step 251/334, epoch 436/501 --> loss:0.8243840444087982
step 301/334, epoch 436/501 --> loss:0.8476522171497345
step 51/334, epoch 437/501 --> loss:0.8193423807621002
step 101/334, epoch 437/501 --> loss:0.8429627192020416
step 151/334, epoch 437/501 --> loss:0.8341641068458557
step 201/334, epoch 437/501 --> loss:0.8135947072505951
step 251/334, epoch 437/501 --> loss:0.8461434185504914
step 301/334, epoch 437/501 --> loss:0.8296697080135346
step 51/334, epoch 438/501 --> loss:0.8211465549468994
step 101/334, epoch 438/501 --> loss:0.838365935087204
step 151/334, epoch 438/501 --> loss:0.7972062528133392
step 201/334, epoch 438/501 --> loss:0.8370821595191955
step 251/334, epoch 438/501 --> loss:0.8404112756252289
step 301/334, epoch 438/501 --> loss:0.8455504143238067
step 51/334, epoch 439/501 --> loss:0.8254582905769348
step 101/334, epoch 439/501 --> loss:0.8395525658130646
step 151/334, epoch 439/501 --> loss:0.8292775642871857
step 201/334, epoch 439/501 --> loss:0.8303951060771942
step 251/334, epoch 439/501 --> loss:0.834988523721695
step 301/334, epoch 439/501 --> loss:0.8194059586524963
step 51/334, epoch 440/501 --> loss:0.8419999551773071
step 101/334, epoch 440/501 --> loss:0.8280287480354309
step 151/334, epoch 440/501 --> loss:0.8069875752925872
step 201/334, epoch 440/501 --> loss:0.8479495930671692
step 251/334, epoch 440/501 --> loss:0.8180023455619811
step 301/334, epoch 440/501 --> loss:0.8302850568294525
step 51/334, epoch 441/501 --> loss:0.8152117884159088
step 101/334, epoch 441/501 --> loss:0.8192889106273651
step 151/334, epoch 441/501 --> loss:0.8474570524692535
step 201/334, epoch 441/501 --> loss:0.8385272490978241
step 251/334, epoch 441/501 --> loss:0.8190683102607728
step 301/334, epoch 441/501 --> loss:0.8167220401763916

##########train dataset##########
acc--> [99.17153185746517]
F1--> {'F1': [0.910756351094298], 'precision': [0.8406454949546799], 'recall': [0.9936377841360833]}
##########eval dataset##########
acc--> [98.5073206410046]
F1--> {'F1': [0.8344465478122663], 'precision': [0.7749779475198783], 'recall': [0.9038121163754245]}
step 51/334, epoch 442/501 --> loss:0.8326395094394684
step 101/334, epoch 442/501 --> loss:0.8472851502895355
step 151/334, epoch 442/501 --> loss:0.8183232247829437
step 201/334, epoch 442/501 --> loss:0.8255139315128326
step 251/334, epoch 442/501 --> loss:0.832716007232666
step 301/334, epoch 442/501 --> loss:0.8230214810371399
step 51/334, epoch 443/501 --> loss:0.8158357334136963
step 101/334, epoch 443/501 --> loss:0.8178994822502136
step 151/334, epoch 443/501 --> loss:0.8448668873310089
step 201/334, epoch 443/501 --> loss:0.8421568834781646
step 251/334, epoch 443/501 --> loss:0.8277251946926117
step 301/334, epoch 443/501 --> loss:0.809493635892868
step 51/334, epoch 444/501 --> loss:0.8569389486312866
step 101/334, epoch 444/501 --> loss:0.8232925093173981
step 151/334, epoch 444/501 --> loss:0.820146015882492
step 201/334, epoch 444/501 --> loss:0.8231962025165558
step 251/334, epoch 444/501 --> loss:0.825304992198944
step 301/334, epoch 444/501 --> loss:0.8182576537132263
step 51/334, epoch 445/501 --> loss:0.8526358270645141
step 101/334, epoch 445/501 --> loss:0.8147140336036682
step 151/334, epoch 445/501 --> loss:0.8089976251125336
step 201/334, epoch 445/501 --> loss:0.8422736096382141
step 251/334, epoch 445/501 --> loss:0.8233981144428253
step 301/334, epoch 445/501 --> loss:0.8408979141712188
step 51/334, epoch 446/501 --> loss:0.8195407712459564
step 101/334, epoch 446/501 --> loss:0.828887779712677
step 151/334, epoch 446/501 --> loss:0.8230443942546845
step 201/334, epoch 446/501 --> loss:0.8352850604057313
step 251/334, epoch 446/501 --> loss:0.8395100820064545
step 301/334, epoch 446/501 --> loss:0.8308726561069488
step 51/334, epoch 447/501 --> loss:0.8181021916866302
step 101/334, epoch 447/501 --> loss:0.8345811188220977
step 151/334, epoch 447/501 --> loss:0.832839537858963
step 201/334, epoch 447/501 --> loss:0.8197288167476654
step 251/334, epoch 447/501 --> loss:0.8286928129196167
step 301/334, epoch 447/501 --> loss:0.824291056394577
step 51/334, epoch 448/501 --> loss:0.8191824030876159
step 101/334, epoch 448/501 --> loss:0.8344422852993012
step 151/334, epoch 448/501 --> loss:0.8449398243427276
step 201/334, epoch 448/501 --> loss:0.8214080703258514
step 251/334, epoch 448/501 --> loss:0.8108916676044464
step 301/334, epoch 448/501 --> loss:0.8408628690242768
step 51/334, epoch 449/501 --> loss:0.8204389834403991
step 101/334, epoch 449/501 --> loss:0.8262924766540527
step 151/334, epoch 449/501 --> loss:0.8216667342185974
step 201/334, epoch 449/501 --> loss:0.8246628046035767
step 251/334, epoch 449/501 --> loss:0.8181375694274903
step 301/334, epoch 449/501 --> loss:0.8377383601665497
step 51/334, epoch 450/501 --> loss:0.8202940571308136
step 101/334, epoch 450/501 --> loss:0.8261230599880218
step 151/334, epoch 450/501 --> loss:0.8154649412631989
step 201/334, epoch 450/501 --> loss:0.8323174250125885
step 251/334, epoch 450/501 --> loss:0.8400287628173828
step 301/334, epoch 450/501 --> loss:0.8344638657569885
step 51/334, epoch 451/501 --> loss:0.811960027217865
step 101/334, epoch 451/501 --> loss:0.8067007315158844
step 151/334, epoch 451/501 --> loss:0.8381654262542725
step 201/334, epoch 451/501 --> loss:0.8287818491458893
step 251/334, epoch 451/501 --> loss:0.8388611781597137
step 301/334, epoch 451/501 --> loss:0.8444790732860565

##########train dataset##########
acc--> [99.00464149820955]
F1--> {'F1': [0.8944544135871075], 'precision': [0.814830681783631], 'recall': [0.9913369891448524]}
##########eval dataset##########
acc--> [98.318102979278]
F1--> {'F1': [0.8159248390353973], 'precision': [0.7492901070075478], 'recall': [0.8955801003068655]}
step 51/334, epoch 452/501 --> loss:0.8267832589149475
step 101/334, epoch 452/501 --> loss:0.829602781534195
step 151/334, epoch 452/501 --> loss:0.8183842766284942
step 201/334, epoch 452/501 --> loss:0.8416558587551117
step 251/334, epoch 452/501 --> loss:0.8385682260990143
step 301/334, epoch 452/501 --> loss:0.8246985328197479
step 51/334, epoch 453/501 --> loss:0.8431723177433014
step 101/334, epoch 453/501 --> loss:0.829356015920639
step 151/334, epoch 453/501 --> loss:0.8240048730373383
step 201/334, epoch 453/501 --> loss:0.8236499655246735
step 251/334, epoch 453/501 --> loss:0.8516150999069214
step 301/334, epoch 453/501 --> loss:0.8075408506393432
step 51/334, epoch 454/501 --> loss:0.8471831977367401
step 101/334, epoch 454/501 --> loss:0.803296355009079
step 151/334, epoch 454/501 --> loss:0.8231947290897369
step 201/334, epoch 454/501 --> loss:0.8289543473720551
step 251/334, epoch 454/501 --> loss:0.8204607284069061
step 301/334, epoch 454/501 --> loss:0.8402195811271668
step 51/334, epoch 455/501 --> loss:0.8496892154216766
step 101/334, epoch 455/501 --> loss:0.8315389931201935
step 151/334, epoch 455/501 --> loss:0.8123383235931396
step 201/334, epoch 455/501 --> loss:0.8142289185523987
step 251/334, epoch 455/501 --> loss:0.8152937078475953
step 301/334, epoch 455/501 --> loss:0.8166740763187409
step 51/334, epoch 456/501 --> loss:0.8158422374725341
step 101/334, epoch 456/501 --> loss:0.8273632383346557
step 151/334, epoch 456/501 --> loss:0.8272312688827514
step 201/334, epoch 456/501 --> loss:0.8436192071437836
step 251/334, epoch 456/501 --> loss:0.8272178912162781
step 301/334, epoch 456/501 --> loss:0.8260564315319061
step 51/334, epoch 457/501 --> loss:0.8123883378505706
step 101/334, epoch 457/501 --> loss:0.8275337266921997
step 151/334, epoch 457/501 --> loss:0.8345422112941742
step 201/334, epoch 457/501 --> loss:0.8242429018020629
step 251/334, epoch 457/501 --> loss:0.8305942320823669
step 301/334, epoch 457/501 --> loss:0.8329043364524842
step 51/334, epoch 458/501 --> loss:0.8196721637248993
step 101/334, epoch 458/501 --> loss:0.8436168801784515
step 151/334, epoch 458/501 --> loss:0.8322877323627472
step 201/334, epoch 458/501 --> loss:0.823709466457367
step 251/334, epoch 458/501 --> loss:0.8249292802810669
step 301/334, epoch 458/501 --> loss:0.8220068538188934
step 51/334, epoch 459/501 --> loss:0.8440460479259491
step 101/334, epoch 459/501 --> loss:0.836737356185913
step 151/334, epoch 459/501 --> loss:0.8298921084403992
step 201/334, epoch 459/501 --> loss:0.8363498663902282
step 251/334, epoch 459/501 --> loss:0.8311889934539795
step 301/334, epoch 459/501 --> loss:0.8135633373260498
step 51/334, epoch 460/501 --> loss:0.8159409415721893
step 101/334, epoch 460/501 --> loss:0.8111804115772248
step 151/334, epoch 460/501 --> loss:0.8207591021060944
step 201/334, epoch 460/501 --> loss:0.8385479426383973
step 251/334, epoch 460/501 --> loss:0.8287059080600738
step 301/334, epoch 460/501 --> loss:0.8270272958278656
step 51/334, epoch 461/501 --> loss:0.8310525715351105
step 101/334, epoch 461/501 --> loss:0.8146363925933838
step 151/334, epoch 461/501 --> loss:0.7950404059886932
step 201/334, epoch 461/501 --> loss:0.8373425102233887
step 251/334, epoch 461/501 --> loss:0.837019727230072
step 301/334, epoch 461/501 --> loss:0.8487127494812011

##########train dataset##########
acc--> [99.20655526960823]
F1--> {'F1': [0.9141979923478446], 'precision': [0.8465953742030206], 'recall': [0.9935457388893564]}
##########eval dataset##########
acc--> [98.57555933406834]
F1--> {'F1': [0.8397512067317464], 'precision': [0.7896050324074345], 'recall': [0.8967100320314689]}
step 51/334, epoch 462/501 --> loss:0.839989709854126
step 101/334, epoch 462/501 --> loss:0.8096885716915131
step 151/334, epoch 462/501 --> loss:0.8004745781421662
step 201/334, epoch 462/501 --> loss:0.8494552397727966
step 251/334, epoch 462/501 --> loss:0.8164581334590912
step 301/334, epoch 462/501 --> loss:0.836615549325943
step 51/334, epoch 463/501 --> loss:0.8144047641754151
step 101/334, epoch 463/501 --> loss:0.8334549832344055
step 151/334, epoch 463/501 --> loss:0.8340946674346924
step 201/334, epoch 463/501 --> loss:0.8217700719833374
step 251/334, epoch 463/501 --> loss:0.8334540605545044
step 301/334, epoch 463/501 --> loss:0.8347227942943573
step 51/334, epoch 464/501 --> loss:0.8306510269641876
step 101/334, epoch 464/501 --> loss:0.8415990400314332
step 151/334, epoch 464/501 --> loss:0.8393932211399079
step 201/334, epoch 464/501 --> loss:0.8240636277198792
step 251/334, epoch 464/501 --> loss:0.8262056887149811
step 301/334, epoch 464/501 --> loss:0.8192043077945709
step 51/334, epoch 465/501 --> loss:0.8230558609962464
step 101/334, epoch 465/501 --> loss:0.8637434351444244
step 151/334, epoch 465/501 --> loss:0.8319743621349335
step 201/334, epoch 465/501 --> loss:0.8310343456268311
step 251/334, epoch 465/501 --> loss:0.8332528138160705
step 301/334, epoch 465/501 --> loss:0.8114142620563507
step 51/334, epoch 466/501 --> loss:0.8286478555202484
step 101/334, epoch 466/501 --> loss:0.8415054905414582
step 151/334, epoch 466/501 --> loss:0.8340574705600738
step 201/334, epoch 466/501 --> loss:0.8256858134269714
step 251/334, epoch 466/501 --> loss:0.8090011751651764
step 301/334, epoch 466/501 --> loss:0.8452135407924652
step 51/334, epoch 467/501 --> loss:0.8233600842952729
step 101/334, epoch 467/501 --> loss:0.8081550848484039
step 151/334, epoch 467/501 --> loss:0.8371327376365661
step 201/334, epoch 467/501 --> loss:0.8160956954956055
step 251/334, epoch 467/501 --> loss:0.8411773431301117
step 301/334, epoch 467/501 --> loss:0.8247136998176575
step 51/334, epoch 468/501 --> loss:0.8039017522335052
step 101/334, epoch 468/501 --> loss:0.8138202571868897
step 151/334, epoch 468/501 --> loss:0.8300090575218201
step 201/334, epoch 468/501 --> loss:0.8283166062831878
step 251/334, epoch 468/501 --> loss:0.8362705016136169
step 301/334, epoch 468/501 --> loss:0.8414236891269684
step 51/334, epoch 469/501 --> loss:0.8342472910881042
step 101/334, epoch 469/501 --> loss:0.8124603617191315
step 151/334, epoch 469/501 --> loss:0.8196587836742402
step 201/334, epoch 469/501 --> loss:0.8327835595607758
step 251/334, epoch 469/501 --> loss:0.8310515189170837
step 301/334, epoch 469/501 --> loss:0.8325369334220887
step 51/334, epoch 470/501 --> loss:0.8130634355545044
step 101/334, epoch 470/501 --> loss:0.8385875236988067
step 151/334, epoch 470/501 --> loss:0.814292904138565
step 201/334, epoch 470/501 --> loss:0.8317478775978089
step 251/334, epoch 470/501 --> loss:0.8459470248222352
step 301/334, epoch 470/501 --> loss:0.8189346933364868
step 51/334, epoch 471/501 --> loss:0.8205276644229889
step 101/334, epoch 471/501 --> loss:0.8538266456127167
step 151/334, epoch 471/501 --> loss:0.8244285666942597
step 201/334, epoch 471/501 --> loss:0.8327011847496033
step 251/334, epoch 471/501 --> loss:0.8162069416046143
step 301/334, epoch 471/501 --> loss:0.8246961891651153

##########train dataset##########
acc--> [99.22764209222098]
F1--> {'F1': [0.916278460542785], 'precision': [0.8502543381575082], 'recall': [0.9934313714936789]}
##########eval dataset##########
acc--> [98.61728538051958]
F1--> {'F1': [0.8398964828644482], 'precision': [0.8106150081030746], 'recall': [0.8713834286822867]}
step 51/334, epoch 472/501 --> loss:0.8140339171886444
step 101/334, epoch 472/501 --> loss:0.821112459897995
step 151/334, epoch 472/501 --> loss:0.8399913799762726
step 201/334, epoch 472/501 --> loss:0.8251543438434601
step 251/334, epoch 472/501 --> loss:0.8468802690505981
step 301/334, epoch 472/501 --> loss:0.8272563171386719
step 51/334, epoch 473/501 --> loss:0.8267234146595002
step 101/334, epoch 473/501 --> loss:0.8224187338352204
step 151/334, epoch 473/501 --> loss:0.8142304337024688
step 201/334, epoch 473/501 --> loss:0.8324962294101715
step 251/334, epoch 473/501 --> loss:0.8330604910850525
step 301/334, epoch 473/501 --> loss:0.8472246205806733
step 51/334, epoch 474/501 --> loss:0.8318552529811859
step 101/334, epoch 474/501 --> loss:0.8214208376407623
step 151/334, epoch 474/501 --> loss:0.7966223299503327
step 201/334, epoch 474/501 --> loss:0.8602021431922913
step 251/334, epoch 474/501 --> loss:0.8390997898578644
step 301/334, epoch 474/501 --> loss:0.8207171607017517
step 51/334, epoch 475/501 --> loss:0.8246743643283844
step 101/334, epoch 475/501 --> loss:0.8489022970199585
step 151/334, epoch 475/501 --> loss:0.821302455663681
step 201/334, epoch 475/501 --> loss:0.8309403395652771
step 251/334, epoch 475/501 --> loss:0.8196315884590148
step 301/334, epoch 475/501 --> loss:0.8321410036087036
step 51/334, epoch 476/501 --> loss:0.8226366901397705
step 101/334, epoch 476/501 --> loss:0.80861971616745
step 151/334, epoch 476/501 --> loss:0.8415100634098053
step 201/334, epoch 476/501 --> loss:0.8439757549762725
step 251/334, epoch 476/501 --> loss:0.8376816844940186
step 301/334, epoch 476/501 --> loss:0.8122486519813538
step 51/334, epoch 477/501 --> loss:0.8090879380702972
step 101/334, epoch 477/501 --> loss:0.8387787866592408
step 151/334, epoch 477/501 --> loss:0.8392020285129547
step 201/334, epoch 477/501 --> loss:0.8221644032001495
step 251/334, epoch 477/501 --> loss:0.8385903429985047
step 301/334, epoch 477/501 --> loss:0.841268379688263
step 51/334, epoch 478/501 --> loss:0.8364791131019592
step 101/334, epoch 478/501 --> loss:0.810487732887268
step 151/334, epoch 478/501 --> loss:0.8296955394744873
step 201/334, epoch 478/501 --> loss:0.8136984384059907
step 251/334, epoch 478/501 --> loss:0.8438626265525818
step 301/334, epoch 478/501 --> loss:0.8329464066028595
step 51/334, epoch 479/501 --> loss:0.8291358876228333
step 101/334, epoch 479/501 --> loss:0.8228396892547607
step 151/334, epoch 479/501 --> loss:0.8285766732692719
step 201/334, epoch 479/501 --> loss:0.8392299818992615
step 251/334, epoch 479/501 --> loss:0.8295740962028504
step 301/334, epoch 479/501 --> loss:0.823969213962555
step 51/334, epoch 480/501 --> loss:0.8218397748470306
step 101/334, epoch 480/501 --> loss:0.8183122956752777
step 151/334, epoch 480/501 --> loss:0.8231829142570496
step 201/334, epoch 480/501 --> loss:0.8453730189800263
step 251/334, epoch 480/501 --> loss:0.8100834333896637
step 301/334, epoch 480/501 --> loss:0.8334510910511017
step 51/334, epoch 481/501 --> loss:0.8328182363510132
step 101/334, epoch 481/501 --> loss:0.8154093396663665
step 151/334, epoch 481/501 --> loss:0.8210727643966674
step 201/334, epoch 481/501 --> loss:0.8383876752853393
step 251/334, epoch 481/501 --> loss:0.840064080953598
step 301/334, epoch 481/501 --> loss:0.8165551006793976

##########train dataset##########
acc--> [99.22589136501696]
F1--> {'F1': [0.9161048667359001], 'precision': [0.8499530714333255], 'recall': [0.9934345987923223]}
##########eval dataset##########
acc--> [98.57101573926745]
F1--> {'F1': [0.840006594593713], 'precision': [0.7865426880185326], 'recall': [0.9012802739019589]}
step 51/334, epoch 482/501 --> loss:0.8151373374462128
step 101/334, epoch 482/501 --> loss:0.83416060090065
step 151/334, epoch 482/501 --> loss:0.818383458852768
step 201/334, epoch 482/501 --> loss:0.8275720000267028
step 251/334, epoch 482/501 --> loss:0.8445526444911957
step 301/334, epoch 482/501 --> loss:0.8333963549137116
step 51/334, epoch 483/501 --> loss:0.8125964426994323
step 101/334, epoch 483/501 --> loss:0.8222746741771698
step 151/334, epoch 483/501 --> loss:0.8317807829380035
step 201/334, epoch 483/501 --> loss:0.8459505307674408
step 251/334, epoch 483/501 --> loss:0.830172131061554
step 301/334, epoch 483/501 --> loss:0.820366826057434
step 51/334, epoch 484/501 --> loss:0.8161701440811158
step 101/334, epoch 484/501 --> loss:0.8359846985340118
step 151/334, epoch 484/501 --> loss:0.8281932687759399
step 201/334, epoch 484/501 --> loss:0.8521282875537872
step 251/334, epoch 484/501 --> loss:0.8176235699653626
step 301/334, epoch 484/501 --> loss:0.8404281294345856
step 51/334, epoch 485/501 --> loss:0.8235373854637146
step 101/334, epoch 485/501 --> loss:0.8231619703769684
step 151/334, epoch 485/501 --> loss:0.825733493566513
step 201/334, epoch 485/501 --> loss:0.8349188423156738
step 251/334, epoch 485/501 --> loss:0.8161254858970642
step 301/334, epoch 485/501 --> loss:0.8302692329883575
step 51/334, epoch 486/501 --> loss:0.8150948798656463
step 101/334, epoch 486/501 --> loss:0.8062994503974914
step 151/334, epoch 486/501 --> loss:0.8264576828479767
step 201/334, epoch 486/501 --> loss:0.8349094724655152
step 251/334, epoch 486/501 --> loss:0.8241091573238373
step 301/334, epoch 486/501 --> loss:0.845942690372467
step 51/334, epoch 487/501 --> loss:0.7888974845409393
step 101/334, epoch 487/501 --> loss:0.8228531312942505
step 151/334, epoch 487/501 --> loss:0.8270045971870422
step 201/334, epoch 487/501 --> loss:0.8506755316257477
step 251/334, epoch 487/501 --> loss:0.8466978061199188
step 301/334, epoch 487/501 --> loss:0.8351058042049408
step 51/334, epoch 488/501 --> loss:0.8406512856483459
step 101/334, epoch 488/501 --> loss:0.8304782164096832
step 151/334, epoch 488/501 --> loss:0.816664776802063
step 201/334, epoch 488/501 --> loss:0.8473156189918518
step 251/334, epoch 488/501 --> loss:0.8258794391155243
step 301/334, epoch 488/501 --> loss:0.8241545557975769
step 51/334, epoch 489/501 --> loss:0.8150729691982269
step 101/334, epoch 489/501 --> loss:0.8364798820018768
step 151/334, epoch 489/501 --> loss:0.8501090323925018
step 201/334, epoch 489/501 --> loss:0.807057523727417
step 251/334, epoch 489/501 --> loss:0.8238583791255951
step 301/334, epoch 489/501 --> loss:0.827114599943161
step 51/334, epoch 490/501 --> loss:0.8194227540493011
step 101/334, epoch 490/501 --> loss:0.8599657559394837
step 151/334, epoch 490/501 --> loss:0.8186629450321198
step 201/334, epoch 490/501 --> loss:0.8150055944919586
step 251/334, epoch 490/501 --> loss:0.8112776923179627
step 301/334, epoch 490/501 --> loss:0.8278464198112487
step 51/334, epoch 491/501 --> loss:0.8276239824295044
step 101/334, epoch 491/501 --> loss:0.8443050825595856
step 151/334, epoch 491/501 --> loss:0.81962926030159
step 201/334, epoch 491/501 --> loss:0.8418510925769805
step 251/334, epoch 491/501 --> loss:0.7949574887752533
step 301/334, epoch 491/501 --> loss:0.8544763839244842

##########train dataset##########
acc--> [99.2763852554596]
F1--> {'F1': [0.9210356894849732], 'precision': [0.8596034732140581], 'recall': [0.9919358547493787]}
##########eval dataset##########
acc--> [98.57299484292321]
F1--> {'F1': [0.8395722114092281], 'precision': [0.7889628526557938], 'recall': [0.8971308297671146]}
step 51/334, epoch 492/501 --> loss:0.8423954331874848
step 101/334, epoch 492/501 --> loss:0.8030951642990112
step 151/334, epoch 492/501 --> loss:0.8465935409069061
step 201/334, epoch 492/501 --> loss:0.8341622817516327
step 251/334, epoch 492/501 --> loss:0.8081708610057831
step 301/334, epoch 492/501 --> loss:0.8236280632019043
step 51/334, epoch 493/501 --> loss:0.835907690525055
step 101/334, epoch 493/501 --> loss:0.8142020773887634
step 151/334, epoch 493/501 --> loss:0.8219016456604004
step 201/334, epoch 493/501 --> loss:0.8354757225513458
step 251/334, epoch 493/501 --> loss:0.8212919807434083
step 301/334, epoch 493/501 --> loss:0.8303650987148284
step 51/334, epoch 494/501 --> loss:0.8046322202682495
step 101/334, epoch 494/501 --> loss:0.8433290493488311
step 151/334, epoch 494/501 --> loss:0.8263213217258454
step 201/334, epoch 494/501 --> loss:0.8398568999767303
step 251/334, epoch 494/501 --> loss:0.8241477584838868
step 301/334, epoch 494/501 --> loss:0.817178213596344
step 51/334, epoch 495/501 --> loss:0.8327755284309387
step 101/334, epoch 495/501 --> loss:0.8295369255542755
step 151/334, epoch 495/501 --> loss:0.8278552496433258
step 201/334, epoch 495/501 --> loss:0.8200188446044921
step 251/334, epoch 495/501 --> loss:0.828802021741867
step 301/334, epoch 495/501 --> loss:0.8234311556816101
step 51/334, epoch 496/501 --> loss:0.8481269896030426
step 101/334, epoch 496/501 --> loss:0.8157591724395752
step 151/334, epoch 496/501 --> loss:0.82559077501297
step 201/334, epoch 496/501 --> loss:0.8371806967258454
step 251/334, epoch 496/501 --> loss:0.8303196465969086
step 301/334, epoch 496/501 --> loss:0.8166091680526734
step 51/334, epoch 497/501 --> loss:0.8239887249469757
step 101/334, epoch 497/501 --> loss:0.8525411248207092
step 151/334, epoch 497/501 --> loss:0.8304682767391205
step 201/334, epoch 497/501 --> loss:0.8150263798236846
step 251/334, epoch 497/501 --> loss:0.8312223887443543
step 301/334, epoch 497/501 --> loss:0.8183213448524476
step 51/334, epoch 498/501 --> loss:0.8330593955516815
step 101/334, epoch 498/501 --> loss:0.8247784578800201
step 151/334, epoch 498/501 --> loss:0.8229575634002686
step 201/334, epoch 498/501 --> loss:0.836782522201538
step 251/334, epoch 498/501 --> loss:0.8301178514957428
step 301/334, epoch 498/501 --> loss:0.8258412206172943
step 51/334, epoch 499/501 --> loss:0.8204850268363952
step 101/334, epoch 499/501 --> loss:0.8250955629348755
step 151/334, epoch 499/501 --> loss:0.8486380791664123
step 201/334, epoch 499/501 --> loss:0.808899689912796
step 251/334, epoch 499/501 --> loss:0.8284893083572388
step 301/334, epoch 499/501 --> loss:0.8215445172786713
step 51/334, epoch 500/501 --> loss:0.836112209558487
step 101/334, epoch 500/501 --> loss:0.8196116888523102
step 151/334, epoch 500/501 --> loss:0.8283207416534424
step 201/334, epoch 500/501 --> loss:0.8351030421257019
step 251/334, epoch 500/501 --> loss:0.8087810778617859
step 301/334, epoch 500/501 --> loss:0.8206878662109375
step 51/334, epoch 501/501 --> loss:0.814302031993866
step 101/334, epoch 501/501 --> loss:0.8100283825397492
step 151/334, epoch 501/501 --> loss:0.8189083755016326
step 201/334, epoch 501/501 --> loss:0.8284975671768189
step 251/334, epoch 501/501 --> loss:0.8307484018802643
step 301/334, epoch 501/501 --> loss:0.8380309677124024

##########train dataset##########
acc--> [99.27924191589415]
F1--> {'F1': [0.9213083336223095], 'precision': [0.8602282434181656], 'recall': [0.9917368379996989]}
##########eval dataset##########
acc--> [98.52563719527687]
F1--> {'F1': [0.8350613217002001], 'precision': [0.7813533340774952], 'recall': [0.8967092302065154]}
