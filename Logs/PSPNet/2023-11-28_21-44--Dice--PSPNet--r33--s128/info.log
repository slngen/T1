##########Config##########
{'device': 'cuda:0', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/PSPNet', 'log_path': 'Logs/PSPNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (features): Sequential(
    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (5): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (6): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (7): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (pyramid_pooling): PyramidPoolingModule(
    (pooling_layers): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (final_conv): Conv2d(4096, 2, kernel_size=(1, 1), stride=(1, 1))
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.8880611073970794
step 101/334, epoch 1/501 --> loss:0.8547773861885071
step 151/334, epoch 1/501 --> loss:0.8503174817562104
step 201/334, epoch 1/501 --> loss:0.847105724811554
step 251/334, epoch 1/501 --> loss:0.8519869744777679
step 301/334, epoch 1/501 --> loss:0.8513583898544311

##########train dataset##########
acc--> [87.1593925896567]
F1--> {'F1': [0.37283711045393475], 'precision': [0.23477867183228865], 'recall': [0.9050632640910082]}
##########eval dataset##########
acc--> [86.82034977255681]
F1--> {'F1': [0.3697240034913687], 'precision': [0.23302872524907434], 'recall': [0.8943935169427587]}
save model!
step 51/334, epoch 2/501 --> loss:0.8457410907745362
step 101/334, epoch 2/501 --> loss:0.8483179175853729
step 151/334, epoch 2/501 --> loss:0.8424284434318543
step 201/334, epoch 2/501 --> loss:0.837215336561203
step 251/334, epoch 2/501 --> loss:0.8529814994335174
step 301/334, epoch 2/501 --> loss:0.8428738522529602
step 51/334, epoch 3/501 --> loss:0.8350152575969696
step 101/334, epoch 3/501 --> loss:0.8558060324192047
step 151/334, epoch 3/501 --> loss:0.864901065826416
step 201/334, epoch 3/501 --> loss:0.8431916499137878
step 251/334, epoch 3/501 --> loss:0.8392377889156342
step 301/334, epoch 3/501 --> loss:0.8418761885166168
step 51/334, epoch 4/501 --> loss:0.8414538741111756
step 101/334, epoch 4/501 --> loss:0.8453309094905853
step 151/334, epoch 4/501 --> loss:0.846814101934433
step 201/334, epoch 4/501 --> loss:0.8397227954864502
step 251/334, epoch 4/501 --> loss:0.8453957784175873
step 301/334, epoch 4/501 --> loss:0.8387089514732361
step 51/334, epoch 5/501 --> loss:0.8597855257987976
step 101/334, epoch 5/501 --> loss:0.843140366077423
step 151/334, epoch 5/501 --> loss:0.8249156248569488
step 201/334, epoch 5/501 --> loss:0.8499561262130737
step 251/334, epoch 5/501 --> loss:0.8441963803768158
step 301/334, epoch 5/501 --> loss:0.8419381546974182
step 51/334, epoch 6/501 --> loss:0.8502902507781982
step 101/334, epoch 6/501 --> loss:0.839833527803421
step 151/334, epoch 6/501 --> loss:0.8268728864192962
step 201/334, epoch 6/501 --> loss:0.8448603439331055
step 251/334, epoch 6/501 --> loss:0.8449275040626526
step 301/334, epoch 6/501 --> loss:0.845514042377472
step 51/334, epoch 7/501 --> loss:0.844776349067688
step 101/334, epoch 7/501 --> loss:0.8533019995689393
step 151/334, epoch 7/501 --> loss:0.8450615274906158
step 201/334, epoch 7/501 --> loss:0.8386460947990417
step 251/334, epoch 7/501 --> loss:0.8353776502609253
step 301/334, epoch 7/501 --> loss:0.8245975375175476
step 51/334, epoch 8/501 --> loss:0.8468045294284821
step 101/334, epoch 8/501 --> loss:0.8441955673694611
step 151/334, epoch 8/501 --> loss:0.8371742582321167
step 201/334, epoch 8/501 --> loss:0.838382875919342
step 251/334, epoch 8/501 --> loss:0.8366668260097504
step 301/334, epoch 8/501 --> loss:0.8477567327022553
step 51/334, epoch 9/501 --> loss:0.8404077053070068
step 101/334, epoch 9/501 --> loss:0.8523305940628052
step 151/334, epoch 9/501 --> loss:0.8400955522060394
step 201/334, epoch 9/501 --> loss:0.8352400243282319
step 251/334, epoch 9/501 --> loss:0.8340245270729065
step 301/334, epoch 9/501 --> loss:0.8439893543720245
step 51/334, epoch 10/501 --> loss:0.8449044239521026
step 101/334, epoch 10/501 --> loss:0.8451228702068329
step 151/334, epoch 10/501 --> loss:0.8403749513626099
step 201/334, epoch 10/501 --> loss:0.8384242844581604
step 251/334, epoch 10/501 --> loss:0.8369565284252167
step 301/334, epoch 10/501 --> loss:0.8527113926410675
step 51/334, epoch 11/501 --> loss:0.8435968923568725
step 101/334, epoch 11/501 --> loss:0.8458374345302582
step 151/334, epoch 11/501 --> loss:0.8459921181201935
step 201/334, epoch 11/501 --> loss:0.8420514512062073
step 251/334, epoch 11/501 --> loss:0.8415066707134247
step 301/334, epoch 11/501 --> loss:0.8420329368114472

##########train dataset##########
acc--> [96.20791944082275]
F1--> {'F1': [0.6646273910404731], 'precision': [0.5299781453190894], 'recall': [0.8910166665383087]}
##########eval dataset##########
acc--> [96.21815452007675]
F1--> {'F1': [0.665699258234273], 'precision': [0.5386432185892879], 'recall': [0.8712153011668933]}
save model!
step 51/334, epoch 12/501 --> loss:0.8328823399543762
step 101/334, epoch 12/501 --> loss:0.8436752665042877
step 151/334, epoch 12/501 --> loss:0.8459791004657745
step 201/334, epoch 12/501 --> loss:0.8503779077529907
step 251/334, epoch 12/501 --> loss:0.8336946940422059
step 301/334, epoch 12/501 --> loss:0.8281260001659393
step 51/334, epoch 13/501 --> loss:0.8346660304069519
step 101/334, epoch 13/501 --> loss:0.8302471518516541
step 151/334, epoch 13/501 --> loss:0.839036340713501
step 201/334, epoch 13/501 --> loss:0.8463493216037751
step 251/334, epoch 13/501 --> loss:0.8524401259422302
step 301/334, epoch 13/501 --> loss:0.832424921989441
step 51/334, epoch 14/501 --> loss:0.8375804579257965
step 101/334, epoch 14/501 --> loss:0.8406956160068512
step 151/334, epoch 14/501 --> loss:0.8364309191703796
step 201/334, epoch 14/501 --> loss:0.8270793068408966
step 251/334, epoch 14/501 --> loss:0.8427938985824585
step 301/334, epoch 14/501 --> loss:0.8321389794349671
step 51/334, epoch 15/501 --> loss:0.8352768099308014
step 101/334, epoch 15/501 --> loss:0.830501959323883
step 151/334, epoch 15/501 --> loss:0.8458775115013123
step 201/334, epoch 15/501 --> loss:0.8422772872447968
step 251/334, epoch 15/501 --> loss:0.8444449627399444
step 301/334, epoch 15/501 --> loss:0.8317743611335754
step 51/334, epoch 16/501 --> loss:0.845047379732132
step 101/334, epoch 16/501 --> loss:0.849512311220169
step 151/334, epoch 16/501 --> loss:0.834789035320282
step 201/334, epoch 16/501 --> loss:0.8287916195392608
step 251/334, epoch 16/501 --> loss:0.8411344742774963
step 301/334, epoch 16/501 --> loss:0.8384286606311798
step 51/334, epoch 17/501 --> loss:0.8434001564979553
step 101/334, epoch 17/501 --> loss:0.8398315715789795
step 151/334, epoch 17/501 --> loss:0.840042793750763
step 201/334, epoch 17/501 --> loss:0.8391479563713073
step 251/334, epoch 17/501 --> loss:0.8318006682395935
step 301/334, epoch 17/501 --> loss:0.8370271444320678
step 51/334, epoch 18/501 --> loss:0.8401456809043885
step 101/334, epoch 18/501 --> loss:0.8296253859996796
step 151/334, epoch 18/501 --> loss:0.8517899668216705
step 201/334, epoch 18/501 --> loss:0.8398985004425049
step 251/334, epoch 18/501 --> loss:0.850997805595398
step 301/334, epoch 18/501 --> loss:0.8376509130001069
step 51/334, epoch 19/501 --> loss:0.8460711443424225
step 101/334, epoch 19/501 --> loss:0.8396282482147217
step 151/334, epoch 19/501 --> loss:0.8305988717079162
step 201/334, epoch 19/501 --> loss:0.8352050662040711
step 251/334, epoch 19/501 --> loss:0.8409713411331177
step 301/334, epoch 19/501 --> loss:0.8427789092063904
step 51/334, epoch 20/501 --> loss:0.8376592230796814
step 101/334, epoch 20/501 --> loss:0.8247869801521301
step 151/334, epoch 20/501 --> loss:0.8454805266857147
step 201/334, epoch 20/501 --> loss:0.8399297511577606
step 251/334, epoch 20/501 --> loss:0.8426929759979248
step 301/334, epoch 20/501 --> loss:0.8322264075279235
step 51/334, epoch 21/501 --> loss:0.8396348142623902
step 101/334, epoch 21/501 --> loss:0.8530839252471923
step 151/334, epoch 21/501 --> loss:0.8366664636135102
step 201/334, epoch 21/501 --> loss:0.8404873657226563
step 251/334, epoch 21/501 --> loss:0.8274104559421539
step 301/334, epoch 21/501 --> loss:0.8307707643508911

##########train dataset##########
acc--> [95.53353445870262]
F1--> {'F1': [0.6321720708738636], 'precision': [0.48427337357916794], 'recall': [0.9101470614464805]}
##########eval dataset##########
acc--> [95.47965449401805]
F1--> {'F1': [0.631369980471296], 'precision': [0.48751907228079344], 'recall': [0.8956640533169762]}
step 51/334, epoch 22/501 --> loss:0.8272512555122375
step 101/334, epoch 22/501 --> loss:0.8212645173072814
step 151/334, epoch 22/501 --> loss:0.8478580117225647
step 201/334, epoch 22/501 --> loss:0.8326498878002166
step 251/334, epoch 22/501 --> loss:0.8349243485927582
step 301/334, epoch 22/501 --> loss:0.8422779464721679
step 51/334, epoch 23/501 --> loss:0.8289470732212066
step 101/334, epoch 23/501 --> loss:0.8488988566398621
step 151/334, epoch 23/501 --> loss:0.8261442351341247
step 201/334, epoch 23/501 --> loss:0.847182377576828
step 251/334, epoch 23/501 --> loss:0.8345839667320252
step 301/334, epoch 23/501 --> loss:0.8336036276817321
step 51/334, epoch 24/501 --> loss:0.832194402217865
step 101/334, epoch 24/501 --> loss:0.8295093941688537
step 151/334, epoch 24/501 --> loss:0.8246903777122497
step 201/334, epoch 24/501 --> loss:0.8313344943523407
step 251/334, epoch 24/501 --> loss:0.8559875738620758
step 301/334, epoch 24/501 --> loss:0.8404128551483154
step 51/334, epoch 25/501 --> loss:0.8395299923419952
step 101/334, epoch 25/501 --> loss:0.8384007334709167
step 151/334, epoch 25/501 --> loss:0.8371293914318084
step 201/334, epoch 25/501 --> loss:0.8376084280014038
step 251/334, epoch 25/501 --> loss:0.8392784023284912
step 301/334, epoch 25/501 --> loss:0.8332099843025208
step 51/334, epoch 26/501 --> loss:0.8278494262695313
step 101/334, epoch 26/501 --> loss:0.8427252912521362
step 151/334, epoch 26/501 --> loss:0.8351769828796387
step 201/334, epoch 26/501 --> loss:0.8481531000137329
step 251/334, epoch 26/501 --> loss:0.8280645322799682
step 301/334, epoch 26/501 --> loss:0.8353860700130462
step 51/334, epoch 27/501 --> loss:0.8306973540782928
step 101/334, epoch 27/501 --> loss:0.8378188776969909
step 151/334, epoch 27/501 --> loss:0.8380030167102813
step 201/334, epoch 27/501 --> loss:0.8355821692943572
step 251/334, epoch 27/501 --> loss:0.8457872593402862
step 301/334, epoch 27/501 --> loss:0.8488418209552765
step 51/334, epoch 28/501 --> loss:0.8354014945030213
step 101/334, epoch 28/501 --> loss:0.8476406347751617
step 151/334, epoch 28/501 --> loss:0.8312701106071472
step 201/334, epoch 28/501 --> loss:0.828458594083786
step 251/334, epoch 28/501 --> loss:0.846568933725357
step 301/334, epoch 28/501 --> loss:0.8228210008144379
step 51/334, epoch 29/501 --> loss:0.8397868943214416
step 101/334, epoch 29/501 --> loss:0.8372404718399048
step 151/334, epoch 29/501 --> loss:0.8425807690620423
step 201/334, epoch 29/501 --> loss:0.8404606592655182
step 251/334, epoch 29/501 --> loss:0.8359168016910553
step 301/334, epoch 29/501 --> loss:0.8264269161224366
step 51/334, epoch 30/501 --> loss:0.8312953388690949
step 101/334, epoch 30/501 --> loss:0.8270944607257843
step 151/334, epoch 30/501 --> loss:0.8359516584873199
step 201/334, epoch 30/501 --> loss:0.8327071249485016
step 251/334, epoch 30/501 --> loss:0.833175812959671
step 301/334, epoch 30/501 --> loss:0.8500241708755493
step 51/334, epoch 31/501 --> loss:0.8315929198265075
step 101/334, epoch 31/501 --> loss:0.8441997182369232
step 151/334, epoch 31/501 --> loss:0.8445559537410736
step 201/334, epoch 31/501 --> loss:0.8377633237838745
step 251/334, epoch 31/501 --> loss:0.8362894213199615
step 301/334, epoch 31/501 --> loss:0.824190057516098

##########train dataset##########
acc--> [96.19809448562306]
F1--> {'F1': [0.6727659914403535], 'precision': [0.5280537395340617], 'recall': [0.9267541780301628]}
##########eval dataset##########
acc--> [96.25427299492193]
F1--> {'F1': [0.6753714941932585], 'precision': [0.5399365525916818], 'recall': [0.9015155937593121]}
save model!
step 51/334, epoch 32/501 --> loss:0.8342593538761139
step 101/334, epoch 32/501 --> loss:0.832835818529129
step 151/334, epoch 32/501 --> loss:0.8452294969558716
step 201/334, epoch 32/501 --> loss:0.8346896135807037
step 251/334, epoch 32/501 --> loss:0.8381774806976319
step 301/334, epoch 32/501 --> loss:0.8440686094760895
step 51/334, epoch 33/501 --> loss:0.848758305311203
step 101/334, epoch 33/501 --> loss:0.8377719342708587
step 151/334, epoch 33/501 --> loss:0.8345529186725616
step 201/334, epoch 33/501 --> loss:0.8544576454162598
step 251/334, epoch 33/501 --> loss:0.8346311676502228
step 301/334, epoch 33/501 --> loss:0.8400342488288879
step 51/334, epoch 34/501 --> loss:0.8191356968879699
step 101/334, epoch 34/501 --> loss:0.8474322474002838
step 151/334, epoch 34/501 --> loss:0.8408129739761353
step 201/334, epoch 34/501 --> loss:0.8364911234378815
step 251/334, epoch 34/501 --> loss:0.832810275554657
step 301/334, epoch 34/501 --> loss:0.8296134984493255
step 51/334, epoch 35/501 --> loss:0.8411643230915069
step 101/334, epoch 35/501 --> loss:0.8270815026760101
step 151/334, epoch 35/501 --> loss:0.8250380063056946
step 201/334, epoch 35/501 --> loss:0.8449624931812286
step 251/334, epoch 35/501 --> loss:0.8585073232650757
step 301/334, epoch 35/501 --> loss:0.8221424722671509
step 51/334, epoch 36/501 --> loss:0.8449520468711853
step 101/334, epoch 36/501 --> loss:0.8351198565959931
step 151/334, epoch 36/501 --> loss:0.8236390316486358
step 201/334, epoch 36/501 --> loss:0.8344326984882354
step 251/334, epoch 36/501 --> loss:0.8399801933765412
step 301/334, epoch 36/501 --> loss:0.83203888297081
step 51/334, epoch 37/501 --> loss:0.8380889284610749
step 101/334, epoch 37/501 --> loss:0.8261490988731385
step 151/334, epoch 37/501 --> loss:0.8450082588195801
step 201/334, epoch 37/501 --> loss:0.8279209017753602
step 251/334, epoch 37/501 --> loss:0.8511937093734742
step 301/334, epoch 37/501 --> loss:0.8289252591133117
step 51/334, epoch 38/501 --> loss:0.841323584318161
step 101/334, epoch 38/501 --> loss:0.8387353229522705
step 151/334, epoch 38/501 --> loss:0.8399091100692749
step 201/334, epoch 38/501 --> loss:0.8355069601535797
step 251/334, epoch 38/501 --> loss:0.8237848806381226
step 301/334, epoch 38/501 --> loss:0.824458303451538
step 51/334, epoch 39/501 --> loss:0.8391382992267609
step 101/334, epoch 39/501 --> loss:0.8284564816951752
step 151/334, epoch 39/501 --> loss:0.8300399279594421
step 201/334, epoch 39/501 --> loss:0.8405116879940033
step 251/334, epoch 39/501 --> loss:0.8298361599445343
step 301/334, epoch 39/501 --> loss:0.8348529267311097
step 51/334, epoch 40/501 --> loss:0.8317054355144501
step 101/334, epoch 40/501 --> loss:0.8497302639484405
step 151/334, epoch 40/501 --> loss:0.8337561154365539
step 201/334, epoch 40/501 --> loss:0.8277767944335938
step 251/334, epoch 40/501 --> loss:0.8261340570449829
step 301/334, epoch 40/501 --> loss:0.8403647303581238
step 51/334, epoch 41/501 --> loss:0.8344264042377472
step 101/334, epoch 41/501 --> loss:0.8320680594444275
step 151/334, epoch 41/501 --> loss:0.842270416021347
step 201/334, epoch 41/501 --> loss:0.8224187231063843
step 251/334, epoch 41/501 --> loss:0.8376564872264862
step 301/334, epoch 41/501 --> loss:0.8218980014324189

##########train dataset##########
acc--> [92.72724061500104]
F1--> {'F1': [0.5145953118666511], 'precision': [0.35808870195974146], 'recall': [0.9141482327989727]}
##########eval dataset##########
acc--> [92.90825217205504]
F1--> {'F1': [0.5238157060168431], 'precision': [0.368997692268067], 'recall': [0.902476024744028]}
step 51/334, epoch 42/501 --> loss:0.8371835649013519
step 101/334, epoch 42/501 --> loss:0.8329386341571808
step 151/334, epoch 42/501 --> loss:0.8302098894119263
step 201/334, epoch 42/501 --> loss:0.8396725869178772
step 251/334, epoch 42/501 --> loss:0.8328598153591156
step 301/334, epoch 42/501 --> loss:0.8309053468704224
step 51/334, epoch 43/501 --> loss:0.8286120736598969
step 101/334, epoch 43/501 --> loss:0.8420301723480225
step 151/334, epoch 43/501 --> loss:0.8350748324394226
step 201/334, epoch 43/501 --> loss:0.8355563282966614
step 251/334, epoch 43/501 --> loss:0.8308277678489685
step 301/334, epoch 43/501 --> loss:0.8318396866321563
step 51/334, epoch 44/501 --> loss:0.8241276526451111
step 101/334, epoch 44/501 --> loss:0.8505002927780151
step 151/334, epoch 44/501 --> loss:0.8319526386260986
step 201/334, epoch 44/501 --> loss:0.8305200624465943
step 251/334, epoch 44/501 --> loss:0.8356446719169617
step 301/334, epoch 44/501 --> loss:0.8388167357444763
step 51/334, epoch 45/501 --> loss:0.8381448757648468
step 101/334, epoch 45/501 --> loss:0.8344423711299896
step 151/334, epoch 45/501 --> loss:0.830484414100647
step 201/334, epoch 45/501 --> loss:0.8378470242023468
step 251/334, epoch 45/501 --> loss:0.851467273235321
step 301/334, epoch 45/501 --> loss:0.8225338470935821
step 51/334, epoch 46/501 --> loss:0.8344491791725158
step 101/334, epoch 46/501 --> loss:0.8328059530258178
step 151/334, epoch 46/501 --> loss:0.8247162365913391
step 201/334, epoch 46/501 --> loss:0.8382398176193238
step 251/334, epoch 46/501 --> loss:0.8289920270442963
step 301/334, epoch 46/501 --> loss:0.8371632719039916
step 51/334, epoch 47/501 --> loss:0.8195585465431213
step 101/334, epoch 47/501 --> loss:0.826826490163803
step 151/334, epoch 47/501 --> loss:0.8422315752506256
step 201/334, epoch 47/501 --> loss:0.8321092307567597
step 251/334, epoch 47/501 --> loss:0.8347200441360474
step 301/334, epoch 47/501 --> loss:0.8349493503570556
step 51/334, epoch 48/501 --> loss:0.8326491284370422
step 101/334, epoch 48/501 --> loss:0.8233808672428131
step 151/334, epoch 48/501 --> loss:0.8350479650497437
step 201/334, epoch 48/501 --> loss:0.8319958782196045
step 251/334, epoch 48/501 --> loss:0.8426062071323395
step 301/334, epoch 48/501 --> loss:0.8434548318386078
step 51/334, epoch 49/501 --> loss:0.8467170071601867
step 101/334, epoch 49/501 --> loss:0.8342154014110565
step 151/334, epoch 49/501 --> loss:0.8405651640892029
step 201/334, epoch 49/501 --> loss:0.8249704504013061
step 251/334, epoch 49/501 --> loss:0.8301093530654907
step 301/334, epoch 49/501 --> loss:0.8281440830230713
step 51/334, epoch 50/501 --> loss:0.8210458135604859
step 101/334, epoch 50/501 --> loss:0.8155914175510407
step 151/334, epoch 50/501 --> loss:0.8558428633213043
step 201/334, epoch 50/501 --> loss:0.8326996624469757
step 251/334, epoch 50/501 --> loss:0.8399588263034821
step 301/334, epoch 50/501 --> loss:0.8309792184829712
step 51/334, epoch 51/501 --> loss:0.8213884210586548
step 101/334, epoch 51/501 --> loss:0.8363207745552063
step 151/334, epoch 51/501 --> loss:0.8260067939758301
step 201/334, epoch 51/501 --> loss:0.8223850774765015
step 251/334, epoch 51/501 --> loss:0.8436825096607208
step 301/334, epoch 51/501 --> loss:0.8447567677497864

##########train dataset##########
acc--> [89.87978408521431]
F1--> {'F1': [0.4412299248185875], 'precision': [0.28757682904083187], 'recall': [0.9474938598615612]}
##########eval dataset##########
acc--> [90.24946024202426]
F1--> {'F1': [0.4524818519706984], 'precision': [0.29874818052357427], 'recall': [0.9322008926943739]}
step 51/334, epoch 52/501 --> loss:0.8345378065109252
step 101/334, epoch 52/501 --> loss:0.8246431303024292
step 151/334, epoch 52/501 --> loss:0.8424726808071137
step 201/334, epoch 52/501 --> loss:0.8237766540050506
step 251/334, epoch 52/501 --> loss:0.8361314487457275
step 301/334, epoch 52/501 --> loss:0.8338951122760773
step 51/334, epoch 53/501 --> loss:0.8358393156528473
step 101/334, epoch 53/501 --> loss:0.8358031558990479
step 151/334, epoch 53/501 --> loss:0.8389174270629883
step 201/334, epoch 53/501 --> loss:0.8371608364582062
step 251/334, epoch 53/501 --> loss:0.8283130407333374
step 301/334, epoch 53/501 --> loss:0.8318602216243743
step 51/334, epoch 54/501 --> loss:0.8488108849525452
step 101/334, epoch 54/501 --> loss:0.837803475856781
step 151/334, epoch 54/501 --> loss:0.8309333825111389
step 201/334, epoch 54/501 --> loss:0.8288666689395905
step 251/334, epoch 54/501 --> loss:0.8297719871997833
step 301/334, epoch 54/501 --> loss:0.8351525628566742
step 51/334, epoch 55/501 --> loss:0.8294811737537384
step 101/334, epoch 55/501 --> loss:0.8420758581161499
step 151/334, epoch 55/501 --> loss:0.8407815766334533
step 201/334, epoch 55/501 --> loss:0.83586510181427
step 251/334, epoch 55/501 --> loss:0.8309476435184479
step 301/334, epoch 55/501 --> loss:0.8130995976924896
step 51/334, epoch 56/501 --> loss:0.8235168516635895
step 101/334, epoch 56/501 --> loss:0.8491794526576996
step 151/334, epoch 56/501 --> loss:0.8184947502613068
step 201/334, epoch 56/501 --> loss:0.8372766816616058
step 251/334, epoch 56/501 --> loss:0.8349970316886902
step 301/334, epoch 56/501 --> loss:0.8467811346054077
step 51/334, epoch 57/501 --> loss:0.8209857070446014
step 101/334, epoch 57/501 --> loss:0.8380550062656402
step 151/334, epoch 57/501 --> loss:0.8245773661136627
step 201/334, epoch 57/501 --> loss:0.8299345898628235
step 251/334, epoch 57/501 --> loss:0.828740348815918
step 301/334, epoch 57/501 --> loss:0.8354471623897552
step 51/334, epoch 58/501 --> loss:0.8361563420295716
step 101/334, epoch 58/501 --> loss:0.8351696336269379
step 151/334, epoch 58/501 --> loss:0.8230313003063202
step 201/334, epoch 58/501 --> loss:0.8479895889759064
step 251/334, epoch 58/501 --> loss:0.8313900661468506
step 301/334, epoch 58/501 --> loss:0.8235160911083221
step 51/334, epoch 59/501 --> loss:0.8239903593063355
step 101/334, epoch 59/501 --> loss:0.8344963204860687
step 151/334, epoch 59/501 --> loss:0.8327623498439789
step 201/334, epoch 59/501 --> loss:0.8287206506729126
step 251/334, epoch 59/501 --> loss:0.8271007478237152
step 301/334, epoch 59/501 --> loss:0.8386335885524749
step 51/334, epoch 60/501 --> loss:0.8328581583499909
step 101/334, epoch 60/501 --> loss:0.8276908040046692
step 151/334, epoch 60/501 --> loss:0.8297286283969879
step 201/334, epoch 60/501 --> loss:0.8401183342933655
step 251/334, epoch 60/501 --> loss:0.8405178725719452
step 301/334, epoch 60/501 --> loss:0.8224536764621735
step 51/334, epoch 61/501 --> loss:0.8306882655620575
step 101/334, epoch 61/501 --> loss:0.8449018740653992
step 151/334, epoch 61/501 --> loss:0.8402346646785737
step 201/334, epoch 61/501 --> loss:0.8171767973899842
step 251/334, epoch 61/501 --> loss:0.8154116737842559
step 301/334, epoch 61/501 --> loss:0.8430138826370239

##########train dataset##########
acc--> [96.91767426166179]
F1--> {'F1': [0.7241246076202068], 'precision': [0.5815743976759288], 'recall': [0.9592628049517256]}
##########eval dataset##########
acc--> [96.73333088764869]
F1--> {'F1': [0.710576921071057], 'precision': [0.5757705049495865], 'recall': [0.92782235285217]}
save model!
step 51/334, epoch 62/501 --> loss:0.8245772528648376
step 101/334, epoch 62/501 --> loss:0.8232259559631347
step 151/334, epoch 62/501 --> loss:0.8463457357883454
step 201/334, epoch 62/501 --> loss:0.8335876071453094
step 251/334, epoch 62/501 --> loss:0.8324543714523316
step 301/334, epoch 62/501 --> loss:0.8391420149803162
step 51/334, epoch 63/501 --> loss:0.8306195700168609
step 101/334, epoch 63/501 --> loss:0.8309073352813721
step 151/334, epoch 63/501 --> loss:0.8457533574104309
step 201/334, epoch 63/501 --> loss:0.8337904226779937
step 251/334, epoch 63/501 --> loss:0.8236857366561889
step 301/334, epoch 63/501 --> loss:0.8280671525001526
step 51/334, epoch 64/501 --> loss:0.8154977416992187
step 101/334, epoch 64/501 --> loss:0.8360323810577392
step 151/334, epoch 64/501 --> loss:0.8245257425308228
step 201/334, epoch 64/501 --> loss:0.8369428050518036
step 251/334, epoch 64/501 --> loss:0.8373069369792938
step 301/334, epoch 64/501 --> loss:0.8368000948429107
step 51/334, epoch 65/501 --> loss:0.8336502194404602
step 101/334, epoch 65/501 --> loss:0.8346086084842682
step 151/334, epoch 65/501 --> loss:0.8166429805755615
step 201/334, epoch 65/501 --> loss:0.8337042093276977
step 251/334, epoch 65/501 --> loss:0.8357490503787994
step 301/334, epoch 65/501 --> loss:0.8351604044437408
step 51/334, epoch 66/501 --> loss:0.8209479606151581
step 101/334, epoch 66/501 --> loss:0.8330987560749054
step 151/334, epoch 66/501 --> loss:0.8326813852787018
step 201/334, epoch 66/501 --> loss:0.8365764737129211
step 251/334, epoch 66/501 --> loss:0.8314243507385254
step 301/334, epoch 66/501 --> loss:0.8348255896568298
step 51/334, epoch 67/501 --> loss:0.8321919429302216
step 101/334, epoch 67/501 --> loss:0.8265619349479675
step 151/334, epoch 67/501 --> loss:0.8434341716766357
step 201/334, epoch 67/501 --> loss:0.8325048506259918
step 251/334, epoch 67/501 --> loss:0.8222754859924316
step 301/334, epoch 67/501 --> loss:0.8157255506515503
step 51/334, epoch 68/501 --> loss:0.8388358628749848
step 101/334, epoch 68/501 --> loss:0.827246470451355
step 151/334, epoch 68/501 --> loss:0.8391261780261994
step 201/334, epoch 68/501 --> loss:0.8433441495895386
step 251/334, epoch 68/501 --> loss:0.8268670237064362
step 301/334, epoch 68/501 --> loss:0.8194599843025208
step 51/334, epoch 69/501 --> loss:0.8163291251659394
step 101/334, epoch 69/501 --> loss:0.8211865615844727
step 151/334, epoch 69/501 --> loss:0.8388372766971588
step 201/334, epoch 69/501 --> loss:0.8342526638507843
step 251/334, epoch 69/501 --> loss:0.8234644842147827
step 301/334, epoch 69/501 --> loss:0.8380893850326538
step 51/334, epoch 70/501 --> loss:0.8222582983970642
step 101/334, epoch 70/501 --> loss:0.8283422029018402
step 151/334, epoch 70/501 --> loss:0.8442327547073364
step 201/334, epoch 70/501 --> loss:0.829040311574936
step 251/334, epoch 70/501 --> loss:0.8325777184963227
step 301/334, epoch 70/501 --> loss:0.8390416026115417
step 51/334, epoch 71/501 --> loss:0.8247712326049804
step 101/334, epoch 71/501 --> loss:0.8281588900089264
step 151/334, epoch 71/501 --> loss:0.8223655414581299
step 201/334, epoch 71/501 --> loss:0.8361081492900848
step 251/334, epoch 71/501 --> loss:0.8344743275642394
step 301/334, epoch 71/501 --> loss:0.832537157535553

##########train dataset##########
acc--> [96.50611033544847]
F1--> {'F1': [0.698881120964286], 'precision': [0.5489629857450461], 'recall': [0.9614629913180575]}
##########eval dataset##########
acc--> [96.26935823695223]
F1--> {'F1': [0.6835197063047581], 'precision': [0.5396126609452584], 'recall': [0.9321139458147477]}
step 51/334, epoch 72/501 --> loss:0.8363953363895417
step 101/334, epoch 72/501 --> loss:0.8275425350666046
step 151/334, epoch 72/501 --> loss:0.8310994553565979
step 201/334, epoch 72/501 --> loss:0.8316230475902557
step 251/334, epoch 72/501 --> loss:0.8277085447311401
step 301/334, epoch 72/501 --> loss:0.8276595497131347
step 51/334, epoch 73/501 --> loss:0.8315603744983673
step 101/334, epoch 73/501 --> loss:0.8426014924049378
step 151/334, epoch 73/501 --> loss:0.8362509083747863
step 201/334, epoch 73/501 --> loss:0.8329188418388367
step 251/334, epoch 73/501 --> loss:0.8336270523071289
step 301/334, epoch 73/501 --> loss:0.8173569738864899
step 51/334, epoch 74/501 --> loss:0.8279722678661346
step 101/334, epoch 74/501 --> loss:0.82501504778862
step 151/334, epoch 74/501 --> loss:0.8402976703643799
step 201/334, epoch 74/501 --> loss:0.830089076757431
step 251/334, epoch 74/501 --> loss:0.8338707160949707
step 301/334, epoch 74/501 --> loss:0.8277449142932892
step 51/334, epoch 75/501 --> loss:0.829744439125061
step 101/334, epoch 75/501 --> loss:0.8266426789760589
step 151/334, epoch 75/501 --> loss:0.8337140035629272
step 201/334, epoch 75/501 --> loss:0.829509185552597
step 251/334, epoch 75/501 --> loss:0.8340712559223175
step 301/334, epoch 75/501 --> loss:0.827609783411026
step 51/334, epoch 76/501 --> loss:0.8340983724594117
step 101/334, epoch 76/501 --> loss:0.8206226742267608
step 151/334, epoch 76/501 --> loss:0.8224545550346375
step 201/334, epoch 76/501 --> loss:0.8360483086109162
step 251/334, epoch 76/501 --> loss:0.8240230762958527
step 301/334, epoch 76/501 --> loss:0.8404724264144897
step 51/334, epoch 77/501 --> loss:0.8362916302680969
step 101/334, epoch 77/501 --> loss:0.8217006838321685
step 151/334, epoch 77/501 --> loss:0.8316323256492615
step 201/334, epoch 77/501 --> loss:0.8480254459381104
step 251/334, epoch 77/501 --> loss:0.8257040762901307
step 301/334, epoch 77/501 --> loss:0.8366679406166077
step 51/334, epoch 78/501 --> loss:0.8417405152320862
step 101/334, epoch 78/501 --> loss:0.8361463785171509
step 151/334, epoch 78/501 --> loss:0.8386915767192841
step 201/334, epoch 78/501 --> loss:0.8226949274539948
step 251/334, epoch 78/501 --> loss:0.840627566576004
step 301/334, epoch 78/501 --> loss:0.8208774614334107
step 51/334, epoch 79/501 --> loss:0.828053320646286
step 101/334, epoch 79/501 --> loss:0.8265953660011292
step 151/334, epoch 79/501 --> loss:0.8305011630058289
step 201/334, epoch 79/501 --> loss:0.8207004129886627
step 251/334, epoch 79/501 --> loss:0.8454193711280823
step 301/334, epoch 79/501 --> loss:0.8251024103164672
step 51/334, epoch 80/501 --> loss:0.8240818929672241
step 101/334, epoch 80/501 --> loss:0.8302677285671234
step 151/334, epoch 80/501 --> loss:0.8205898928642273
step 201/334, epoch 80/501 --> loss:0.8303655469417572
step 251/334, epoch 80/501 --> loss:0.8406506097316742
step 301/334, epoch 80/501 --> loss:0.8344752311706543
step 51/334, epoch 81/501 --> loss:0.8289975309371949
step 101/334, epoch 81/501 --> loss:0.8380645036697387
step 151/334, epoch 81/501 --> loss:0.8342792296409607
step 201/334, epoch 81/501 --> loss:0.837161465883255
step 251/334, epoch 81/501 --> loss:0.8235766518115998
step 301/334, epoch 81/501 --> loss:0.8447777009010315

##########train dataset##########
acc--> [96.705228950718]
F1--> {'F1': [0.7101658584408714], 'precision': [0.5644965537390547], 'recall': [0.957180513572435]}
##########eval dataset##########
acc--> [96.5445023717674]
F1--> {'F1': [0.697733841180079], 'precision': [0.5609447827459686], 'recall': [0.922767542342323]}
step 51/334, epoch 82/501 --> loss:0.8279969322681427
step 101/334, epoch 82/501 --> loss:0.8347743439674378
step 151/334, epoch 82/501 --> loss:0.8372299134731293
step 201/334, epoch 82/501 --> loss:0.8355213320255279
step 251/334, epoch 82/501 --> loss:0.8138358604907989
step 301/334, epoch 82/501 --> loss:0.8241759133338928
step 51/334, epoch 83/501 --> loss:0.8381308209896088
step 101/334, epoch 83/501 --> loss:0.8287230551242828
step 151/334, epoch 83/501 --> loss:0.8236167085170746
step 201/334, epoch 83/501 --> loss:0.8250085508823395
step 251/334, epoch 83/501 --> loss:0.8198436737060547
step 301/334, epoch 83/501 --> loss:0.8390987265110016
step 51/334, epoch 84/501 --> loss:0.8390310204029083
step 101/334, epoch 84/501 --> loss:0.8443911969661713
step 151/334, epoch 84/501 --> loss:0.8237862908840179
step 201/334, epoch 84/501 --> loss:0.8215472197532654
step 251/334, epoch 84/501 --> loss:0.8377899181842804
step 301/334, epoch 84/501 --> loss:0.809420086145401
step 51/334, epoch 85/501 --> loss:0.8241999018192291
step 101/334, epoch 85/501 --> loss:0.8323188829421997
step 151/334, epoch 85/501 --> loss:0.8192975640296936
step 201/334, epoch 85/501 --> loss:0.8394889664649964
step 251/334, epoch 85/501 --> loss:0.8211634922027587
step 301/334, epoch 85/501 --> loss:0.8444800627231598
step 51/334, epoch 86/501 --> loss:0.8395537984371185
step 101/334, epoch 86/501 --> loss:0.8440402328968049
step 151/334, epoch 86/501 --> loss:0.8370987367630005
step 201/334, epoch 86/501 --> loss:0.8291785454750061
step 251/334, epoch 86/501 --> loss:0.8178703701496124
step 301/334, epoch 86/501 --> loss:0.8189578127861022
step 51/334, epoch 87/501 --> loss:0.838340972661972
step 101/334, epoch 87/501 --> loss:0.8408942151069642
step 151/334, epoch 87/501 --> loss:0.8226927292346954
step 201/334, epoch 87/501 --> loss:0.8236926245689392
step 251/334, epoch 87/501 --> loss:0.8206308603286743
step 301/334, epoch 87/501 --> loss:0.8342567574977875
step 51/334, epoch 88/501 --> loss:0.8480617153644562
step 101/334, epoch 88/501 --> loss:0.8308553552627563
step 151/334, epoch 88/501 --> loss:0.8243289434909821
step 201/334, epoch 88/501 --> loss:0.8299333095550537
step 251/334, epoch 88/501 --> loss:0.8273698508739471
step 301/334, epoch 88/501 --> loss:0.8312809491157531
step 51/334, epoch 89/501 --> loss:0.8322399914264679
step 101/334, epoch 89/501 --> loss:0.8303497934341431
step 151/334, epoch 89/501 --> loss:0.8183900332450866
step 201/334, epoch 89/501 --> loss:0.8361739265918732
step 251/334, epoch 89/501 --> loss:0.834539006948471
step 301/334, epoch 89/501 --> loss:0.8225591838359833
step 51/334, epoch 90/501 --> loss:0.8380847358703614
step 101/334, epoch 90/501 --> loss:0.8348775243759156
step 151/334, epoch 90/501 --> loss:0.8285453987121582
step 201/334, epoch 90/501 --> loss:0.8324602091312409
step 251/334, epoch 90/501 --> loss:0.8184018766880036
step 301/334, epoch 90/501 --> loss:0.834559223651886
step 51/334, epoch 91/501 --> loss:0.8189594686031342
step 101/334, epoch 91/501 --> loss:0.8352755117416382
step 151/334, epoch 91/501 --> loss:0.8298570024967193
step 201/334, epoch 91/501 --> loss:0.8411531937122345
step 251/334, epoch 91/501 --> loss:0.82122642993927
step 301/334, epoch 91/501 --> loss:0.8398174285888672

##########train dataset##########
acc--> [97.14176963230881]
F1--> {'F1': [0.7402570062395588], 'precision': [0.600112258531052], 'recall': [0.9658175762968357]}
##########eval dataset##########
acc--> [96.91272178242086]
F1--> {'F1': [0.7223269811461915], 'precision': [0.5908456419151576], 'recall': [0.9290904182671085]}
save model!
step 51/334, epoch 92/501 --> loss:0.8518956077098846
step 101/334, epoch 92/501 --> loss:0.8202877414226531
step 151/334, epoch 92/501 --> loss:0.8189963686466217
step 201/334, epoch 92/501 --> loss:0.8280641853809356
step 251/334, epoch 92/501 --> loss:0.8249535369873047
step 301/334, epoch 92/501 --> loss:0.8364867484569549
step 51/334, epoch 93/501 --> loss:0.8205796778202057
step 101/334, epoch 93/501 --> loss:0.8337536597251892
step 151/334, epoch 93/501 --> loss:0.8362534677982331
step 201/334, epoch 93/501 --> loss:0.8243923580646515
step 251/334, epoch 93/501 --> loss:0.8260430312156677
step 301/334, epoch 93/501 --> loss:0.8285897886753082
step 51/334, epoch 94/501 --> loss:0.8198630344867707
step 101/334, epoch 94/501 --> loss:0.8345604228973389
step 151/334, epoch 94/501 --> loss:0.8271373438835145
step 201/334, epoch 94/501 --> loss:0.8356170988082886
step 251/334, epoch 94/501 --> loss:0.8261444735527038
step 301/334, epoch 94/501 --> loss:0.8263002073764801
step 51/334, epoch 95/501 --> loss:0.8258052027225494
step 101/334, epoch 95/501 --> loss:0.8246792888641358
step 151/334, epoch 95/501 --> loss:0.829930555820465
step 201/334, epoch 95/501 --> loss:0.8325800132751465
step 251/334, epoch 95/501 --> loss:0.8217975807189941
step 301/334, epoch 95/501 --> loss:0.8382755708694458
step 51/334, epoch 96/501 --> loss:0.8250750124454498
step 101/334, epoch 96/501 --> loss:0.8502621150016785
step 151/334, epoch 96/501 --> loss:0.824007179737091
step 201/334, epoch 96/501 --> loss:0.8257994365692138
step 251/334, epoch 96/501 --> loss:0.8385751366615295
step 301/334, epoch 96/501 --> loss:0.8205536389350891
step 51/334, epoch 97/501 --> loss:0.8141826391220093
step 101/334, epoch 97/501 --> loss:0.8300678265094757
step 151/334, epoch 97/501 --> loss:0.8212226057052612
step 201/334, epoch 97/501 --> loss:0.8283007001876831
step 251/334, epoch 97/501 --> loss:0.8333949613571167
step 301/334, epoch 97/501 --> loss:0.8401777637004852
step 51/334, epoch 98/501 --> loss:0.8333993279933929
step 101/334, epoch 98/501 --> loss:0.8271401858329773
step 151/334, epoch 98/501 --> loss:0.8271402204036713
step 201/334, epoch 98/501 --> loss:0.8381503403186799
step 251/334, epoch 98/501 --> loss:0.8254447090625763
step 301/334, epoch 98/501 --> loss:0.8151572906970977
step 51/334, epoch 99/501 --> loss:0.828568559885025
step 101/334, epoch 99/501 --> loss:0.8260067129135131
step 151/334, epoch 99/501 --> loss:0.8261863660812377
step 201/334, epoch 99/501 --> loss:0.8266827011108399
step 251/334, epoch 99/501 --> loss:0.8271063649654389
step 301/334, epoch 99/501 --> loss:0.8322026419639588
step 51/334, epoch 100/501 --> loss:0.8310678184032441
step 101/334, epoch 100/501 --> loss:0.8244187164306641
step 151/334, epoch 100/501 --> loss:0.8324890327453613
step 201/334, epoch 100/501 --> loss:0.8237756621837616
step 251/334, epoch 100/501 --> loss:0.8371602296829224
step 301/334, epoch 100/501 --> loss:0.8238290166854858
step 51/334, epoch 101/501 --> loss:0.8265370285511017
step 101/334, epoch 101/501 --> loss:0.8342387413978577
step 151/334, epoch 101/501 --> loss:0.8368468034267426
step 201/334, epoch 101/501 --> loss:0.8335698843002319
step 251/334, epoch 101/501 --> loss:0.8169127225875854
step 301/334, epoch 101/501 --> loss:0.8185015940666198

##########train dataset##########
acc--> [96.0552646106282]
F1--> {'F1': [0.6716877314049453], 'precision': [0.5174680569112371], 'recall': [0.9568760080506695]}
##########eval dataset##########
acc--> [95.87900623978369]
F1--> {'F1': [0.6595834935447384], 'precision': [0.5129204974716638], 'recall': [0.9237230314084812]}
step 51/334, epoch 102/501 --> loss:0.826298838853836
step 101/334, epoch 102/501 --> loss:0.8298671412467956
step 151/334, epoch 102/501 --> loss:0.8282541370391846
step 201/334, epoch 102/501 --> loss:0.8307671868801116
step 251/334, epoch 102/501 --> loss:0.8236760056018829
step 301/334, epoch 102/501 --> loss:0.829211333990097
step 51/334, epoch 103/501 --> loss:0.8380710196495056
step 101/334, epoch 103/501 --> loss:0.8257758367061615
step 151/334, epoch 103/501 --> loss:0.8196543943881989
step 201/334, epoch 103/501 --> loss:0.8326232063770295
step 251/334, epoch 103/501 --> loss:0.8234088218212128
step 301/334, epoch 103/501 --> loss:0.8159439444541932
step 51/334, epoch 104/501 --> loss:0.8173882067203522
step 101/334, epoch 104/501 --> loss:0.8260530042648315
step 151/334, epoch 104/501 --> loss:0.8413793158531189
step 201/334, epoch 104/501 --> loss:0.8324215722084045
step 251/334, epoch 104/501 --> loss:0.8251265037059784
step 301/334, epoch 104/501 --> loss:0.8292085206508637
step 51/334, epoch 105/501 --> loss:0.830756425857544
step 101/334, epoch 105/501 --> loss:0.8237682199478149
step 151/334, epoch 105/501 --> loss:0.8267270731925964
step 201/334, epoch 105/501 --> loss:0.8402211332321167
step 251/334, epoch 105/501 --> loss:0.8354282486438751
step 301/334, epoch 105/501 --> loss:0.8304717421531678
step 51/334, epoch 106/501 --> loss:0.8279885196685791
step 101/334, epoch 106/501 --> loss:0.8336029696464539
step 151/334, epoch 106/501 --> loss:0.8205524027347565
step 201/334, epoch 106/501 --> loss:0.8424487435817718
step 251/334, epoch 106/501 --> loss:0.8163040900230407
step 301/334, epoch 106/501 --> loss:0.8253531563282013
step 51/334, epoch 107/501 --> loss:0.8129696810245514
step 101/334, epoch 107/501 --> loss:0.827347195148468
step 151/334, epoch 107/501 --> loss:0.8363753569126129
step 201/334, epoch 107/501 --> loss:0.8346183729171753
step 251/334, epoch 107/501 --> loss:0.8307329249382019
step 301/334, epoch 107/501 --> loss:0.8224656343460083
step 51/334, epoch 108/501 --> loss:0.8288345062732696
step 101/334, epoch 108/501 --> loss:0.8317571055889129
step 151/334, epoch 108/501 --> loss:0.8204013597965241
step 201/334, epoch 108/501 --> loss:0.833426069021225
step 251/334, epoch 108/501 --> loss:0.8236596834659576
step 301/334, epoch 108/501 --> loss:0.8360602402687073
step 51/334, epoch 109/501 --> loss:0.8270994770526886
step 101/334, epoch 109/501 --> loss:0.8287208640575409
step 151/334, epoch 109/501 --> loss:0.8290509188175201
step 201/334, epoch 109/501 --> loss:0.8179921758174896
step 251/334, epoch 109/501 --> loss:0.8224938213825226
step 301/334, epoch 109/501 --> loss:0.8312814021110535
step 51/334, epoch 110/501 --> loss:0.8203234052658082
step 101/334, epoch 110/501 --> loss:0.8425608694553375
step 151/334, epoch 110/501 --> loss:0.8299115324020385
step 201/334, epoch 110/501 --> loss:0.8312455141544342
step 251/334, epoch 110/501 --> loss:0.8241413760185242
step 301/334, epoch 110/501 --> loss:0.8343295860290527
step 51/334, epoch 111/501 --> loss:0.8153984546661377
step 101/334, epoch 111/501 --> loss:0.843830326795578
step 151/334, epoch 111/501 --> loss:0.8254360258579254
step 201/334, epoch 111/501 --> loss:0.8201902961730957
step 251/334, epoch 111/501 --> loss:0.8402220475673675
step 301/334, epoch 111/501 --> loss:0.8361571609973908

##########train dataset##########
acc--> [98.04248731145874]
F1--> {'F1': [0.8049453277828038], 'precision': [0.6941693267288888], 'recall': [0.9578039053623715]}
##########eval dataset##########
acc--> [97.77623441066406]
F1--> {'F1': [0.7780360570911093], 'precision': [0.6841747951510603], 'recall': [0.9017588288133284]}
save model!
step 51/334, epoch 112/501 --> loss:0.8413733434677124
step 101/334, epoch 112/501 --> loss:0.8339340269565583
step 151/334, epoch 112/501 --> loss:0.8235806000232696
step 201/334, epoch 112/501 --> loss:0.8061737859249115
step 251/334, epoch 112/501 --> loss:0.826186112165451
step 301/334, epoch 112/501 --> loss:0.8272111248970032
step 51/334, epoch 113/501 --> loss:0.8329985666275025
step 101/334, epoch 113/501 --> loss:0.8193707501888275
step 151/334, epoch 113/501 --> loss:0.832099369764328
step 201/334, epoch 113/501 --> loss:0.8262077438831329
step 251/334, epoch 113/501 --> loss:0.8259395623207092
step 301/334, epoch 113/501 --> loss:0.8341487658023834
step 51/334, epoch 114/501 --> loss:0.8307859981060028
step 101/334, epoch 114/501 --> loss:0.8277154195308686
step 151/334, epoch 114/501 --> loss:0.8363918483257293
step 201/334, epoch 114/501 --> loss:0.8374016153812408
step 251/334, epoch 114/501 --> loss:0.8208901953697204
step 301/334, epoch 114/501 --> loss:0.8192301213741302
step 51/334, epoch 115/501 --> loss:0.8223340368270874
step 101/334, epoch 115/501 --> loss:0.8326950192451477
step 151/334, epoch 115/501 --> loss:0.8446715068817139
step 201/334, epoch 115/501 --> loss:0.8172750198841094
step 251/334, epoch 115/501 --> loss:0.8326736426353455
step 301/334, epoch 115/501 --> loss:0.811446237564087
step 51/334, epoch 116/501 --> loss:0.8235090863704682
step 101/334, epoch 116/501 --> loss:0.8345539152622223
step 151/334, epoch 116/501 --> loss:0.8308962941169739
step 201/334, epoch 116/501 --> loss:0.8185325348377228
step 251/334, epoch 116/501 --> loss:0.8229777598381043
step 301/334, epoch 116/501 --> loss:0.8294487714767456
step 51/334, epoch 117/501 --> loss:0.832005809545517
step 101/334, epoch 117/501 --> loss:0.8274495470523834
step 151/334, epoch 117/501 --> loss:0.8297059035301209
step 201/334, epoch 117/501 --> loss:0.8108337056636811
step 251/334, epoch 117/501 --> loss:0.8342853939533234
step 301/334, epoch 117/501 --> loss:0.8289567124843598
step 51/334, epoch 118/501 --> loss:0.8297794139385224
step 101/334, epoch 118/501 --> loss:0.8324887847900391
step 151/334, epoch 118/501 --> loss:0.8292716228961945
step 201/334, epoch 118/501 --> loss:0.8284278631210327
step 251/334, epoch 118/501 --> loss:0.8322928082942963
step 301/334, epoch 118/501 --> loss:0.8240479922294617
step 51/334, epoch 119/501 --> loss:0.8318959605693818
step 101/334, epoch 119/501 --> loss:0.8263369143009186
step 151/334, epoch 119/501 --> loss:0.8379731404781342
step 201/334, epoch 119/501 --> loss:0.8313462233543396
step 251/334, epoch 119/501 --> loss:0.8210738861560821
step 301/334, epoch 119/501 --> loss:0.8203329992294311
step 51/334, epoch 120/501 --> loss:0.844232189655304
step 101/334, epoch 120/501 --> loss:0.8259549605846405
step 151/334, epoch 120/501 --> loss:0.8103419649600982
step 201/334, epoch 120/501 --> loss:0.826476331949234
step 251/334, epoch 120/501 --> loss:0.8274644720554352
step 301/334, epoch 120/501 --> loss:0.8231359386444091
step 51/334, epoch 121/501 --> loss:0.8309304070472717
step 101/334, epoch 121/501 --> loss:0.8167003607749939
step 151/334, epoch 121/501 --> loss:0.8213737034797668
step 201/334, epoch 121/501 --> loss:0.834666862487793
step 251/334, epoch 121/501 --> loss:0.8305281007289886
step 301/334, epoch 121/501 --> loss:0.8352901709079742

##########train dataset##########
acc--> [98.05723089468883]
F1--> {'F1': [0.8081906016182578], 'precision': [0.6923638094399678], 'recall': [0.97057081998611]}
##########eval dataset##########
acc--> [97.66655268630929]
F1--> {'F1': [0.7727311533239033], 'precision': [0.667245002701009], 'recall': [0.9178470902432663]}
step 51/334, epoch 122/501 --> loss:0.8086468374729157
step 101/334, epoch 122/501 --> loss:0.8353780734539032
step 151/334, epoch 122/501 --> loss:0.8328135979175567
step 201/334, epoch 122/501 --> loss:0.8220274674892426
step 251/334, epoch 122/501 --> loss:0.8298332607746124
step 301/334, epoch 122/501 --> loss:0.8296657645702362
step 51/334, epoch 123/501 --> loss:0.826197624206543
step 101/334, epoch 123/501 --> loss:0.8203665602207184
step 151/334, epoch 123/501 --> loss:0.820692526102066
step 201/334, epoch 123/501 --> loss:0.8255352556705475
step 251/334, epoch 123/501 --> loss:0.8306534016132354
step 301/334, epoch 123/501 --> loss:0.8286545729637146
step 51/334, epoch 124/501 --> loss:0.8141173362731934
step 101/334, epoch 124/501 --> loss:0.849830584526062
step 151/334, epoch 124/501 --> loss:0.8195732212066651
step 201/334, epoch 124/501 --> loss:0.8244674217700958
step 251/334, epoch 124/501 --> loss:0.8221411025524139
step 301/334, epoch 124/501 --> loss:0.8312908673286438
step 51/334, epoch 125/501 --> loss:0.8363211321830749
step 101/334, epoch 125/501 --> loss:0.8289711320400238
step 151/334, epoch 125/501 --> loss:0.8173477065563202
step 201/334, epoch 125/501 --> loss:0.8247598826885223
step 251/334, epoch 125/501 --> loss:0.8345826423168182
step 301/334, epoch 125/501 --> loss:0.8118697977066041
step 51/334, epoch 126/501 --> loss:0.8345227777957916
step 101/334, epoch 126/501 --> loss:0.8192752611637115
step 151/334, epoch 126/501 --> loss:0.831018078327179
step 201/334, epoch 126/501 --> loss:0.8395056259632111
step 251/334, epoch 126/501 --> loss:0.8377595460414886
step 301/334, epoch 126/501 --> loss:0.8230658400058747
step 51/334, epoch 127/501 --> loss:0.82363356590271
step 101/334, epoch 127/501 --> loss:0.8232557046413421
step 151/334, epoch 127/501 --> loss:0.8252125787734985
step 201/334, epoch 127/501 --> loss:0.8351202297210694
step 251/334, epoch 127/501 --> loss:0.8230187499523163
step 301/334, epoch 127/501 --> loss:0.825749442577362
step 51/334, epoch 128/501 --> loss:0.816013822555542
step 101/334, epoch 128/501 --> loss:0.8289788389205932
step 151/334, epoch 128/501 --> loss:0.8347353196144104
step 201/334, epoch 128/501 --> loss:0.8315256178379059
step 251/334, epoch 128/501 --> loss:0.8272956192493439
step 301/334, epoch 128/501 --> loss:0.8323544538021088
step 51/334, epoch 129/501 --> loss:0.8308343684673309
step 101/334, epoch 129/501 --> loss:0.8291057550907135
step 151/334, epoch 129/501 --> loss:0.8228263413906097
step 201/334, epoch 129/501 --> loss:0.8445697450637817
step 251/334, epoch 129/501 --> loss:0.8188016700744629
step 301/334, epoch 129/501 --> loss:0.8303718614578247
step 51/334, epoch 130/501 --> loss:0.8322986662387848
step 101/334, epoch 130/501 --> loss:0.8041862440109253
step 151/334, epoch 130/501 --> loss:0.8286569678783416
step 201/334, epoch 130/501 --> loss:0.8195561587810516
step 251/334, epoch 130/501 --> loss:0.8356812191009522
step 301/334, epoch 130/501 --> loss:0.8326733195781708
step 51/334, epoch 131/501 --> loss:0.817767471075058
step 101/334, epoch 131/501 --> loss:0.84193718791008
step 151/334, epoch 131/501 --> loss:0.8360278725624084
step 201/334, epoch 131/501 --> loss:0.8261852610111237
step 251/334, epoch 131/501 --> loss:0.8121581947803498
step 301/334, epoch 131/501 --> loss:0.8246190273761749

##########train dataset##########
acc--> [97.95429786135462]
F1--> {'F1': [0.8003504728688866], 'precision': [0.680069389936049], 'recall': [0.972335174769162]}
##########eval dataset##########
acc--> [97.52421341542471]
F1--> {'F1': [0.7622085992513659], 'precision': [0.6515967163028419], 'recall': [0.918066851184133]}
step 51/334, epoch 132/501 --> loss:0.8319339048862457
step 101/334, epoch 132/501 --> loss:0.8198760712146759
step 151/334, epoch 132/501 --> loss:0.815662852525711
step 201/334, epoch 132/501 --> loss:0.819968113899231
step 251/334, epoch 132/501 --> loss:0.8319368529319763
step 301/334, epoch 132/501 --> loss:0.8315930020809174
step 51/334, epoch 133/501 --> loss:0.8336287093162537
step 101/334, epoch 133/501 --> loss:0.8171419215202331
step 151/334, epoch 133/501 --> loss:0.8182360661029816
step 201/334, epoch 133/501 --> loss:0.8422337031364441
step 251/334, epoch 133/501 --> loss:0.8259691488742829
step 301/334, epoch 133/501 --> loss:0.8250406563282013
step 51/334, epoch 134/501 --> loss:0.8177211999893188
step 101/334, epoch 134/501 --> loss:0.8231262910366058
step 151/334, epoch 134/501 --> loss:0.8322997891902923
step 201/334, epoch 134/501 --> loss:0.828365079164505
step 251/334, epoch 134/501 --> loss:0.8268947386741639
step 301/334, epoch 134/501 --> loss:0.8193175768852234
step 51/334, epoch 135/501 --> loss:0.8165731382369995
step 101/334, epoch 135/501 --> loss:0.8390316808223724
step 151/334, epoch 135/501 --> loss:0.8376703917980194
step 201/334, epoch 135/501 --> loss:0.8277714216709137
step 251/334, epoch 135/501 --> loss:0.8400038838386535
step 301/334, epoch 135/501 --> loss:0.8262487292289734
step 51/334, epoch 136/501 --> loss:0.8283821523189545
step 101/334, epoch 136/501 --> loss:0.8438627874851227
step 151/334, epoch 136/501 --> loss:0.8141937839984894
step 201/334, epoch 136/501 --> loss:0.8296906197071076
step 251/334, epoch 136/501 --> loss:0.8235230255126953
step 301/334, epoch 136/501 --> loss:0.8306227684020996
step 51/334, epoch 137/501 --> loss:0.8243286323547363
step 101/334, epoch 137/501 --> loss:0.8300839817523956
step 151/334, epoch 137/501 --> loss:0.8310628986358642
step 201/334, epoch 137/501 --> loss:0.8351341700553894
step 251/334, epoch 137/501 --> loss:0.8315514278411865
step 301/334, epoch 137/501 --> loss:0.8185628962516784
step 51/334, epoch 138/501 --> loss:0.8207043361663818
step 101/334, epoch 138/501 --> loss:0.8203328323364257
step 151/334, epoch 138/501 --> loss:0.821774629354477
step 201/334, epoch 138/501 --> loss:0.8329790699481964
step 251/334, epoch 138/501 --> loss:0.834348076581955
step 301/334, epoch 138/501 --> loss:0.8398311913013459
step 51/334, epoch 139/501 --> loss:0.8188504123687744
step 101/334, epoch 139/501 --> loss:0.8086823487281799
step 151/334, epoch 139/501 --> loss:0.8240238857269288
step 201/334, epoch 139/501 --> loss:0.8430002844333648
step 251/334, epoch 139/501 --> loss:0.8306695020198822
step 301/334, epoch 139/501 --> loss:0.8375224792957305
step 51/334, epoch 140/501 --> loss:0.8270675683021546
step 101/334, epoch 140/501 --> loss:0.8201984429359436
step 151/334, epoch 140/501 --> loss:0.8376558029651642
step 201/334, epoch 140/501 --> loss:0.8340397036075592
step 251/334, epoch 140/501 --> loss:0.8359735584259034
step 301/334, epoch 140/501 --> loss:0.8125274515151978
step 51/334, epoch 141/501 --> loss:0.819020608663559
step 101/334, epoch 141/501 --> loss:0.8371059107780456
step 151/334, epoch 141/501 --> loss:0.8291683852672577
step 201/334, epoch 141/501 --> loss:0.8225486028194428
step 251/334, epoch 141/501 --> loss:0.8169623076915741
step 301/334, epoch 141/501 --> loss:0.8261806702613831

##########train dataset##########
acc--> [97.92009402983892]
F1--> {'F1': [0.7985796453289462], 'precision': [0.6749200720689753], 'recall': [0.9777327030308862]}
##########eval dataset##########
acc--> [97.48773382925482]
F1--> {'F1': [0.7600563799583667], 'precision': [0.6471841133632307], 'recall': [0.9206309347408526]}
step 51/334, epoch 142/501 --> loss:0.831255373954773
step 101/334, epoch 142/501 --> loss:0.8195847404003144
step 151/334, epoch 142/501 --> loss:0.8323858511447907
step 201/334, epoch 142/501 --> loss:0.8250480246543884
step 251/334, epoch 142/501 --> loss:0.821348351240158
step 301/334, epoch 142/501 --> loss:0.8227413427829743
step 51/334, epoch 143/501 --> loss:0.8327691042423249
step 101/334, epoch 143/501 --> loss:0.802819905281067
step 151/334, epoch 143/501 --> loss:0.84930020570755
step 201/334, epoch 143/501 --> loss:0.832667452096939
step 251/334, epoch 143/501 --> loss:0.8255570757389069
step 301/334, epoch 143/501 --> loss:0.8248404753208161
step 51/334, epoch 144/501 --> loss:0.8164593350887298
step 101/334, epoch 144/501 --> loss:0.8354136669635772
step 151/334, epoch 144/501 --> loss:0.8253612399101258
step 201/334, epoch 144/501 --> loss:0.8173737215995789
step 251/334, epoch 144/501 --> loss:0.8300800240039825
step 301/334, epoch 144/501 --> loss:0.82281130194664
step 51/334, epoch 145/501 --> loss:0.820332647562027
step 101/334, epoch 145/501 --> loss:0.8139702320098877
step 151/334, epoch 145/501 --> loss:0.833346712589264
step 201/334, epoch 145/501 --> loss:0.8296687662601471
step 251/334, epoch 145/501 --> loss:0.8296366798877716
step 301/334, epoch 145/501 --> loss:0.8393152153491974
step 51/334, epoch 146/501 --> loss:0.8367356729507446
step 101/334, epoch 146/501 --> loss:0.8411874175071716
step 151/334, epoch 146/501 --> loss:0.8160420954227448
step 201/334, epoch 146/501 --> loss:0.8249191820621491
step 251/334, epoch 146/501 --> loss:0.8272557592391968
step 301/334, epoch 146/501 --> loss:0.8253880262374877
step 51/334, epoch 147/501 --> loss:0.8397652673721313
step 101/334, epoch 147/501 --> loss:0.8307083296775818
step 151/334, epoch 147/501 --> loss:0.8274714767932891
step 201/334, epoch 147/501 --> loss:0.8170930790901184
step 251/334, epoch 147/501 --> loss:0.8222511613368988
step 301/334, epoch 147/501 --> loss:0.835188833475113
step 51/334, epoch 148/501 --> loss:0.8285215389728546
step 101/334, epoch 148/501 --> loss:0.8137897288799286
step 151/334, epoch 148/501 --> loss:0.8345422291755676
step 201/334, epoch 148/501 --> loss:0.8271315705776214
step 251/334, epoch 148/501 --> loss:0.8329340600967408
step 301/334, epoch 148/501 --> loss:0.8114085805416107
step 51/334, epoch 149/501 --> loss:0.8328706812858582
step 101/334, epoch 149/501 --> loss:0.8307861137390137
step 151/334, epoch 149/501 --> loss:0.8137533807754517
step 201/334, epoch 149/501 --> loss:0.8222194075584411
step 251/334, epoch 149/501 --> loss:0.827960866689682
step 301/334, epoch 149/501 --> loss:0.8357114565372467
step 51/334, epoch 150/501 --> loss:0.8223414516448975
step 101/334, epoch 150/501 --> loss:0.8262354671955109
step 151/334, epoch 150/501 --> loss:0.8295850706100464
step 201/334, epoch 150/501 --> loss:0.8178361022472381
step 251/334, epoch 150/501 --> loss:0.8314898800849915
step 301/334, epoch 150/501 --> loss:0.8165981531143188
step 51/334, epoch 151/501 --> loss:0.8215714192390442
step 101/334, epoch 151/501 --> loss:0.8119675743579865
step 151/334, epoch 151/501 --> loss:0.8320353674888611
step 201/334, epoch 151/501 --> loss:0.8373821175098419
step 251/334, epoch 151/501 --> loss:0.8265841591358185
step 301/334, epoch 151/501 --> loss:0.8300765478610992

##########train dataset##########
acc--> [97.85740655327349]
F1--> {'F1': [0.7943520283399547], 'precision': [0.6672559297622987], 'recall': [0.9812726051590511]}
##########eval dataset##########
acc--> [97.41970940002697]
F1--> {'F1': [0.7576616203690721], 'precision': [0.6376832163467279], 'recall': [0.9332647950988936]}
step 51/334, epoch 152/501 --> loss:0.8178051042556763
step 101/334, epoch 152/501 --> loss:0.8289736437797547
step 151/334, epoch 152/501 --> loss:0.8352822983264923
step 201/334, epoch 152/501 --> loss:0.8203713881969452
step 251/334, epoch 152/501 --> loss:0.8146520256996155
step 301/334, epoch 152/501 --> loss:0.82535400390625
step 51/334, epoch 153/501 --> loss:0.8343774557113648
step 101/334, epoch 153/501 --> loss:0.8233098900318145
step 151/334, epoch 153/501 --> loss:0.8367254602909088
step 201/334, epoch 153/501 --> loss:0.8213248968124389
step 251/334, epoch 153/501 --> loss:0.8176705181598664
step 301/334, epoch 153/501 --> loss:0.8130124580860137
step 51/334, epoch 154/501 --> loss:0.8382171666622162
step 101/334, epoch 154/501 --> loss:0.8253833734989167
step 151/334, epoch 154/501 --> loss:0.8262277805805206
step 201/334, epoch 154/501 --> loss:0.8136702632904053
step 251/334, epoch 154/501 --> loss:0.8193355906009674
step 301/334, epoch 154/501 --> loss:0.822973781824112
step 51/334, epoch 155/501 --> loss:0.838991185426712
step 101/334, epoch 155/501 --> loss:0.8286544919013977
step 151/334, epoch 155/501 --> loss:0.825303566455841
step 201/334, epoch 155/501 --> loss:0.8288145434856414
step 251/334, epoch 155/501 --> loss:0.8131886553764344
step 301/334, epoch 155/501 --> loss:0.8257467198371887
step 51/334, epoch 156/501 --> loss:0.8180571901798248
step 101/334, epoch 156/501 --> loss:0.8310428559780121
step 151/334, epoch 156/501 --> loss:0.8077373147010803
step 201/334, epoch 156/501 --> loss:0.841332060098648
step 251/334, epoch 156/501 --> loss:0.8481604480743408
step 301/334, epoch 156/501 --> loss:0.8254636073112488
step 51/334, epoch 157/501 --> loss:0.8283032786846161
step 101/334, epoch 157/501 --> loss:0.8146958994865418
step 151/334, epoch 157/501 --> loss:0.8311199903488159
step 201/334, epoch 157/501 --> loss:0.8355807781219482
step 251/334, epoch 157/501 --> loss:0.8090531814098358
step 301/334, epoch 157/501 --> loss:0.8419299030303955
step 51/334, epoch 158/501 --> loss:0.8136555016040802
step 101/334, epoch 158/501 --> loss:0.8238654398918152
step 151/334, epoch 158/501 --> loss:0.8370227873325348
step 201/334, epoch 158/501 --> loss:0.8271748280525207
step 251/334, epoch 158/501 --> loss:0.8171445631980896
step 301/334, epoch 158/501 --> loss:0.837503707408905
step 51/334, epoch 159/501 --> loss:0.8353620541095733
step 101/334, epoch 159/501 --> loss:0.8368284702301025
step 151/334, epoch 159/501 --> loss:0.8357985877990722
step 201/334, epoch 159/501 --> loss:0.8147808599472046
step 251/334, epoch 159/501 --> loss:0.8063007616996765
step 301/334, epoch 159/501 --> loss:0.8364528107643128
step 51/334, epoch 160/501 --> loss:0.8342121708393097
step 101/334, epoch 160/501 --> loss:0.8196439468860626
step 151/334, epoch 160/501 --> loss:0.8246240401268006
step 201/334, epoch 160/501 --> loss:0.8220130097866059
step 251/334, epoch 160/501 --> loss:0.830898687839508
step 301/334, epoch 160/501 --> loss:0.8175628328323364
step 51/334, epoch 161/501 --> loss:0.8222890436649323
step 101/334, epoch 161/501 --> loss:0.8223572838306427
step 151/334, epoch 161/501 --> loss:0.8465365159511566
step 201/334, epoch 161/501 --> loss:0.8212756562232971
step 251/334, epoch 161/501 --> loss:0.8200589144229888
step 301/334, epoch 161/501 --> loss:0.826474381685257

##########train dataset##########
acc--> [98.0922497297673]
F1--> {'F1': [0.8123588474519158], 'precision': [0.6940658188696232], 'recall': [0.9792727995704639]}
##########eval dataset##########
acc--> [97.61237864522874]
F1--> {'F1': [0.7691466632181788], 'precision': [0.6606558394828976], 'recall': [0.9202845371369424]}
step 51/334, epoch 162/501 --> loss:0.8204632937908173
step 101/334, epoch 162/501 --> loss:0.8265226912498475
step 151/334, epoch 162/501 --> loss:0.8351315653324127
step 201/334, epoch 162/501 --> loss:0.8281107902526855
step 251/334, epoch 162/501 --> loss:0.8197285461425782
step 301/334, epoch 162/501 --> loss:0.8178211975097657
step 51/334, epoch 163/501 --> loss:0.8214412772655487
step 101/334, epoch 163/501 --> loss:0.8307874321937561
step 151/334, epoch 163/501 --> loss:0.8286253035068512
step 201/334, epoch 163/501 --> loss:0.8234869682788849
step 251/334, epoch 163/501 --> loss:0.8238972640037536
step 301/334, epoch 163/501 --> loss:0.8255464470386505
step 51/334, epoch 164/501 --> loss:0.8292610001564026
step 101/334, epoch 164/501 --> loss:0.8166632044315338
step 151/334, epoch 164/501 --> loss:0.8331240367889404
step 201/334, epoch 164/501 --> loss:0.8285424840450287
step 251/334, epoch 164/501 --> loss:0.8293432438373566
step 301/334, epoch 164/501 --> loss:0.8203300487995148
step 51/334, epoch 165/501 --> loss:0.8315395951271057
step 101/334, epoch 165/501 --> loss:0.82947425365448
step 151/334, epoch 165/501 --> loss:0.8170636177062989
step 201/334, epoch 165/501 --> loss:0.8221652555465698
step 251/334, epoch 165/501 --> loss:0.82718554854393
step 301/334, epoch 165/501 --> loss:0.8375674152374267
step 51/334, epoch 166/501 --> loss:0.8348233234882355
step 101/334, epoch 166/501 --> loss:0.8209672248363495
step 151/334, epoch 166/501 --> loss:0.8221489131450653
step 201/334, epoch 166/501 --> loss:0.8139980220794678
step 251/334, epoch 166/501 --> loss:0.8370747303962708
step 301/334, epoch 166/501 --> loss:0.8303119516372681
step 51/334, epoch 167/501 --> loss:0.8218867719173432
step 101/334, epoch 167/501 --> loss:0.8275650417804719
step 151/334, epoch 167/501 --> loss:0.8217692255973816
step 201/334, epoch 167/501 --> loss:0.8316979038715363
step 251/334, epoch 167/501 --> loss:0.8228155922889709
step 301/334, epoch 167/501 --> loss:0.825762380361557
step 51/334, epoch 168/501 --> loss:0.8167804396152496
step 101/334, epoch 168/501 --> loss:0.8357344532012939
step 151/334, epoch 168/501 --> loss:0.8226284301280975
step 201/334, epoch 168/501 --> loss:0.8339461219310761
step 251/334, epoch 168/501 --> loss:0.8358227550983429
step 301/334, epoch 168/501 --> loss:0.8222137773036957
step 51/334, epoch 169/501 --> loss:0.8293315613269806
step 101/334, epoch 169/501 --> loss:0.8155489242076874
step 151/334, epoch 169/501 --> loss:0.842959178686142
step 201/334, epoch 169/501 --> loss:0.8243036651611328
step 251/334, epoch 169/501 --> loss:0.8295293545722962
step 301/334, epoch 169/501 --> loss:0.8333933532238007
step 51/334, epoch 170/501 --> loss:0.8227918803691864
step 101/334, epoch 170/501 --> loss:0.8307123625278473
step 151/334, epoch 170/501 --> loss:0.8262619066238404
step 201/334, epoch 170/501 --> loss:0.825523829460144
step 251/334, epoch 170/501 --> loss:0.8125923919677734
step 301/334, epoch 170/501 --> loss:0.8342394578456879
step 51/334, epoch 171/501 --> loss:0.8460073304176331
step 101/334, epoch 171/501 --> loss:0.8329376888275146
step 151/334, epoch 171/501 --> loss:0.8166975128650665
step 201/334, epoch 171/501 --> loss:0.8174316048622131
step 251/334, epoch 171/501 --> loss:0.8221590149402619
step 301/334, epoch 171/501 --> loss:0.8192405498027802

##########train dataset##########
acc--> [98.34783559083031]
F1--> {'F1': [0.8328222387662075], 'precision': [0.7263591275066933], 'recall': [0.9758674795218674]}
##########eval dataset##########
acc--> [97.93278918595215]
F1--> {'F1': [0.7915683373281643], 'precision': [0.7014752418670237], 'recall': [0.9082265647258391]}
save model!
step 51/334, epoch 172/501 --> loss:0.8197501063346863
step 101/334, epoch 172/501 --> loss:0.8182538330554963
step 151/334, epoch 172/501 --> loss:0.8430357122421265
step 201/334, epoch 172/501 --> loss:0.8193499362468719
step 251/334, epoch 172/501 --> loss:0.8326745986938476
step 301/334, epoch 172/501 --> loss:0.8245446813106537
step 51/334, epoch 173/501 --> loss:0.8439555251598359
step 101/334, epoch 173/501 --> loss:0.8335868990421296
step 151/334, epoch 173/501 --> loss:0.8131451904773712
step 201/334, epoch 173/501 --> loss:0.8282644379138947
step 251/334, epoch 173/501 --> loss:0.8133952713012695
step 301/334, epoch 173/501 --> loss:0.8321971392631531
step 51/334, epoch 174/501 --> loss:0.8445030248165131
step 101/334, epoch 174/501 --> loss:0.8184414184093476
step 151/334, epoch 174/501 --> loss:0.8228182411193847
step 201/334, epoch 174/501 --> loss:0.8248194646835327
step 251/334, epoch 174/501 --> loss:0.8291869294643402
step 301/334, epoch 174/501 --> loss:0.8275263953208923
step 51/334, epoch 175/501 --> loss:0.8298094069957733
step 101/334, epoch 175/501 --> loss:0.8185758090019226
step 151/334, epoch 175/501 --> loss:0.8284060788154602
step 201/334, epoch 175/501 --> loss:0.8283715724945069
step 251/334, epoch 175/501 --> loss:0.8418758130073547
step 301/334, epoch 175/501 --> loss:0.8195119988918305
step 51/334, epoch 176/501 --> loss:0.8312442338466645
step 101/334, epoch 176/501 --> loss:0.8335818696022034
step 151/334, epoch 176/501 --> loss:0.8299347066879272
step 201/334, epoch 176/501 --> loss:0.8274010634422302
step 251/334, epoch 176/501 --> loss:0.814568464756012
step 301/334, epoch 176/501 --> loss:0.8188475108146668
step 51/334, epoch 177/501 --> loss:0.839229006767273
step 101/334, epoch 177/501 --> loss:0.8277229142189025
step 151/334, epoch 177/501 --> loss:0.8136868906021119
step 201/334, epoch 177/501 --> loss:0.8364217710494996
step 251/334, epoch 177/501 --> loss:0.8142004930973052
step 301/334, epoch 177/501 --> loss:0.822360497713089
step 51/334, epoch 178/501 --> loss:0.8214010906219482
step 101/334, epoch 178/501 --> loss:0.8349010360240936
step 151/334, epoch 178/501 --> loss:0.8245167398452758
step 201/334, epoch 178/501 --> loss:0.8218030178546906
step 251/334, epoch 178/501 --> loss:0.8198085367679596
step 301/334, epoch 178/501 --> loss:0.8198026990890503
step 51/334, epoch 179/501 --> loss:0.8133249056339263
step 101/334, epoch 179/501 --> loss:0.8226513946056366
step 151/334, epoch 179/501 --> loss:0.8308525896072387
step 201/334, epoch 179/501 --> loss:0.8319684004783631
step 251/334, epoch 179/501 --> loss:0.8231010293960571
step 301/334, epoch 179/501 --> loss:0.8373861467838287
step 51/334, epoch 180/501 --> loss:0.8254315507411957
step 101/334, epoch 180/501 --> loss:0.8285954773426056
step 151/334, epoch 180/501 --> loss:0.8204494869709015
step 201/334, epoch 180/501 --> loss:0.8264849281311035
step 251/334, epoch 180/501 --> loss:0.8301623082160949
step 301/334, epoch 180/501 --> loss:0.8218336844444275
step 51/334, epoch 181/501 --> loss:0.8217729425430298
step 101/334, epoch 181/501 --> loss:0.8347947716712951
step 151/334, epoch 181/501 --> loss:0.8304289615154267
step 201/334, epoch 181/501 --> loss:0.8247539675235749
step 251/334, epoch 181/501 --> loss:0.8221428489685059
step 301/334, epoch 181/501 --> loss:0.8318881487846375

##########train dataset##########
acc--> [98.30106085143007]
F1--> {'F1': [0.8287392625324307], 'precision': [0.7207699565176049], 'recall': [0.9747683020937792]}
##########eval dataset##########
acc--> [97.77220077714242]
F1--> {'F1': [0.7786863376182374], 'precision': [0.6822954758249834], 'recall': [0.9068062264453386]}
step 51/334, epoch 182/501 --> loss:0.8199140095710754
step 101/334, epoch 182/501 --> loss:0.8251168429851532
step 151/334, epoch 182/501 --> loss:0.8197408854961395
step 201/334, epoch 182/501 --> loss:0.8349733328819275
step 251/334, epoch 182/501 --> loss:0.8287407600879669
step 301/334, epoch 182/501 --> loss:0.8229324388504028
step 51/334, epoch 183/501 --> loss:0.825965564250946
step 101/334, epoch 183/501 --> loss:0.8340686404705048
step 151/334, epoch 183/501 --> loss:0.8163785910606385
step 201/334, epoch 183/501 --> loss:0.8273498487472534
step 251/334, epoch 183/501 --> loss:0.8288288354873657
step 301/334, epoch 183/501 --> loss:0.8233986079692841
step 51/334, epoch 184/501 --> loss:0.8426579916477204
step 101/334, epoch 184/501 --> loss:0.8130827462673187
step 151/334, epoch 184/501 --> loss:0.839405255317688
step 201/334, epoch 184/501 --> loss:0.8190659666061402
step 251/334, epoch 184/501 --> loss:0.8149731826782226
step 301/334, epoch 184/501 --> loss:0.8258614623546601
step 51/334, epoch 185/501 --> loss:0.8250498962402344
step 101/334, epoch 185/501 --> loss:0.828895103931427
step 151/334, epoch 185/501 --> loss:0.8160574817657471
step 201/334, epoch 185/501 --> loss:0.8272680425643921
step 251/334, epoch 185/501 --> loss:0.8138620710372925
step 301/334, epoch 185/501 --> loss:0.827493851184845
step 51/334, epoch 186/501 --> loss:0.8371878921985626
step 101/334, epoch 186/501 --> loss:0.8253433895111084
step 151/334, epoch 186/501 --> loss:0.8180342137813568
step 201/334, epoch 186/501 --> loss:0.8172761833667755
step 251/334, epoch 186/501 --> loss:0.8131445515155792
step 301/334, epoch 186/501 --> loss:0.8415084648132324
step 51/334, epoch 187/501 --> loss:0.8270847809314728
step 101/334, epoch 187/501 --> loss:0.8172634971141816
step 151/334, epoch 187/501 --> loss:0.8131393194198608
step 201/334, epoch 187/501 --> loss:0.8231160986423492
step 251/334, epoch 187/501 --> loss:0.824744975566864
step 301/334, epoch 187/501 --> loss:0.8349725592136383
step 51/334, epoch 188/501 --> loss:0.8196501481533051
step 101/334, epoch 188/501 --> loss:0.82895467877388
step 151/334, epoch 188/501 --> loss:0.8321047258377076
step 201/334, epoch 188/501 --> loss:0.8243882834911347
step 251/334, epoch 188/501 --> loss:0.8174457561969757
step 301/334, epoch 188/501 --> loss:0.8252618622779846
step 51/334, epoch 189/501 --> loss:0.8132789707183838
step 101/334, epoch 189/501 --> loss:0.8315443539619446
step 151/334, epoch 189/501 --> loss:0.8265516448020935
step 201/334, epoch 189/501 --> loss:0.8189924275875091
step 251/334, epoch 189/501 --> loss:0.8331238329410553
step 301/334, epoch 189/501 --> loss:0.831675853729248
step 51/334, epoch 190/501 --> loss:0.816897863149643
step 101/334, epoch 190/501 --> loss:0.8207852804660797
step 151/334, epoch 190/501 --> loss:0.8309101068973541
step 201/334, epoch 190/501 --> loss:0.827466003894806
step 251/334, epoch 190/501 --> loss:0.8336039245128631
step 301/334, epoch 190/501 --> loss:0.8313194513320923
step 51/334, epoch 191/501 --> loss:0.8241910886764526
step 101/334, epoch 191/501 --> loss:0.8404404234886169
step 151/334, epoch 191/501 --> loss:0.8199413847923279
step 201/334, epoch 191/501 --> loss:0.8232677280902863
step 251/334, epoch 191/501 --> loss:0.8194692730903625
step 301/334, epoch 191/501 --> loss:0.8270330464839936

##########train dataset##########
acc--> [98.0053696062012]
F1--> {'F1': [0.805630591297039], 'precision': [0.6838289014716236], 'recall': [0.9802399725999431]}
##########eval dataset##########
acc--> [97.45917332766224]
F1--> {'F1': [0.7576114880978599], 'precision': [0.6445741914135589], 'recall': [0.9187403420225871]}
step 51/334, epoch 192/501 --> loss:0.8155162298679351
step 101/334, epoch 192/501 --> loss:0.8294124686717987
step 151/334, epoch 192/501 --> loss:0.8296000671386718
step 201/334, epoch 192/501 --> loss:0.8392835962772369
step 251/334, epoch 192/501 --> loss:0.7983832204341889
step 301/334, epoch 192/501 --> loss:0.8326272058486939
step 51/334, epoch 193/501 --> loss:0.8242324531078339
step 101/334, epoch 193/501 --> loss:0.8325808382034302
step 151/334, epoch 193/501 --> loss:0.8249858009815216
step 201/334, epoch 193/501 --> loss:0.8187626326084136
step 251/334, epoch 193/501 --> loss:0.8109260666370391
step 301/334, epoch 193/501 --> loss:0.8382684862613679
step 51/334, epoch 194/501 --> loss:0.8261087572574616
step 101/334, epoch 194/501 --> loss:0.8242478203773499
step 151/334, epoch 194/501 --> loss:0.821766095161438
step 201/334, epoch 194/501 --> loss:0.8226139783859253
step 251/334, epoch 194/501 --> loss:0.8251092720031739
step 301/334, epoch 194/501 --> loss:0.8275714325904846
step 51/334, epoch 195/501 --> loss:0.8179342031478882
step 101/334, epoch 195/501 --> loss:0.8280147528648376
step 151/334, epoch 195/501 --> loss:0.8245833873748779
step 201/334, epoch 195/501 --> loss:0.823636292219162
step 251/334, epoch 195/501 --> loss:0.8355244767665863
step 301/334, epoch 195/501 --> loss:0.8221334826946258
step 51/334, epoch 196/501 --> loss:0.8258507299423218
step 101/334, epoch 196/501 --> loss:0.8129356217384338
step 151/334, epoch 196/501 --> loss:0.8169024741649628
step 201/334, epoch 196/501 --> loss:0.8342167615890503
step 251/334, epoch 196/501 --> loss:0.8241582190990449
step 301/334, epoch 196/501 --> loss:0.8383964312076568
step 51/334, epoch 197/501 --> loss:0.8305660271644593
step 101/334, epoch 197/501 --> loss:0.8314077091217041
step 151/334, epoch 197/501 --> loss:0.8169136595726013
step 201/334, epoch 197/501 --> loss:0.8191114568710327
step 251/334, epoch 197/501 --> loss:0.8349358308315277
step 301/334, epoch 197/501 --> loss:0.8208622944355011
step 51/334, epoch 198/501 --> loss:0.8292729914188385
step 101/334, epoch 198/501 --> loss:0.8319612872600556
step 151/334, epoch 198/501 --> loss:0.8269043147563935
step 201/334, epoch 198/501 --> loss:0.8161759161949158
step 251/334, epoch 198/501 --> loss:0.8227367842197418
step 301/334, epoch 198/501 --> loss:0.8176536703109741
step 51/334, epoch 199/501 --> loss:0.8287994265556335
step 101/334, epoch 199/501 --> loss:0.8227355229854584
step 151/334, epoch 199/501 --> loss:0.8243399095535279
step 201/334, epoch 199/501 --> loss:0.8261858773231506
step 251/334, epoch 199/501 --> loss:0.8203641843795776
step 301/334, epoch 199/501 --> loss:0.8288204216957092
step 51/334, epoch 200/501 --> loss:0.82159153342247
step 101/334, epoch 200/501 --> loss:0.8176073253154754
step 151/334, epoch 200/501 --> loss:0.8397382390499115
step 201/334, epoch 200/501 --> loss:0.8391499745845795
step 251/334, epoch 200/501 --> loss:0.8358759641647339
step 301/334, epoch 200/501 --> loss:0.8216661059856415
step 51/334, epoch 201/501 --> loss:0.8155072474479675
step 101/334, epoch 201/501 --> loss:0.8272956204414368
step 151/334, epoch 201/501 --> loss:0.830994987487793
step 201/334, epoch 201/501 --> loss:0.8244872117042541
step 251/334, epoch 201/501 --> loss:0.8250530958175659
step 301/334, epoch 201/501 --> loss:0.8267144536972046

##########train dataset##########
acc--> [98.49531347491194]
F1--> {'F1': [0.8457426642170772], 'precision': [0.7449185676751164], 'recall': [0.9781451319844024]}
##########eval dataset##########
acc--> [98.03915482701458]
F1--> {'F1': [0.7975728651785433], 'precision': [0.7200734778494938], 'recall': [0.8937787113871783]}
save model!
step 51/334, epoch 202/501 --> loss:0.8254875278472901
step 101/334, epoch 202/501 --> loss:0.814025241136551
step 151/334, epoch 202/501 --> loss:0.8236813592910767
step 201/334, epoch 202/501 --> loss:0.8195620334148407
step 251/334, epoch 202/501 --> loss:0.8386351835727691
step 301/334, epoch 202/501 --> loss:0.8341270792484283
step 51/334, epoch 203/501 --> loss:0.8124791824817658
step 101/334, epoch 203/501 --> loss:0.8267341721057891
step 151/334, epoch 203/501 --> loss:0.8361177206039428
step 201/334, epoch 203/501 --> loss:0.8192912185192108
step 251/334, epoch 203/501 --> loss:0.8213232970237732
step 301/334, epoch 203/501 --> loss:0.8259201395511627
step 51/334, epoch 204/501 --> loss:0.8230064713954925
step 101/334, epoch 204/501 --> loss:0.8270138251781464
step 151/334, epoch 204/501 --> loss:0.8330953061580658
step 201/334, epoch 204/501 --> loss:0.8252426433563232
step 251/334, epoch 204/501 --> loss:0.8240874040126801
step 301/334, epoch 204/501 --> loss:0.821311776638031
step 51/334, epoch 205/501 --> loss:0.8155356359481811
step 101/334, epoch 205/501 --> loss:0.8228526544570923
step 151/334, epoch 205/501 --> loss:0.828898024559021
step 201/334, epoch 205/501 --> loss:0.8253695690631866
step 251/334, epoch 205/501 --> loss:0.8381274521350861
step 301/334, epoch 205/501 --> loss:0.8151696920394897
step 51/334, epoch 206/501 --> loss:0.8107892596721649
step 101/334, epoch 206/501 --> loss:0.829440381526947
step 151/334, epoch 206/501 --> loss:0.8217964458465576
step 201/334, epoch 206/501 --> loss:0.8269105195999146
step 251/334, epoch 206/501 --> loss:0.8232155025005341
step 301/334, epoch 206/501 --> loss:0.8224589955806733
step 51/334, epoch 207/501 --> loss:0.818635984659195
step 101/334, epoch 207/501 --> loss:0.8217329227924347
step 151/334, epoch 207/501 --> loss:0.822827513217926
step 201/334, epoch 207/501 --> loss:0.8305667054653167
step 251/334, epoch 207/501 --> loss:0.8195152926445007
step 301/334, epoch 207/501 --> loss:0.8314279294013978
step 51/334, epoch 208/501 --> loss:0.8251879167556763
step 101/334, epoch 208/501 --> loss:0.8224336647987366
step 151/334, epoch 208/501 --> loss:0.820978010892868
step 201/334, epoch 208/501 --> loss:0.8055663895606995
step 251/334, epoch 208/501 --> loss:0.8189798736572266
step 301/334, epoch 208/501 --> loss:0.8450682151317597
step 51/334, epoch 209/501 --> loss:0.8152504920959472
step 101/334, epoch 209/501 --> loss:0.8363403844833374
step 151/334, epoch 209/501 --> loss:0.8244945931434632
step 201/334, epoch 209/501 --> loss:0.828331948518753
step 251/334, epoch 209/501 --> loss:0.8305280888080597
step 301/334, epoch 209/501 --> loss:0.82394859790802
step 51/334, epoch 210/501 --> loss:0.8175690305233002
step 101/334, epoch 210/501 --> loss:0.8388963210582733
step 151/334, epoch 210/501 --> loss:0.8241296327114105
step 201/334, epoch 210/501 --> loss:0.8184817445278167
step 251/334, epoch 210/501 --> loss:0.834247316122055
step 301/334, epoch 210/501 --> loss:0.8236708641052246
step 51/334, epoch 211/501 --> loss:0.8136413979530335
step 101/334, epoch 211/501 --> loss:0.8347590672969818
step 151/334, epoch 211/501 --> loss:0.823788720369339
step 201/334, epoch 211/501 --> loss:0.8245930862426758
step 251/334, epoch 211/501 --> loss:0.8395241963863372
step 301/334, epoch 211/501 --> loss:0.8332065558433532

##########train dataset##########
acc--> [98.4189989328088]
F1--> {'F1': [0.8387191302340353], 'precision': [0.7359664601018581], 'recall': [0.974832744117766]}
##########eval dataset##########
acc--> [98.0005793266865]
F1--> {'F1': [0.7949160091416869], 'precision': [0.7139811846984507], 'recall': [0.8965583861409816]}
step 51/334, epoch 212/501 --> loss:0.8300923109054565
step 101/334, epoch 212/501 --> loss:0.8243627262115478
step 151/334, epoch 212/501 --> loss:0.8223314201831817
step 201/334, epoch 212/501 --> loss:0.8108017671108246
step 251/334, epoch 212/501 --> loss:0.8334123921394349
step 301/334, epoch 212/501 --> loss:0.8297473442554474
step 51/334, epoch 213/501 --> loss:0.8314645373821259
step 101/334, epoch 213/501 --> loss:0.8400059187412262
step 151/334, epoch 213/501 --> loss:0.8184044933319092
step 201/334, epoch 213/501 --> loss:0.8254979801177978
step 251/334, epoch 213/501 --> loss:0.8192005312442779
step 301/334, epoch 213/501 --> loss:0.8128022003173828
step 51/334, epoch 214/501 --> loss:0.8249156987667083
step 101/334, epoch 214/501 --> loss:0.8289794397354125
step 151/334, epoch 214/501 --> loss:0.8271861720085144
step 201/334, epoch 214/501 --> loss:0.830253517627716
step 251/334, epoch 214/501 --> loss:0.8243321776390076
step 301/334, epoch 214/501 --> loss:0.8165587115287781
step 51/334, epoch 215/501 --> loss:0.8257494843006135
step 101/334, epoch 215/501 --> loss:0.836264796257019
step 151/334, epoch 215/501 --> loss:0.828663934469223
step 201/334, epoch 215/501 --> loss:0.8265224289894104
step 251/334, epoch 215/501 --> loss:0.8191003847122192
step 301/334, epoch 215/501 --> loss:0.8239470553398133
step 51/334, epoch 216/501 --> loss:0.8080112624168396
step 101/334, epoch 216/501 --> loss:0.8199297070503235
step 151/334, epoch 216/501 --> loss:0.8152125763893128
step 201/334, epoch 216/501 --> loss:0.8197413992881775
step 251/334, epoch 216/501 --> loss:0.8518088674545288
step 301/334, epoch 216/501 --> loss:0.8282610154151917
step 51/334, epoch 217/501 --> loss:0.824899697303772
step 101/334, epoch 217/501 --> loss:0.8288783049583435
step 151/334, epoch 217/501 --> loss:0.8287397193908691
step 201/334, epoch 217/501 --> loss:0.8159649121761322
step 251/334, epoch 217/501 --> loss:0.8295712232589721
step 301/334, epoch 217/501 --> loss:0.831417840719223
step 51/334, epoch 218/501 --> loss:0.8082554256916046
step 101/334, epoch 218/501 --> loss:0.8430404913425446
step 151/334, epoch 218/501 --> loss:0.8399133670330048
step 201/334, epoch 218/501 --> loss:0.8218075406551361
step 251/334, epoch 218/501 --> loss:0.8295251321792603
step 301/334, epoch 218/501 --> loss:0.8230921447277069
step 51/334, epoch 219/501 --> loss:0.8358852124214172
step 101/334, epoch 219/501 --> loss:0.8218505561351777
step 151/334, epoch 219/501 --> loss:0.8222108495235443
step 201/334, epoch 219/501 --> loss:0.8292687964439392
step 251/334, epoch 219/501 --> loss:0.8226247227191925
step 301/334, epoch 219/501 --> loss:0.8192110776901245
step 51/334, epoch 220/501 --> loss:0.8104299581050873
step 101/334, epoch 220/501 --> loss:0.8273126816749573
step 151/334, epoch 220/501 --> loss:0.8147894084453583
step 201/334, epoch 220/501 --> loss:0.8319413506984711
step 251/334, epoch 220/501 --> loss:0.8337477481365204
step 301/334, epoch 220/501 --> loss:0.8350588607788086
step 51/334, epoch 221/501 --> loss:0.8060424160957337
step 101/334, epoch 221/501 --> loss:0.818593270778656
step 151/334, epoch 221/501 --> loss:0.8369071590900421
step 201/334, epoch 221/501 --> loss:0.8356085622310638
step 251/334, epoch 221/501 --> loss:0.8227920532226562
step 301/334, epoch 221/501 --> loss:0.8226904284954071

##########train dataset##########
acc--> [98.1290747889579]
F1--> {'F1': [0.8155025010100153], 'precision': [0.6980375402554456], 'recall': [0.9805139529524517]}
##########eval dataset##########
acc--> [97.57210772389371]
F1--> {'F1': [0.7646659627808173], 'precision': [0.6579885347596355], 'recall': [0.912640933477587]}
step 51/334, epoch 222/501 --> loss:0.8263024580478668
step 101/334, epoch 222/501 --> loss:0.8346334230899811
step 151/334, epoch 222/501 --> loss:0.8274195253849029
step 201/334, epoch 222/501 --> loss:0.8268602025508881
step 251/334, epoch 222/501 --> loss:0.8272798538208008
step 301/334, epoch 222/501 --> loss:0.8039573574066162
step 51/334, epoch 223/501 --> loss:0.8232948648929596
step 101/334, epoch 223/501 --> loss:0.8117248904705048
step 151/334, epoch 223/501 --> loss:0.8288100326061248
step 201/334, epoch 223/501 --> loss:0.8315475261211396
step 251/334, epoch 223/501 --> loss:0.8150264370441437
step 301/334, epoch 223/501 --> loss:0.8365863227844238
step 51/334, epoch 224/501 --> loss:0.8222146964073181
step 101/334, epoch 224/501 --> loss:0.8343917810916901
step 151/334, epoch 224/501 --> loss:0.8247565317153931
step 201/334, epoch 224/501 --> loss:0.8201446437835693
step 251/334, epoch 224/501 --> loss:0.8292547035217285
step 301/334, epoch 224/501 --> loss:0.8174884736537933
step 51/334, epoch 225/501 --> loss:0.8239438033103943
step 101/334, epoch 225/501 --> loss:0.8194514751434326
step 151/334, epoch 225/501 --> loss:0.8104452621936798
step 201/334, epoch 225/501 --> loss:0.8354872703552246
step 251/334, epoch 225/501 --> loss:0.830280944108963
step 301/334, epoch 225/501 --> loss:0.822385596036911
step 51/334, epoch 226/501 --> loss:0.8143505370616912
step 101/334, epoch 226/501 --> loss:0.8329576683044434
step 151/334, epoch 226/501 --> loss:0.8355072283744812
step 201/334, epoch 226/501 --> loss:0.8248296296596527
step 251/334, epoch 226/501 --> loss:0.8300057888031006
step 301/334, epoch 226/501 --> loss:0.8237527346611023
step 51/334, epoch 227/501 --> loss:0.8240411722660065
step 101/334, epoch 227/501 --> loss:0.8214314901828765
step 151/334, epoch 227/501 --> loss:0.8192659628391266
step 201/334, epoch 227/501 --> loss:0.8287967228889466
step 251/334, epoch 227/501 --> loss:0.8125155091285705
step 301/334, epoch 227/501 --> loss:0.8345165717601776
step 51/334, epoch 228/501 --> loss:0.8360829973220825
step 101/334, epoch 228/501 --> loss:0.8080902600288391
step 151/334, epoch 228/501 --> loss:0.8353582835197448
step 201/334, epoch 228/501 --> loss:0.8241278016567231
step 251/334, epoch 228/501 --> loss:0.8201134765148163
step 301/334, epoch 228/501 --> loss:0.819784129858017
step 51/334, epoch 229/501 --> loss:0.818957941532135
step 101/334, epoch 229/501 --> loss:0.8232767570018769
step 151/334, epoch 229/501 --> loss:0.8167372453212738
step 201/334, epoch 229/501 --> loss:0.8345111751556397
step 251/334, epoch 229/501 --> loss:0.8244104588031769
step 301/334, epoch 229/501 --> loss:0.8394306099414826
step 51/334, epoch 230/501 --> loss:0.8218385231494904
step 101/334, epoch 230/501 --> loss:0.8304890990257263
step 151/334, epoch 230/501 --> loss:0.8286872053146362
step 201/334, epoch 230/501 --> loss:0.8233825504779816
step 251/334, epoch 230/501 --> loss:0.8283446180820465
step 301/334, epoch 230/501 --> loss:0.8174075293540954
step 51/334, epoch 231/501 --> loss:0.8210350263118744
step 101/334, epoch 231/501 --> loss:0.8216750156879425
step 151/334, epoch 231/501 --> loss:0.8349901378154755
step 201/334, epoch 231/501 --> loss:0.8354761099815369
step 251/334, epoch 231/501 --> loss:0.8165306448936462
step 301/334, epoch 231/501 --> loss:0.8317947351932525

##########train dataset##########
acc--> [97.31142425250763]
F1--> {'F1': [0.7485955623557091], 'precision': [0.6179973595642306], 'recall': [0.9491964859690047]}
##########eval dataset##########
acc--> [97.00729957506495]
F1--> {'F1': [0.7161275059035672], 'precision': [0.6068598199730697], 'recall': [0.873398393689798]}
step 51/334, epoch 232/501 --> loss:0.8321957433223724
step 101/334, epoch 232/501 --> loss:0.8233791327476502
step 151/334, epoch 232/501 --> loss:0.8177007937431335
step 201/334, epoch 232/501 --> loss:0.8002089869976043
step 251/334, epoch 232/501 --> loss:0.8304442465305328
step 301/334, epoch 232/501 --> loss:0.8286960673332214
step 51/334, epoch 233/501 --> loss:0.8270448386669159
step 101/334, epoch 233/501 --> loss:0.8429650592803956
step 151/334, epoch 233/501 --> loss:0.8293626224994659
step 201/334, epoch 233/501 --> loss:0.8308108162879944
step 251/334, epoch 233/501 --> loss:0.8098248100280762
step 301/334, epoch 233/501 --> loss:0.8215930056571961
step 51/334, epoch 234/501 --> loss:0.8376697015762329
step 101/334, epoch 234/501 --> loss:0.8265564835071564
step 151/334, epoch 234/501 --> loss:0.8154662585258484
step 201/334, epoch 234/501 --> loss:0.8233050405979156
step 251/334, epoch 234/501 --> loss:0.8235663974285126
step 301/334, epoch 234/501 --> loss:0.8279937386512757
step 51/334, epoch 235/501 --> loss:0.8198101246356964
step 101/334, epoch 235/501 --> loss:0.8229486870765687
step 151/334, epoch 235/501 --> loss:0.8109039461612702
step 201/334, epoch 235/501 --> loss:0.8326468563079834
step 251/334, epoch 235/501 --> loss:0.8210274112224579
step 301/334, epoch 235/501 --> loss:0.8314173650741578
step 51/334, epoch 236/501 --> loss:0.8326160717010498
step 101/334, epoch 236/501 --> loss:0.833060290813446
step 151/334, epoch 236/501 --> loss:0.8253096532821655
step 201/334, epoch 236/501 --> loss:0.836388213634491
step 251/334, epoch 236/501 --> loss:0.8110896980762482
step 301/334, epoch 236/501 --> loss:0.8098484909534455
step 51/334, epoch 237/501 --> loss:0.8207149624824523
step 101/334, epoch 237/501 --> loss:0.8183864426612854
step 151/334, epoch 237/501 --> loss:0.8264102613925934
step 201/334, epoch 237/501 --> loss:0.826061840057373
step 251/334, epoch 237/501 --> loss:0.8281730651855469
step 301/334, epoch 237/501 --> loss:0.8295753717422485
step 51/334, epoch 238/501 --> loss:0.8161808800697327
step 101/334, epoch 238/501 --> loss:0.823479859828949
step 151/334, epoch 238/501 --> loss:0.8319537401199341
step 201/334, epoch 238/501 --> loss:0.8290892720222474
step 251/334, epoch 238/501 --> loss:0.8258415412902832
step 301/334, epoch 238/501 --> loss:0.8118029212951661
step 51/334, epoch 239/501 --> loss:0.8355263328552246
step 101/334, epoch 239/501 --> loss:0.818938707113266
step 151/334, epoch 239/501 --> loss:0.8206644642353058
step 201/334, epoch 239/501 --> loss:0.8189521133899689
step 251/334, epoch 239/501 --> loss:0.8293690538406372
step 301/334, epoch 239/501 --> loss:0.8147538697719574
step 51/334, epoch 240/501 --> loss:0.8359104931354523
step 101/334, epoch 240/501 --> loss:0.8101825737953186
step 151/334, epoch 240/501 --> loss:0.8292524158954621
step 201/334, epoch 240/501 --> loss:0.8249401307106018
step 251/334, epoch 240/501 --> loss:0.8171467709541321
step 301/334, epoch 240/501 --> loss:0.8168010282516479
step 51/334, epoch 241/501 --> loss:0.8205201244354248
step 101/334, epoch 241/501 --> loss:0.8429342722892761
step 151/334, epoch 241/501 --> loss:0.8266361570358276
step 201/334, epoch 241/501 --> loss:0.8239281678199768
step 251/334, epoch 241/501 --> loss:0.8084337294101716
step 301/334, epoch 241/501 --> loss:0.8238490164279938

##########train dataset##########
acc--> [98.75154269961577]
F1--> {'F1': [0.8688823954099498], 'precision': [0.7798117945357917], 'recall': [0.9809367604635152]}
##########eval dataset##########
acc--> [98.2514268769927]
F1--> {'F1': [0.8167786900603857], 'precision': [0.7464328610886103], 'recall': [0.901775353353506]}
save model!
step 51/334, epoch 242/501 --> loss:0.8200163972377777
step 101/334, epoch 242/501 --> loss:0.8241442477703095
step 151/334, epoch 242/501 --> loss:0.8258539938926697
step 201/334, epoch 242/501 --> loss:0.8270199394226074
step 251/334, epoch 242/501 --> loss:0.8329161560535431
step 301/334, epoch 242/501 --> loss:0.8178722083568573
step 51/334, epoch 243/501 --> loss:0.8338431632518768
step 101/334, epoch 243/501 --> loss:0.815740487575531
step 151/334, epoch 243/501 --> loss:0.8273285090923309
step 201/334, epoch 243/501 --> loss:0.8198745930194855
step 251/334, epoch 243/501 --> loss:0.8284142947196961
step 301/334, epoch 243/501 --> loss:0.8271229493618012
step 51/334, epoch 244/501 --> loss:0.8134822428226471
step 101/334, epoch 244/501 --> loss:0.8348256707191467
step 151/334, epoch 244/501 --> loss:0.8275075006484985
step 201/334, epoch 244/501 --> loss:0.8310333108901977
step 251/334, epoch 244/501 --> loss:0.8261091470718384
step 301/334, epoch 244/501 --> loss:0.8383413898944855
step 51/334, epoch 245/501 --> loss:0.8246118938922882
step 101/334, epoch 245/501 --> loss:0.8265891826152801
step 151/334, epoch 245/501 --> loss:0.8263643229007721
step 201/334, epoch 245/501 --> loss:0.8200776767730713
step 251/334, epoch 245/501 --> loss:0.8331102848052978
step 301/334, epoch 245/501 --> loss:0.828296000957489
step 51/334, epoch 246/501 --> loss:0.8329485142230988
step 101/334, epoch 246/501 --> loss:0.8258702051639557
step 151/334, epoch 246/501 --> loss:0.8126208472251892
step 201/334, epoch 246/501 --> loss:0.8372333073616027
step 251/334, epoch 246/501 --> loss:0.808442280292511
step 301/334, epoch 246/501 --> loss:0.8299128544330597
step 51/334, epoch 247/501 --> loss:0.8270529997348786
step 101/334, epoch 247/501 --> loss:0.8160884130001068
step 151/334, epoch 247/501 --> loss:0.8359428870677948
step 201/334, epoch 247/501 --> loss:0.8323885202407837
step 251/334, epoch 247/501 --> loss:0.8152471899986267
step 301/334, epoch 247/501 --> loss:0.8143869233131409
step 51/334, epoch 248/501 --> loss:0.8132457768917084
step 101/334, epoch 248/501 --> loss:0.8069285190105439
step 151/334, epoch 248/501 --> loss:0.8198458433151246
step 201/334, epoch 248/501 --> loss:0.8333278548717499
step 251/334, epoch 248/501 --> loss:0.840076562166214
step 301/334, epoch 248/501 --> loss:0.8322646808624268
step 51/334, epoch 249/501 --> loss:0.8236437726020813
step 101/334, epoch 249/501 --> loss:0.8327530550956727
step 151/334, epoch 249/501 --> loss:0.8109229016304016
step 201/334, epoch 249/501 --> loss:0.8208069396018982
step 251/334, epoch 249/501 --> loss:0.8266395938396454
step 301/334, epoch 249/501 --> loss:0.8355848002433777
step 51/334, epoch 250/501 --> loss:0.8207082521915435
step 101/334, epoch 250/501 --> loss:0.827176777124405
step 151/334, epoch 250/501 --> loss:0.8213475251197815
step 201/334, epoch 250/501 --> loss:0.8240514755249023
step 251/334, epoch 250/501 --> loss:0.8229346311092377
step 301/334, epoch 250/501 --> loss:0.8252263736724853
step 51/334, epoch 251/501 --> loss:0.8292568862438202
step 101/334, epoch 251/501 --> loss:0.8313741028308869
step 151/334, epoch 251/501 --> loss:0.8135579955577851
step 201/334, epoch 251/501 --> loss:0.8281411731243133
step 251/334, epoch 251/501 --> loss:0.8260559225082398
step 301/334, epoch 251/501 --> loss:0.8261162376403809

##########train dataset##########
acc--> [98.46946450266432]
F1--> {'F1': [0.8442614998084144], 'precision': [0.7394194853318112], 'recall': [0.9837598637838173]}
##########eval dataset##########
acc--> [97.9699429355933]
F1--> {'F1': [0.7950290762195104], 'precision': [0.7053013573544112], 'recall': [0.9109276320875824]}
step 51/334, epoch 252/501 --> loss:0.8159218108654023
step 101/334, epoch 252/501 --> loss:0.8176696705818176
step 151/334, epoch 252/501 --> loss:0.8352705836296082
step 201/334, epoch 252/501 --> loss:0.8298052251338959
step 251/334, epoch 252/501 --> loss:0.8332837975025177
step 301/334, epoch 252/501 --> loss:0.8154712188243866
step 51/334, epoch 253/501 --> loss:0.825308004617691
step 101/334, epoch 253/501 --> loss:0.8164250314235687
step 151/334, epoch 253/501 --> loss:0.8262318086624145
step 201/334, epoch 253/501 --> loss:0.8308044385910034
step 251/334, epoch 253/501 --> loss:0.8136347901821136
step 301/334, epoch 253/501 --> loss:0.8279726588726044
step 51/334, epoch 254/501 --> loss:0.83824587225914
step 101/334, epoch 254/501 --> loss:0.819716432094574
step 151/334, epoch 254/501 --> loss:0.8275667464733124
step 201/334, epoch 254/501 --> loss:0.8247860550880433
step 251/334, epoch 254/501 --> loss:0.8052446162700653
step 301/334, epoch 254/501 --> loss:0.8321508026123047
step 51/334, epoch 255/501 --> loss:0.8191168582439423
step 101/334, epoch 255/501 --> loss:0.812038607597351
step 151/334, epoch 255/501 --> loss:0.8196022284030914
step 201/334, epoch 255/501 --> loss:0.8344791388511658
step 251/334, epoch 255/501 --> loss:0.8209027707576751
step 301/334, epoch 255/501 --> loss:0.8303475165367127
step 51/334, epoch 256/501 --> loss:0.8355045795440674
step 101/334, epoch 256/501 --> loss:0.8228203380107879
step 151/334, epoch 256/501 --> loss:0.8252426695823669
step 201/334, epoch 256/501 --> loss:0.8124564111232757
step 251/334, epoch 256/501 --> loss:0.8286857724189758
step 301/334, epoch 256/501 --> loss:0.8215877974033355
step 51/334, epoch 257/501 --> loss:0.8321221327781677
step 101/334, epoch 257/501 --> loss:0.8172422790527344
step 151/334, epoch 257/501 --> loss:0.8226917970180512
step 201/334, epoch 257/501 --> loss:0.8148973035812378
step 251/334, epoch 257/501 --> loss:0.8314875984191894
step 301/334, epoch 257/501 --> loss:0.8265381598472595
step 51/334, epoch 258/501 --> loss:0.8121336472034454
step 101/334, epoch 258/501 --> loss:0.8266624879837036
step 151/334, epoch 258/501 --> loss:0.8441381347179413
step 201/334, epoch 258/501 --> loss:0.8223824155330658
step 251/334, epoch 258/501 --> loss:0.8235832786560059
step 301/334, epoch 258/501 --> loss:0.8129724550247193
step 51/334, epoch 259/501 --> loss:0.8177512884140015
step 101/334, epoch 259/501 --> loss:0.8073378431797028
step 151/334, epoch 259/501 --> loss:0.829042032957077
step 201/334, epoch 259/501 --> loss:0.8289377474784851
step 251/334, epoch 259/501 --> loss:0.8355049359798431
step 301/334, epoch 259/501 --> loss:0.8217198956012726
step 51/334, epoch 260/501 --> loss:0.8280652570724487
step 101/334, epoch 260/501 --> loss:0.822192690372467
step 151/334, epoch 260/501 --> loss:0.8454966986179352
step 201/334, epoch 260/501 --> loss:0.8156055521965027
step 251/334, epoch 260/501 --> loss:0.8346339988708497
step 301/334, epoch 260/501 --> loss:0.8206307208538055
step 51/334, epoch 261/501 --> loss:0.8287233459949493
step 101/334, epoch 261/501 --> loss:0.8341064333915711
step 151/334, epoch 261/501 --> loss:0.8329575705528259
step 201/334, epoch 261/501 --> loss:0.8135456705093383
step 251/334, epoch 261/501 --> loss:0.8172754240036011
step 301/334, epoch 261/501 --> loss:0.8195686388015747

##########train dataset##########
acc--> [98.711629266096]
F1--> {'F1': [0.86560019601139], 'precision': [0.7727368531418811], 'recall': [0.9838445202532233]}
##########eval dataset##########
acc--> [98.17687440939953]
F1--> {'F1': [0.8083345693714653], 'precision': [0.7407460993741063], 'recall': [0.8895075038386515]}
step 51/334, epoch 262/501 --> loss:0.8237983500957489
step 101/334, epoch 262/501 --> loss:0.8315458452701568
step 151/334, epoch 262/501 --> loss:0.8178074109554291
step 201/334, epoch 262/501 --> loss:0.8499609506130219
step 251/334, epoch 262/501 --> loss:0.8179010379314423
step 301/334, epoch 262/501 --> loss:0.8194064140319824
step 51/334, epoch 263/501 --> loss:0.8289052832126618
step 101/334, epoch 263/501 --> loss:0.8359842729568482
step 151/334, epoch 263/501 --> loss:0.8194166123867035
step 201/334, epoch 263/501 --> loss:0.8161577880382538
step 251/334, epoch 263/501 --> loss:0.8202328777313233
step 301/334, epoch 263/501 --> loss:0.8326869642734528
step 51/334, epoch 264/501 --> loss:0.8184468376636506
step 101/334, epoch 264/501 --> loss:0.8317113888263702
step 151/334, epoch 264/501 --> loss:0.8379758870601655
step 201/334, epoch 264/501 --> loss:0.8122858667373657
step 251/334, epoch 264/501 --> loss:0.8270325267314911
step 301/334, epoch 264/501 --> loss:0.8196156775951385
step 51/334, epoch 265/501 --> loss:0.8342338609695434
step 101/334, epoch 265/501 --> loss:0.8170159232616424
step 151/334, epoch 265/501 --> loss:0.8298890936374664
step 201/334, epoch 265/501 --> loss:0.8211148202419281
step 251/334, epoch 265/501 --> loss:0.8211479663848877
step 301/334, epoch 265/501 --> loss:0.8237146925926209
step 51/334, epoch 266/501 --> loss:0.8172853684425354
step 101/334, epoch 266/501 --> loss:0.8232465636730194
step 151/334, epoch 266/501 --> loss:0.8163357329368591
step 201/334, epoch 266/501 --> loss:0.8240693187713624
step 251/334, epoch 266/501 --> loss:0.8257302057743072
step 301/334, epoch 266/501 --> loss:0.8382532167434692
step 51/334, epoch 267/501 --> loss:0.816532028913498
step 101/334, epoch 267/501 --> loss:0.8209578561782837
step 151/334, epoch 267/501 --> loss:0.8272060775756835
step 201/334, epoch 267/501 --> loss:0.806648166179657
step 251/334, epoch 267/501 --> loss:0.8459064996242524
step 301/334, epoch 267/501 --> loss:0.8306364369392395
step 51/334, epoch 268/501 --> loss:0.8258344686031341
step 101/334, epoch 268/501 --> loss:0.8102599859237671
step 151/334, epoch 268/501 --> loss:0.8287934410572052
step 201/334, epoch 268/501 --> loss:0.8224382996559143
step 251/334, epoch 268/501 --> loss:0.8244096899032592
step 301/334, epoch 268/501 --> loss:0.8396030139923095
step 51/334, epoch 269/501 --> loss:0.83001411318779
step 101/334, epoch 269/501 --> loss:0.814153915643692
step 151/334, epoch 269/501 --> loss:0.8145665276050568
step 201/334, epoch 269/501 --> loss:0.8146503460407257
step 251/334, epoch 269/501 --> loss:0.8320942103862763
step 301/334, epoch 269/501 --> loss:0.8265997314453125
step 51/334, epoch 270/501 --> loss:0.8264159536361695
step 101/334, epoch 270/501 --> loss:0.8208336007595062
step 151/334, epoch 270/501 --> loss:0.812953850030899
step 201/334, epoch 270/501 --> loss:0.8251834678649902
step 251/334, epoch 270/501 --> loss:0.8296372699737549
step 301/334, epoch 270/501 --> loss:0.8220339620113373
step 51/334, epoch 271/501 --> loss:0.830189757347107
step 101/334, epoch 271/501 --> loss:0.8128317153453827
step 151/334, epoch 271/501 --> loss:0.8403476631641388
step 201/334, epoch 271/501 --> loss:0.8174622201919556
step 251/334, epoch 271/501 --> loss:0.8248589849472046
step 301/334, epoch 271/501 --> loss:0.8237599158287048

##########train dataset##########
acc--> [98.49219792426177]
F1--> {'F1': [0.8454234604548675], 'precision': [0.7446367371529066], 'recall': [0.9777771341105825]}
##########eval dataset##########
acc--> [97.96916731385706]
F1--> {'F1': [0.7923641255410468], 'precision': [0.7098707584665458], 'recall': [0.8965641002343141]}
step 51/334, epoch 272/501 --> loss:0.8102286100387573
step 101/334, epoch 272/501 --> loss:0.8219896054267883
step 151/334, epoch 272/501 --> loss:0.8266891503334045
step 201/334, epoch 272/501 --> loss:0.830782276391983
step 251/334, epoch 272/501 --> loss:0.823379967212677
step 301/334, epoch 272/501 --> loss:0.8214280378818511
step 51/334, epoch 273/501 --> loss:0.8358713805675506
step 101/334, epoch 273/501 --> loss:0.8276743090152741
step 151/334, epoch 273/501 --> loss:0.8227296197414398
step 201/334, epoch 273/501 --> loss:0.8113037955760956
step 251/334, epoch 273/501 --> loss:0.832333791255951
step 301/334, epoch 273/501 --> loss:0.8236791968345643
step 51/334, epoch 274/501 --> loss:0.8184639132022857
step 101/334, epoch 274/501 --> loss:0.8287088811397553
step 151/334, epoch 274/501 --> loss:0.8210895657539368
step 201/334, epoch 274/501 --> loss:0.8303976595401764
step 251/334, epoch 274/501 --> loss:0.813859555721283
step 301/334, epoch 274/501 --> loss:0.8226102030277253
step 51/334, epoch 275/501 --> loss:0.8171862483024597
step 101/334, epoch 275/501 --> loss:0.8200755906105042
step 151/334, epoch 275/501 --> loss:0.8273484003543854
step 201/334, epoch 275/501 --> loss:0.8310775804519653
step 251/334, epoch 275/501 --> loss:0.8295734238624572
step 301/334, epoch 275/501 --> loss:0.8118921601772309
step 51/334, epoch 276/501 --> loss:0.8306024944782258
step 101/334, epoch 276/501 --> loss:0.8236124956607819
step 151/334, epoch 276/501 --> loss:0.8062191724777221
step 201/334, epoch 276/501 --> loss:0.8303223824501038
step 251/334, epoch 276/501 --> loss:0.8226516008377075
step 301/334, epoch 276/501 --> loss:0.8263945257663727
step 51/334, epoch 277/501 --> loss:0.8300660490989685
step 101/334, epoch 277/501 --> loss:0.8263717246055603
step 151/334, epoch 277/501 --> loss:0.8189876616001129
step 201/334, epoch 277/501 --> loss:0.8217259931564331
step 251/334, epoch 277/501 --> loss:0.8226243841648102
step 301/334, epoch 277/501 --> loss:0.8292890286445618
step 51/334, epoch 278/501 --> loss:0.8309181010723115
step 101/334, epoch 278/501 --> loss:0.8128084528446198
step 151/334, epoch 278/501 --> loss:0.8259718072414398
step 201/334, epoch 278/501 --> loss:0.8324602591991425
step 251/334, epoch 278/501 --> loss:0.8225846469402314
step 301/334, epoch 278/501 --> loss:0.828201036453247
step 51/334, epoch 279/501 --> loss:0.8251846671104431
step 101/334, epoch 279/501 --> loss:0.8230889821052552
step 151/334, epoch 279/501 --> loss:0.8281815385818482
step 201/334, epoch 279/501 --> loss:0.8137594234943389
step 251/334, epoch 279/501 --> loss:0.8258225834369659
step 301/334, epoch 279/501 --> loss:0.8235752141475677
step 51/334, epoch 280/501 --> loss:0.8319789099693299
step 101/334, epoch 280/501 --> loss:0.8234854817390442
step 151/334, epoch 280/501 --> loss:0.8187079393863678
step 201/334, epoch 280/501 --> loss:0.8242018544673919
step 251/334, epoch 280/501 --> loss:0.8324242854118347
step 301/334, epoch 280/501 --> loss:0.826222550868988
step 51/334, epoch 281/501 --> loss:0.8270461547374726
step 101/334, epoch 281/501 --> loss:0.825268394947052
step 151/334, epoch 281/501 --> loss:0.8301589334011078
step 201/334, epoch 281/501 --> loss:0.8181515526771546
step 251/334, epoch 281/501 --> loss:0.8255603575706482
step 301/334, epoch 281/501 --> loss:0.8161978054046631

##########train dataset##########
acc--> [98.74075913545192]
F1--> {'F1': [0.8682359391413791], 'precision': [0.7769615906329189], 'recall': [0.9838226777987772]}
##########eval dataset##########
acc--> [98.14249540980794]
F1--> {'F1': [0.8074440272407728], 'precision': [0.731434377267269], 'recall': [0.9010956851168547]}
step 51/334, epoch 282/501 --> loss:0.8307048809528351
step 101/334, epoch 282/501 --> loss:0.806516844034195
step 151/334, epoch 282/501 --> loss:0.8249594688415527
step 201/334, epoch 282/501 --> loss:0.8308931803703308
step 251/334, epoch 282/501 --> loss:0.8238042032718659
step 301/334, epoch 282/501 --> loss:0.8292202603816986
step 51/334, epoch 283/501 --> loss:0.8137462842464447
step 101/334, epoch 283/501 --> loss:0.8232780683040619
step 151/334, epoch 283/501 --> loss:0.8163904666900634
step 201/334, epoch 283/501 --> loss:0.8194024956226349
step 251/334, epoch 283/501 --> loss:0.8295321691036225
step 301/334, epoch 283/501 --> loss:0.8324405014514923
step 51/334, epoch 284/501 --> loss:0.8207923305034638
step 101/334, epoch 284/501 --> loss:0.8209822857379914
step 151/334, epoch 284/501 --> loss:0.829884946346283
step 201/334, epoch 284/501 --> loss:0.8315675365924835
step 251/334, epoch 284/501 --> loss:0.8141315960884095
step 301/334, epoch 284/501 --> loss:0.8248803567886352
step 51/334, epoch 285/501 --> loss:0.8326568746566773
step 101/334, epoch 285/501 --> loss:0.8249855530261994
step 151/334, epoch 285/501 --> loss:0.8111618435382844
step 201/334, epoch 285/501 --> loss:0.8178691494464875
step 251/334, epoch 285/501 --> loss:0.8327740883827209
step 301/334, epoch 285/501 --> loss:0.8190527904033661
step 51/334, epoch 286/501 --> loss:0.811452089548111
step 101/334, epoch 286/501 --> loss:0.8232521605491638
step 151/334, epoch 286/501 --> loss:0.82952303647995
step 201/334, epoch 286/501 --> loss:0.8279387450218201
step 251/334, epoch 286/501 --> loss:0.8284712719917298
step 301/334, epoch 286/501 --> loss:0.8278112435340881
step 51/334, epoch 287/501 --> loss:0.8171128582954407
step 101/334, epoch 287/501 --> loss:0.8356556129455567
step 151/334, epoch 287/501 --> loss:0.8305910778045654
step 201/334, epoch 287/501 --> loss:0.8246538937091827
step 251/334, epoch 287/501 --> loss:0.8249147713184357
step 301/334, epoch 287/501 --> loss:0.8184093725681305
step 51/334, epoch 288/501 --> loss:0.8174300360679626
step 101/334, epoch 288/501 --> loss:0.826177384853363
step 151/334, epoch 288/501 --> loss:0.8306762385368347
step 201/334, epoch 288/501 --> loss:0.826529688835144
step 251/334, epoch 288/501 --> loss:0.8230950844287872
step 301/334, epoch 288/501 --> loss:0.8245647704601288
step 51/334, epoch 289/501 --> loss:0.8269658041000366
step 101/334, epoch 289/501 --> loss:0.8384752345085144
step 151/334, epoch 289/501 --> loss:0.8272645258903504
step 201/334, epoch 289/501 --> loss:0.8203619098663331
step 251/334, epoch 289/501 --> loss:0.8192069387435913
step 301/334, epoch 289/501 --> loss:0.8283124387264251
step 51/334, epoch 290/501 --> loss:0.8208754062652588
step 101/334, epoch 290/501 --> loss:0.8300930881500244
step 151/334, epoch 290/501 --> loss:0.8250195932388306
step 201/334, epoch 290/501 --> loss:0.8219373619556427
step 251/334, epoch 290/501 --> loss:0.8271205127239227
step 301/334, epoch 290/501 --> loss:0.8313803267478943
step 51/334, epoch 291/501 --> loss:0.8342671084403992
step 101/334, epoch 291/501 --> loss:0.8068171668052674
step 151/334, epoch 291/501 --> loss:0.8299570679664612
step 201/334, epoch 291/501 --> loss:0.8107820236682892
step 251/334, epoch 291/501 --> loss:0.8315630531311036
step 301/334, epoch 291/501 --> loss:0.8191410505771637

##########train dataset##########
acc--> [98.75839485136707]
F1--> {'F1': [0.869885801312509], 'precision': [0.779363880747929], 'recall': [0.9842116362888202]}
##########eval dataset##########
acc--> [98.18996519553662]
F1--> {'F1': [0.812593573312103], 'precision': [0.7353713329587556], 'recall': [0.9079495084166929]}
step 51/334, epoch 292/501 --> loss:0.8234291553497315
step 101/334, epoch 292/501 --> loss:0.8084188985824585
step 151/334, epoch 292/501 --> loss:0.841455010175705
step 201/334, epoch 292/501 --> loss:0.8198232352733612
step 251/334, epoch 292/501 --> loss:0.827773118019104
step 301/334, epoch 292/501 --> loss:0.8157145345211029
step 51/334, epoch 293/501 --> loss:0.8145262026786804
step 101/334, epoch 293/501 --> loss:0.8265517330169678
step 151/334, epoch 293/501 --> loss:0.8197723793983459
step 201/334, epoch 293/501 --> loss:0.8299990952014923
step 251/334, epoch 293/501 --> loss:0.840978170633316
step 301/334, epoch 293/501 --> loss:0.8223918414115906
step 51/334, epoch 294/501 --> loss:0.8232270705699921
step 101/334, epoch 294/501 --> loss:0.8293513834476471
step 151/334, epoch 294/501 --> loss:0.8231657648086548
step 201/334, epoch 294/501 --> loss:0.8220741713047027
step 251/334, epoch 294/501 --> loss:0.8109678173065186
step 301/334, epoch 294/501 --> loss:0.8319792604446411
step 51/334, epoch 295/501 --> loss:0.8252876162528991
step 101/334, epoch 295/501 --> loss:0.8254668319225311
step 151/334, epoch 295/501 --> loss:0.8230645620822906
step 201/334, epoch 295/501 --> loss:0.8202339661121368
step 251/334, epoch 295/501 --> loss:0.8277273499965667
step 301/334, epoch 295/501 --> loss:0.8236461985111236
step 51/334, epoch 296/501 --> loss:0.8308562064170837
step 101/334, epoch 296/501 --> loss:0.8076000022888183
step 151/334, epoch 296/501 --> loss:0.831102294921875
step 201/334, epoch 296/501 --> loss:0.8266130089759827
step 251/334, epoch 296/501 --> loss:0.8318525755405426
step 301/334, epoch 296/501 --> loss:0.8200939893722534
step 51/334, epoch 297/501 --> loss:0.8152440977096558
step 101/334, epoch 297/501 --> loss:0.8221428906917572
step 151/334, epoch 297/501 --> loss:0.8278938007354736
step 201/334, epoch 297/501 --> loss:0.832004462480545
step 251/334, epoch 297/501 --> loss:0.8244086182117463
step 301/334, epoch 297/501 --> loss:0.828041582107544
step 51/334, epoch 298/501 --> loss:0.8136418449878693
step 101/334, epoch 298/501 --> loss:0.8309038555622101
step 151/334, epoch 298/501 --> loss:0.8155790996551514
step 201/334, epoch 298/501 --> loss:0.8340491676330566
step 251/334, epoch 298/501 --> loss:0.8284313225746155
step 301/334, epoch 298/501 --> loss:0.8229663002490998
step 51/334, epoch 299/501 --> loss:0.8367560184001923
step 101/334, epoch 299/501 --> loss:0.83631844997406
step 151/334, epoch 299/501 --> loss:0.8172993910312653
step 201/334, epoch 299/501 --> loss:0.8063634741306305
step 251/334, epoch 299/501 --> loss:0.8143689262866974
step 301/334, epoch 299/501 --> loss:0.8251418495178222
step 51/334, epoch 300/501 --> loss:0.8274981749057769
step 101/334, epoch 300/501 --> loss:0.8193525469303131
step 151/334, epoch 300/501 --> loss:0.8212400031089783
step 201/334, epoch 300/501 --> loss:0.82544930934906
step 251/334, epoch 300/501 --> loss:0.8149316799640656
step 301/334, epoch 300/501 --> loss:0.8410863423347473
step 51/334, epoch 301/501 --> loss:0.8344069373607635
step 101/334, epoch 301/501 --> loss:0.8332388865947723
step 151/334, epoch 301/501 --> loss:0.8041256666183472
step 201/334, epoch 301/501 --> loss:0.8119968843460083
step 251/334, epoch 301/501 --> loss:0.8211157429218292
step 301/334, epoch 301/501 --> loss:0.8314682710170745

##########train dataset##########
acc--> [98.67978290890846]
F1--> {'F1': [0.8626403132326507], 'precision': [0.7685073309295578], 'recall': [0.9830653144326574]}
##########eval dataset##########
acc--> [98.0804737054287]
F1--> {'F1': [0.802604040055183], 'precision': [0.7223668229630726], 'recall': [0.902905817223601]}
step 51/334, epoch 302/501 --> loss:0.8122619354724884
step 101/334, epoch 302/501 --> loss:0.8340007984638214
step 151/334, epoch 302/501 --> loss:0.8293101835250855
step 201/334, epoch 302/501 --> loss:0.8185061681270599
step 251/334, epoch 302/501 --> loss:0.8229818212985992
step 301/334, epoch 302/501 --> loss:0.8264801383018494
step 51/334, epoch 303/501 --> loss:0.8250719630718231
step 101/334, epoch 303/501 --> loss:0.8134474194049836
step 151/334, epoch 303/501 --> loss:0.8193341529369355
step 201/334, epoch 303/501 --> loss:0.8208206725120545
step 251/334, epoch 303/501 --> loss:0.8277025103569031
step 301/334, epoch 303/501 --> loss:0.8232487607002258
step 51/334, epoch 304/501 --> loss:0.818069179058075
step 101/334, epoch 304/501 --> loss:0.8354444134235383
step 151/334, epoch 304/501 --> loss:0.811703325510025
step 201/334, epoch 304/501 --> loss:0.820623105764389
step 251/334, epoch 304/501 --> loss:0.8229262208938599
step 301/334, epoch 304/501 --> loss:0.8305576407909393
step 51/334, epoch 305/501 --> loss:0.8213663077354432
step 101/334, epoch 305/501 --> loss:0.8006873917579651
step 151/334, epoch 305/501 --> loss:0.8256364011764527
step 201/334, epoch 305/501 --> loss:0.8376116538047791
step 251/334, epoch 305/501 --> loss:0.8224274861812592
step 301/334, epoch 305/501 --> loss:0.8243705379962921
step 51/334, epoch 306/501 --> loss:0.8361860394477845
step 101/334, epoch 306/501 --> loss:0.8248359048366547
step 151/334, epoch 306/501 --> loss:0.8180090963840485
step 201/334, epoch 306/501 --> loss:0.8275479769706726
step 251/334, epoch 306/501 --> loss:0.8243808257579803
step 301/334, epoch 306/501 --> loss:0.8074783432483673
step 51/334, epoch 307/501 --> loss:0.8214720606803894
step 101/334, epoch 307/501 --> loss:0.8167458462715149
step 151/334, epoch 307/501 --> loss:0.8332434344291687
step 201/334, epoch 307/501 --> loss:0.8273454737663269
step 251/334, epoch 307/501 --> loss:0.8244272220134735
step 301/334, epoch 307/501 --> loss:0.8270523726940155
step 51/334, epoch 308/501 --> loss:0.8270002806186676
step 101/334, epoch 308/501 --> loss:0.8227058601379394
step 151/334, epoch 308/501 --> loss:0.8148572468757629
step 201/334, epoch 308/501 --> loss:0.8286612355709075
step 251/334, epoch 308/501 --> loss:0.8342878723144531
step 301/334, epoch 308/501 --> loss:0.8208408427238464
step 51/334, epoch 309/501 --> loss:0.836450321674347
step 101/334, epoch 309/501 --> loss:0.8239790070056915
step 151/334, epoch 309/501 --> loss:0.8197792375087738
step 201/334, epoch 309/501 --> loss:0.8232865965366364
step 251/334, epoch 309/501 --> loss:0.8168629693984986
step 301/334, epoch 309/501 --> loss:0.8244481337070465
step 51/334, epoch 310/501 --> loss:0.8150649285316467
step 101/334, epoch 310/501 --> loss:0.8180669236183167
step 151/334, epoch 310/501 --> loss:0.8265375304222107
step 201/334, epoch 310/501 --> loss:0.825223376750946
step 251/334, epoch 310/501 --> loss:0.8240397095680236
step 301/334, epoch 310/501 --> loss:0.828630781173706
step 51/334, epoch 311/501 --> loss:0.8320451402664184
step 101/334, epoch 311/501 --> loss:0.8203208649158478
step 151/334, epoch 311/501 --> loss:0.8077862381935119
step 201/334, epoch 311/501 --> loss:0.8290961921215058
step 251/334, epoch 311/501 --> loss:0.8168245768547058
step 301/334, epoch 311/501 --> loss:0.8229382681846619

##########train dataset##########
acc--> [98.73627618675678]
F1--> {'F1': [0.8680739516653304], 'precision': [0.7753945313233574], 'recall': [0.9859287788100882]}
##########eval dataset##########
acc--> [98.16682136381466]
F1--> {'F1': [0.8101216038215492], 'precision': [0.7333714437913289], 'recall': [0.9048262158881681]}
step 51/334, epoch 312/501 --> loss:0.8185668838024139
step 101/334, epoch 312/501 --> loss:0.8151325011253356
step 151/334, epoch 312/501 --> loss:0.8140078103542328
step 201/334, epoch 312/501 --> loss:0.8353479313850403
step 251/334, epoch 312/501 --> loss:0.8114788627624512
step 301/334, epoch 312/501 --> loss:0.8292015683650971
step 51/334, epoch 313/501 --> loss:0.8198838543891906
step 101/334, epoch 313/501 --> loss:0.8245260310173035
step 151/334, epoch 313/501 --> loss:0.8224849963188171
step 201/334, epoch 313/501 --> loss:0.8160205793380737
step 251/334, epoch 313/501 --> loss:0.8222286951541901
step 301/334, epoch 313/501 --> loss:0.8235625445842742
step 51/334, epoch 314/501 --> loss:0.8285102152824402
step 101/334, epoch 314/501 --> loss:0.8223549103736878
step 151/334, epoch 314/501 --> loss:0.8359487664699554
step 201/334, epoch 314/501 --> loss:0.8013700985908508
step 251/334, epoch 314/501 --> loss:0.8249125492572784
step 301/334, epoch 314/501 --> loss:0.8286924767494201
step 51/334, epoch 315/501 --> loss:0.8166112089157105
step 101/334, epoch 315/501 --> loss:0.8190451741218567
step 151/334, epoch 315/501 --> loss:0.8148668694496155
step 201/334, epoch 315/501 --> loss:0.8300638747215271
step 251/334, epoch 315/501 --> loss:0.8275660264492035
step 301/334, epoch 315/501 --> loss:0.8282409358024597
step 51/334, epoch 316/501 --> loss:0.8174094629287719
step 101/334, epoch 316/501 --> loss:0.833124281167984
step 151/334, epoch 316/501 --> loss:0.8197269296646118
step 201/334, epoch 316/501 --> loss:0.8137404787540435
step 251/334, epoch 316/501 --> loss:0.8270634138584136
step 301/334, epoch 316/501 --> loss:0.8380409169197083
step 51/334, epoch 317/501 --> loss:0.8076558303833008
step 101/334, epoch 317/501 --> loss:0.8182982122898101
step 151/334, epoch 317/501 --> loss:0.8106054902076721
step 201/334, epoch 317/501 --> loss:0.8324894452095032
step 251/334, epoch 317/501 --> loss:0.839744291305542
step 301/334, epoch 317/501 --> loss:0.8255486524105072
step 51/334, epoch 318/501 --> loss:0.826998655796051
step 101/334, epoch 318/501 --> loss:0.8291483497619629
step 151/334, epoch 318/501 --> loss:0.8207951867580414
step 201/334, epoch 318/501 --> loss:0.8309982216358185
step 251/334, epoch 318/501 --> loss:0.8286217057704925
step 301/334, epoch 318/501 --> loss:0.8138245189189911
step 51/334, epoch 319/501 --> loss:0.8270189929008483
step 101/334, epoch 319/501 --> loss:0.8265427947044373
step 151/334, epoch 319/501 --> loss:0.8047955286502838
step 201/334, epoch 319/501 --> loss:0.8277371144294738
step 251/334, epoch 319/501 --> loss:0.8359660995006561
step 301/334, epoch 319/501 --> loss:0.8293948805332184
step 51/334, epoch 320/501 --> loss:0.8306796777248383
step 101/334, epoch 320/501 --> loss:0.816855845451355
step 151/334, epoch 320/501 --> loss:0.8232692635059357
step 201/334, epoch 320/501 --> loss:0.8359025764465332
step 251/334, epoch 320/501 --> loss:0.8120376026630401
step 301/334, epoch 320/501 --> loss:0.8223661148548126
step 51/334, epoch 321/501 --> loss:0.8236314702033997
step 101/334, epoch 321/501 --> loss:0.820803987979889
step 151/334, epoch 321/501 --> loss:0.8090347397327423
step 201/334, epoch 321/501 --> loss:0.8173455631732941
step 251/334, epoch 321/501 --> loss:0.8318346095085144
step 301/334, epoch 321/501 --> loss:0.8318622434139251

##########train dataset##########
acc--> [98.80059338438124]
F1--> {'F1': [0.8733708652625654], 'precision': [0.7871328244812401], 'recall': [0.9808428107759133]}
##########eval dataset##########
acc--> [98.20151808797291]
F1--> {'F1': [0.8100248454332412], 'precision': [0.7452507211592725], 'recall': [0.887142486938838]}
step 51/334, epoch 322/501 --> loss:0.8076409733295441
step 101/334, epoch 322/501 --> loss:0.8267606258392334
step 151/334, epoch 322/501 --> loss:0.8239769911766053
step 201/334, epoch 322/501 --> loss:0.8195189750194549
step 251/334, epoch 322/501 --> loss:0.8344150388240814
step 301/334, epoch 322/501 --> loss:0.8289221847057342
step 51/334, epoch 323/501 --> loss:0.8338063406944275
step 101/334, epoch 323/501 --> loss:0.8320271301269532
step 151/334, epoch 323/501 --> loss:0.822926071882248
step 201/334, epoch 323/501 --> loss:0.8313151371479034
step 251/334, epoch 323/501 --> loss:0.8189135611057281
step 301/334, epoch 323/501 --> loss:0.8182911539077758
step 51/334, epoch 324/501 --> loss:0.8154451608657837
step 101/334, epoch 324/501 --> loss:0.8243603384494782
step 151/334, epoch 324/501 --> loss:0.8246359848976135
step 201/334, epoch 324/501 --> loss:0.8276397824287415
step 251/334, epoch 324/501 --> loss:0.8218762242794037
step 301/334, epoch 324/501 --> loss:0.8272206842899322
step 51/334, epoch 325/501 --> loss:0.808296011686325
step 101/334, epoch 325/501 --> loss:0.8186511075496674
step 151/334, epoch 325/501 --> loss:0.8322680842876434
step 201/334, epoch 325/501 --> loss:0.8284430825710296
step 251/334, epoch 325/501 --> loss:0.8285165166854859
step 301/334, epoch 325/501 --> loss:0.8275772070884705
step 51/334, epoch 326/501 --> loss:0.8058577525615692
step 101/334, epoch 326/501 --> loss:0.821819167137146
step 151/334, epoch 326/501 --> loss:0.8223587667942047
step 201/334, epoch 326/501 --> loss:0.8276871728897095
step 251/334, epoch 326/501 --> loss:0.812534636259079
step 301/334, epoch 326/501 --> loss:0.8349145805835724
step 51/334, epoch 327/501 --> loss:0.8268356204032898
step 101/334, epoch 327/501 --> loss:0.8223746883869171
step 151/334, epoch 327/501 --> loss:0.8136378967761994
step 201/334, epoch 327/501 --> loss:0.8284463453292846
step 251/334, epoch 327/501 --> loss:0.8287352597713471
step 301/334, epoch 327/501 --> loss:0.817919842004776
step 51/334, epoch 328/501 --> loss:0.8240533113479614
step 101/334, epoch 328/501 --> loss:0.8239842104911804
step 151/334, epoch 328/501 --> loss:0.8159123051166535
step 201/334, epoch 328/501 --> loss:0.8248765468597412
step 251/334, epoch 328/501 --> loss:0.800071622133255
step 301/334, epoch 328/501 --> loss:0.8472729742527008
step 51/334, epoch 329/501 --> loss:0.8288631629943848
step 101/334, epoch 329/501 --> loss:0.8286307096481323
step 151/334, epoch 329/501 --> loss:0.8265766763687133
step 201/334, epoch 329/501 --> loss:0.8256628632545471
step 251/334, epoch 329/501 --> loss:0.8225331711769104
step 301/334, epoch 329/501 --> loss:0.8135325753688812
step 51/334, epoch 330/501 --> loss:0.8133080637454987
step 101/334, epoch 330/501 --> loss:0.8248737239837647
step 151/334, epoch 330/501 --> loss:0.8152866792678833
step 201/334, epoch 330/501 --> loss:0.8110570383071899
step 251/334, epoch 330/501 --> loss:0.8267112922668457
step 301/334, epoch 330/501 --> loss:0.837337189912796
step 51/334, epoch 331/501 --> loss:0.8136640667915345
step 101/334, epoch 331/501 --> loss:0.81203573346138
step 151/334, epoch 331/501 --> loss:0.8301487410068512
step 201/334, epoch 331/501 --> loss:0.8421986377239228
step 251/334, epoch 331/501 --> loss:0.8140737569332123
step 301/334, epoch 331/501 --> loss:0.8285023689270019

##########train dataset##########
acc--> [98.62271778405346]
F1--> {'F1': [0.8578182929351459], 'precision': [0.7595909106760911], 'recall': [0.9852366036387594]}
##########eval dataset##########
acc--> [98.02009869538968]
F1--> {'F1': [0.7968071828592312], 'precision': [0.7159912097253932], 'recall': [0.898200801886673]}
step 51/334, epoch 332/501 --> loss:0.8042958056926728
step 101/334, epoch 332/501 --> loss:0.84289266705513
step 151/334, epoch 332/501 --> loss:0.8268432366847992
step 201/334, epoch 332/501 --> loss:0.8094933784008026
step 251/334, epoch 332/501 --> loss:0.8250841403007507
step 301/334, epoch 332/501 --> loss:0.8283515405654908
step 51/334, epoch 333/501 --> loss:0.8211711347103119
step 101/334, epoch 333/501 --> loss:0.8413232231140136
step 151/334, epoch 333/501 --> loss:0.8298901391029357
step 201/334, epoch 333/501 --> loss:0.8145181846618652
step 251/334, epoch 333/501 --> loss:0.8245208656787872
step 301/334, epoch 333/501 --> loss:0.8193910026550293
step 51/334, epoch 334/501 --> loss:0.8282041394710541
step 101/334, epoch 334/501 --> loss:0.8189489197731018
step 151/334, epoch 334/501 --> loss:0.8384349417686462
step 201/334, epoch 334/501 --> loss:0.8068856835365296
step 251/334, epoch 334/501 --> loss:0.8253769147396087
step 301/334, epoch 334/501 --> loss:0.8320490348339081
step 51/334, epoch 335/501 --> loss:0.8150286877155304
step 101/334, epoch 335/501 --> loss:0.8378963053226471
step 151/334, epoch 335/501 --> loss:0.8173916101455688
step 201/334, epoch 335/501 --> loss:0.8171141171455383
step 251/334, epoch 335/501 --> loss:0.8333547151088715
step 301/334, epoch 335/501 --> loss:0.8183564853668213
step 51/334, epoch 336/501 --> loss:0.8487246680259705
step 101/334, epoch 336/501 --> loss:0.814799622297287
step 151/334, epoch 336/501 --> loss:0.8267247068881989
step 201/334, epoch 336/501 --> loss:0.817597998380661
step 251/334, epoch 336/501 --> loss:0.8077923238277436
step 301/334, epoch 336/501 --> loss:0.8230805790424347
step 51/334, epoch 337/501 --> loss:0.8273304629325867
step 101/334, epoch 337/501 --> loss:0.820998318195343
step 151/334, epoch 337/501 --> loss:0.814565578699112
step 201/334, epoch 337/501 --> loss:0.8292720198631287
step 251/334, epoch 337/501 --> loss:0.8149495625495911
step 301/334, epoch 337/501 --> loss:0.819241384267807
step 51/334, epoch 338/501 --> loss:0.8262460970878601
step 101/334, epoch 338/501 --> loss:0.8339627683162689
step 151/334, epoch 338/501 --> loss:0.8300477302074433
step 201/334, epoch 338/501 --> loss:0.809302179813385
step 251/334, epoch 338/501 --> loss:0.8266540658473969
step 301/334, epoch 338/501 --> loss:0.8161059427261352
step 51/334, epoch 339/501 --> loss:0.8233344960212707
step 101/334, epoch 339/501 --> loss:0.8235043954849243
step 151/334, epoch 339/501 --> loss:0.8245497047901154
step 201/334, epoch 339/501 --> loss:0.8192524123191833
step 251/334, epoch 339/501 --> loss:0.819938393831253
step 301/334, epoch 339/501 --> loss:0.8219861853122711
step 51/334, epoch 340/501 --> loss:0.8202526140213012
step 101/334, epoch 340/501 --> loss:0.8206934249401092
step 151/334, epoch 340/501 --> loss:0.8125725555419921
step 201/334, epoch 340/501 --> loss:0.8280929028987885
step 251/334, epoch 340/501 --> loss:0.8352586793899536
step 301/334, epoch 340/501 --> loss:0.8285844707489014
step 51/334, epoch 341/501 --> loss:0.8276294291019439
step 101/334, epoch 341/501 --> loss:0.8320626819133758
step 151/334, epoch 341/501 --> loss:0.8316215753555298
step 201/334, epoch 341/501 --> loss:0.8193920648097992
step 251/334, epoch 341/501 --> loss:0.8252562081813812
step 301/334, epoch 341/501 --> loss:0.8012331211566925

##########train dataset##########
acc--> [98.86567724032382]
F1--> {'F1': [0.8798394027333781], 'precision': [0.7951073381270437], 'recall': [0.9847971768688781]}
##########eval dataset##########
acc--> [98.26801864076026]
F1--> {'F1': [0.815987828422577], 'precision': [0.7544134691490801], 'recall': [0.8885185023872734]}
step 51/334, epoch 342/501 --> loss:0.8239177811145783
step 101/334, epoch 342/501 --> loss:0.8225434923171997
step 151/334, epoch 342/501 --> loss:0.8206725144386291
step 201/334, epoch 342/501 --> loss:0.8185480618476868
step 251/334, epoch 342/501 --> loss:0.831064784526825
step 301/334, epoch 342/501 --> loss:0.8309216177463532
step 51/334, epoch 343/501 --> loss:0.8112336671352387
step 101/334, epoch 343/501 --> loss:0.8299954271316529
step 151/334, epoch 343/501 --> loss:0.8215669405460357
step 201/334, epoch 343/501 --> loss:0.8251533234119415
step 251/334, epoch 343/501 --> loss:0.8178648746013641
step 301/334, epoch 343/501 --> loss:0.8355913746356964
step 51/334, epoch 344/501 --> loss:0.8208562135696411
step 101/334, epoch 344/501 --> loss:0.8245106887817383
step 151/334, epoch 344/501 --> loss:0.825314552783966
step 201/334, epoch 344/501 --> loss:0.8348746085166932
step 251/334, epoch 344/501 --> loss:0.8223992049694061
step 301/334, epoch 344/501 --> loss:0.8137600076198578
step 51/334, epoch 345/501 --> loss:0.821631475687027
step 101/334, epoch 345/501 --> loss:0.828925678730011
step 151/334, epoch 345/501 --> loss:0.8178962564468384
step 201/334, epoch 345/501 --> loss:0.8178674376010895
step 251/334, epoch 345/501 --> loss:0.8195735120773315
step 301/334, epoch 345/501 --> loss:0.8169945883750915
step 51/334, epoch 346/501 --> loss:0.843450540304184
step 101/334, epoch 346/501 --> loss:0.8075789844989777
step 151/334, epoch 346/501 --> loss:0.8238218891620636
step 201/334, epoch 346/501 --> loss:0.8185708570480347
step 251/334, epoch 346/501 --> loss:0.8383725130558014
step 301/334, epoch 346/501 --> loss:0.814182858467102
step 51/334, epoch 347/501 --> loss:0.8139111125469207
step 101/334, epoch 347/501 --> loss:0.8317837154865265
step 151/334, epoch 347/501 --> loss:0.8230189013481141
step 201/334, epoch 347/501 --> loss:0.8189836096763611
step 251/334, epoch 347/501 --> loss:0.8361777782440185
step 301/334, epoch 347/501 --> loss:0.8119719576835632
step 51/334, epoch 348/501 --> loss:0.8257331538200379
step 101/334, epoch 348/501 --> loss:0.8347800695896148
step 151/334, epoch 348/501 --> loss:0.8213612234592438
step 201/334, epoch 348/501 --> loss:0.8253039145469665
step 251/334, epoch 348/501 --> loss:0.8126674711704254
step 301/334, epoch 348/501 --> loss:0.8188965427875519
step 51/334, epoch 349/501 --> loss:0.8146033561229706
step 101/334, epoch 349/501 --> loss:0.8250387108325958
step 151/334, epoch 349/501 --> loss:0.8149271702766419
step 201/334, epoch 349/501 --> loss:0.8219246971607208
step 251/334, epoch 349/501 --> loss:0.8391818881034852
step 301/334, epoch 349/501 --> loss:0.8117904233932495
step 51/334, epoch 350/501 --> loss:0.8197256195545196
step 101/334, epoch 350/501 --> loss:0.8167660105228424
step 151/334, epoch 350/501 --> loss:0.8149760293960572
step 201/334, epoch 350/501 --> loss:0.8336734426021576
step 251/334, epoch 350/501 --> loss:0.8206173026561737
step 301/334, epoch 350/501 --> loss:0.8280863308906555
step 51/334, epoch 351/501 --> loss:0.8094031417369842
step 101/334, epoch 351/501 --> loss:0.815111311674118
step 151/334, epoch 351/501 --> loss:0.8289725351333618
step 201/334, epoch 351/501 --> loss:0.8262714076042176
step 251/334, epoch 351/501 --> loss:0.8134011149406433
step 301/334, epoch 351/501 --> loss:0.8245424568653107

##########train dataset##########
acc--> [98.76852074856326]
F1--> {'F1': [0.8711415978549424], 'precision': [0.7795576827913561], 'recall': [0.98712170242465]}
##########eval dataset##########
acc--> [98.15337080133004]
F1--> {'F1': [0.8067852231655526], 'precision': [0.7364152326803498], 'recall': [0.8920369939654665]}
step 51/334, epoch 352/501 --> loss:0.8139198815822601
step 101/334, epoch 352/501 --> loss:0.8267923963069915
step 151/334, epoch 352/501 --> loss:0.8303277206420898
step 201/334, epoch 352/501 --> loss:0.8284240603446961
step 251/334, epoch 352/501 --> loss:0.8279703426361084
step 301/334, epoch 352/501 --> loss:0.8157957947254181
step 51/334, epoch 353/501 --> loss:0.8172172927856445
step 101/334, epoch 353/501 --> loss:0.8243606197834015
step 151/334, epoch 353/501 --> loss:0.8226468777656555
step 201/334, epoch 353/501 --> loss:0.8144520246982574
step 251/334, epoch 353/501 --> loss:0.8373494827747345
step 301/334, epoch 353/501 --> loss:0.8213474559783935
step 51/334, epoch 354/501 --> loss:0.8308431589603424
step 101/334, epoch 354/501 --> loss:0.8336408591270447
step 151/334, epoch 354/501 --> loss:0.8300657045841217
step 201/334, epoch 354/501 --> loss:0.8175281083583832
step 251/334, epoch 354/501 --> loss:0.8138676118850708
step 301/334, epoch 354/501 --> loss:0.8121199536323548
step 51/334, epoch 355/501 --> loss:0.8076679015159607
step 101/334, epoch 355/501 --> loss:0.8328606057167053
step 151/334, epoch 355/501 --> loss:0.8111526322364807
step 201/334, epoch 355/501 --> loss:0.8216769182682038
step 251/334, epoch 355/501 --> loss:0.8324882340431213
step 301/334, epoch 355/501 --> loss:0.832925568819046
step 51/334, epoch 356/501 --> loss:0.8196274018287659
step 101/334, epoch 356/501 --> loss:0.8138514924049377
step 151/334, epoch 356/501 --> loss:0.8298320698738099
step 201/334, epoch 356/501 --> loss:0.8446866118907929
step 251/334, epoch 356/501 --> loss:0.8179800438880921
step 301/334, epoch 356/501 --> loss:0.8220135021209717
step 51/334, epoch 357/501 --> loss:0.8137264621257781
step 101/334, epoch 357/501 --> loss:0.8340389788150787
step 151/334, epoch 357/501 --> loss:0.82826465010643
step 201/334, epoch 357/501 --> loss:0.8305354833602905
step 251/334, epoch 357/501 --> loss:0.8132264876365661
step 301/334, epoch 357/501 --> loss:0.8198245143890381
step 51/334, epoch 358/501 --> loss:0.8081647098064423
step 101/334, epoch 358/501 --> loss:0.8263015794754028
step 151/334, epoch 358/501 --> loss:0.8267398607730866
step 201/334, epoch 358/501 --> loss:0.8187435030937195
step 251/334, epoch 358/501 --> loss:0.8296739256381989
step 301/334, epoch 358/501 --> loss:0.8294458997249603
step 51/334, epoch 359/501 --> loss:0.8270509195327759
step 101/334, epoch 359/501 --> loss:0.8272091341018677
step 151/334, epoch 359/501 --> loss:0.8223942959308624
step 201/334, epoch 359/501 --> loss:0.8270202481746673
step 251/334, epoch 359/501 --> loss:0.8352876245975495
step 301/334, epoch 359/501 --> loss:0.8092381000518799
step 51/334, epoch 360/501 --> loss:0.8253985583782196
step 101/334, epoch 360/501 --> loss:0.8227548217773437
step 151/334, epoch 360/501 --> loss:0.8178838205337524
step 201/334, epoch 360/501 --> loss:0.8298645448684693
step 251/334, epoch 360/501 --> loss:0.8180995619297028
step 301/334, epoch 360/501 --> loss:0.822796767950058
step 51/334, epoch 361/501 --> loss:0.8243790340423583
step 101/334, epoch 361/501 --> loss:0.8198803317546844
step 151/334, epoch 361/501 --> loss:0.8325324916839599
step 201/334, epoch 361/501 --> loss:0.8120029830932617
step 251/334, epoch 361/501 --> loss:0.8333926177024842
step 301/334, epoch 361/501 --> loss:0.8209490931034088

##########train dataset##########
acc--> [98.6869620347111]
F1--> {'F1': [0.8637431341994687], 'precision': [0.7679257768700433], 'recall': [0.986893238491189]}
##########eval dataset##########
acc--> [98.04879069223358]
F1--> {'F1': [0.8006100547222375], 'precision': [0.7169585461064983], 'recall': [0.9063727275268473]}
step 51/334, epoch 362/501 --> loss:0.8332163953781128
step 101/334, epoch 362/501 --> loss:0.7994605505466461
step 151/334, epoch 362/501 --> loss:0.8130375170707702
step 201/334, epoch 362/501 --> loss:0.828142237663269
step 251/334, epoch 362/501 --> loss:0.8270737981796265
step 301/334, epoch 362/501 --> loss:0.8270107436180115
step 51/334, epoch 363/501 --> loss:0.8093211221694946
step 101/334, epoch 363/501 --> loss:0.8173185575008393
step 151/334, epoch 363/501 --> loss:0.821903579235077
step 201/334, epoch 363/501 --> loss:0.8344190096855164
step 251/334, epoch 363/501 --> loss:0.8280894124507904
step 301/334, epoch 363/501 --> loss:0.8321196556091308
step 51/334, epoch 364/501 --> loss:0.8339986729621888
step 101/334, epoch 364/501 --> loss:0.813217476606369
step 151/334, epoch 364/501 --> loss:0.812911639213562
step 201/334, epoch 364/501 --> loss:0.82087562084198
step 251/334, epoch 364/501 --> loss:0.8349619317054748
step 301/334, epoch 364/501 --> loss:0.8177205228805542
step 51/334, epoch 365/501 --> loss:0.8043082964420318
step 101/334, epoch 365/501 --> loss:0.8275545048713684
step 151/334, epoch 365/501 --> loss:0.8253839015960693
step 201/334, epoch 365/501 --> loss:0.8242301106452942
step 251/334, epoch 365/501 --> loss:0.8295319139957428
step 301/334, epoch 365/501 --> loss:0.8161829340457917
step 51/334, epoch 366/501 --> loss:0.8270610785484314
step 101/334, epoch 366/501 --> loss:0.8133732306957245
step 151/334, epoch 366/501 --> loss:0.8290149641036987
step 201/334, epoch 366/501 --> loss:0.8322570192813873
step 251/334, epoch 366/501 --> loss:0.8148192799091339
step 301/334, epoch 366/501 --> loss:0.8078303980827332
step 51/334, epoch 367/501 --> loss:0.8196500754356384
step 101/334, epoch 367/501 --> loss:0.8204356253147125
step 151/334, epoch 367/501 --> loss:0.8286045563220977
step 201/334, epoch 367/501 --> loss:0.8203578686714172
step 251/334, epoch 367/501 --> loss:0.8290303862094879
step 301/334, epoch 367/501 --> loss:0.8244763481616973
step 51/334, epoch 368/501 --> loss:0.8210834527015686
step 101/334, epoch 368/501 --> loss:0.8381025552749634
step 151/334, epoch 368/501 --> loss:0.8313931441307068
step 201/334, epoch 368/501 --> loss:0.8227795708179474
step 251/334, epoch 368/501 --> loss:0.7942160391807556
step 301/334, epoch 368/501 --> loss:0.8265116918087005
step 51/334, epoch 369/501 --> loss:0.8309733593463897
step 101/334, epoch 369/501 --> loss:0.8254027915000915
step 151/334, epoch 369/501 --> loss:0.8245328462123871
step 201/334, epoch 369/501 --> loss:0.8365850377082825
step 251/334, epoch 369/501 --> loss:0.8156454956531525
step 301/334, epoch 369/501 --> loss:0.8126468384265899
step 51/334, epoch 370/501 --> loss:0.8032001256942749
step 101/334, epoch 370/501 --> loss:0.8275166475772857
step 151/334, epoch 370/501 --> loss:0.8320738458633423
step 201/334, epoch 370/501 --> loss:0.8290479171276093
step 251/334, epoch 370/501 --> loss:0.8365270078182221
step 301/334, epoch 370/501 --> loss:0.8132039093971253
step 51/334, epoch 371/501 --> loss:0.8197186577320099
step 101/334, epoch 371/501 --> loss:0.8257091975212097
step 151/334, epoch 371/501 --> loss:0.8189099955558777
step 201/334, epoch 371/501 --> loss:0.8232180011272431
step 251/334, epoch 371/501 --> loss:0.8232054710388184
step 301/334, epoch 371/501 --> loss:0.8354311323165894

##########train dataset##########
acc--> [98.72650844486442]
F1--> {'F1': [0.8669779063732537], 'precision': [0.7747656033503688], 'recall': [0.9841185006057318]}
##########eval dataset##########
acc--> [97.92517247370401]
F1--> {'F1': [0.7907249342123155], 'precision': [0.700923081182303], 'recall': [0.9069317820636975]}
step 51/334, epoch 372/501 --> loss:0.8265898823738098
step 101/334, epoch 372/501 --> loss:0.8220982837677002
step 151/334, epoch 372/501 --> loss:0.8292993354797363
step 201/334, epoch 372/501 --> loss:0.8214020156860351
step 251/334, epoch 372/501 --> loss:0.8120546984672546
step 301/334, epoch 372/501 --> loss:0.8182473313808442
step 51/334, epoch 373/501 --> loss:0.8185068321228027
step 101/334, epoch 373/501 --> loss:0.8307795405387879
step 151/334, epoch 373/501 --> loss:0.8225582063198089
step 201/334, epoch 373/501 --> loss:0.8270758223533631
step 251/334, epoch 373/501 --> loss:0.8171092569828033
step 301/334, epoch 373/501 --> loss:0.8299525487422943
step 51/334, epoch 374/501 --> loss:0.8360043954849243
step 101/334, epoch 374/501 --> loss:0.814148371219635
step 151/334, epoch 374/501 --> loss:0.8283234763145447
step 201/334, epoch 374/501 --> loss:0.8232352578639984
step 251/334, epoch 374/501 --> loss:0.8118966066837311
step 301/334, epoch 374/501 --> loss:0.8162887215614318
step 51/334, epoch 375/501 --> loss:0.8148499751091003
step 101/334, epoch 375/501 --> loss:0.8363813090324402
step 151/334, epoch 375/501 --> loss:0.8218531835079194
step 201/334, epoch 375/501 --> loss:0.8205440878868103
step 251/334, epoch 375/501 --> loss:0.8170119595527648
step 301/334, epoch 375/501 --> loss:0.8244559729099273
step 51/334, epoch 376/501 --> loss:0.8392500531673431
step 101/334, epoch 376/501 --> loss:0.8125248742103577
step 151/334, epoch 376/501 --> loss:0.8136688077449798
step 201/334, epoch 376/501 --> loss:0.8185476350784302
step 251/334, epoch 376/501 --> loss:0.8219654333591461
step 301/334, epoch 376/501 --> loss:0.8273044741153717
step 51/334, epoch 377/501 --> loss:0.819812103509903
step 101/334, epoch 377/501 --> loss:0.8254258131980896
step 151/334, epoch 377/501 --> loss:0.8237643086910248
step 201/334, epoch 377/501 --> loss:0.8175049245357513
step 251/334, epoch 377/501 --> loss:0.8285723781585693
step 301/334, epoch 377/501 --> loss:0.8158764040470123
step 51/334, epoch 378/501 --> loss:0.8238151967525482
step 101/334, epoch 378/501 --> loss:0.81154181599617
step 151/334, epoch 378/501 --> loss:0.8279067635536194
step 201/334, epoch 378/501 --> loss:0.8306629407405853
step 251/334, epoch 378/501 --> loss:0.8125227200984955
step 301/334, epoch 378/501 --> loss:0.830250426530838
step 51/334, epoch 379/501 --> loss:0.7971007883548736
step 101/334, epoch 379/501 --> loss:0.8264665913581848
step 151/334, epoch 379/501 --> loss:0.8288708102703094
step 201/334, epoch 379/501 --> loss:0.8329081976413727
step 251/334, epoch 379/501 --> loss:0.8251631724834442
step 301/334, epoch 379/501 --> loss:0.8233235454559327
step 51/334, epoch 380/501 --> loss:0.8323751640319824
step 101/334, epoch 380/501 --> loss:0.8259396386146546
step 151/334, epoch 380/501 --> loss:0.8249359798431396
step 201/334, epoch 380/501 --> loss:0.8105867600440979
step 251/334, epoch 380/501 --> loss:0.8246419966220856
step 301/334, epoch 380/501 --> loss:0.8204643476009369
step 51/334, epoch 381/501 --> loss:0.8113590025901795
step 101/334, epoch 381/501 --> loss:0.8256758892536163
step 151/334, epoch 381/501 --> loss:0.8376756894588471
step 201/334, epoch 381/501 --> loss:0.811397020816803
step 251/334, epoch 381/501 --> loss:0.8209605193138123
step 301/334, epoch 381/501 --> loss:0.8151958692073822

##########train dataset##########
acc--> [98.91269542234672]
F1--> {'F1': [0.8844020749501688], 'precision': [0.8015784098133313], 'recall': [0.986325945178976]}
##########eval dataset##########
acc--> [98.30051799249534]
F1--> {'F1': [0.8192162523345076], 'precision': [0.7581981185355636], 'recall': [0.8909269155094224]}
save model!
step 51/334, epoch 382/501 --> loss:0.8238168084621429
step 101/334, epoch 382/501 --> loss:0.8387154150009155
step 151/334, epoch 382/501 --> loss:0.811604460477829
step 201/334, epoch 382/501 --> loss:0.820262621641159
step 251/334, epoch 382/501 --> loss:0.8051629412174225
step 301/334, epoch 382/501 --> loss:0.8321049523353576
step 51/334, epoch 383/501 --> loss:0.8214994585514068
step 101/334, epoch 383/501 --> loss:0.8234743690490722
step 151/334, epoch 383/501 --> loss:0.8392526483535767
step 201/334, epoch 383/501 --> loss:0.8231898367404937
step 251/334, epoch 383/501 --> loss:0.8271239352226257
step 301/334, epoch 383/501 --> loss:0.8060763239860534
step 51/334, epoch 384/501 --> loss:0.839938393831253
step 101/334, epoch 384/501 --> loss:0.8312649083137512
step 151/334, epoch 384/501 --> loss:0.8159387934207917
step 201/334, epoch 384/501 --> loss:0.8166426491737365
step 251/334, epoch 384/501 --> loss:0.8227755725383759
step 301/334, epoch 384/501 --> loss:0.8130399191379547
step 51/334, epoch 385/501 --> loss:0.8208333230018616
step 101/334, epoch 385/501 --> loss:0.8116496682167054
step 151/334, epoch 385/501 --> loss:0.8346600127220154
step 201/334, epoch 385/501 --> loss:0.8180433142185212
step 251/334, epoch 385/501 --> loss:0.8087975931167602
step 301/334, epoch 385/501 --> loss:0.840150556564331
step 51/334, epoch 386/501 --> loss:0.822687394618988
step 101/334, epoch 386/501 --> loss:0.8231265032291413
step 151/334, epoch 386/501 --> loss:0.8109015691280365
step 201/334, epoch 386/501 --> loss:0.8382895290851593
step 251/334, epoch 386/501 --> loss:0.8327899014949799
step 301/334, epoch 386/501 --> loss:0.8225767469406128
step 51/334, epoch 387/501 --> loss:0.8282206773757934
step 101/334, epoch 387/501 --> loss:0.8219338715076446
step 151/334, epoch 387/501 --> loss:0.8224710321426392
step 201/334, epoch 387/501 --> loss:0.8323969280719757
step 251/334, epoch 387/501 --> loss:0.8256492781639099
step 301/334, epoch 387/501 --> loss:0.812478334903717
step 51/334, epoch 388/501 --> loss:0.8178275656700135
step 101/334, epoch 388/501 --> loss:0.8149792754650116
step 151/334, epoch 388/501 --> loss:0.8234461951255798
step 201/334, epoch 388/501 --> loss:0.8358538794517517
step 251/334, epoch 388/501 --> loss:0.830160630941391
step 301/334, epoch 388/501 --> loss:0.8267321991920471
step 51/334, epoch 389/501 --> loss:0.8195606815814972
step 101/334, epoch 389/501 --> loss:0.827912677526474
step 151/334, epoch 389/501 --> loss:0.8351096844673157
step 201/334, epoch 389/501 --> loss:0.8181390917301178
step 251/334, epoch 389/501 --> loss:0.8187928295135498
step 301/334, epoch 389/501 --> loss:0.8121682572364807
step 51/334, epoch 390/501 --> loss:0.8199494862556458
step 101/334, epoch 390/501 --> loss:0.8189889693260193
step 151/334, epoch 390/501 --> loss:0.8101026499271393
step 201/334, epoch 390/501 --> loss:0.8193077254295349
step 251/334, epoch 390/501 --> loss:0.8374743509292603
step 301/334, epoch 390/501 --> loss:0.8182644128799439
step 51/334, epoch 391/501 --> loss:0.8299022150039673
step 101/334, epoch 391/501 --> loss:0.817395408153534
step 151/334, epoch 391/501 --> loss:0.813758054971695
step 201/334, epoch 391/501 --> loss:0.825099333524704
step 251/334, epoch 391/501 --> loss:0.82226442694664
step 301/334, epoch 391/501 --> loss:0.8223038506507874

##########train dataset##########
acc--> [98.8702809090974]
F1--> {'F1': [0.8803383947253877], 'precision': [0.7954974199788953], 'recall': [0.9854490588167878]}
##########eval dataset##########
acc--> [98.10830931441753]
F1--> {'F1': [0.805145288153908], 'precision': [0.7256124763546187], 'recall': [0.9042716399650108]}
step 51/334, epoch 392/501 --> loss:0.8284440076351166
step 101/334, epoch 392/501 --> loss:0.811518714427948
step 151/334, epoch 392/501 --> loss:0.8212815964221954
step 201/334, epoch 392/501 --> loss:0.8084061765670776
step 251/334, epoch 392/501 --> loss:0.8339491951465606
step 301/334, epoch 392/501 --> loss:0.8437163126468659
step 51/334, epoch 393/501 --> loss:0.817181179523468
step 101/334, epoch 393/501 --> loss:0.8244965887069702
step 151/334, epoch 393/501 --> loss:0.8180361866950989
step 201/334, epoch 393/501 --> loss:0.8400890862941742
step 251/334, epoch 393/501 --> loss:0.8157029700279236
step 301/334, epoch 393/501 --> loss:0.8098390007019043
step 51/334, epoch 394/501 --> loss:0.8206289660930634
step 101/334, epoch 394/501 --> loss:0.8332544195652009
step 151/334, epoch 394/501 --> loss:0.8327462697029113
step 201/334, epoch 394/501 --> loss:0.8200934541225433
step 251/334, epoch 394/501 --> loss:0.8235820293426513
step 301/334, epoch 394/501 --> loss:0.8168697082996368
step 51/334, epoch 395/501 --> loss:0.8193063485622406
step 101/334, epoch 395/501 --> loss:0.8285084438323974
step 151/334, epoch 395/501 --> loss:0.8398938310146332
step 201/334, epoch 395/501 --> loss:0.8225366234779358
step 251/334, epoch 395/501 --> loss:0.8204670989513397
step 301/334, epoch 395/501 --> loss:0.8163235902786254
step 51/334, epoch 396/501 --> loss:0.8265928483009338
step 101/334, epoch 396/501 --> loss:0.8223644542694092
step 151/334, epoch 396/501 --> loss:0.8175572669506073
step 201/334, epoch 396/501 --> loss:0.8248964047431946
step 251/334, epoch 396/501 --> loss:0.8165386068820953
step 301/334, epoch 396/501 --> loss:0.8202082931995391
step 51/334, epoch 397/501 --> loss:0.8154525685310364
step 101/334, epoch 397/501 --> loss:0.8220926201343537
step 151/334, epoch 397/501 --> loss:0.8223459839820861
step 201/334, epoch 397/501 --> loss:0.8207990503311158
step 251/334, epoch 397/501 --> loss:0.8207273054122924
step 301/334, epoch 397/501 --> loss:0.8232677066326142
step 51/334, epoch 398/501 --> loss:0.8210843586921692
step 101/334, epoch 398/501 --> loss:0.8234826922416687
step 151/334, epoch 398/501 --> loss:0.8266145050525665
step 201/334, epoch 398/501 --> loss:0.8247007250785827
step 251/334, epoch 398/501 --> loss:0.8183081638813019
step 301/334, epoch 398/501 --> loss:0.8224962174892425
step 51/334, epoch 399/501 --> loss:0.8156368613243103
step 101/334, epoch 399/501 --> loss:0.8178950333595276
step 151/334, epoch 399/501 --> loss:0.8216565525531769
step 201/334, epoch 399/501 --> loss:0.8328404092788696
step 251/334, epoch 399/501 --> loss:0.8298345470428466
step 301/334, epoch 399/501 --> loss:0.8276948571205139
step 51/334, epoch 400/501 --> loss:0.8216562044620513
step 101/334, epoch 400/501 --> loss:0.8196784687042237
step 151/334, epoch 400/501 --> loss:0.8241604793071747
step 201/334, epoch 400/501 --> loss:0.8241957819461823
step 251/334, epoch 400/501 --> loss:0.8266768026351928
step 301/334, epoch 400/501 --> loss:0.8145502042770386
step 51/334, epoch 401/501 --> loss:0.8174558174610138
step 101/334, epoch 401/501 --> loss:0.8229509902000427
step 151/334, epoch 401/501 --> loss:0.8267816066741943
step 201/334, epoch 401/501 --> loss:0.8161068904399872
step 251/334, epoch 401/501 --> loss:0.8248487079143524
step 301/334, epoch 401/501 --> loss:0.8308665776252746

##########train dataset##########
acc--> [98.90524453333431]
F1--> {'F1': [0.8836833379228419], 'precision': [0.8005168170089709], 'recall': [0.9861461180151977]}
##########eval dataset##########
acc--> [98.27860167228195]
F1--> {'F1': [0.8177648206202152], 'precision': [0.7537724248240746], 'recall': [0.8936424997569292]}
step 51/334, epoch 402/501 --> loss:0.8192246699333191
step 101/334, epoch 402/501 --> loss:0.8150939857959747
step 151/334, epoch 402/501 --> loss:0.8119129860401153
step 201/334, epoch 402/501 --> loss:0.8254955208301544
step 251/334, epoch 402/501 --> loss:0.8256232690811157
step 301/334, epoch 402/501 --> loss:0.8326104927062988
step 51/334, epoch 403/501 --> loss:0.8049851322174072
step 101/334, epoch 403/501 --> loss:0.834262787103653
step 151/334, epoch 403/501 --> loss:0.837819277048111
step 201/334, epoch 403/501 --> loss:0.8246618020534515
step 251/334, epoch 403/501 --> loss:0.8286898851394653
step 301/334, epoch 403/501 --> loss:0.8151028203964233
step 51/334, epoch 404/501 --> loss:0.8219664061069488
step 101/334, epoch 404/501 --> loss:0.8299840569496155
step 151/334, epoch 404/501 --> loss:0.8171937215328217
step 201/334, epoch 404/501 --> loss:0.8171081733703613
step 251/334, epoch 404/501 --> loss:0.8198891270160675
step 301/334, epoch 404/501 --> loss:0.827376765012741
step 51/334, epoch 405/501 --> loss:0.8185630869865418
step 101/334, epoch 405/501 --> loss:0.8262077343463897
step 151/334, epoch 405/501 --> loss:0.8247354245185852
step 201/334, epoch 405/501 --> loss:0.8220614957809448
step 251/334, epoch 405/501 --> loss:0.8146400630474091
step 301/334, epoch 405/501 --> loss:0.8211776769161224
step 51/334, epoch 406/501 --> loss:0.8276600253582
step 101/334, epoch 406/501 --> loss:0.8123848378658295
step 151/334, epoch 406/501 --> loss:0.8232855498790741
step 201/334, epoch 406/501 --> loss:0.810021984577179
step 251/334, epoch 406/501 --> loss:0.8346932554244995
step 301/334, epoch 406/501 --> loss:0.8282115435600281
step 51/334, epoch 407/501 --> loss:0.8309111607074737
step 101/334, epoch 407/501 --> loss:0.8216984665393829
step 151/334, epoch 407/501 --> loss:0.8332939875125885
step 201/334, epoch 407/501 --> loss:0.8182605957984924
step 251/334, epoch 407/501 --> loss:0.809062329530716
step 301/334, epoch 407/501 --> loss:0.815482326745987
step 51/334, epoch 408/501 --> loss:0.8271833515167236
step 101/334, epoch 408/501 --> loss:0.8003675055503845
step 151/334, epoch 408/501 --> loss:0.8368490421772004
step 201/334, epoch 408/501 --> loss:0.8304777467250823
step 251/334, epoch 408/501 --> loss:0.8232552909851074
step 301/334, epoch 408/501 --> loss:0.8148842716217041
step 51/334, epoch 409/501 --> loss:0.8183888411521911
step 101/334, epoch 409/501 --> loss:0.8351706910133362
step 151/334, epoch 409/501 --> loss:0.836233627796173
step 201/334, epoch 409/501 --> loss:0.8252543950080872
step 251/334, epoch 409/501 --> loss:0.8251984548568726
step 301/334, epoch 409/501 --> loss:0.8045640254020691
step 51/334, epoch 410/501 --> loss:0.8246831440925598
step 101/334, epoch 410/501 --> loss:0.8244749915599823
step 151/334, epoch 410/501 --> loss:0.811813417673111
step 201/334, epoch 410/501 --> loss:0.8214087069034577
step 251/334, epoch 410/501 --> loss:0.8267958045005799
step 301/334, epoch 410/501 --> loss:0.8240518951416016
step 51/334, epoch 411/501 --> loss:0.8133509314060211
step 101/334, epoch 411/501 --> loss:0.8212905764579773
step 151/334, epoch 411/501 --> loss:0.82114506483078
step 201/334, epoch 411/501 --> loss:0.8229092252254486
step 251/334, epoch 411/501 --> loss:0.8201461446285248
step 301/334, epoch 411/501 --> loss:0.8366358757019043

##########train dataset##########
acc--> [98.96467313991474]
F1--> {'F1': [0.8892406206926688], 'precision': [0.8100745974274021], 'recall': [0.9855680391431805]}
##########eval dataset##########
acc--> [98.28270138717355]
F1--> {'F1': [0.8162462612835845], 'precision': [0.7592509969592821], 'recall': [0.8825046508073049]}
step 51/334, epoch 412/501 --> loss:0.8163186061382294
step 101/334, epoch 412/501 --> loss:0.8243939638137817
step 151/334, epoch 412/501 --> loss:0.8088168406486511
step 201/334, epoch 412/501 --> loss:0.8424176263809204
step 251/334, epoch 412/501 --> loss:0.8196637606620789
step 301/334, epoch 412/501 --> loss:0.8130838441848754
step 51/334, epoch 413/501 --> loss:0.8074913346767425
step 101/334, epoch 413/501 --> loss:0.801979341506958
step 151/334, epoch 413/501 --> loss:0.8244035792350769
step 201/334, epoch 413/501 --> loss:0.8410353076457977
step 251/334, epoch 413/501 --> loss:0.8255032932758332
step 301/334, epoch 413/501 --> loss:0.8280909168720245
step 51/334, epoch 414/501 --> loss:0.8127555322647094
step 101/334, epoch 414/501 --> loss:0.825278846025467
step 151/334, epoch 414/501 --> loss:0.8230414080619812
step 201/334, epoch 414/501 --> loss:0.8268216967582702
step 251/334, epoch 414/501 --> loss:0.8307536685466766
step 301/334, epoch 414/501 --> loss:0.8144122064113617
step 51/334, epoch 415/501 --> loss:0.8221533381938935
step 101/334, epoch 415/501 --> loss:0.8234161734580994
step 151/334, epoch 415/501 --> loss:0.8322527003288269
step 201/334, epoch 415/501 --> loss:0.8171806383132935
step 251/334, epoch 415/501 --> loss:0.8200331020355225
step 301/334, epoch 415/501 --> loss:0.8161736500263214
step 51/334, epoch 416/501 --> loss:0.8214349973201752
step 101/334, epoch 416/501 --> loss:0.8289630508422852
step 151/334, epoch 416/501 --> loss:0.8441256618499756
step 201/334, epoch 416/501 --> loss:0.8205632019042969
step 251/334, epoch 416/501 --> loss:0.8037104105949402
step 301/334, epoch 416/501 --> loss:0.8219286441802979
step 51/334, epoch 417/501 --> loss:0.8171423017978668
step 101/334, epoch 417/501 --> loss:0.8320990169048309
step 151/334, epoch 417/501 --> loss:0.8194854640960694
step 201/334, epoch 417/501 --> loss:0.8297348308563233
step 251/334, epoch 417/501 --> loss:0.8163211333751679
step 301/334, epoch 417/501 --> loss:0.8184661889076232
step 51/334, epoch 418/501 --> loss:0.8202533292770385
step 101/334, epoch 418/501 --> loss:0.8362284219264984
step 151/334, epoch 418/501 --> loss:0.8267597854137421
step 201/334, epoch 418/501 --> loss:0.8236971187591553
step 251/334, epoch 418/501 --> loss:0.812601603269577
step 301/334, epoch 418/501 --> loss:0.8250985479354859
step 51/334, epoch 419/501 --> loss:0.8089257574081421
step 101/334, epoch 419/501 --> loss:0.83355424284935
step 151/334, epoch 419/501 --> loss:0.8345387482643127
step 201/334, epoch 419/501 --> loss:0.8220494258403778
step 251/334, epoch 419/501 --> loss:0.8263817310333252
step 301/334, epoch 419/501 --> loss:0.8162524545192719
step 51/334, epoch 420/501 --> loss:0.8132499086856843
step 101/334, epoch 420/501 --> loss:0.8180656015872956
step 151/334, epoch 420/501 --> loss:0.830526419878006
step 201/334, epoch 420/501 --> loss:0.8366130101680755
step 251/334, epoch 420/501 --> loss:0.8134450316429138
step 301/334, epoch 420/501 --> loss:0.8155532360076905
step 51/334, epoch 421/501 --> loss:0.8249308729171753
step 101/334, epoch 421/501 --> loss:0.8194075751304627
step 151/334, epoch 421/501 --> loss:0.814301962852478
step 201/334, epoch 421/501 --> loss:0.828902325630188
step 251/334, epoch 421/501 --> loss:0.8271560990810394
step 301/334, epoch 421/501 --> loss:0.8283031356334686

##########train dataset##########
acc--> [98.66135879361843]
F1--> {'F1': [0.8610440421695423], 'precision': [0.7657074769349022], 'recall': [0.9835099643981671]}
##########eval dataset##########
acc--> [97.8935161600523]
F1--> {'F1': [0.7875512546983297], 'precision': [0.6980610396763772], 'recall': [0.9033734462671321]}
step 51/334, epoch 422/501 --> loss:0.8225275838375091
step 101/334, epoch 422/501 --> loss:0.8229864120483399
step 151/334, epoch 422/501 --> loss:0.8284545755386352
step 201/334, epoch 422/501 --> loss:0.8272138702869415
step 251/334, epoch 422/501 --> loss:0.8173676311969758
step 301/334, epoch 422/501 --> loss:0.8260066473484039
step 51/334, epoch 423/501 --> loss:0.8282295072078705
step 101/334, epoch 423/501 --> loss:0.8229771733283997
step 151/334, epoch 423/501 --> loss:0.8196730506420136
step 201/334, epoch 423/501 --> loss:0.8291245877742768
step 251/334, epoch 423/501 --> loss:0.8280641674995423
step 301/334, epoch 423/501 --> loss:0.817940719127655
step 51/334, epoch 424/501 --> loss:0.8354256618022918
step 101/334, epoch 424/501 --> loss:0.8206162524223327
step 151/334, epoch 424/501 --> loss:0.8196019876003265
step 201/334, epoch 424/501 --> loss:0.8191371166706085
step 251/334, epoch 424/501 --> loss:0.8353549861907958
step 301/334, epoch 424/501 --> loss:0.8150306344032288
step 51/334, epoch 425/501 --> loss:0.8282199895381928
step 101/334, epoch 425/501 --> loss:0.8228664183616639
step 151/334, epoch 425/501 --> loss:0.8154555451869965
step 201/334, epoch 425/501 --> loss:0.8319728672504425
step 251/334, epoch 425/501 --> loss:0.8155060470104217
step 301/334, epoch 425/501 --> loss:0.8276424443721772
step 51/334, epoch 426/501 --> loss:0.818547533750534
step 101/334, epoch 426/501 --> loss:0.8167677235603332
step 151/334, epoch 426/501 --> loss:0.8202559912204742
step 201/334, epoch 426/501 --> loss:0.8297354018688202
step 251/334, epoch 426/501 --> loss:0.8288881146907806
step 301/334, epoch 426/501 --> loss:0.8225786101818084
step 51/334, epoch 427/501 --> loss:0.8328393411636352
step 101/334, epoch 427/501 --> loss:0.8094765043258667
step 151/334, epoch 427/501 --> loss:0.8258049857616424
step 201/334, epoch 427/501 --> loss:0.8238147044181824
step 251/334, epoch 427/501 --> loss:0.8183003687858581
step 301/334, epoch 427/501 --> loss:0.8311707723140717
step 51/334, epoch 428/501 --> loss:0.8287382411956787
step 101/334, epoch 428/501 --> loss:0.8172003042697906
step 151/334, epoch 428/501 --> loss:0.8103447520732879
step 201/334, epoch 428/501 --> loss:0.8337569427490235
step 251/334, epoch 428/501 --> loss:0.8326340270042419
step 301/334, epoch 428/501 --> loss:0.8110879433155059
step 51/334, epoch 429/501 --> loss:0.8160961842536927
step 101/334, epoch 429/501 --> loss:0.8311172437667846
step 151/334, epoch 429/501 --> loss:0.8241357052326203
step 201/334, epoch 429/501 --> loss:0.8201954770088196
step 251/334, epoch 429/501 --> loss:0.8123829352855683
step 301/334, epoch 429/501 --> loss:0.8190603280067443
step 51/334, epoch 430/501 --> loss:0.8287067639827729
step 101/334, epoch 430/501 --> loss:0.8184365570545197
step 151/334, epoch 430/501 --> loss:0.831296558380127
step 201/334, epoch 430/501 --> loss:0.8124565529823303
step 251/334, epoch 430/501 --> loss:0.816377876996994
step 301/334, epoch 430/501 --> loss:0.8211860632896424
step 51/334, epoch 431/501 --> loss:0.8277079319953918
step 101/334, epoch 431/501 --> loss:0.8229093623161315
step 151/334, epoch 431/501 --> loss:0.812705602645874
step 201/334, epoch 431/501 --> loss:0.8140344631671905
step 251/334, epoch 431/501 --> loss:0.816610689163208
step 301/334, epoch 431/501 --> loss:0.8357854437828064

##########train dataset##########
acc--> [98.98576883059012]
F1--> {'F1': [0.8913658894673951], 'precision': [0.8128200599199876], 'recall': [0.9867280634086543]}
##########eval dataset##########
acc--> [98.34192364442556]
F1--> {'F1': [0.8221175642375516], 'precision': [0.7664420840007797], 'recall': [0.886526909208483]}
save model!
step 51/334, epoch 432/501 --> loss:0.8098224508762359
step 101/334, epoch 432/501 --> loss:0.8065184354782104
step 151/334, epoch 432/501 --> loss:0.8275987029075622
step 201/334, epoch 432/501 --> loss:0.8316666269302369
step 251/334, epoch 432/501 --> loss:0.8199904334545135
step 301/334, epoch 432/501 --> loss:0.8401418030261993
step 51/334, epoch 433/501 --> loss:0.8449855935573578
step 101/334, epoch 433/501 --> loss:0.8102607202529907
step 151/334, epoch 433/501 --> loss:0.8147156929969788
step 201/334, epoch 433/501 --> loss:0.8171639657020568
step 251/334, epoch 433/501 --> loss:0.8283664298057556
step 301/334, epoch 433/501 --> loss:0.8235273349285126
step 51/334, epoch 434/501 --> loss:0.831392353773117
step 101/334, epoch 434/501 --> loss:0.8224732458591462
step 151/334, epoch 434/501 --> loss:0.8140605211257934
step 201/334, epoch 434/501 --> loss:0.8143842244148254
step 251/334, epoch 434/501 --> loss:0.8257264292240143
step 301/334, epoch 434/501 --> loss:0.8375141537189483
step 51/334, epoch 435/501 --> loss:0.815482656955719
step 101/334, epoch 435/501 --> loss:0.8416432797908783
step 151/334, epoch 435/501 --> loss:0.8085730528831482
step 201/334, epoch 435/501 --> loss:0.8304196238517761
step 251/334, epoch 435/501 --> loss:0.8244099247455597
step 301/334, epoch 435/501 --> loss:0.818947993516922
step 51/334, epoch 436/501 --> loss:0.8252503824234009
step 101/334, epoch 436/501 --> loss:0.8233177781105041
step 151/334, epoch 436/501 --> loss:0.8288362622261047
step 201/334, epoch 436/501 --> loss:0.8239158868789673
step 251/334, epoch 436/501 --> loss:0.805068598985672
step 301/334, epoch 436/501 --> loss:0.8257176804542542
step 51/334, epoch 437/501 --> loss:0.8243356430530548
step 101/334, epoch 437/501 --> loss:0.8350083470344544
step 151/334, epoch 437/501 --> loss:0.8244936442375184
step 201/334, epoch 437/501 --> loss:0.8246335124969483
step 251/334, epoch 437/501 --> loss:0.8244754016399384
step 301/334, epoch 437/501 --> loss:0.8186595940589905
step 51/334, epoch 438/501 --> loss:0.8166769790649414
step 101/334, epoch 438/501 --> loss:0.8321135509014129
step 151/334, epoch 438/501 --> loss:0.8301274490356445
step 201/334, epoch 438/501 --> loss:0.8095047473907471
step 251/334, epoch 438/501 --> loss:0.8271698367595672
step 301/334, epoch 438/501 --> loss:0.8147266817092895
step 51/334, epoch 439/501 --> loss:0.8260768496990204
step 101/334, epoch 439/501 --> loss:0.8263217699527741
step 151/334, epoch 439/501 --> loss:0.8065471887588501
step 201/334, epoch 439/501 --> loss:0.8090359425544739
step 251/334, epoch 439/501 --> loss:0.8335506272315979
step 301/334, epoch 439/501 --> loss:0.8325699067115784
step 51/334, epoch 440/501 --> loss:0.8315544617176056
step 101/334, epoch 440/501 --> loss:0.8269895458221436
step 151/334, epoch 440/501 --> loss:0.8192778527736664
step 201/334, epoch 440/501 --> loss:0.811829059123993
step 251/334, epoch 440/501 --> loss:0.8281503307819367
step 301/334, epoch 440/501 --> loss:0.8214868152141571
step 51/334, epoch 441/501 --> loss:0.8207535350322723
step 101/334, epoch 441/501 --> loss:0.812021963596344
step 151/334, epoch 441/501 --> loss:0.820326200723648
step 201/334, epoch 441/501 --> loss:0.824967566728592
step 251/334, epoch 441/501 --> loss:0.830412415266037
step 301/334, epoch 441/501 --> loss:0.834461258649826

##########train dataset##########
acc--> [98.38995402521424]
F1--> {'F1': [0.8376890332997663], 'precision': [0.7285895656812381], 'recall': [0.9852294132655567]}
##########eval dataset##########
acc--> [97.71235241559728]
F1--> {'F1': [0.7733453008977068], 'precision': [0.6762626198363468], 'recall': [0.9029875133147595]}
step 51/334, epoch 442/501 --> loss:0.8274982202053071
step 101/334, epoch 442/501 --> loss:0.820802356004715
step 151/334, epoch 442/501 --> loss:0.8233630931377411
step 201/334, epoch 442/501 --> loss:0.831593633890152
step 251/334, epoch 442/501 --> loss:0.8112287425994873
step 301/334, epoch 442/501 --> loss:0.8293067669868469
step 51/334, epoch 443/501 --> loss:0.8383299255371094
step 101/334, epoch 443/501 --> loss:0.8244881987571716
step 151/334, epoch 443/501 --> loss:0.8193794703483581
step 201/334, epoch 443/501 --> loss:0.816011689901352
step 251/334, epoch 443/501 --> loss:0.8254284131526947
step 301/334, epoch 443/501 --> loss:0.8038550102710724
step 51/334, epoch 444/501 --> loss:0.8323758375644684
step 101/334, epoch 444/501 --> loss:0.8233462166786194
step 151/334, epoch 444/501 --> loss:0.8147833287715912
step 201/334, epoch 444/501 --> loss:0.8246804082393646
step 251/334, epoch 444/501 --> loss:0.8251573872566224
step 301/334, epoch 444/501 --> loss:0.8129302394390107
step 51/334, epoch 445/501 --> loss:0.8107102608680725
step 101/334, epoch 445/501 --> loss:0.8232348549365998
step 151/334, epoch 445/501 --> loss:0.8248360884189606
step 201/334, epoch 445/501 --> loss:0.8202047193050385
step 251/334, epoch 445/501 --> loss:0.8127756917476654
step 301/334, epoch 445/501 --> loss:0.8299692046642303
step 51/334, epoch 446/501 --> loss:0.8156363534927368
step 101/334, epoch 446/501 --> loss:0.8209622490406037
step 151/334, epoch 446/501 --> loss:0.8226397013664246
step 201/334, epoch 446/501 --> loss:0.8204841876029968
step 251/334, epoch 446/501 --> loss:0.8224542927742005
step 301/334, epoch 446/501 --> loss:0.8323424100875855
step 51/334, epoch 447/501 --> loss:0.8244371533393859
step 101/334, epoch 447/501 --> loss:0.8159690296649933
step 151/334, epoch 447/501 --> loss:0.8312064218521118
step 201/334, epoch 447/501 --> loss:0.8409542143344879
step 251/334, epoch 447/501 --> loss:0.811507796049118
step 301/334, epoch 447/501 --> loss:0.8144809699058533
step 51/334, epoch 448/501 --> loss:0.8193068313598633
step 101/334, epoch 448/501 --> loss:0.8214837908744812
step 151/334, epoch 448/501 --> loss:0.8403165411949157
step 201/334, epoch 448/501 --> loss:0.8214865946769714
step 251/334, epoch 448/501 --> loss:0.8151062536239624
step 301/334, epoch 448/501 --> loss:0.8168832290172577
step 51/334, epoch 449/501 --> loss:0.8262176859378815
step 101/334, epoch 449/501 --> loss:0.8173950958251953
step 151/334, epoch 449/501 --> loss:0.8284554958343506
step 201/334, epoch 449/501 --> loss:0.8230600774288177
step 251/334, epoch 449/501 --> loss:0.8311447703838348
step 301/334, epoch 449/501 --> loss:0.8074069714546204
step 51/334, epoch 450/501 --> loss:0.8461967480182647
step 101/334, epoch 450/501 --> loss:0.8112579572200775
step 151/334, epoch 450/501 --> loss:0.8144295871257782
step 201/334, epoch 450/501 --> loss:0.8132698523998261
step 251/334, epoch 450/501 --> loss:0.8277289211750031
step 301/334, epoch 450/501 --> loss:0.8254650127887726
step 51/334, epoch 451/501 --> loss:0.8354559957981109
step 101/334, epoch 451/501 --> loss:0.8086594927310944
step 151/334, epoch 451/501 --> loss:0.8331776964664459
step 201/334, epoch 451/501 --> loss:0.8068031358718872
step 251/334, epoch 451/501 --> loss:0.8469709384441376
step 301/334, epoch 451/501 --> loss:0.812609714269638

##########train dataset##########
acc--> [98.93746048848713]
F1--> {'F1': [0.8867732917565901], 'precision': [0.8052430407579203], 'recall': [0.9866854638391134]}
##########eval dataset##########
acc--> [98.31264359013792]
F1--> {'F1': [0.8209605149467019], 'precision': [0.758178880360931], 'recall': [0.8950900185894975]}
step 51/334, epoch 452/501 --> loss:0.8246455204486847
step 101/334, epoch 452/501 --> loss:0.8247260665893554
step 151/334, epoch 452/501 --> loss:0.8112478017807007
step 201/334, epoch 452/501 --> loss:0.8319732594490051
step 251/334, epoch 452/501 --> loss:0.8182988202571869
step 301/334, epoch 452/501 --> loss:0.8265337526798249
step 51/334, epoch 453/501 --> loss:0.8282746469974518
step 101/334, epoch 453/501 --> loss:0.8195640110969543
step 151/334, epoch 453/501 --> loss:0.8123943710327148
step 201/334, epoch 453/501 --> loss:0.8401354944705963
step 251/334, epoch 453/501 --> loss:0.8166770386695862
step 301/334, epoch 453/501 --> loss:0.8186936140060425
step 51/334, epoch 454/501 --> loss:0.837317795753479
step 101/334, epoch 454/501 --> loss:0.8247136783599853
step 151/334, epoch 454/501 --> loss:0.8263759338855743
step 201/334, epoch 454/501 --> loss:0.822834609746933
step 251/334, epoch 454/501 --> loss:0.8159901916980743
step 301/334, epoch 454/501 --> loss:0.8169881093502045
step 51/334, epoch 455/501 --> loss:0.8137525737285614
step 101/334, epoch 455/501 --> loss:0.8101356196403503
step 151/334, epoch 455/501 --> loss:0.839229338169098
step 201/334, epoch 455/501 --> loss:0.8181854140758514
step 251/334, epoch 455/501 --> loss:0.8294609248638153
step 301/334, epoch 455/501 --> loss:0.8214575338363648
step 51/334, epoch 456/501 --> loss:0.8262801110744477
step 101/334, epoch 456/501 --> loss:0.8185664248466492
step 151/334, epoch 456/501 --> loss:0.8236540675163269
step 201/334, epoch 456/501 --> loss:0.8141236793994904
step 251/334, epoch 456/501 --> loss:0.8220259416103363
step 301/334, epoch 456/501 --> loss:0.8319655668735504
step 51/334, epoch 457/501 --> loss:0.8258489954471588
step 101/334, epoch 457/501 --> loss:0.8203574657440186
step 151/334, epoch 457/501 --> loss:0.8249182081222535
step 201/334, epoch 457/501 --> loss:0.8137714827060699
step 251/334, epoch 457/501 --> loss:0.8201042366027832
step 301/334, epoch 457/501 --> loss:0.8233313953876495
step 51/334, epoch 458/501 --> loss:0.8118362998962403
step 101/334, epoch 458/501 --> loss:0.8180667328834533
step 151/334, epoch 458/501 --> loss:0.8161833703517913
step 201/334, epoch 458/501 --> loss:0.8405142426490784
step 251/334, epoch 458/501 --> loss:0.8233642590045929
step 301/334, epoch 458/501 --> loss:0.8276115787029267
step 51/334, epoch 459/501 --> loss:0.824141595363617
step 101/334, epoch 459/501 --> loss:0.8287510812282562
step 151/334, epoch 459/501 --> loss:0.8101415705680847
step 201/334, epoch 459/501 --> loss:0.8225167775154114
step 251/334, epoch 459/501 --> loss:0.8281617474555969
step 301/334, epoch 459/501 --> loss:0.8262020254135132
step 51/334, epoch 460/501 --> loss:0.8134402024745941
step 101/334, epoch 460/501 --> loss:0.8253277337551117
step 151/334, epoch 460/501 --> loss:0.8284688067436218
step 201/334, epoch 460/501 --> loss:0.8150262999534607
step 251/334, epoch 460/501 --> loss:0.829923449754715
step 301/334, epoch 460/501 --> loss:0.8214127659797669
step 51/334, epoch 461/501 --> loss:0.8120069110393524
step 101/334, epoch 461/501 --> loss:0.8404866480827331
step 151/334, epoch 461/501 --> loss:0.8214698171615601
step 201/334, epoch 461/501 --> loss:0.8095350325107574
step 251/334, epoch 461/501 --> loss:0.8208092045783997
step 301/334, epoch 461/501 --> loss:0.8263989555835723

##########train dataset##########
acc--> [98.89728873688479]
F1--> {'F1': [0.8829549227920445], 'precision': [0.7992099263238438], 'recall': [0.9863169232956178]}
##########eval dataset##########
acc--> [98.22288306261476]
F1--> {'F1': [0.8119276282792749], 'precision': [0.7481864891036403], 'recall': [0.8875528206140897]}
step 51/334, epoch 462/501 --> loss:0.8227134990692139
step 101/334, epoch 462/501 --> loss:0.8198188114166259
step 151/334, epoch 462/501 --> loss:0.8279257380962372
step 201/334, epoch 462/501 --> loss:0.8184396064281464
step 251/334, epoch 462/501 --> loss:0.8292128193378449
step 301/334, epoch 462/501 --> loss:0.8250509119033813
step 51/334, epoch 463/501 --> loss:0.8253503489494324
step 101/334, epoch 463/501 --> loss:0.8346947145462036
step 151/334, epoch 463/501 --> loss:0.8285488939285278
step 201/334, epoch 463/501 --> loss:0.8179932940006256
step 251/334, epoch 463/501 --> loss:0.8322606527805329
step 301/334, epoch 463/501 --> loss:0.8044842755794526
step 51/334, epoch 464/501 --> loss:0.8166077709197999
step 101/334, epoch 464/501 --> loss:0.826350920200348
step 151/334, epoch 464/501 --> loss:0.8190618658065796
step 201/334, epoch 464/501 --> loss:0.8247135198116302
step 251/334, epoch 464/501 --> loss:0.8235495030879975
step 301/334, epoch 464/501 --> loss:0.8235432052612305
step 51/334, epoch 465/501 --> loss:0.8286589753627777
step 101/334, epoch 465/501 --> loss:0.8170889925956726
step 151/334, epoch 465/501 --> loss:0.8286085081100464
step 201/334, epoch 465/501 --> loss:0.8152010571956635
step 251/334, epoch 465/501 --> loss:0.814713008403778
step 301/334, epoch 465/501 --> loss:0.8235282063484192
step 51/334, epoch 466/501 --> loss:0.8199811387062073
step 101/334, epoch 466/501 --> loss:0.8300791525840759
step 151/334, epoch 466/501 --> loss:0.8243988072872162
step 201/334, epoch 466/501 --> loss:0.8075409746170044
step 251/334, epoch 466/501 --> loss:0.834937926530838
step 301/334, epoch 466/501 --> loss:0.8196835803985596
step 51/334, epoch 467/501 --> loss:0.8280417943000793
step 101/334, epoch 467/501 --> loss:0.8238194644451141
step 151/334, epoch 467/501 --> loss:0.8215199732780456
step 201/334, epoch 467/501 --> loss:0.8137230110168457
step 251/334, epoch 467/501 --> loss:0.8200171422958374
step 301/334, epoch 467/501 --> loss:0.8309001553058625
step 51/334, epoch 468/501 --> loss:0.8269028854370117
step 101/334, epoch 468/501 --> loss:0.8306047463417053
step 151/334, epoch 468/501 --> loss:0.829709587097168
step 201/334, epoch 468/501 --> loss:0.8345625376701356
step 251/334, epoch 468/501 --> loss:0.804956487417221
step 301/334, epoch 468/501 --> loss:0.8248334300518035
step 51/334, epoch 469/501 --> loss:0.8170718765258789
step 101/334, epoch 469/501 --> loss:0.8285405564308167
step 151/334, epoch 469/501 --> loss:0.8178516483306885
step 201/334, epoch 469/501 --> loss:0.8213561022281647
step 251/334, epoch 469/501 --> loss:0.8208655667304993
step 301/334, epoch 469/501 --> loss:0.8298387145996093
step 51/334, epoch 470/501 --> loss:0.8164081251621247
step 101/334, epoch 470/501 --> loss:0.8211817169189453
step 151/334, epoch 470/501 --> loss:0.8403303420543671
step 201/334, epoch 470/501 --> loss:0.8165264797210693
step 251/334, epoch 470/501 --> loss:0.8300160229206085
step 301/334, epoch 470/501 --> loss:0.8155441272258759
step 51/334, epoch 471/501 --> loss:0.8183586418628692
step 101/334, epoch 471/501 --> loss:0.8329656171798706
step 151/334, epoch 471/501 --> loss:0.8148260760307312
step 201/334, epoch 471/501 --> loss:0.829594007730484
step 251/334, epoch 471/501 --> loss:0.8236001014709473
step 301/334, epoch 471/501 --> loss:0.8131639468669891

##########train dataset##########
acc--> [97.74563549429088]
F1--> {'F1': [0.7854643431518188], 'precision': [0.6559970854841402], 'recall': [0.9786153552583785]}
##########eval dataset##########
acc--> [97.15053795307408]
F1--> {'F1': [0.733186556399195], 'precision': [0.6158186585848998], 'recall': [0.9058413168469296]}
step 51/334, epoch 472/501 --> loss:0.82673468708992
step 101/334, epoch 472/501 --> loss:0.8237606060504913
step 151/334, epoch 472/501 --> loss:0.8297368323802948
step 201/334, epoch 472/501 --> loss:0.8178242516517639
step 251/334, epoch 472/501 --> loss:0.8130155611038208
step 301/334, epoch 472/501 --> loss:0.8276211285591125
step 51/334, epoch 473/501 --> loss:0.8278088235855102
step 101/334, epoch 473/501 --> loss:0.824535207748413
step 151/334, epoch 473/501 --> loss:0.8315270972251892
step 201/334, epoch 473/501 --> loss:0.8236930787563324
step 251/334, epoch 473/501 --> loss:0.8134296941757202
step 301/334, epoch 473/501 --> loss:0.8243942129611969
step 51/334, epoch 474/501 --> loss:0.8162685596942901
step 101/334, epoch 474/501 --> loss:0.8130039870738983
step 151/334, epoch 474/501 --> loss:0.8248120224475861
step 201/334, epoch 474/501 --> loss:0.8188155233860016
step 251/334, epoch 474/501 --> loss:0.8230595409870147
step 301/334, epoch 474/501 --> loss:0.8300665152072907
step 51/334, epoch 475/501 --> loss:0.8316830372810364
step 101/334, epoch 475/501 --> loss:0.8255665683746338
step 151/334, epoch 475/501 --> loss:0.8198795509338379
step 201/334, epoch 475/501 --> loss:0.8244539141654968
step 251/334, epoch 475/501 --> loss:0.8291834008693695
step 301/334, epoch 475/501 --> loss:0.8109512293338775
step 51/334, epoch 476/501 --> loss:0.8270726943016052
step 101/334, epoch 476/501 --> loss:0.8280081260204315
step 151/334, epoch 476/501 --> loss:0.8277700650691986
step 201/334, epoch 476/501 --> loss:0.8088625288009643
step 251/334, epoch 476/501 --> loss:0.8378167128562928
step 301/334, epoch 476/501 --> loss:0.8211777353286743
step 51/334, epoch 477/501 --> loss:0.8131067991256714
step 101/334, epoch 477/501 --> loss:0.8141327011585235
step 151/334, epoch 477/501 --> loss:0.8285825800895691
step 201/334, epoch 477/501 --> loss:0.8141516506671905
step 251/334, epoch 477/501 --> loss:0.8329662680625916
step 301/334, epoch 477/501 --> loss:0.8316140854358673
step 51/334, epoch 478/501 --> loss:0.8250527966022492
step 101/334, epoch 478/501 --> loss:0.8101221692562103
step 151/334, epoch 478/501 --> loss:0.822325792312622
step 201/334, epoch 478/501 --> loss:0.8236976337432861
step 251/334, epoch 478/501 --> loss:0.8203121733665466
step 301/334, epoch 478/501 --> loss:0.8255412745475769
step 51/334, epoch 479/501 --> loss:0.820867315530777
step 101/334, epoch 479/501 --> loss:0.8270010662078857
step 151/334, epoch 479/501 --> loss:0.818281044960022
step 201/334, epoch 479/501 --> loss:0.8174097418785096
step 251/334, epoch 479/501 --> loss:0.8188858759403229
step 301/334, epoch 479/501 --> loss:0.8328515255451202
step 51/334, epoch 480/501 --> loss:0.8193838167190551
step 101/334, epoch 480/501 --> loss:0.8154649722576142
step 151/334, epoch 480/501 --> loss:0.8158313941955566
step 201/334, epoch 480/501 --> loss:0.8238947904109954
step 251/334, epoch 480/501 --> loss:0.8283246207237244
step 301/334, epoch 480/501 --> loss:0.832352318763733
step 51/334, epoch 481/501 --> loss:0.8185914528369903
step 101/334, epoch 481/501 --> loss:0.8319495940208435
step 151/334, epoch 481/501 --> loss:0.8218038260936738
step 201/334, epoch 481/501 --> loss:0.8135362398624421
step 251/334, epoch 481/501 --> loss:0.8191739916801453
step 301/334, epoch 481/501 --> loss:0.8313072502613068

##########train dataset##########
acc--> [98.99134483952162]
F1--> {'F1': [0.8919273605776158], 'precision': [0.8135538038091277], 'recall': [0.9870230043773859]}
##########eval dataset##########
acc--> [98.26241774404008]
F1--> {'F1': [0.8155292621551722], 'precision': [0.7535165104243415], 'recall': [0.8886761804762578]}
step 51/334, epoch 482/501 --> loss:0.8268993735313416
step 101/334, epoch 482/501 --> loss:0.8345192635059356
step 151/334, epoch 482/501 --> loss:0.8304841423034668
step 201/334, epoch 482/501 --> loss:0.820236793756485
step 251/334, epoch 482/501 --> loss:0.8226096487045288
step 301/334, epoch 482/501 --> loss:0.8106338000297546
step 51/334, epoch 483/501 --> loss:0.8216476881504059
step 101/334, epoch 483/501 --> loss:0.8262069952487946
step 151/334, epoch 483/501 --> loss:0.8341302466392517
step 201/334, epoch 483/501 --> loss:0.8196181726455688
step 251/334, epoch 483/501 --> loss:0.8260237276554108
step 301/334, epoch 483/501 --> loss:0.8275909769535065
step 51/334, epoch 484/501 --> loss:0.8195040512084961
step 101/334, epoch 484/501 --> loss:0.8389947545528412
step 151/334, epoch 484/501 --> loss:0.8144337546825409
step 201/334, epoch 484/501 --> loss:0.8158578777313232
step 251/334, epoch 484/501 --> loss:0.8321035420894622
step 301/334, epoch 484/501 --> loss:0.8205350315570832
step 51/334, epoch 485/501 --> loss:0.81657874584198
step 101/334, epoch 485/501 --> loss:0.8177865719795228
step 151/334, epoch 485/501 --> loss:0.8343022072315216
step 201/334, epoch 485/501 --> loss:0.8392999053001404
step 251/334, epoch 485/501 --> loss:0.8110377252101898
step 301/334, epoch 485/501 --> loss:0.8155348515510559
step 51/334, epoch 486/501 --> loss:0.8236552059650422
step 101/334, epoch 486/501 --> loss:0.8210398030281066
step 151/334, epoch 486/501 --> loss:0.8160063421726227
step 201/334, epoch 486/501 --> loss:0.8296487069129944
step 251/334, epoch 486/501 --> loss:0.8134960544109344
step 301/334, epoch 486/501 --> loss:0.8341411113739013
step 51/334, epoch 487/501 --> loss:0.8183833134174346
step 101/334, epoch 487/501 --> loss:0.8324552261829377
step 151/334, epoch 487/501 --> loss:0.8256957995891571
step 201/334, epoch 487/501 --> loss:0.8267003345489502
step 251/334, epoch 487/501 --> loss:0.8190861511230468
step 301/334, epoch 487/501 --> loss:0.822550641298294
step 51/334, epoch 488/501 --> loss:0.8269813179969787
step 101/334, epoch 488/501 --> loss:0.8141459918022156
step 151/334, epoch 488/501 --> loss:0.8130550181865692
step 201/334, epoch 488/501 --> loss:0.8367836141586303
step 251/334, epoch 488/501 --> loss:0.8189985704421997
step 301/334, epoch 488/501 --> loss:0.8175899183750153
step 51/334, epoch 489/501 --> loss:0.8441915345191956
step 101/334, epoch 489/501 --> loss:0.8112836802005767
step 151/334, epoch 489/501 --> loss:0.8184947335720062
step 201/334, epoch 489/501 --> loss:0.8216242289543152
step 251/334, epoch 489/501 --> loss:0.8226072084903717
step 301/334, epoch 489/501 --> loss:0.8137530565261841
step 51/334, epoch 490/501 --> loss:0.8172415995597839
step 101/334, epoch 490/501 --> loss:0.824497503042221
step 151/334, epoch 490/501 --> loss:0.829841492176056
step 201/334, epoch 490/501 --> loss:0.8222040414810181
step 251/334, epoch 490/501 --> loss:0.8190375638008117
step 301/334, epoch 490/501 --> loss:0.8214823043346405
step 51/334, epoch 491/501 --> loss:0.8217106282711029
step 101/334, epoch 491/501 --> loss:0.8308351445198059
step 151/334, epoch 491/501 --> loss:0.8395270025730133
step 201/334, epoch 491/501 --> loss:0.8154132544994355
step 251/334, epoch 491/501 --> loss:0.8053343427181244
step 301/334, epoch 491/501 --> loss:0.8178814899921417

##########train dataset##########
acc--> [99.06130269711485]
F1--> {'F1': [0.8985521295554254], 'precision': [0.8254858941144545], 'recall': [0.9858209910457564]}
##########eval dataset##########
acc--> [98.38685296859818]
F1--> {'F1': [0.8232925276861244], 'precision': [0.7817731291667817], 'recall': [0.8694805333181532]}
save model!
step 51/334, epoch 492/501 --> loss:0.8322004044055938
step 101/334, epoch 492/501 --> loss:0.8046124565601349
step 151/334, epoch 492/501 --> loss:0.8094702100753784
step 201/334, epoch 492/501 --> loss:0.8221193611621856
step 251/334, epoch 492/501 --> loss:0.826933388710022
step 301/334, epoch 492/501 --> loss:0.8144560658931732
step 51/334, epoch 493/501 --> loss:0.824852567911148
step 101/334, epoch 493/501 --> loss:0.8081115019321442
step 151/334, epoch 493/501 --> loss:0.8304566264152526
step 201/334, epoch 493/501 --> loss:0.8226942265033722
step 251/334, epoch 493/501 --> loss:0.8157764673233032
step 301/334, epoch 493/501 --> loss:0.8256818163394928
step 51/334, epoch 494/501 --> loss:0.8214510428905487
step 101/334, epoch 494/501 --> loss:0.8308907985687256
step 151/334, epoch 494/501 --> loss:0.8288607895374298
step 201/334, epoch 494/501 --> loss:0.8123363518714904
step 251/334, epoch 494/501 --> loss:0.831170563697815
step 301/334, epoch 494/501 --> loss:0.8064802324771881
step 51/334, epoch 495/501 --> loss:0.8291062653064728
step 101/334, epoch 495/501 --> loss:0.8301756727695465
step 151/334, epoch 495/501 --> loss:0.8334186291694641
step 201/334, epoch 495/501 --> loss:0.8057346844673157
step 251/334, epoch 495/501 --> loss:0.8116134703159332
step 301/334, epoch 495/501 --> loss:0.8374762761592865
step 51/334, epoch 496/501 --> loss:0.8229691982269287
step 101/334, epoch 496/501 --> loss:0.8103556776046753
step 151/334, epoch 496/501 --> loss:0.828410005569458
step 201/334, epoch 496/501 --> loss:0.8286115980148315
step 251/334, epoch 496/501 --> loss:0.8195773184299469
step 301/334, epoch 496/501 --> loss:0.8200971210002899
step 51/334, epoch 497/501 --> loss:0.8167170405387878
step 101/334, epoch 497/501 --> loss:0.8297830367088318
step 151/334, epoch 497/501 --> loss:0.8222036933898926
step 201/334, epoch 497/501 --> loss:0.8118293190002441
step 251/334, epoch 497/501 --> loss:0.8283525931835175
step 301/334, epoch 497/501 --> loss:0.8290553247928619
step 51/334, epoch 498/501 --> loss:0.8179278361797333
step 101/334, epoch 498/501 --> loss:0.8242803490161896
step 151/334, epoch 498/501 --> loss:0.8178058755397797
step 201/334, epoch 498/501 --> loss:0.8171015512943268
step 251/334, epoch 498/501 --> loss:0.8306499624252319
step 301/334, epoch 498/501 --> loss:0.8208936619758606
step 51/334, epoch 499/501 --> loss:0.8283619177341461
step 101/334, epoch 499/501 --> loss:0.8137734925746918
step 151/334, epoch 499/501 --> loss:0.8332916903495788
step 201/334, epoch 499/501 --> loss:0.8139880740642548
step 251/334, epoch 499/501 --> loss:0.8314730954170227
step 301/334, epoch 499/501 --> loss:0.8122458243370057
step 51/334, epoch 500/501 --> loss:0.8205662655830384
step 101/334, epoch 500/501 --> loss:0.8374644243717193
step 151/334, epoch 500/501 --> loss:0.8209863531589509
step 201/334, epoch 500/501 --> loss:0.8151452088356018
step 251/334, epoch 500/501 --> loss:0.8262408697605133
step 301/334, epoch 500/501 --> loss:0.8128825283050537
step 51/334, epoch 501/501 --> loss:0.8145059931278229
step 101/334, epoch 501/501 --> loss:0.8283814477920532
step 151/334, epoch 501/501 --> loss:0.829334180355072
step 201/334, epoch 501/501 --> loss:0.8258636832237244
step 251/334, epoch 501/501 --> loss:0.8292308831214905
step 301/334, epoch 501/501 --> loss:0.7995321452617645

##########train dataset##########
acc--> [99.02813442646168]
F1--> {'F1': [0.895366576689584], 'precision': [0.8199518969073006], 'recall': [0.9860708904314067]}
##########eval dataset##########
acc--> [98.33541095829771]
F1--> {'F1': [0.8205736806084035], 'precision': [0.7681459447704815], 'recall': [0.8806937465257839]}
