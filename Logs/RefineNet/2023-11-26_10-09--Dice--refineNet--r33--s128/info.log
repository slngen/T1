##########Config##########
{'device': 'cuda:0', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/RefineNet', 'log_path': 'Logs/RefineNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (backbone): ResNet(
    (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (final): Sequential(
    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (ResidualConvUnit): ResidualConvUnit(
    (0): Sequential(
      (0): ReLU()
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (1): Sequential(
      (0): ReLU()
      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): Sequential(
      (0): ReLU()
      (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): Sequential(
      (0): ReLU()
      (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (MultiResolutionFusion): MultiResolutionFusion(
    (0): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): Sequential(
      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (ChainedResidualPool): ChainedResidualPool(
    (0): ReLU()
    (1-4): 4 x Sequential(
      (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.8848383033275604
step 101/334, epoch 1/501 --> loss:0.8470589470863342
step 151/334, epoch 1/501 --> loss:0.8378191876411438
step 201/334, epoch 1/501 --> loss:0.8479935812950135
step 251/334, epoch 1/501 --> loss:0.8457091009616852
step 301/334, epoch 1/501 --> loss:0.8507360291481018

##########train dataset##########
acc--> [18.03363446756604]
F1--> {'F1': [0.0902786013033971], 'precision': [0.047356264656846545], 'recall': [0.9644180312032591]}
##########eval dataset##########
acc--> [17.650623605214673]
F1--> {'F1': [0.09176995731047476], 'precision': [0.048182216469985446], 'recall': [0.9625902942557879]}
save model!
step 51/334, epoch 2/501 --> loss:0.8510464894771576
step 101/334, epoch 2/501 --> loss:0.8462377226352692
step 151/334, epoch 2/501 --> loss:0.8403970491886139
step 201/334, epoch 2/501 --> loss:0.8525963151454925
step 251/334, epoch 2/501 --> loss:0.856724169254303
step 301/334, epoch 2/501 --> loss:0.8275418376922608
step 51/334, epoch 3/501 --> loss:0.8352572977542877
step 101/334, epoch 3/501 --> loss:0.8435321974754334
step 151/334, epoch 3/501 --> loss:0.8272169983386993
step 201/334, epoch 3/501 --> loss:0.8460357856750488
step 251/334, epoch 3/501 --> loss:0.8517157137393951
step 301/334, epoch 3/501 --> loss:0.8460841584205627
step 51/334, epoch 4/501 --> loss:0.8391902840137482
step 101/334, epoch 4/501 --> loss:0.8389217674732208
step 151/334, epoch 4/501 --> loss:0.8438009548187256
step 201/334, epoch 4/501 --> loss:0.8294090020656586
step 251/334, epoch 4/501 --> loss:0.8555166029930115
step 301/334, epoch 4/501 --> loss:0.8302608346939087
step 51/334, epoch 5/501 --> loss:0.8526965534687042
step 101/334, epoch 5/501 --> loss:0.8313896870613098
step 151/334, epoch 5/501 --> loss:0.8283929967880249
step 201/334, epoch 5/501 --> loss:0.8500033867359161
step 251/334, epoch 5/501 --> loss:0.8466022288799286
step 301/334, epoch 5/501 --> loss:0.8334189093112946
step 51/334, epoch 6/501 --> loss:0.8418572652339935
step 101/334, epoch 6/501 --> loss:0.8482964682579041
step 151/334, epoch 6/501 --> loss:0.8335702860355377
step 201/334, epoch 6/501 --> loss:0.8420953440666199
step 251/334, epoch 6/501 --> loss:0.8413044953346253
step 301/334, epoch 6/501 --> loss:0.8437410378456116
step 51/334, epoch 7/501 --> loss:0.8365205132961273
step 101/334, epoch 7/501 --> loss:0.8314972758293152
step 151/334, epoch 7/501 --> loss:0.8431502902507781
step 201/334, epoch 7/501 --> loss:0.8408560752868652
step 251/334, epoch 7/501 --> loss:0.8531676816940308
step 301/334, epoch 7/501 --> loss:0.836837329864502
step 51/334, epoch 8/501 --> loss:0.8375279414653778
step 101/334, epoch 8/501 --> loss:0.8327865660190582
step 151/334, epoch 8/501 --> loss:0.8406129539012909
step 201/334, epoch 8/501 --> loss:0.8440947103500366
step 251/334, epoch 8/501 --> loss:0.8368259513378143
step 301/334, epoch 8/501 --> loss:0.8434657263755798
step 51/334, epoch 9/501 --> loss:0.8446822583675384
step 101/334, epoch 9/501 --> loss:0.8232358860969543
step 151/334, epoch 9/501 --> loss:0.8527063703536988
step 201/334, epoch 9/501 --> loss:0.8336103618144989
step 251/334, epoch 9/501 --> loss:0.8449723947048188
step 301/334, epoch 9/501 --> loss:0.8416657340526581
step 51/334, epoch 10/501 --> loss:0.8354993546009064
step 101/334, epoch 10/501 --> loss:0.8262827706336975
step 151/334, epoch 10/501 --> loss:0.8360694539546967
step 201/334, epoch 10/501 --> loss:0.8538275110721588
step 251/334, epoch 10/501 --> loss:0.8340963613986969
step 301/334, epoch 10/501 --> loss:0.8447154867649078
step 51/334, epoch 11/501 --> loss:0.8335771453380585
step 101/334, epoch 11/501 --> loss:0.8244610965251923
step 151/334, epoch 11/501 --> loss:0.8393035733699798
step 201/334, epoch 11/501 --> loss:0.8406683933734894
step 251/334, epoch 11/501 --> loss:0.8306508791446686
step 301/334, epoch 11/501 --> loss:0.8472309601306915

##########train dataset##########
acc--> [91.7119026541639]
F1--> {'F1': [0.49175124723674185], 'precision': [0.3316424968403246], 'recall': [0.9507780967387718]}
##########eval dataset##########
acc--> [91.94988874833635]
F1--> {'F1': [0.5004617566918553], 'precision': [0.34194241024249344], 'recall': [0.9330021012405559]}
save model!
step 51/334, epoch 12/501 --> loss:0.8415376901626587
step 101/334, epoch 12/501 --> loss:0.8431334376335144
step 151/334, epoch 12/501 --> loss:0.8233877122402191
step 201/334, epoch 12/501 --> loss:0.8310158443450928
step 251/334, epoch 12/501 --> loss:0.8465570735931397
step 301/334, epoch 12/501 --> loss:0.83991051197052
step 51/334, epoch 13/501 --> loss:0.8403351330757141
step 101/334, epoch 13/501 --> loss:0.831492612361908
step 151/334, epoch 13/501 --> loss:0.8387983083724976
step 201/334, epoch 13/501 --> loss:0.8475173032283783
step 251/334, epoch 13/501 --> loss:0.842525361776352
step 301/334, epoch 13/501 --> loss:0.8254065251350403
step 51/334, epoch 14/501 --> loss:0.8375238692760467
step 101/334, epoch 14/501 --> loss:0.8419767892360688
step 151/334, epoch 14/501 --> loss:0.831906076669693
step 201/334, epoch 14/501 --> loss:0.8452463030815125
step 251/334, epoch 14/501 --> loss:0.837977796792984
step 301/334, epoch 14/501 --> loss:0.8436740469932557
step 51/334, epoch 15/501 --> loss:0.8381472623348236
step 101/334, epoch 15/501 --> loss:0.8392537271976471
step 151/334, epoch 15/501 --> loss:0.828170895576477
step 201/334, epoch 15/501 --> loss:0.8402152419090271
step 251/334, epoch 15/501 --> loss:0.8302894258499145
step 301/334, epoch 15/501 --> loss:0.8463670313358307
step 51/334, epoch 16/501 --> loss:0.8471728503704071
step 101/334, epoch 16/501 --> loss:0.8265299987792969
step 151/334, epoch 16/501 --> loss:0.8395290422439575
step 201/334, epoch 16/501 --> loss:0.8358347845077515
step 251/334, epoch 16/501 --> loss:0.8293599307537078
step 301/334, epoch 16/501 --> loss:0.8414704275131225
step 51/334, epoch 17/501 --> loss:0.8470585989952087
step 101/334, epoch 17/501 --> loss:0.8265721476078034
step 151/334, epoch 17/501 --> loss:0.8455940508842468
step 201/334, epoch 17/501 --> loss:0.8270148396492004
step 251/334, epoch 17/501 --> loss:0.8379961431026459
step 301/334, epoch 17/501 --> loss:0.8309015166759491
step 51/334, epoch 18/501 --> loss:0.8378305160999298
step 101/334, epoch 18/501 --> loss:0.8305075871944427
step 151/334, epoch 18/501 --> loss:0.8339609694480896
step 201/334, epoch 18/501 --> loss:0.8467188119888306
step 251/334, epoch 18/501 --> loss:0.8381824457645416
step 301/334, epoch 18/501 --> loss:0.830319344997406
step 51/334, epoch 19/501 --> loss:0.8304534316062927
step 101/334, epoch 19/501 --> loss:0.8379560983181
step 151/334, epoch 19/501 --> loss:0.8370550894737243
step 201/334, epoch 19/501 --> loss:0.8446107757091522
step 251/334, epoch 19/501 --> loss:0.841479343175888
step 301/334, epoch 19/501 --> loss:0.8348175251483917
step 51/334, epoch 20/501 --> loss:0.8307724702358246
step 101/334, epoch 20/501 --> loss:0.8404806208610535
step 151/334, epoch 20/501 --> loss:0.8412319314479828
step 201/334, epoch 20/501 --> loss:0.8433687090873718
step 251/334, epoch 20/501 --> loss:0.8388712632656098
step 301/334, epoch 20/501 --> loss:0.8303392279148102
step 51/334, epoch 21/501 --> loss:0.8418294262886047
step 101/334, epoch 21/501 --> loss:0.8429542601108551
step 151/334, epoch 21/501 --> loss:0.8290115118026733
step 201/334, epoch 21/501 --> loss:0.8354977905750275
step 251/334, epoch 21/501 --> loss:0.8360534763336182
step 301/334, epoch 21/501 --> loss:0.8204216289520264

##########train dataset##########
acc--> [95.80329548878747]
F1--> {'F1': [0.6523490317398083], 'precision': [0.5013030095852071], 'recall': [0.9336888859819579]}
##########eval dataset##########
acc--> [95.87119195091428]
F1--> {'F1': [0.6578621779318251], 'precision': [0.5124795678267885], 'recall': [0.9184154108774123]}
save model!
step 51/334, epoch 22/501 --> loss:0.8411328577995301
step 101/334, epoch 22/501 --> loss:0.8316742980480194
step 151/334, epoch 22/501 --> loss:0.8310085844993591
step 201/334, epoch 22/501 --> loss:0.8451116359233857
step 251/334, epoch 22/501 --> loss:0.8381548535823822
step 301/334, epoch 22/501 --> loss:0.8361360359191895
step 51/334, epoch 23/501 --> loss:0.8441970229148865
step 101/334, epoch 23/501 --> loss:0.8306794726848602
step 151/334, epoch 23/501 --> loss:0.8233183252811432
step 201/334, epoch 23/501 --> loss:0.8503427112102508
step 251/334, epoch 23/501 --> loss:0.8402702951431275
step 301/334, epoch 23/501 --> loss:0.8266113245487213
step 51/334, epoch 24/501 --> loss:0.8273944771289825
step 101/334, epoch 24/501 --> loss:0.8408358144760132
step 151/334, epoch 24/501 --> loss:0.8285978055000305
step 201/334, epoch 24/501 --> loss:0.8425371694564819
step 251/334, epoch 24/501 --> loss:0.8393512964248657
step 301/334, epoch 24/501 --> loss:0.8280266177654266
step 51/334, epoch 25/501 --> loss:0.8364655113220215
step 101/334, epoch 25/501 --> loss:0.8298557937145233
step 151/334, epoch 25/501 --> loss:0.826497540473938
step 201/334, epoch 25/501 --> loss:0.8418170881271362
step 251/334, epoch 25/501 --> loss:0.8318242228031159
step 301/334, epoch 25/501 --> loss:0.8334220206737518
step 51/334, epoch 26/501 --> loss:0.8442521691322327
step 101/334, epoch 26/501 --> loss:0.8333754897117615
step 151/334, epoch 26/501 --> loss:0.8418518102169037
step 201/334, epoch 26/501 --> loss:0.8371664154529571
step 251/334, epoch 26/501 --> loss:0.8322593581676483
step 301/334, epoch 26/501 --> loss:0.8283850288391114
step 51/334, epoch 27/501 --> loss:0.8403944134712219
step 101/334, epoch 27/501 --> loss:0.82663583278656
step 151/334, epoch 27/501 --> loss:0.826907811164856
step 201/334, epoch 27/501 --> loss:0.8268374180793763
step 251/334, epoch 27/501 --> loss:0.846846854686737
step 301/334, epoch 27/501 --> loss:0.8282848680019379
step 51/334, epoch 28/501 --> loss:0.8407530450820923
step 101/334, epoch 28/501 --> loss:0.8294217526912689
step 151/334, epoch 28/501 --> loss:0.8302676892280578
step 201/334, epoch 28/501 --> loss:0.8440129554271698
step 251/334, epoch 28/501 --> loss:0.8260596001148224
step 301/334, epoch 28/501 --> loss:0.8306946229934692
step 51/334, epoch 29/501 --> loss:0.8307881546020508
step 101/334, epoch 29/501 --> loss:0.8290995252132416
step 151/334, epoch 29/501 --> loss:0.8273161292076111
step 201/334, epoch 29/501 --> loss:0.8431707692146301
step 251/334, epoch 29/501 --> loss:0.8430591535568237
step 301/334, epoch 29/501 --> loss:0.8230215895175934
step 51/334, epoch 30/501 --> loss:0.8300023317337036
step 101/334, epoch 30/501 --> loss:0.8299572312831879
step 151/334, epoch 30/501 --> loss:0.8377927148342132
step 201/334, epoch 30/501 --> loss:0.8339382827281951
step 251/334, epoch 30/501 --> loss:0.8294756245613099
step 301/334, epoch 30/501 --> loss:0.8391127812862397
step 51/334, epoch 31/501 --> loss:0.8334183132648468
step 101/334, epoch 31/501 --> loss:0.8377022516727447
step 151/334, epoch 31/501 --> loss:0.8238050222396851
step 201/334, epoch 31/501 --> loss:0.8286853802204132
step 251/334, epoch 31/501 --> loss:0.831212865114212
step 301/334, epoch 31/501 --> loss:0.8401552546024322

##########train dataset##########
acc--> [97.1082932679957]
F1--> {'F1': [0.7267678003600049], 'precision': [0.6040995630505489], 'recall': [0.9119626306801747]}
##########eval dataset##########
acc--> [97.15352696695955]
F1--> {'F1': [0.7295856546671309], 'precision': [0.6189206606033278], 'recall': [0.8884562651004361]}
save model!
step 51/334, epoch 32/501 --> loss:0.833305344581604
step 101/334, epoch 32/501 --> loss:0.839855613708496
step 151/334, epoch 32/501 --> loss:0.8357252728939056
step 201/334, epoch 32/501 --> loss:0.8328900480270386
step 251/334, epoch 32/501 --> loss:0.8349753987789154
step 301/334, epoch 32/501 --> loss:0.8315758979320527
step 51/334, epoch 33/501 --> loss:0.8310287773609162
step 101/334, epoch 33/501 --> loss:0.8317642295360566
step 151/334, epoch 33/501 --> loss:0.8328900480270386
step 201/334, epoch 33/501 --> loss:0.8198864614963531
step 251/334, epoch 33/501 --> loss:0.8381865525245666
step 301/334, epoch 33/501 --> loss:0.8423703014850616
step 51/334, epoch 34/501 --> loss:0.8406605112552643
step 101/334, epoch 34/501 --> loss:0.8384804391860962
step 151/334, epoch 34/501 --> loss:0.8180814409255981
step 201/334, epoch 34/501 --> loss:0.8329486811161041
step 251/334, epoch 34/501 --> loss:0.8242231643199921
step 301/334, epoch 34/501 --> loss:0.8284838390350342
step 51/334, epoch 35/501 --> loss:0.8377931272983551
step 101/334, epoch 35/501 --> loss:0.8451150989532471
step 151/334, epoch 35/501 --> loss:0.8222341227531433
step 201/334, epoch 35/501 --> loss:0.8345193862915039
step 251/334, epoch 35/501 --> loss:0.8207834398746491
step 301/334, epoch 35/501 --> loss:0.8319487249851227
step 51/334, epoch 36/501 --> loss:0.8535010707378388
step 101/334, epoch 36/501 --> loss:0.8315536785125732
step 151/334, epoch 36/501 --> loss:0.824610503911972
step 201/334, epoch 36/501 --> loss:0.841023577451706
step 251/334, epoch 36/501 --> loss:0.8237620067596435
step 301/334, epoch 36/501 --> loss:0.8144315421581269
step 51/334, epoch 37/501 --> loss:0.8186165082454682
step 101/334, epoch 37/501 --> loss:0.8346883940696717
step 151/334, epoch 37/501 --> loss:0.8182702958583832
step 201/334, epoch 37/501 --> loss:0.8262887454032898
step 251/334, epoch 37/501 --> loss:0.8464742934703827
step 301/334, epoch 37/501 --> loss:0.8298187911510467
step 51/334, epoch 38/501 --> loss:0.8352698361873627
step 101/334, epoch 38/501 --> loss:0.8300147187709809
step 151/334, epoch 38/501 --> loss:0.8305286371707916
step 201/334, epoch 38/501 --> loss:0.8246915864944458
step 251/334, epoch 38/501 --> loss:0.8291887640953064
step 301/334, epoch 38/501 --> loss:0.8364730155467988
step 51/334, epoch 39/501 --> loss:0.8441619861125946
step 101/334, epoch 39/501 --> loss:0.8340002393722534
step 151/334, epoch 39/501 --> loss:0.825202157497406
step 201/334, epoch 39/501 --> loss:0.8137358939647674
step 251/334, epoch 39/501 --> loss:0.8282866311073304
step 301/334, epoch 39/501 --> loss:0.8378171682357788
step 51/334, epoch 40/501 --> loss:0.8388040947914124
step 101/334, epoch 40/501 --> loss:0.813386685848236
step 151/334, epoch 40/501 --> loss:0.8387104403972626
step 201/334, epoch 40/501 --> loss:0.8408276879787445
step 251/334, epoch 40/501 --> loss:0.8369099116325378
step 301/334, epoch 40/501 --> loss:0.8214903378486633
step 51/334, epoch 41/501 --> loss:0.8338467466831208
step 101/334, epoch 41/501 --> loss:0.8247530746459961
step 151/334, epoch 41/501 --> loss:0.8382338917255402
step 201/334, epoch 41/501 --> loss:0.824258235692978
step 251/334, epoch 41/501 --> loss:0.8323992884159088
step 301/334, epoch 41/501 --> loss:0.8338234281539917

##########train dataset##########
acc--> [96.67924667146433]
F1--> {'F1': [0.7065919859107738], 'precision': [0.5631204159456544], 'recall': [0.9481818971766129]}
##########eval dataset##########
acc--> [96.63905813728815]
F1--> {'F1': [0.7043385397444506], 'precision': [0.5682124122916442], 'recall': [0.9262523671003413]}
step 51/334, epoch 42/501 --> loss:0.8301150298118591
step 101/334, epoch 42/501 --> loss:0.8213150906562805
step 151/334, epoch 42/501 --> loss:0.8311123037338257
step 201/334, epoch 42/501 --> loss:0.8335563993453979
step 251/334, epoch 42/501 --> loss:0.8427696645259857
step 301/334, epoch 42/501 --> loss:0.8401674783229828
step 51/334, epoch 43/501 --> loss:0.8385465157032013
step 101/334, epoch 43/501 --> loss:0.8318235230445862
step 151/334, epoch 43/501 --> loss:0.8265940070152282
step 201/334, epoch 43/501 --> loss:0.834562863111496
step 251/334, epoch 43/501 --> loss:0.832486002445221
step 301/334, epoch 43/501 --> loss:0.8303648972511292
step 51/334, epoch 44/501 --> loss:0.8282418119907379
step 101/334, epoch 44/501 --> loss:0.8435007166862488
step 151/334, epoch 44/501 --> loss:0.8249055552482605
step 201/334, epoch 44/501 --> loss:0.8371187770366668
step 251/334, epoch 44/501 --> loss:0.8316770029067994
step 301/334, epoch 44/501 --> loss:0.8236677086353302
step 51/334, epoch 45/501 --> loss:0.8247812294960022
step 101/334, epoch 45/501 --> loss:0.8386982178688049
step 151/334, epoch 45/501 --> loss:0.8226471292972565
step 201/334, epoch 45/501 --> loss:0.8414353895187378
step 251/334, epoch 45/501 --> loss:0.8316441738605499
step 301/334, epoch 45/501 --> loss:0.8366897368431091
step 51/334, epoch 46/501 --> loss:0.8303318166732788
step 101/334, epoch 46/501 --> loss:0.845295580625534
step 151/334, epoch 46/501 --> loss:0.8388996148109436
step 201/334, epoch 46/501 --> loss:0.8262387895584107
step 251/334, epoch 46/501 --> loss:0.8206791901588439
step 301/334, epoch 46/501 --> loss:0.8198467648029327
step 51/334, epoch 47/501 --> loss:0.8282346451282501
step 101/334, epoch 47/501 --> loss:0.8266326355934143
step 151/334, epoch 47/501 --> loss:0.8262533164024353
step 201/334, epoch 47/501 --> loss:0.8295199012756348
step 251/334, epoch 47/501 --> loss:0.8410661697387696
step 301/334, epoch 47/501 --> loss:0.8204227042198181
step 51/334, epoch 48/501 --> loss:0.8269685626029968
step 101/334, epoch 48/501 --> loss:0.822698929309845
step 151/334, epoch 48/501 --> loss:0.8267988324165344
step 201/334, epoch 48/501 --> loss:0.833660340309143
step 251/334, epoch 48/501 --> loss:0.8367925357818603
step 301/334, epoch 48/501 --> loss:0.8226184904575348
step 51/334, epoch 49/501 --> loss:0.8245209360122681
step 101/334, epoch 49/501 --> loss:0.8391159749031067
step 151/334, epoch 49/501 --> loss:0.8341995680332184
step 201/334, epoch 49/501 --> loss:0.8236310052871704
step 251/334, epoch 49/501 --> loss:0.818147828578949
step 301/334, epoch 49/501 --> loss:0.8270934402942658
step 51/334, epoch 50/501 --> loss:0.8303736114501953
step 101/334, epoch 50/501 --> loss:0.8168899571895599
step 151/334, epoch 50/501 --> loss:0.8455030083656311
step 201/334, epoch 50/501 --> loss:0.8137845313549041
step 251/334, epoch 50/501 --> loss:0.828352153301239
step 301/334, epoch 50/501 --> loss:0.8285367584228516
step 51/334, epoch 51/501 --> loss:0.8288946330547333
step 101/334, epoch 51/501 --> loss:0.8189035654067993
step 151/334, epoch 51/501 --> loss:0.8373561513423919
step 201/334, epoch 51/501 --> loss:0.8432812309265136
step 251/334, epoch 51/501 --> loss:0.8326723992824554
step 301/334, epoch 51/501 --> loss:0.8188602340221405

##########train dataset##########
acc--> [98.2088613206158]
F1--> {'F1': [0.8148494528688142], 'precision': [0.7222848222803705], 'recall': [0.9346397789211668]}
##########eval dataset##########
acc--> [98.04346079587907]
F1--> {'F1': [0.7991352404485228], 'precision': [0.7182812703435182], 'recall': [0.9005134653367648]}
save model!
step 51/334, epoch 52/501 --> loss:0.8296079039573669
step 101/334, epoch 52/501 --> loss:0.8272024989128113
step 151/334, epoch 52/501 --> loss:0.8355152773857116
step 201/334, epoch 52/501 --> loss:0.8249328792095184
step 251/334, epoch 52/501 --> loss:0.8285302138328552
step 301/334, epoch 52/501 --> loss:0.8369528305530548
step 51/334, epoch 53/501 --> loss:0.8289679920673371
step 101/334, epoch 53/501 --> loss:0.8203324294090271
step 151/334, epoch 53/501 --> loss:0.8362657308578492
step 201/334, epoch 53/501 --> loss:0.8375859832763672
step 251/334, epoch 53/501 --> loss:0.8282735252380371
step 301/334, epoch 53/501 --> loss:0.8183160936832428
step 51/334, epoch 54/501 --> loss:0.8308372116088867
step 101/334, epoch 54/501 --> loss:0.8313660669326782
step 151/334, epoch 54/501 --> loss:0.8215482068061829
step 201/334, epoch 54/501 --> loss:0.8402164876461029
step 251/334, epoch 54/501 --> loss:0.8274523949623108
step 301/334, epoch 54/501 --> loss:0.8229140853881836
step 51/334, epoch 55/501 --> loss:0.824776599407196
step 101/334, epoch 55/501 --> loss:0.8321642935276031
step 151/334, epoch 55/501 --> loss:0.8229908287525177
step 201/334, epoch 55/501 --> loss:0.8268493390083314
step 251/334, epoch 55/501 --> loss:0.8342824172973633
step 301/334, epoch 55/501 --> loss:0.8370124864578247
step 51/334, epoch 56/501 --> loss:0.8207344353199005
step 101/334, epoch 56/501 --> loss:0.8314871621131897
step 151/334, epoch 56/501 --> loss:0.8348549914360046
step 201/334, epoch 56/501 --> loss:0.8306193137168885
step 251/334, epoch 56/501 --> loss:0.8378132569789887
step 301/334, epoch 56/501 --> loss:0.8287375664710999
step 51/334, epoch 57/501 --> loss:0.8398351800441742
step 101/334, epoch 57/501 --> loss:0.8271268701553345
step 151/334, epoch 57/501 --> loss:0.8297133696079254
step 201/334, epoch 57/501 --> loss:0.8291155982017517
step 251/334, epoch 57/501 --> loss:0.8340909230709076
step 301/334, epoch 57/501 --> loss:0.8271911203861236
step 51/334, epoch 58/501 --> loss:0.8223694467544556
step 101/334, epoch 58/501 --> loss:0.8247155618667602
step 151/334, epoch 58/501 --> loss:0.8353018260002136
step 201/334, epoch 58/501 --> loss:0.8342969679832458
step 251/334, epoch 58/501 --> loss:0.8230747175216675
step 301/334, epoch 58/501 --> loss:0.8366226541996002
step 51/334, epoch 59/501 --> loss:0.8413659822940827
step 101/334, epoch 59/501 --> loss:0.8325366759300232
step 151/334, epoch 59/501 --> loss:0.8329433679580689
step 201/334, epoch 59/501 --> loss:0.8164625227451324
step 251/334, epoch 59/501 --> loss:0.824029483795166
step 301/334, epoch 59/501 --> loss:0.8199717676639557
step 51/334, epoch 60/501 --> loss:0.8385590946674347
step 101/334, epoch 60/501 --> loss:0.8254944002628326
step 151/334, epoch 60/501 --> loss:0.818111480474472
step 201/334, epoch 60/501 --> loss:0.8457643604278564
step 251/334, epoch 60/501 --> loss:0.8182122373580932
step 301/334, epoch 60/501 --> loss:0.8302322554588318
step 51/334, epoch 61/501 --> loss:0.8268477547168732
step 101/334, epoch 61/501 --> loss:0.8300688076019287
step 151/334, epoch 61/501 --> loss:0.8276490974426269
step 201/334, epoch 61/501 --> loss:0.83709552526474
step 251/334, epoch 61/501 --> loss:0.8261054301261902
step 301/334, epoch 61/501 --> loss:0.8274360525608063

##########train dataset##########
acc--> [97.90876550892284]
F1--> {'F1': [0.794149729867893], 'precision': [0.6788886621371857], 'recall': [0.9565663471669851]}
##########eval dataset##########
acc--> [97.70406888225233]
F1--> {'F1': [0.7774340878586562], 'precision': [0.669024140084581], 'recall': [0.9277859062028062]}
step 51/334, epoch 62/501 --> loss:0.8242505550384521
step 101/334, epoch 62/501 --> loss:0.8247359645366669
step 151/334, epoch 62/501 --> loss:0.8252456152439117
step 201/334, epoch 62/501 --> loss:0.8363614785671234
step 251/334, epoch 62/501 --> loss:0.8327007246017456
step 301/334, epoch 62/501 --> loss:0.8264071881771088
step 51/334, epoch 63/501 --> loss:0.823490492105484
step 101/334, epoch 63/501 --> loss:0.8201638782024383
step 151/334, epoch 63/501 --> loss:0.8311762177944183
step 201/334, epoch 63/501 --> loss:0.8383866775035859
step 251/334, epoch 63/501 --> loss:0.8256900155544281
step 301/334, epoch 63/501 --> loss:0.8357755398750305
step 51/334, epoch 64/501 --> loss:0.8259152328968048
step 101/334, epoch 64/501 --> loss:0.8273917472362519
step 151/334, epoch 64/501 --> loss:0.8181598997116089
step 201/334, epoch 64/501 --> loss:0.827114337682724
step 251/334, epoch 64/501 --> loss:0.837912027835846
step 301/334, epoch 64/501 --> loss:0.8365927577018738
step 51/334, epoch 65/501 --> loss:0.8339214932918548
step 101/334, epoch 65/501 --> loss:0.8375080323219299
step 151/334, epoch 65/501 --> loss:0.8053898227214813
step 201/334, epoch 65/501 --> loss:0.8211092031002045
step 251/334, epoch 65/501 --> loss:0.8317955601215362
step 301/334, epoch 65/501 --> loss:0.8323378050327301
step 51/334, epoch 66/501 --> loss:0.8243202531337738
step 101/334, epoch 66/501 --> loss:0.8264602744579315
step 151/334, epoch 66/501 --> loss:0.8301935231685639
step 201/334, epoch 66/501 --> loss:0.827570811510086
step 251/334, epoch 66/501 --> loss:0.8264786577224732
step 301/334, epoch 66/501 --> loss:0.8193972337245942
step 51/334, epoch 67/501 --> loss:0.8358083462715149
step 101/334, epoch 67/501 --> loss:0.8282741093635559
step 151/334, epoch 67/501 --> loss:0.8188685274124146
step 201/334, epoch 67/501 --> loss:0.8351655101776123
step 251/334, epoch 67/501 --> loss:0.8304646873474121
step 301/334, epoch 67/501 --> loss:0.8173664128780365
step 51/334, epoch 68/501 --> loss:0.8267950236797332
step 101/334, epoch 68/501 --> loss:0.8294576549530029
step 151/334, epoch 68/501 --> loss:0.8228022003173828
step 201/334, epoch 68/501 --> loss:0.8318499958515168
step 251/334, epoch 68/501 --> loss:0.8143034636974334
step 301/334, epoch 68/501 --> loss:0.8407397866249084
step 51/334, epoch 69/501 --> loss:0.835287035703659
step 101/334, epoch 69/501 --> loss:0.8314153802394867
step 151/334, epoch 69/501 --> loss:0.8253308761119843
step 201/334, epoch 69/501 --> loss:0.8213875794410705
step 251/334, epoch 69/501 --> loss:0.8265514624118805
step 301/334, epoch 69/501 --> loss:0.8244423580169677
step 51/334, epoch 70/501 --> loss:0.824618147611618
step 101/334, epoch 70/501 --> loss:0.8256636023521423
step 151/334, epoch 70/501 --> loss:0.8297452676296234
step 201/334, epoch 70/501 --> loss:0.8234515476226807
step 251/334, epoch 70/501 --> loss:0.8219041228294373
step 301/334, epoch 70/501 --> loss:0.8336767828464509
step 51/334, epoch 71/501 --> loss:0.8170191287994385
step 101/334, epoch 71/501 --> loss:0.8214652383327484
step 151/334, epoch 71/501 --> loss:0.8296460127830505
step 201/334, epoch 71/501 --> loss:0.8237557983398438
step 251/334, epoch 71/501 --> loss:0.8349474549293519
step 301/334, epoch 71/501 --> loss:0.8283912897109985

##########train dataset##########
acc--> [97.62924932387322]
F1--> {'F1': [0.7742974353199938], 'precision': [0.6468463646909263], 'recall': [0.9643122106165016]}
##########eval dataset##########
acc--> [97.5288624734221]
F1--> {'F1': [0.7658387301313092], 'precision': [0.6485258371093299], 'recall': [0.9349807218831321]}
step 51/334, epoch 72/501 --> loss:0.8205618345737458
step 101/334, epoch 72/501 --> loss:0.8278793096542358
step 151/334, epoch 72/501 --> loss:0.8314328300952911
step 201/334, epoch 72/501 --> loss:0.8328213882446289
step 251/334, epoch 72/501 --> loss:0.8234637749195098
step 301/334, epoch 72/501 --> loss:0.8297885811328888
step 51/334, epoch 73/501 --> loss:0.8269419324398041
step 101/334, epoch 73/501 --> loss:0.8180479693412781
step 151/334, epoch 73/501 --> loss:0.8332744574546814
step 201/334, epoch 73/501 --> loss:0.8248283207416535
step 251/334, epoch 73/501 --> loss:0.8392137742042541
step 301/334, epoch 73/501 --> loss:0.8241864156723022
step 51/334, epoch 74/501 --> loss:0.8246326863765716
step 101/334, epoch 74/501 --> loss:0.8396696639060974
step 151/334, epoch 74/501 --> loss:0.8229229927062989
step 201/334, epoch 74/501 --> loss:0.8212646222114564
step 251/334, epoch 74/501 --> loss:0.8286511087417603
step 301/334, epoch 74/501 --> loss:0.8209104096889496
step 51/334, epoch 75/501 --> loss:0.8254031765460969
step 101/334, epoch 75/501 --> loss:0.8215950977802277
step 151/334, epoch 75/501 --> loss:0.83220174908638
step 201/334, epoch 75/501 --> loss:0.8266288638114929
step 251/334, epoch 75/501 --> loss:0.8279039597511292
step 301/334, epoch 75/501 --> loss:0.8255278158187866
step 51/334, epoch 76/501 --> loss:0.8222180521488189
step 101/334, epoch 76/501 --> loss:0.8310995984077454
step 151/334, epoch 76/501 --> loss:0.8227494394779206
step 201/334, epoch 76/501 --> loss:0.8284567701816559
step 251/334, epoch 76/501 --> loss:0.8333278799057007
step 301/334, epoch 76/501 --> loss:0.8308360719680786
step 51/334, epoch 77/501 --> loss:0.8176047527790069
step 101/334, epoch 77/501 --> loss:0.8089924466609955
step 151/334, epoch 77/501 --> loss:0.8219973564147949
step 201/334, epoch 77/501 --> loss:0.8489100205898285
step 251/334, epoch 77/501 --> loss:0.8378187394142151
step 301/334, epoch 77/501 --> loss:0.8202189326286315
step 51/334, epoch 78/501 --> loss:0.817551108598709
step 101/334, epoch 78/501 --> loss:0.833823903799057
step 151/334, epoch 78/501 --> loss:0.821355232000351
step 201/334, epoch 78/501 --> loss:0.836071959733963
step 251/334, epoch 78/501 --> loss:0.8238610982894897
step 301/334, epoch 78/501 --> loss:0.8251595282554627
step 51/334, epoch 79/501 --> loss:0.8391174685955047
step 101/334, epoch 79/501 --> loss:0.8221046328544617
step 151/334, epoch 79/501 --> loss:0.8316445064544677
step 201/334, epoch 79/501 --> loss:0.8317439520359039
step 251/334, epoch 79/501 --> loss:0.8129998075962067
step 301/334, epoch 79/501 --> loss:0.8283185636997223
step 51/334, epoch 80/501 --> loss:0.8334590244293213
step 101/334, epoch 80/501 --> loss:0.8249947690963745
step 151/334, epoch 80/501 --> loss:0.82437735080719
step 201/334, epoch 80/501 --> loss:0.8461279106140137
step 251/334, epoch 80/501 --> loss:0.8307537114620209
step 301/334, epoch 80/501 --> loss:0.8192825591564179
step 51/334, epoch 81/501 --> loss:0.8278084325790406
step 101/334, epoch 81/501 --> loss:0.8387259483337403
step 151/334, epoch 81/501 --> loss:0.8391779911518097
step 201/334, epoch 81/501 --> loss:0.8152029144763947
step 251/334, epoch 81/501 --> loss:0.8206069350242615
step 301/334, epoch 81/501 --> loss:0.8213225412368774

##########train dataset##########
acc--> [98.0507208785153]
F1--> {'F1': [0.8055684941403227], 'precision': [0.695218397626618], 'recall': [0.95757299941537]}
##########eval dataset##########
acc--> [97.74640835071354]
F1--> {'F1': [0.7793964720826339], 'precision': [0.6754886086166229], 'recall': [0.9210968649998794]}
step 51/334, epoch 82/501 --> loss:0.8341828072071076
step 101/334, epoch 82/501 --> loss:0.8138144838809968
step 151/334, epoch 82/501 --> loss:0.8256258416175842
step 201/334, epoch 82/501 --> loss:0.8290370166301727
step 251/334, epoch 82/501 --> loss:0.8226943492889405
step 301/334, epoch 82/501 --> loss:0.8253406667709351
step 51/334, epoch 83/501 --> loss:0.8502481639385223
step 101/334, epoch 83/501 --> loss:0.8338218748569488
step 151/334, epoch 83/501 --> loss:0.8274601769447326
step 201/334, epoch 83/501 --> loss:0.8059942996501923
step 251/334, epoch 83/501 --> loss:0.837994738817215
step 301/334, epoch 83/501 --> loss:0.8217438566684723
step 51/334, epoch 84/501 --> loss:0.8299172174930572
step 101/334, epoch 84/501 --> loss:0.8186197435855865
step 151/334, epoch 84/501 --> loss:0.81758669257164
step 201/334, epoch 84/501 --> loss:0.8321384358406066
step 251/334, epoch 84/501 --> loss:0.8307473969459533
step 301/334, epoch 84/501 --> loss:0.8248859751224518
step 51/334, epoch 85/501 --> loss:0.8297073757648468
step 101/334, epoch 85/501 --> loss:0.8433664906024932
step 151/334, epoch 85/501 --> loss:0.8364872467517853
step 201/334, epoch 85/501 --> loss:0.8251178681850433
step 251/334, epoch 85/501 --> loss:0.8271632790565491
step 301/334, epoch 85/501 --> loss:0.8081360542774201
step 51/334, epoch 86/501 --> loss:0.8406042444705963
step 101/334, epoch 86/501 --> loss:0.8219344282150268
step 151/334, epoch 86/501 --> loss:0.8399887895584106
step 201/334, epoch 86/501 --> loss:0.8226350855827331
step 251/334, epoch 86/501 --> loss:0.821625452041626
step 301/334, epoch 86/501 --> loss:0.830149929523468
step 51/334, epoch 87/501 --> loss:0.8173053300380707
step 101/334, epoch 87/501 --> loss:0.8126426041126251
step 151/334, epoch 87/501 --> loss:0.8335690033435822
step 201/334, epoch 87/501 --> loss:0.822864488363266
step 251/334, epoch 87/501 --> loss:0.8184169328212738
step 301/334, epoch 87/501 --> loss:0.8356004381179809
step 51/334, epoch 88/501 --> loss:0.8281552970409394
step 101/334, epoch 88/501 --> loss:0.8273260366916656
step 151/334, epoch 88/501 --> loss:0.8383333349227905
step 201/334, epoch 88/501 --> loss:0.821205552816391
step 251/334, epoch 88/501 --> loss:0.811574000120163
step 301/334, epoch 88/501 --> loss:0.8301300609111786
step 51/334, epoch 89/501 --> loss:0.8219949436187745
step 101/334, epoch 89/501 --> loss:0.8359983062744141
step 151/334, epoch 89/501 --> loss:0.8302555453777313
step 201/334, epoch 89/501 --> loss:0.8280725347995758
step 251/334, epoch 89/501 --> loss:0.8199741792678833
step 301/334, epoch 89/501 --> loss:0.8342553007602692
step 51/334, epoch 90/501 --> loss:0.8318300008773803
step 101/334, epoch 90/501 --> loss:0.827894583940506
step 151/334, epoch 90/501 --> loss:0.8195626735687256
step 201/334, epoch 90/501 --> loss:0.825783120393753
step 251/334, epoch 90/501 --> loss:0.8305929839611054
step 301/334, epoch 90/501 --> loss:0.8154094910621643
step 51/334, epoch 91/501 --> loss:0.8290540218353272
step 101/334, epoch 91/501 --> loss:0.8103664565086365
step 151/334, epoch 91/501 --> loss:0.8384457290172577
step 201/334, epoch 91/501 --> loss:0.8436494827270508
step 251/334, epoch 91/501 --> loss:0.8247154223918914
step 301/334, epoch 91/501 --> loss:0.827479156255722

##########train dataset##########
acc--> [97.01356347866518]
F1--> {'F1': [0.718951055269204], 'precision': [0.5960156550375723], 'recall': [0.9057940366430197]}
##########eval dataset##########
acc--> [96.8220554728873]
F1--> {'F1': [0.7034675038161079], 'precision': [0.5894588716545751], 'recall': [0.872167392664043]}
step 51/334, epoch 92/501 --> loss:0.8326359057426452
step 101/334, epoch 92/501 --> loss:0.829119359254837
step 151/334, epoch 92/501 --> loss:0.8355296766757965
step 201/334, epoch 92/501 --> loss:0.8279336130619049
step 251/334, epoch 92/501 --> loss:0.8141766834259033
step 301/334, epoch 92/501 --> loss:0.8221859276294708
step 51/334, epoch 93/501 --> loss:0.8278554487228393
step 101/334, epoch 93/501 --> loss:0.8354916310310364
step 151/334, epoch 93/501 --> loss:0.8269962763786316
step 201/334, epoch 93/501 --> loss:0.8207358765602112
step 251/334, epoch 93/501 --> loss:0.8235426354408264
step 301/334, epoch 93/501 --> loss:0.825709594488144
step 51/334, epoch 94/501 --> loss:0.82903648853302
step 101/334, epoch 94/501 --> loss:0.8083101916313171
step 151/334, epoch 94/501 --> loss:0.8326880860328675
step 201/334, epoch 94/501 --> loss:0.8353303670883179
step 251/334, epoch 94/501 --> loss:0.826106538772583
step 301/334, epoch 94/501 --> loss:0.8204439616203308
step 51/334, epoch 95/501 --> loss:0.8418332028388977
step 101/334, epoch 95/501 --> loss:0.8184424078464508
step 151/334, epoch 95/501 --> loss:0.8319801270961762
step 201/334, epoch 95/501 --> loss:0.8138123416900634
step 251/334, epoch 95/501 --> loss:0.8256396043300629
step 301/334, epoch 95/501 --> loss:0.8324378299713134
step 51/334, epoch 96/501 --> loss:0.8172687768936158
step 101/334, epoch 96/501 --> loss:0.840366690158844
step 151/334, epoch 96/501 --> loss:0.8198531186580658
step 201/334, epoch 96/501 --> loss:0.8262351322174072
step 251/334, epoch 96/501 --> loss:0.8299828684329986
step 301/334, epoch 96/501 --> loss:0.8260763835906982
step 51/334, epoch 97/501 --> loss:0.8270337700843811
step 101/334, epoch 97/501 --> loss:0.824924920797348
step 151/334, epoch 97/501 --> loss:0.8198823153972625
step 201/334, epoch 97/501 --> loss:0.8372552335262299
step 251/334, epoch 97/501 --> loss:0.8399384808540344
step 301/334, epoch 97/501 --> loss:0.8207674908638001
step 51/334, epoch 98/501 --> loss:0.8107037329673767
step 101/334, epoch 98/501 --> loss:0.8298825764656067
step 151/334, epoch 98/501 --> loss:0.8308236014842987
step 201/334, epoch 98/501 --> loss:0.8332603168487549
step 251/334, epoch 98/501 --> loss:0.8155079352855682
step 301/334, epoch 98/501 --> loss:0.8357160568237305
step 51/334, epoch 99/501 --> loss:0.809555196762085
step 101/334, epoch 99/501 --> loss:0.8365279412269593
step 151/334, epoch 99/501 --> loss:0.8258544743061066
step 201/334, epoch 99/501 --> loss:0.8212040531635284
step 251/334, epoch 99/501 --> loss:0.8206007361412049
step 301/334, epoch 99/501 --> loss:0.8276713693141937
step 51/334, epoch 100/501 --> loss:0.8297608876228333
step 101/334, epoch 100/501 --> loss:0.8103742861747741
step 151/334, epoch 100/501 --> loss:0.8244139134883881
step 201/334, epoch 100/501 --> loss:0.8375986802577973
step 251/334, epoch 100/501 --> loss:0.826942572593689
step 301/334, epoch 100/501 --> loss:0.8234900403022766
step 51/334, epoch 101/501 --> loss:0.8166993999481201
step 101/334, epoch 101/501 --> loss:0.8272733438014984
step 151/334, epoch 101/501 --> loss:0.822458610534668
step 201/334, epoch 101/501 --> loss:0.8232435810565949
step 251/334, epoch 101/501 --> loss:0.8422481989860535
step 301/334, epoch 101/501 --> loss:0.8305021500587464

##########train dataset##########
acc--> [98.48996345854435]
F1--> {'F1': [0.845349502505822], 'precision': [0.744003873403981], 'recall': [0.9786721999069059]}
##########eval dataset##########
acc--> [98.18475411215032]
F1--> {'F1': [0.8164116295727363], 'precision': [0.7252080041285112], 'recall': [0.9338680180328541]}
save model!
step 51/334, epoch 102/501 --> loss:0.8031550252437591
step 101/334, epoch 102/501 --> loss:0.813243637084961
step 151/334, epoch 102/501 --> loss:0.8236679577827454
step 201/334, epoch 102/501 --> loss:0.8175471031665802
step 251/334, epoch 102/501 --> loss:0.8422227227687835
step 301/334, epoch 102/501 --> loss:0.8388460683822632
step 51/334, epoch 103/501 --> loss:0.8171403217315674
step 101/334, epoch 103/501 --> loss:0.8169426155090332
step 151/334, epoch 103/501 --> loss:0.8386772179603577
step 201/334, epoch 103/501 --> loss:0.8274699294567108
step 251/334, epoch 103/501 --> loss:0.8150612878799438
step 301/334, epoch 103/501 --> loss:0.830308598279953
step 51/334, epoch 104/501 --> loss:0.827599948644638
step 101/334, epoch 104/501 --> loss:0.8184653341770172
step 151/334, epoch 104/501 --> loss:0.8347815597057342
step 201/334, epoch 104/501 --> loss:0.8260066258907318
step 251/334, epoch 104/501 --> loss:0.8192344725131988
step 301/334, epoch 104/501 --> loss:0.8198830378055573
step 51/334, epoch 105/501 --> loss:0.8194192028045655
step 101/334, epoch 105/501 --> loss:0.819548556804657
step 151/334, epoch 105/501 --> loss:0.8230281066894531
step 201/334, epoch 105/501 --> loss:0.8252014124393463
step 251/334, epoch 105/501 --> loss:0.8182089531421661
step 301/334, epoch 105/501 --> loss:0.8274013519287109
step 51/334, epoch 106/501 --> loss:0.8233971548080444
step 101/334, epoch 106/501 --> loss:0.8227039170265198
step 151/334, epoch 106/501 --> loss:0.8271122348308563
step 201/334, epoch 106/501 --> loss:0.8248219358921051
step 251/334, epoch 106/501 --> loss:0.8355030179023742
step 301/334, epoch 106/501 --> loss:0.8192182219028473
step 51/334, epoch 107/501 --> loss:0.8155713570117951
step 101/334, epoch 107/501 --> loss:0.8264586448669433
step 151/334, epoch 107/501 --> loss:0.8170610511302948
step 201/334, epoch 107/501 --> loss:0.821688494682312
step 251/334, epoch 107/501 --> loss:0.8413739120960235
step 301/334, epoch 107/501 --> loss:0.8289407813549041
step 51/334, epoch 108/501 --> loss:0.827648196220398
step 101/334, epoch 108/501 --> loss:0.813065220117569
step 151/334, epoch 108/501 --> loss:0.8375767028331756
step 201/334, epoch 108/501 --> loss:0.8252745831012726
step 251/334, epoch 108/501 --> loss:0.8174645209312439
step 301/334, epoch 108/501 --> loss:0.8250217592716217
step 51/334, epoch 109/501 --> loss:0.8262746870517731
step 101/334, epoch 109/501 --> loss:0.8186704897880555
step 151/334, epoch 109/501 --> loss:0.8262443137168884
step 201/334, epoch 109/501 --> loss:0.8212780618667602
step 251/334, epoch 109/501 --> loss:0.8401326644420624
step 301/334, epoch 109/501 --> loss:0.8218848311901092
step 51/334, epoch 110/501 --> loss:0.8293058693408966
step 101/334, epoch 110/501 --> loss:0.8201070141792297
step 151/334, epoch 110/501 --> loss:0.8158391857147217
step 201/334, epoch 110/501 --> loss:0.8236084198951721
step 251/334, epoch 110/501 --> loss:0.8195393717288971
step 301/334, epoch 110/501 --> loss:0.8369194567203522
step 51/334, epoch 111/501 --> loss:0.8324238002300263
step 101/334, epoch 111/501 --> loss:0.8417193400859833
step 151/334, epoch 111/501 --> loss:0.8231837141513825
step 201/334, epoch 111/501 --> loss:0.819579975605011
step 251/334, epoch 111/501 --> loss:0.825681185722351
step 301/334, epoch 111/501 --> loss:0.8097212898731232

##########train dataset##########
acc--> [98.77722975820366]
F1--> {'F1': [0.871012798482324], 'precision': [0.7844785456706678], 'recall': [0.9790173378206378]}
##########eval dataset##########
acc--> [98.34032367428634]
F1--> {'F1': [0.8268796033942692], 'precision': [0.7528462860367992], 'recall': [0.9170736799889717]}
save model!
step 51/334, epoch 112/501 --> loss:0.8339135432243348
step 101/334, epoch 112/501 --> loss:0.8219501948356629
step 151/334, epoch 112/501 --> loss:0.8126601672172546
step 201/334, epoch 112/501 --> loss:0.8172851252555847
step 251/334, epoch 112/501 --> loss:0.8218831551074982
step 301/334, epoch 112/501 --> loss:0.8309277427196503
step 51/334, epoch 113/501 --> loss:0.8065464353561401
step 101/334, epoch 113/501 --> loss:0.8283630466461182
step 151/334, epoch 113/501 --> loss:0.8199599170684815
step 201/334, epoch 113/501 --> loss:0.8264173316955566
step 251/334, epoch 113/501 --> loss:0.824446382522583
step 301/334, epoch 113/501 --> loss:0.8330623924732208
step 51/334, epoch 114/501 --> loss:0.8347567403316498
step 101/334, epoch 114/501 --> loss:0.8189952230453491
step 151/334, epoch 114/501 --> loss:0.8169439256191253
step 201/334, epoch 114/501 --> loss:0.8231611859798431
step 251/334, epoch 114/501 --> loss:0.8304889726638794
step 301/334, epoch 114/501 --> loss:0.8177786934375763
step 51/334, epoch 115/501 --> loss:0.8242957115173339
step 101/334, epoch 115/501 --> loss:0.8297721314430236
step 151/334, epoch 115/501 --> loss:0.8345458745956421
step 201/334, epoch 115/501 --> loss:0.8234869706630706
step 251/334, epoch 115/501 --> loss:0.8217103040218353
step 301/334, epoch 115/501 --> loss:0.8213596773147583
step 51/334, epoch 116/501 --> loss:0.8205439627170563
step 101/334, epoch 116/501 --> loss:0.83442462682724
step 151/334, epoch 116/501 --> loss:0.8401685643196106
step 201/334, epoch 116/501 --> loss:0.8212933719158173
step 251/334, epoch 116/501 --> loss:0.819214836359024
step 301/334, epoch 116/501 --> loss:0.8247612452507019
step 51/334, epoch 117/501 --> loss:0.8255973494052887
step 101/334, epoch 117/501 --> loss:0.8217688381671906
step 151/334, epoch 117/501 --> loss:0.8291882240772247
step 201/334, epoch 117/501 --> loss:0.8294535267353058
step 251/334, epoch 117/501 --> loss:0.8199384200572968
step 301/334, epoch 117/501 --> loss:0.8189558184146881
step 51/334, epoch 118/501 --> loss:0.8179430603981018
step 101/334, epoch 118/501 --> loss:0.8293419766426087
step 151/334, epoch 118/501 --> loss:0.8273024368286133
step 201/334, epoch 118/501 --> loss:0.8237301862239838
step 251/334, epoch 118/501 --> loss:0.8326856005191803
step 301/334, epoch 118/501 --> loss:0.8118973302841187
step 51/334, epoch 119/501 --> loss:0.8266929841041565
step 101/334, epoch 119/501 --> loss:0.8150319373607635
step 151/334, epoch 119/501 --> loss:0.8221156525611878
step 201/334, epoch 119/501 --> loss:0.8278368997573853
step 251/334, epoch 119/501 --> loss:0.8172340440750122
step 301/334, epoch 119/501 --> loss:0.8349863409996032
step 51/334, epoch 120/501 --> loss:0.8333182001113891
step 101/334, epoch 120/501 --> loss:0.8223866760730744
step 151/334, epoch 120/501 --> loss:0.817708077430725
step 201/334, epoch 120/501 --> loss:0.8232084238529205
step 251/334, epoch 120/501 --> loss:0.8267212522029876
step 301/334, epoch 120/501 --> loss:0.8168812024593354
step 51/334, epoch 121/501 --> loss:0.8297267389297486
step 101/334, epoch 121/501 --> loss:0.822594221830368
step 151/334, epoch 121/501 --> loss:0.8237013864517212
step 201/334, epoch 121/501 --> loss:0.8180448508262634
step 251/334, epoch 121/501 --> loss:0.8227992689609528
step 301/334, epoch 121/501 --> loss:0.8228119587898255

##########train dataset##########
acc--> [97.8563406693581]
F1--> {'F1': [0.7950289645326455], 'precision': [0.6661090711620322], 'recall': [0.9858425621653647]}
##########eval dataset##########
acc--> [97.39420265871266]
F1--> {'F1': [0.7569546268245029], 'precision': [0.6340988225971416], 'recall': [0.9388717105726189]}
step 51/334, epoch 122/501 --> loss:0.8191681027412414
step 101/334, epoch 122/501 --> loss:0.8272236812114716
step 151/334, epoch 122/501 --> loss:0.8332610440254211
step 201/334, epoch 122/501 --> loss:0.8109137547016144
step 251/334, epoch 122/501 --> loss:0.8213966655731201
step 301/334, epoch 122/501 --> loss:0.8251963913440704
step 51/334, epoch 123/501 --> loss:0.8263912212848663
step 101/334, epoch 123/501 --> loss:0.8201277577877044
step 151/334, epoch 123/501 --> loss:0.8140130758285522
step 201/334, epoch 123/501 --> loss:0.8241148447990417
step 251/334, epoch 123/501 --> loss:0.8305725133419037
step 301/334, epoch 123/501 --> loss:0.8274269366264343
step 51/334, epoch 124/501 --> loss:0.8244293653964996
step 101/334, epoch 124/501 --> loss:0.8217122566699981
step 151/334, epoch 124/501 --> loss:0.829193776845932
step 201/334, epoch 124/501 --> loss:0.8140841782093048
step 251/334, epoch 124/501 --> loss:0.8208392894268036
step 301/334, epoch 124/501 --> loss:0.833040577173233
step 51/334, epoch 125/501 --> loss:0.8243441843986511
step 101/334, epoch 125/501 --> loss:0.8277344942092896
step 151/334, epoch 125/501 --> loss:0.8294184195995331
step 201/334, epoch 125/501 --> loss:0.8302270376682281
step 251/334, epoch 125/501 --> loss:0.8140335404872894
step 301/334, epoch 125/501 --> loss:0.8150579857826233
step 51/334, epoch 126/501 --> loss:0.8286356151103973
step 101/334, epoch 126/501 --> loss:0.8244454777240753
step 151/334, epoch 126/501 --> loss:0.8096142899990082
step 201/334, epoch 126/501 --> loss:0.8298183727264404
step 251/334, epoch 126/501 --> loss:0.8396865057945252
step 301/334, epoch 126/501 --> loss:0.8202664363384247
step 51/334, epoch 127/501 --> loss:0.8244005477428437
step 101/334, epoch 127/501 --> loss:0.8323258006572724
step 151/334, epoch 127/501 --> loss:0.8207667279243469
step 201/334, epoch 127/501 --> loss:0.8262833893299103
step 251/334, epoch 127/501 --> loss:0.8269347989559174
step 301/334, epoch 127/501 --> loss:0.8070280611515045
step 51/334, epoch 128/501 --> loss:0.8236131882667541
step 101/334, epoch 128/501 --> loss:0.8261933219432831
step 151/334, epoch 128/501 --> loss:0.8353698861598968
step 201/334, epoch 128/501 --> loss:0.8187570643424987
step 251/334, epoch 128/501 --> loss:0.8177903759479522
step 301/334, epoch 128/501 --> loss:0.8237509870529175
step 51/334, epoch 129/501 --> loss:0.8244050741195679
step 101/334, epoch 129/501 --> loss:0.8154507839679718
step 151/334, epoch 129/501 --> loss:0.8294853091239929
step 201/334, epoch 129/501 --> loss:0.8340089404582978
step 251/334, epoch 129/501 --> loss:0.8317181861400604
step 301/334, epoch 129/501 --> loss:0.8177521920204163
step 51/334, epoch 130/501 --> loss:0.8273780286312103
step 101/334, epoch 130/501 --> loss:0.821394110918045
step 151/334, epoch 130/501 --> loss:0.8303098225593567
step 201/334, epoch 130/501 --> loss:0.8144762778282165
step 251/334, epoch 130/501 --> loss:0.8257383251190186
step 301/334, epoch 130/501 --> loss:0.8276695871353149
step 51/334, epoch 131/501 --> loss:0.820711236000061
step 101/334, epoch 131/501 --> loss:0.8158727300167083
step 151/334, epoch 131/501 --> loss:0.8206088149547577
step 201/334, epoch 131/501 --> loss:0.8337284207344056
step 251/334, epoch 131/501 --> loss:0.8270807456970215
step 301/334, epoch 131/501 --> loss:0.8207590103149414

##########train dataset##########
acc--> [98.06998002202569]
F1--> {'F1': [0.810466034122846], 'precision': [0.6916802949586363], 'recall': [0.9785247294225401]}
##########eval dataset##########
acc--> [97.67572197700709]
F1--> {'F1': [0.7745090378956062], 'precision': [0.6668851587375749], 'recall': [0.9235689053234601]}
step 51/334, epoch 132/501 --> loss:0.8268944609165192
step 101/334, epoch 132/501 --> loss:0.8082836163043976
step 151/334, epoch 132/501 --> loss:0.8270953667163848
step 201/334, epoch 132/501 --> loss:0.8298610842227936
step 251/334, epoch 132/501 --> loss:0.8179967844486237
step 301/334, epoch 132/501 --> loss:0.8122607219219208
step 51/334, epoch 133/501 --> loss:0.8236750960350037
step 101/334, epoch 133/501 --> loss:0.8125123059749604
step 151/334, epoch 133/501 --> loss:0.8331339824199676
step 201/334, epoch 133/501 --> loss:0.8299580383300781
step 251/334, epoch 133/501 --> loss:0.827051751613617
step 301/334, epoch 133/501 --> loss:0.8262536180019379
step 51/334, epoch 134/501 --> loss:0.8104190421104431
step 101/334, epoch 134/501 --> loss:0.8290069591999054
step 151/334, epoch 134/501 --> loss:0.8238722336292267
step 201/334, epoch 134/501 --> loss:0.8256741988658906
step 251/334, epoch 134/501 --> loss:0.8201975440979004
step 301/334, epoch 134/501 --> loss:0.8289352881908417
step 51/334, epoch 135/501 --> loss:0.8133071625232696
step 101/334, epoch 135/501 --> loss:0.8313348186016083
step 151/334, epoch 135/501 --> loss:0.828624678850174
step 201/334, epoch 135/501 --> loss:0.8247335207462311
step 251/334, epoch 135/501 --> loss:0.8251231348514557
step 301/334, epoch 135/501 --> loss:0.827722179889679
step 51/334, epoch 136/501 --> loss:0.8230804109573364
step 101/334, epoch 136/501 --> loss:0.8162016594409942
step 151/334, epoch 136/501 --> loss:0.8186831140518188
step 201/334, epoch 136/501 --> loss:0.8288412439823151
step 251/334, epoch 136/501 --> loss:0.8324390637874604
step 301/334, epoch 136/501 --> loss:0.821590529680252
step 51/334, epoch 137/501 --> loss:0.8220828998088837
step 101/334, epoch 137/501 --> loss:0.8250658249855042
step 151/334, epoch 137/501 --> loss:0.8282592380046845
step 201/334, epoch 137/501 --> loss:0.8112654781341553
step 251/334, epoch 137/501 --> loss:0.8306078946590424
step 301/334, epoch 137/501 --> loss:0.82375643491745
step 51/334, epoch 138/501 --> loss:0.821128910779953
step 101/334, epoch 138/501 --> loss:0.8255008232593536
step 151/334, epoch 138/501 --> loss:0.8265251350402832
step 201/334, epoch 138/501 --> loss:0.8283638262748718
step 251/334, epoch 138/501 --> loss:0.8265367305278778
step 301/334, epoch 138/501 --> loss:0.8205491411685943
step 51/334, epoch 139/501 --> loss:0.8117096495628356
step 101/334, epoch 139/501 --> loss:0.8288946199417114
step 151/334, epoch 139/501 --> loss:0.8260972726345063
step 201/334, epoch 139/501 --> loss:0.8121351516246795
step 251/334, epoch 139/501 --> loss:0.8122831892967224
step 301/334, epoch 139/501 --> loss:0.8320203065872193
step 51/334, epoch 140/501 --> loss:0.8107282316684723
step 101/334, epoch 140/501 --> loss:0.8264147174358368
step 151/334, epoch 140/501 --> loss:0.8255146861076355
step 201/334, epoch 140/501 --> loss:0.8124455976486206
step 251/334, epoch 140/501 --> loss:0.8243117773532868
step 301/334, epoch 140/501 --> loss:0.8278249049186707
step 51/334, epoch 141/501 --> loss:0.8161198496818542
step 101/334, epoch 141/501 --> loss:0.8135988295078278
step 151/334, epoch 141/501 --> loss:0.8275298726558685
step 201/334, epoch 141/501 --> loss:0.8380229127407074
step 251/334, epoch 141/501 --> loss:0.8213270020484924
step 301/334, epoch 141/501 --> loss:0.8208558213710785

##########train dataset##########
acc--> [98.9766064054892]
F1--> {'F1': [0.8897071672845731], 'precision': [0.8154587433122852], 'recall': [0.9788428016861975]}
##########eval dataset##########
acc--> [98.47513233895032]
F1--> {'F1': [0.8373861158984065], 'precision': [0.7766622412841528], 'recall': [0.9084225426836465]}
save model!
step 51/334, epoch 142/501 --> loss:0.8130886507034302
step 101/334, epoch 142/501 --> loss:0.8273872017860413
step 151/334, epoch 142/501 --> loss:0.8328619003295898
step 201/334, epoch 142/501 --> loss:0.8251800167560578
step 251/334, epoch 142/501 --> loss:0.821754812002182
step 301/334, epoch 142/501 --> loss:0.8325405311584473
step 51/334, epoch 143/501 --> loss:0.8243436205387116
step 101/334, epoch 143/501 --> loss:0.8260085332393646
step 151/334, epoch 143/501 --> loss:0.8215270161628723
step 201/334, epoch 143/501 --> loss:0.814123215675354
step 251/334, epoch 143/501 --> loss:0.8157052230834961
step 301/334, epoch 143/501 --> loss:0.8182900559902191
step 51/334, epoch 144/501 --> loss:0.8314031863212585
step 101/334, epoch 144/501 --> loss:0.8247210693359375
step 151/334, epoch 144/501 --> loss:0.8242717206478118
step 201/334, epoch 144/501 --> loss:0.8151252937316894
step 251/334, epoch 144/501 --> loss:0.823043167591095
step 301/334, epoch 144/501 --> loss:0.8212025821208954
step 51/334, epoch 145/501 --> loss:0.8183057916164398
step 101/334, epoch 145/501 --> loss:0.809153311252594
step 151/334, epoch 145/501 --> loss:0.826439175605774
step 201/334, epoch 145/501 --> loss:0.8322832071781159
step 251/334, epoch 145/501 --> loss:0.8281926667690277
step 301/334, epoch 145/501 --> loss:0.8251617789268494
step 51/334, epoch 146/501 --> loss:0.8257077300548553
step 101/334, epoch 146/501 --> loss:0.8276936161518097
step 151/334, epoch 146/501 --> loss:0.8242565965652466
step 201/334, epoch 146/501 --> loss:0.826919115781784
step 251/334, epoch 146/501 --> loss:0.8193425059318542
step 301/334, epoch 146/501 --> loss:0.8170895063877106
step 51/334, epoch 147/501 --> loss:0.8184266746044159
step 101/334, epoch 147/501 --> loss:0.8111910927295685
step 151/334, epoch 147/501 --> loss:0.8223420262336731
step 201/334, epoch 147/501 --> loss:0.8353420698642731
step 251/334, epoch 147/501 --> loss:0.8279347932338714
step 301/334, epoch 147/501 --> loss:0.8300841605663299
step 51/334, epoch 148/501 --> loss:0.8237527227401733
step 101/334, epoch 148/501 --> loss:0.8140304136276245
step 151/334, epoch 148/501 --> loss:0.8248699593544007
step 201/334, epoch 148/501 --> loss:0.8279442775249481
step 251/334, epoch 148/501 --> loss:0.8104296028614044
step 301/334, epoch 148/501 --> loss:0.833791447877884
step 51/334, epoch 149/501 --> loss:0.8090624475479126
step 101/334, epoch 149/501 --> loss:0.829949404001236
step 151/334, epoch 149/501 --> loss:0.8162516045570374
step 201/334, epoch 149/501 --> loss:0.8224159574508667
step 251/334, epoch 149/501 --> loss:0.8275046288967133
step 301/334, epoch 149/501 --> loss:0.8199365711212159
step 51/334, epoch 150/501 --> loss:0.8257958459854126
step 101/334, epoch 150/501 --> loss:0.8233669877052308
step 151/334, epoch 150/501 --> loss:0.8342519080638886
step 201/334, epoch 150/501 --> loss:0.8225233674049377
step 251/334, epoch 150/501 --> loss:0.8168340003490449
step 301/334, epoch 150/501 --> loss:0.8158059859275818
step 51/334, epoch 151/501 --> loss:0.8294415974617004
step 101/334, epoch 151/501 --> loss:0.8163925385475159
step 151/334, epoch 151/501 --> loss:0.8155831718444824
step 201/334, epoch 151/501 --> loss:0.8157136917114258
step 251/334, epoch 151/501 --> loss:0.8210150933265686
step 301/334, epoch 151/501 --> loss:0.8353039979934692

##########train dataset##########
acc--> [99.00694004283116]
F1--> {'F1': [0.8932802533899601], 'precision': [0.8167998993926558], 'recall': [0.985574619012998]}
##########eval dataset##########
acc--> [98.48298534215768]
F1--> {'F1': [0.8394168892500501], 'precision': [0.773666408790705], 'recall': [0.9173928970408142]}
save model!
step 51/334, epoch 152/501 --> loss:0.8252890634536744
step 101/334, epoch 152/501 --> loss:0.8305103552341461
step 151/334, epoch 152/501 --> loss:0.8071099603176117
step 201/334, epoch 152/501 --> loss:0.826898193359375
step 251/334, epoch 152/501 --> loss:0.8489500701427459
step 301/334, epoch 152/501 --> loss:0.8154337251186371
step 51/334, epoch 153/501 --> loss:0.8344761097431183
step 101/334, epoch 153/501 --> loss:0.8236911499500275
step 151/334, epoch 153/501 --> loss:0.8188507401943207
step 201/334, epoch 153/501 --> loss:0.8197401809692383
step 251/334, epoch 153/501 --> loss:0.8373098719120026
step 301/334, epoch 153/501 --> loss:0.8197188377380371
step 51/334, epoch 154/501 --> loss:0.823797345161438
step 101/334, epoch 154/501 --> loss:0.8231459856033325
step 151/334, epoch 154/501 --> loss:0.8217784404754639
step 201/334, epoch 154/501 --> loss:0.8155858612060547
step 251/334, epoch 154/501 --> loss:0.8283119165897369
step 301/334, epoch 154/501 --> loss:0.8204659473896027
step 51/334, epoch 155/501 --> loss:0.826000235080719
step 101/334, epoch 155/501 --> loss:0.8235306978225708
step 151/334, epoch 155/501 --> loss:0.8234146428108216
step 201/334, epoch 155/501 --> loss:0.8330485379695892
step 251/334, epoch 155/501 --> loss:0.8125296616554261
step 301/334, epoch 155/501 --> loss:0.8221154272556305
step 51/334, epoch 156/501 --> loss:0.8229578208923339
step 101/334, epoch 156/501 --> loss:0.8299130618572235
step 151/334, epoch 156/501 --> loss:0.8311539947986603
step 201/334, epoch 156/501 --> loss:0.8203842473030091
step 251/334, epoch 156/501 --> loss:0.814380578994751
step 301/334, epoch 156/501 --> loss:0.8292477214336396
step 51/334, epoch 157/501 --> loss:0.8173894429206848
step 101/334, epoch 157/501 --> loss:0.823991094827652
step 151/334, epoch 157/501 --> loss:0.8296842145919799
step 201/334, epoch 157/501 --> loss:0.825740624666214
step 251/334, epoch 157/501 --> loss:0.8060821080207825
step 301/334, epoch 157/501 --> loss:0.8392391955852508
step 51/334, epoch 158/501 --> loss:0.8193221580982208
step 101/334, epoch 158/501 --> loss:0.8194443488121033
step 151/334, epoch 158/501 --> loss:0.8255614614486695
step 201/334, epoch 158/501 --> loss:0.8244169020652771
step 251/334, epoch 158/501 --> loss:0.8271551191806793
step 301/334, epoch 158/501 --> loss:0.8170143043994904
step 51/334, epoch 159/501 --> loss:0.8208691799640655
step 101/334, epoch 159/501 --> loss:0.8226060152053833
step 151/334, epoch 159/501 --> loss:0.8164152193069458
step 201/334, epoch 159/501 --> loss:0.8177673447132111
step 251/334, epoch 159/501 --> loss:0.8207016932964325
step 301/334, epoch 159/501 --> loss:0.8212110793590546
step 51/334, epoch 160/501 --> loss:0.8215158426761627
step 101/334, epoch 160/501 --> loss:0.8154082131385804
step 151/334, epoch 160/501 --> loss:0.8243356275558472
step 201/334, epoch 160/501 --> loss:0.8227683115005493
step 251/334, epoch 160/501 --> loss:0.8193312466144562
step 301/334, epoch 160/501 --> loss:0.8300465476512909
step 51/334, epoch 161/501 --> loss:0.8115930712223053
step 101/334, epoch 161/501 --> loss:0.8278965020179748
step 151/334, epoch 161/501 --> loss:0.8243894624710083
step 201/334, epoch 161/501 --> loss:0.825551118850708
step 251/334, epoch 161/501 --> loss:0.8258326864242553
step 301/334, epoch 161/501 --> loss:0.8233945643901825

##########train dataset##########
acc--> [99.1164471712431]
F1--> {'F1': [0.9038613837137607], 'precision': [0.8351244319162626], 'recall': [0.9849401024947102]}
##########eval dataset##########
acc--> [98.65259552668782]
F1--> {'F1': [0.8535005630373096], 'precision': [0.8050682984425744], 'recall': [0.9081444053298158]}
save model!
step 51/334, epoch 162/501 --> loss:0.8386981475353241
step 101/334, epoch 162/501 --> loss:0.8169066286087037
step 151/334, epoch 162/501 --> loss:0.8111347007751465
step 201/334, epoch 162/501 --> loss:0.8251480877399444
step 251/334, epoch 162/501 --> loss:0.8281957852840424
step 301/334, epoch 162/501 --> loss:0.8191231322288514
step 51/334, epoch 163/501 --> loss:0.8128367817401886
step 101/334, epoch 163/501 --> loss:0.8132905113697052
step 151/334, epoch 163/501 --> loss:0.8153507125377655
step 201/334, epoch 163/501 --> loss:0.8295606946945191
step 251/334, epoch 163/501 --> loss:0.8239988565444947
step 301/334, epoch 163/501 --> loss:0.8373940861225129
step 51/334, epoch 164/501 --> loss:0.812845207452774
step 101/334, epoch 164/501 --> loss:0.83265922665596
step 151/334, epoch 164/501 --> loss:0.8215095162391662
step 201/334, epoch 164/501 --> loss:0.8227675604820252
step 251/334, epoch 164/501 --> loss:0.8153110146522522
step 301/334, epoch 164/501 --> loss:0.832453521490097
step 51/334, epoch 165/501 --> loss:0.8151217436790467
step 101/334, epoch 165/501 --> loss:0.8329995560646057
step 151/334, epoch 165/501 --> loss:0.822263697385788
step 201/334, epoch 165/501 --> loss:0.828962653875351
step 251/334, epoch 165/501 --> loss:0.826142201423645
step 301/334, epoch 165/501 --> loss:0.8088097095489502
step 51/334, epoch 166/501 --> loss:0.8282415425777435
step 101/334, epoch 166/501 --> loss:0.8145533263683319
step 151/334, epoch 166/501 --> loss:0.8193583250045776
step 201/334, epoch 166/501 --> loss:0.8142764341831207
step 251/334, epoch 166/501 --> loss:0.834619802236557
step 301/334, epoch 166/501 --> loss:0.8233450186252594
step 51/334, epoch 167/501 --> loss:0.8384037923812866
step 101/334, epoch 167/501 --> loss:0.8184501385688782
step 151/334, epoch 167/501 --> loss:0.8124611020088196
step 201/334, epoch 167/501 --> loss:0.829178055524826
step 251/334, epoch 167/501 --> loss:0.8150722980499268
step 301/334, epoch 167/501 --> loss:0.8289317107200622
step 51/334, epoch 168/501 --> loss:0.8332420098781586
step 101/334, epoch 168/501 --> loss:0.8263447725772858
step 151/334, epoch 168/501 --> loss:0.8253770506381989
step 201/334, epoch 168/501 --> loss:0.8160385763645173
step 251/334, epoch 168/501 --> loss:0.8041701912879944
step 301/334, epoch 168/501 --> loss:0.8265448892116547
step 51/334, epoch 169/501 --> loss:0.8153621959686279
step 101/334, epoch 169/501 --> loss:0.8290304601192474
step 151/334, epoch 169/501 --> loss:0.8213571894168854
step 201/334, epoch 169/501 --> loss:0.8207479405403137
step 251/334, epoch 169/501 --> loss:0.8227299571037292
step 301/334, epoch 169/501 --> loss:0.8288578808307647
step 51/334, epoch 170/501 --> loss:0.8179857039451599
step 101/334, epoch 170/501 --> loss:0.8026221406459808
step 151/334, epoch 170/501 --> loss:0.8287253177165985
step 201/334, epoch 170/501 --> loss:0.8178274047374725
step 251/334, epoch 170/501 --> loss:0.8245389652252197
step 301/334, epoch 170/501 --> loss:0.836829069852829
step 51/334, epoch 171/501 --> loss:0.8162352299690246
step 101/334, epoch 171/501 --> loss:0.8270530188083649
step 151/334, epoch 171/501 --> loss:0.8255729949474335
step 201/334, epoch 171/501 --> loss:0.819479125738144
step 251/334, epoch 171/501 --> loss:0.8335134637355804
step 301/334, epoch 171/501 --> loss:0.8168551230430603

##########train dataset##########
acc--> [98.98092143312735]
F1--> {'F1': [0.8909055127815352], 'precision': [0.8120398761699946], 'recall': [0.9867500415305193]}
##########eval dataset##########
acc--> [98.45860131664402]
F1--> {'F1': [0.8383214069338828], 'precision': [0.7667689566577481], 'recall': [0.924614429968343]}
step 51/334, epoch 172/501 --> loss:0.8303077685832977
step 101/334, epoch 172/501 --> loss:0.8136131870746612
step 151/334, epoch 172/501 --> loss:0.8270499539375306
step 201/334, epoch 172/501 --> loss:0.8307941424846649
step 251/334, epoch 172/501 --> loss:0.8164005768299103
step 301/334, epoch 172/501 --> loss:0.8246499860286712
step 51/334, epoch 173/501 --> loss:0.8155026769638062
step 101/334, epoch 173/501 --> loss:0.8145420300960541
step 151/334, epoch 173/501 --> loss:0.8251151597499847
step 201/334, epoch 173/501 --> loss:0.8298608314990997
step 251/334, epoch 173/501 --> loss:0.8183414959907531
step 301/334, epoch 173/501 --> loss:0.8321254789829254
step 51/334, epoch 174/501 --> loss:0.8174302089214325
step 101/334, epoch 174/501 --> loss:0.8295231223106384
step 151/334, epoch 174/501 --> loss:0.822624237537384
step 201/334, epoch 174/501 --> loss:0.8152499639987946
step 251/334, epoch 174/501 --> loss:0.8310061490535736
step 301/334, epoch 174/501 --> loss:0.8174302697181701
step 51/334, epoch 175/501 --> loss:0.8105340552330017
step 101/334, epoch 175/501 --> loss:0.8195334815979004
step 151/334, epoch 175/501 --> loss:0.8334191501140594
step 201/334, epoch 175/501 --> loss:0.8253079676628112
step 251/334, epoch 175/501 --> loss:0.8172596168518066
step 301/334, epoch 175/501 --> loss:0.8228116238117218
step 51/334, epoch 176/501 --> loss:0.8197183561325073
step 101/334, epoch 176/501 --> loss:0.8305830514431
step 151/334, epoch 176/501 --> loss:0.8142076206207275
step 201/334, epoch 176/501 --> loss:0.8232301557064057
step 251/334, epoch 176/501 --> loss:0.8207147181034088
step 301/334, epoch 176/501 --> loss:0.812947781085968
step 51/334, epoch 177/501 --> loss:0.8189701151847839
step 101/334, epoch 177/501 --> loss:0.8210859107971191
step 151/334, epoch 177/501 --> loss:0.8284018862247468
step 201/334, epoch 177/501 --> loss:0.8155369091033936
step 251/334, epoch 177/501 --> loss:0.8225024056434631
step 301/334, epoch 177/501 --> loss:0.8393658208847046
step 51/334, epoch 178/501 --> loss:0.8280614829063415
step 101/334, epoch 178/501 --> loss:0.8206642508506775
step 151/334, epoch 178/501 --> loss:0.829906678199768
step 201/334, epoch 178/501 --> loss:0.8250741052627564
step 251/334, epoch 178/501 --> loss:0.8227177667617798
step 301/334, epoch 178/501 --> loss:0.8085775983333587
step 51/334, epoch 179/501 --> loss:0.8239296317100525
step 101/334, epoch 179/501 --> loss:0.8245730841159821
step 151/334, epoch 179/501 --> loss:0.8133471119403839
step 201/334, epoch 179/501 --> loss:0.8253449881076813
step 251/334, epoch 179/501 --> loss:0.8231181752681732
step 301/334, epoch 179/501 --> loss:0.8165300714969635
step 51/334, epoch 180/501 --> loss:0.8332976615428924
step 101/334, epoch 180/501 --> loss:0.8210677325725555
step 151/334, epoch 180/501 --> loss:0.8251134443283081
step 201/334, epoch 180/501 --> loss:0.8080804848670959
step 251/334, epoch 180/501 --> loss:0.8137381553649903
step 301/334, epoch 180/501 --> loss:0.8282421362400055
step 51/334, epoch 181/501 --> loss:0.827230966091156
step 101/334, epoch 181/501 --> loss:0.8349573230743408
step 151/334, epoch 181/501 --> loss:0.8203148484230042
step 201/334, epoch 181/501 --> loss:0.8364652228355408
step 251/334, epoch 181/501 --> loss:0.8002989554405212
step 301/334, epoch 181/501 --> loss:0.8260364496707916

##########train dataset##########
acc--> [99.327877518115]
F1--> {'F1': [0.9250351419167379], 'precision': [0.8732217201983737], 'recall': [0.9833964786022407]}
##########eval dataset##########
acc--> [98.80547110489313]
F1--> {'F1': [0.8660844260960975], 'precision': [0.8400891762837998], 'recall': [0.8937504497904261]}
save model!
step 51/334, epoch 182/501 --> loss:0.8238833892345429
step 101/334, epoch 182/501 --> loss:0.8308532786369324
step 151/334, epoch 182/501 --> loss:0.8318703496456146
step 201/334, epoch 182/501 --> loss:0.8233188498020172
step 251/334, epoch 182/501 --> loss:0.7969245076179504
step 301/334, epoch 182/501 --> loss:0.8165927839279175
step 51/334, epoch 183/501 --> loss:0.8170430433750152
step 101/334, epoch 183/501 --> loss:0.8105751490592956
step 151/334, epoch 183/501 --> loss:0.8187537324428559
step 201/334, epoch 183/501 --> loss:0.8304349434375763
step 251/334, epoch 183/501 --> loss:0.8283677029609681
step 301/334, epoch 183/501 --> loss:0.8146688580513001
step 51/334, epoch 184/501 --> loss:0.8073531651496887
step 101/334, epoch 184/501 --> loss:0.8241310715675354
step 151/334, epoch 184/501 --> loss:0.8281334435939789
step 201/334, epoch 184/501 --> loss:0.8220969939231872
step 251/334, epoch 184/501 --> loss:0.830961389541626
step 301/334, epoch 184/501 --> loss:0.8155581748485565
step 51/334, epoch 185/501 --> loss:0.8202505457401276
step 101/334, epoch 185/501 --> loss:0.8172721755504608
step 151/334, epoch 185/501 --> loss:0.8156575357913971
step 201/334, epoch 185/501 --> loss:0.829015885591507
step 251/334, epoch 185/501 --> loss:0.8187875640392304
step 301/334, epoch 185/501 --> loss:0.8218171417713165
step 51/334, epoch 186/501 --> loss:0.8242207801342011
step 101/334, epoch 186/501 --> loss:0.8218637597560883
step 151/334, epoch 186/501 --> loss:0.8133100521564484
step 201/334, epoch 186/501 --> loss:0.8264256656169892
step 251/334, epoch 186/501 --> loss:0.8192259764671326
step 301/334, epoch 186/501 --> loss:0.8164722001552582
step 51/334, epoch 187/501 --> loss:0.8409491014480591
step 101/334, epoch 187/501 --> loss:0.813606151342392
step 151/334, epoch 187/501 --> loss:0.8288520777225494
step 201/334, epoch 187/501 --> loss:0.8108732450008392
step 251/334, epoch 187/501 --> loss:0.825474648475647
step 301/334, epoch 187/501 --> loss:0.8126763486862183
step 51/334, epoch 188/501 --> loss:0.8058737480640411
step 101/334, epoch 188/501 --> loss:0.8284878706932068
step 151/334, epoch 188/501 --> loss:0.8360824394226074
step 201/334, epoch 188/501 --> loss:0.8233600735664368
step 251/334, epoch 188/501 --> loss:0.8243825566768647
step 301/334, epoch 188/501 --> loss:0.8190588438510895
step 51/334, epoch 189/501 --> loss:0.8184032011032104
step 101/334, epoch 189/501 --> loss:0.8016248536109924
step 151/334, epoch 189/501 --> loss:0.8196100664138793
step 201/334, epoch 189/501 --> loss:0.836679230928421
step 251/334, epoch 189/501 --> loss:0.8353784143924713
step 301/334, epoch 189/501 --> loss:0.836849468946457
step 51/334, epoch 190/501 --> loss:0.8146658766269684
step 101/334, epoch 190/501 --> loss:0.8285696005821228
step 151/334, epoch 190/501 --> loss:0.8227547073364258
step 201/334, epoch 190/501 --> loss:0.8291155242919922
step 251/334, epoch 190/501 --> loss:0.8284846234321595
step 301/334, epoch 190/501 --> loss:0.8181098330020905
step 51/334, epoch 191/501 --> loss:0.8225973045825958
step 101/334, epoch 191/501 --> loss:0.8394228053092957
step 151/334, epoch 191/501 --> loss:0.828805822134018
step 201/334, epoch 191/501 --> loss:0.8169964444637299
step 251/334, epoch 191/501 --> loss:0.8269413781166076
step 301/334, epoch 191/501 --> loss:0.815980578660965

##########train dataset##########
acc--> [99.2476587399203]
F1--> {'F1': [0.9164535121863867], 'precision': [0.8617925282671097], 'recall': [0.9785294099484928]}
##########eval dataset##########
acc--> [98.73091663483422]
F1--> {'F1': [0.8597209234590449], 'precision': [0.8230717668527997], 'recall': [0.8997968871458849]}
step 51/334, epoch 192/501 --> loss:0.8119448626041412
step 101/334, epoch 192/501 --> loss:0.8069915342330932
step 151/334, epoch 192/501 --> loss:0.8262881410121917
step 201/334, epoch 192/501 --> loss:0.8401342117786408
step 251/334, epoch 192/501 --> loss:0.8163891196250915
step 301/334, epoch 192/501 --> loss:0.8284278500080109
step 51/334, epoch 193/501 --> loss:0.8094909596443176
step 101/334, epoch 193/501 --> loss:0.8196377491950989
step 151/334, epoch 193/501 --> loss:0.8086827516555786
step 201/334, epoch 193/501 --> loss:0.8474324643611908
step 251/334, epoch 193/501 --> loss:0.8153665053844452
step 301/334, epoch 193/501 --> loss:0.823601211309433
step 51/334, epoch 194/501 --> loss:0.8134994387626648
step 101/334, epoch 194/501 --> loss:0.8034253537654876
step 151/334, epoch 194/501 --> loss:0.8308642923831939
step 201/334, epoch 194/501 --> loss:0.8269214534759521
step 251/334, epoch 194/501 --> loss:0.8225329780578613
step 301/334, epoch 194/501 --> loss:0.8299910342693329
step 51/334, epoch 195/501 --> loss:0.8270277762413025
step 101/334, epoch 195/501 --> loss:0.8281782364845276
step 151/334, epoch 195/501 --> loss:0.807628298997879
step 201/334, epoch 195/501 --> loss:0.8303835463523864
step 251/334, epoch 195/501 --> loss:0.8315361189842224
step 301/334, epoch 195/501 --> loss:0.8142539536952973
step 51/334, epoch 196/501 --> loss:0.8304978740215302
step 101/334, epoch 196/501 --> loss:0.8254187655448914
step 151/334, epoch 196/501 --> loss:0.833949145078659
step 201/334, epoch 196/501 --> loss:0.8078627073764801
step 251/334, epoch 196/501 --> loss:0.8166365325450897
step 301/334, epoch 196/501 --> loss:0.815348961353302
step 51/334, epoch 197/501 --> loss:0.8296336710453034
step 101/334, epoch 197/501 --> loss:0.8202161633968353
step 151/334, epoch 197/501 --> loss:0.8182475423812866
step 201/334, epoch 197/501 --> loss:0.8267440617084503
step 251/334, epoch 197/501 --> loss:0.8139961504936218
step 301/334, epoch 197/501 --> loss:0.8068758749961853
step 51/334, epoch 198/501 --> loss:0.8241172468662262
step 101/334, epoch 198/501 --> loss:0.8254103136062622
step 151/334, epoch 198/501 --> loss:0.8169742906093598
step 201/334, epoch 198/501 --> loss:0.8215760016441345
step 251/334, epoch 198/501 --> loss:0.8258808648586273
step 301/334, epoch 198/501 --> loss:0.8215828013420104
step 51/334, epoch 199/501 --> loss:0.8298409759998322
step 101/334, epoch 199/501 --> loss:0.8188254487514496
step 151/334, epoch 199/501 --> loss:0.8207158887386322
step 201/334, epoch 199/501 --> loss:0.8378661179542541
step 251/334, epoch 199/501 --> loss:0.8100206398963928
step 301/334, epoch 199/501 --> loss:0.8114347422122955
step 51/334, epoch 200/501 --> loss:0.8231143939495087
step 101/334, epoch 200/501 --> loss:0.8254989922046662
step 151/334, epoch 200/501 --> loss:0.824821093082428
step 201/334, epoch 200/501 --> loss:0.8157428121566772
step 251/334, epoch 200/501 --> loss:0.8323362171649933
step 301/334, epoch 200/501 --> loss:0.8136661028862
step 51/334, epoch 201/501 --> loss:0.8309364235401153
step 101/334, epoch 201/501 --> loss:0.8173052620887756
step 151/334, epoch 201/501 --> loss:0.8274600076675415
step 201/334, epoch 201/501 --> loss:0.8218202865123749
step 251/334, epoch 201/501 --> loss:0.8210170423984527
step 301/334, epoch 201/501 --> loss:0.8154336249828339

##########train dataset##########
acc--> [99.32751736034542]
F1--> {'F1': [0.9247456119772747], 'precision': [0.8755332596029306], 'recall': [0.9798309353318999]}
##########eval dataset##########
acc--> [98.77952916102112]
F1--> {'F1': [0.8633153531161811], 'precision': [0.8366037284259443], 'recall': [0.8917996274397375]}
step 51/334, epoch 202/501 --> loss:0.8480098235607147
step 101/334, epoch 202/501 --> loss:0.8266914892196655
step 151/334, epoch 202/501 --> loss:0.8158710598945618
step 201/334, epoch 202/501 --> loss:0.8077257418632507
step 251/334, epoch 202/501 --> loss:0.8096556222438812
step 301/334, epoch 202/501 --> loss:0.8256575465202332
step 51/334, epoch 203/501 --> loss:0.8286495292186737
step 101/334, epoch 203/501 --> loss:0.8134950339794159
step 151/334, epoch 203/501 --> loss:0.8258585011959076
step 201/334, epoch 203/501 --> loss:0.811169090270996
step 251/334, epoch 203/501 --> loss:0.8280891096591949
step 301/334, epoch 203/501 --> loss:0.8255520141124726
step 51/334, epoch 204/501 --> loss:0.8256328296661377
step 101/334, epoch 204/501 --> loss:0.81537752866745
step 151/334, epoch 204/501 --> loss:0.8333261775970459
step 201/334, epoch 204/501 --> loss:0.8232540464401246
step 251/334, epoch 204/501 --> loss:0.8184165620803833
step 301/334, epoch 204/501 --> loss:0.8273926222324371
step 51/334, epoch 205/501 --> loss:0.8310571527481079
step 101/334, epoch 205/501 --> loss:0.8237860596179962
step 151/334, epoch 205/501 --> loss:0.8315099442005157
step 201/334, epoch 205/501 --> loss:0.8266898083686829
step 251/334, epoch 205/501 --> loss:0.8269149184226989
step 301/334, epoch 205/501 --> loss:0.8049442136287689
step 51/334, epoch 206/501 --> loss:0.8365050685405732
step 101/334, epoch 206/501 --> loss:0.8176048088073731
step 151/334, epoch 206/501 --> loss:0.81630535364151
step 201/334, epoch 206/501 --> loss:0.8232999765872955
step 251/334, epoch 206/501 --> loss:0.813385647535324
step 301/334, epoch 206/501 --> loss:0.8250222730636597
step 51/334, epoch 207/501 --> loss:0.812859400510788
step 101/334, epoch 207/501 --> loss:0.8291070604324341
step 151/334, epoch 207/501 --> loss:0.8376396250724792
step 201/334, epoch 207/501 --> loss:0.8195336878299713
step 251/334, epoch 207/501 --> loss:0.8212092137336731
step 301/334, epoch 207/501 --> loss:0.8100766026973725
step 51/334, epoch 208/501 --> loss:0.8231884133815766
step 101/334, epoch 208/501 --> loss:0.8260020184516906
step 151/334, epoch 208/501 --> loss:0.8187726008892059
step 201/334, epoch 208/501 --> loss:0.8160619854927063
step 251/334, epoch 208/501 --> loss:0.8091798412799835
step 301/334, epoch 208/501 --> loss:0.8172100412845612
step 51/334, epoch 209/501 --> loss:0.8244079887866974
step 101/334, epoch 209/501 --> loss:0.8195428943634033
step 151/334, epoch 209/501 --> loss:0.8295737969875335
step 201/334, epoch 209/501 --> loss:0.8128163838386535
step 251/334, epoch 209/501 --> loss:0.8321991908550262
step 301/334, epoch 209/501 --> loss:0.8141184377670289
step 51/334, epoch 210/501 --> loss:0.8110762250423431
step 101/334, epoch 210/501 --> loss:0.8172573184967041
step 151/334, epoch 210/501 --> loss:0.8243014359474182
step 201/334, epoch 210/501 --> loss:0.8229586923122406
step 251/334, epoch 210/501 --> loss:0.8205130016803741
step 301/334, epoch 210/501 --> loss:0.8301563739776612
step 51/334, epoch 211/501 --> loss:0.8241187047958374
step 101/334, epoch 211/501 --> loss:0.8369900798797607
step 151/334, epoch 211/501 --> loss:0.8283715164661407
step 201/334, epoch 211/501 --> loss:0.8258171772956848
step 251/334, epoch 211/501 --> loss:0.8054914546012878
step 301/334, epoch 211/501 --> loss:0.810974805355072

##########train dataset##########
acc--> [99.3527315509483]
F1--> {'F1': [0.9279660831581423], 'precision': [0.8742781011271911], 'recall': [0.9886905604570386]}
##########eval dataset##########
acc--> [98.76646974684755]
F1--> {'F1': [0.8624536561529664], 'precision': [0.8323725571450785], 'recall': [0.8948012252237766]}
step 51/334, epoch 212/501 --> loss:0.8136758685112
step 101/334, epoch 212/501 --> loss:0.8356502473354339
step 151/334, epoch 212/501 --> loss:0.8105096960067749
step 201/334, epoch 212/501 --> loss:0.8245307552814484
step 251/334, epoch 212/501 --> loss:0.823532202243805
step 301/334, epoch 212/501 --> loss:0.8236781239509583
step 51/334, epoch 213/501 --> loss:0.8122915840148925
step 101/334, epoch 213/501 --> loss:0.8233580601215362
step 151/334, epoch 213/501 --> loss:0.8215118420124053
step 201/334, epoch 213/501 --> loss:0.8115188133716583
step 251/334, epoch 213/501 --> loss:0.8398889458179474
step 301/334, epoch 213/501 --> loss:0.8257807278633118
step 51/334, epoch 214/501 --> loss:0.8164106070995331
step 101/334, epoch 214/501 --> loss:0.8277485406398774
step 151/334, epoch 214/501 --> loss:0.8351774132251739
step 201/334, epoch 214/501 --> loss:0.8077245962619781
step 251/334, epoch 214/501 --> loss:0.8266363120079041
step 301/334, epoch 214/501 --> loss:0.8211270380020141
step 51/334, epoch 215/501 --> loss:0.8146711695194244
step 101/334, epoch 215/501 --> loss:0.8180890309810639
step 151/334, epoch 215/501 --> loss:0.8299470949172973
step 201/334, epoch 215/501 --> loss:0.8309473967552186
step 251/334, epoch 215/501 --> loss:0.8056012678146363
step 301/334, epoch 215/501 --> loss:0.8247886025905609
step 51/334, epoch 216/501 --> loss:0.840866186618805
step 101/334, epoch 216/501 --> loss:0.8166259908676148
step 151/334, epoch 216/501 --> loss:0.8272820734977722
step 201/334, epoch 216/501 --> loss:0.8107004988193512
step 251/334, epoch 216/501 --> loss:0.8223440051078796
step 301/334, epoch 216/501 --> loss:0.8199613213539123
step 51/334, epoch 217/501 --> loss:0.817661589384079
step 101/334, epoch 217/501 --> loss:0.8149761021137237
step 151/334, epoch 217/501 --> loss:0.8167070126533509
step 201/334, epoch 217/501 --> loss:0.8194713366031646
step 251/334, epoch 217/501 --> loss:0.8152054703235626
step 301/334, epoch 217/501 --> loss:0.8354187476634979
step 51/334, epoch 218/501 --> loss:0.8291970336437225
step 101/334, epoch 218/501 --> loss:0.8328073918819427
step 151/334, epoch 218/501 --> loss:0.8253028202056885
step 201/334, epoch 218/501 --> loss:0.8223480689525604
step 251/334, epoch 218/501 --> loss:0.8137376260757446
step 301/334, epoch 218/501 --> loss:0.812854722738266
step 51/334, epoch 219/501 --> loss:0.8019428479671479
step 101/334, epoch 219/501 --> loss:0.8290068352222443
step 151/334, epoch 219/501 --> loss:0.8307168531417847
step 201/334, epoch 219/501 --> loss:0.8102044820785522
step 251/334, epoch 219/501 --> loss:0.8346960723400116
step 301/334, epoch 219/501 --> loss:0.8281719756126403
step 51/334, epoch 220/501 --> loss:0.830035160779953
step 101/334, epoch 220/501 --> loss:0.8155769920349121
step 151/334, epoch 220/501 --> loss:0.8150413131713867
step 201/334, epoch 220/501 --> loss:0.8272173607349396
step 251/334, epoch 220/501 --> loss:0.825718673467636
step 301/334, epoch 220/501 --> loss:0.8170366406440734
step 51/334, epoch 221/501 --> loss:0.8148939263820648
step 101/334, epoch 221/501 --> loss:0.8229355657100678
step 151/334, epoch 221/501 --> loss:0.8218210649490356
step 201/334, epoch 221/501 --> loss:0.8408422136306762
step 251/334, epoch 221/501 --> loss:0.8064948832988739
step 301/334, epoch 221/501 --> loss:0.816006314754486

##########train dataset##########
acc--> [99.27988985659955]
F1--> {'F1': [0.9204010188537787], 'precision': [0.8620060475238749], 'recall': [0.9872940678803876]}
##########eval dataset##########
acc--> [98.68830282856439]
F1--> {'F1': [0.8573064972447544], 'precision': [0.8090418957411786], 'recall': [0.9117062931303446]}
step 51/334, epoch 222/501 --> loss:0.8112919521331787
step 101/334, epoch 222/501 --> loss:0.8200195956230164
step 151/334, epoch 222/501 --> loss:0.8200501465797424
step 201/334, epoch 222/501 --> loss:0.8345929670333863
step 251/334, epoch 222/501 --> loss:0.8302418959140777
step 301/334, epoch 222/501 --> loss:0.8079374969005585
step 51/334, epoch 223/501 --> loss:0.8138128471374512
step 101/334, epoch 223/501 --> loss:0.8225857579708099
step 151/334, epoch 223/501 --> loss:0.824856526851654
step 201/334, epoch 223/501 --> loss:0.8196954965591431
step 251/334, epoch 223/501 --> loss:0.8201524436473846
step 301/334, epoch 223/501 --> loss:0.8190007412433624
step 51/334, epoch 224/501 --> loss:0.8203385961055756
step 101/334, epoch 224/501 --> loss:0.8333206808567047
step 151/334, epoch 224/501 --> loss:0.8334209692478179
step 201/334, epoch 224/501 --> loss:0.8115128874778748
step 251/334, epoch 224/501 --> loss:0.8117171382904053
step 301/334, epoch 224/501 --> loss:0.825544445514679
step 51/334, epoch 225/501 --> loss:0.8233979952335357
step 101/334, epoch 225/501 --> loss:0.823980770111084
step 151/334, epoch 225/501 --> loss:0.8176923978328705
step 201/334, epoch 225/501 --> loss:0.8327942967414856
step 251/334, epoch 225/501 --> loss:0.8145779538154602
step 301/334, epoch 225/501 --> loss:0.8120185708999634
step 51/334, epoch 226/501 --> loss:0.8118321847915649
step 101/334, epoch 226/501 --> loss:0.8188341319561004
step 151/334, epoch 226/501 --> loss:0.8201327300071717
step 201/334, epoch 226/501 --> loss:0.8032259297370911
step 251/334, epoch 226/501 --> loss:0.8349482333660125
step 301/334, epoch 226/501 --> loss:0.8279667532444
step 51/334, epoch 227/501 --> loss:0.8101367402076721
step 101/334, epoch 227/501 --> loss:0.8244110810756683
step 151/334, epoch 227/501 --> loss:0.8284781360626221
step 201/334, epoch 227/501 --> loss:0.8100783967971802
step 251/334, epoch 227/501 --> loss:0.8278218162059784
step 301/334, epoch 227/501 --> loss:0.8189312469959259
step 51/334, epoch 228/501 --> loss:0.8140799403190613
step 101/334, epoch 228/501 --> loss:0.8178845989704132
step 151/334, epoch 228/501 --> loss:0.8290774023532868
step 201/334, epoch 228/501 --> loss:0.8239853549003601
step 251/334, epoch 228/501 --> loss:0.823198367357254
step 301/334, epoch 228/501 --> loss:0.8248378658294677
step 51/334, epoch 229/501 --> loss:0.8269701147079468
step 101/334, epoch 229/501 --> loss:0.8153988695144654
step 151/334, epoch 229/501 --> loss:0.8107404339313508
step 201/334, epoch 229/501 --> loss:0.8167584979534149
step 251/334, epoch 229/501 --> loss:0.8159291088581085
step 301/334, epoch 229/501 --> loss:0.8407277786731719
step 51/334, epoch 230/501 --> loss:0.8227688705921173
step 101/334, epoch 230/501 --> loss:0.8297183871269226
step 151/334, epoch 230/501 --> loss:0.8083007383346558
step 201/334, epoch 230/501 --> loss:0.8347426378726959
step 251/334, epoch 230/501 --> loss:0.8151406383514405
step 301/334, epoch 230/501 --> loss:0.811320744752884
step 51/334, epoch 231/501 --> loss:0.811587483882904
step 101/334, epoch 231/501 --> loss:0.8223050379753113
step 151/334, epoch 231/501 --> loss:0.8169417250156402
step 201/334, epoch 231/501 --> loss:0.8281312620639801
step 251/334, epoch 231/501 --> loss:0.8291388475894927
step 301/334, epoch 231/501 --> loss:0.8206464684009552

##########train dataset##########
acc--> [98.47534688885654]
F1--> {'F1': [0.8451191777824952], 'precision': [0.7392425206158938], 'recall': [0.986406938628071]}
##########eval dataset##########
acc--> [97.9236132203685]
F1--> {'F1': [0.7940957441482539], 'precision': [0.6948641743227425], 'recall': [0.9264043310959934]}
step 51/334, epoch 232/501 --> loss:0.8228778171539307
step 101/334, epoch 232/501 --> loss:0.818484171628952
step 151/334, epoch 232/501 --> loss:0.8288919925689697
step 201/334, epoch 232/501 --> loss:0.8221667420864105
step 251/334, epoch 232/501 --> loss:0.8199711990356445
step 301/334, epoch 232/501 --> loss:0.8342007184028626
step 51/334, epoch 233/501 --> loss:0.8248586666584015
step 101/334, epoch 233/501 --> loss:0.832588666677475
step 151/334, epoch 233/501 --> loss:0.8187544810771942
step 201/334, epoch 233/501 --> loss:0.8124825894832611
step 251/334, epoch 233/501 --> loss:0.8178504145145417
step 301/334, epoch 233/501 --> loss:0.8192074084281922
step 51/334, epoch 234/501 --> loss:0.8244818150997162
step 101/334, epoch 234/501 --> loss:0.8149503469467163
step 151/334, epoch 234/501 --> loss:0.8268410646915436
step 201/334, epoch 234/501 --> loss:0.8250328409671783
step 251/334, epoch 234/501 --> loss:0.8199815368652343
step 301/334, epoch 234/501 --> loss:0.8150458884239197
step 51/334, epoch 235/501 --> loss:0.8394269001483917
step 101/334, epoch 235/501 --> loss:0.8195544910430909
step 151/334, epoch 235/501 --> loss:0.8124582433700561
step 201/334, epoch 235/501 --> loss:0.8167174291610718
step 251/334, epoch 235/501 --> loss:0.8174822926521301
step 301/334, epoch 235/501 --> loss:0.8243748986721039
step 51/334, epoch 236/501 --> loss:0.8201649153232574
step 101/334, epoch 236/501 --> loss:0.8202767539024353
step 151/334, epoch 236/501 --> loss:0.8399915885925293
step 201/334, epoch 236/501 --> loss:0.8200261735916138
step 251/334, epoch 236/501 --> loss:0.8102894616127014
step 301/334, epoch 236/501 --> loss:0.8125778436660767
step 51/334, epoch 237/501 --> loss:0.8143810081481934
step 101/334, epoch 237/501 --> loss:0.8176156890392303
step 151/334, epoch 237/501 --> loss:0.8294178426265717
step 201/334, epoch 237/501 --> loss:0.8203175759315491
step 251/334, epoch 237/501 --> loss:0.8258349633216858
step 301/334, epoch 237/501 --> loss:0.8100801813602447
step 51/334, epoch 238/501 --> loss:0.8194227135181427
step 101/334, epoch 238/501 --> loss:0.8308460855484009
step 151/334, epoch 238/501 --> loss:0.8216271770000457
step 201/334, epoch 238/501 --> loss:0.8255753064155579
step 251/334, epoch 238/501 --> loss:0.8244761562347412
step 301/334, epoch 238/501 --> loss:0.8010164248943329
step 51/334, epoch 239/501 --> loss:0.8263436210155487
step 101/334, epoch 239/501 --> loss:0.8138567793369293
step 151/334, epoch 239/501 --> loss:0.8212886941432953
step 201/334, epoch 239/501 --> loss:0.8321991372108459
step 251/334, epoch 239/501 --> loss:0.8249724388122559
step 301/334, epoch 239/501 --> loss:0.8255773746967315
step 51/334, epoch 240/501 --> loss:0.8262777149677276
step 101/334, epoch 240/501 --> loss:0.8178356230258942
step 151/334, epoch 240/501 --> loss:0.8104509568214416
step 201/334, epoch 240/501 --> loss:0.8144367253780365
step 251/334, epoch 240/501 --> loss:0.8293664669990539
step 301/334, epoch 240/501 --> loss:0.8288050627708435
step 51/334, epoch 241/501 --> loss:0.8210061776638031
step 101/334, epoch 241/501 --> loss:0.8169545924663544
step 151/334, epoch 241/501 --> loss:0.8218248188495636
step 201/334, epoch 241/501 --> loss:0.8168249666690827
step 251/334, epoch 241/501 --> loss:0.8161565780639648
step 301/334, epoch 241/501 --> loss:0.8194582855701447

##########train dataset##########
acc--> [99.36547524302098]
F1--> {'F1': [0.9294705874062036], 'precision': [0.8747494181036845], 'recall': [0.9915062699030096]}
##########eval dataset##########
acc--> [98.77125297005061]
F1--> {'F1': [0.8650301266607721], 'precision': [0.8234364092987326], 'recall': [0.9110604461488231]}
step 51/334, epoch 242/501 --> loss:0.8089200663566589
step 101/334, epoch 242/501 --> loss:0.8128157615661621
step 151/334, epoch 242/501 --> loss:0.8340542674064636
step 201/334, epoch 242/501 --> loss:0.8147192287445069
step 251/334, epoch 242/501 --> loss:0.8240520691871643
step 301/334, epoch 242/501 --> loss:0.8266279327869416
step 51/334, epoch 243/501 --> loss:0.8185021448135376
step 101/334, epoch 243/501 --> loss:0.8330478060245514
step 151/334, epoch 243/501 --> loss:0.8193117034435272
step 201/334, epoch 243/501 --> loss:0.8183828973770142
step 251/334, epoch 243/501 --> loss:0.8380553209781647
step 301/334, epoch 243/501 --> loss:0.8105364072322846
step 51/334, epoch 244/501 --> loss:0.817587149143219
step 101/334, epoch 244/501 --> loss:0.82206591963768
step 151/334, epoch 244/501 --> loss:0.8200753319263459
step 201/334, epoch 244/501 --> loss:0.8112880456447601
step 251/334, epoch 244/501 --> loss:0.823570088148117
step 301/334, epoch 244/501 --> loss:0.8210738945007324
step 51/334, epoch 245/501 --> loss:0.8227197277545929
step 101/334, epoch 245/501 --> loss:0.821016286611557
step 151/334, epoch 245/501 --> loss:0.8312890505790711
step 201/334, epoch 245/501 --> loss:0.8171692633628845
step 251/334, epoch 245/501 --> loss:0.824404730796814
step 301/334, epoch 245/501 --> loss:0.8212894415855407
step 51/334, epoch 246/501 --> loss:0.8258627080917358
step 101/334, epoch 246/501 --> loss:0.8186609542369843
step 151/334, epoch 246/501 --> loss:0.8156256651878357
step 201/334, epoch 246/501 --> loss:0.8163920068740844
step 251/334, epoch 246/501 --> loss:0.8238806521892548
step 301/334, epoch 246/501 --> loss:0.8346398913860321
step 51/334, epoch 247/501 --> loss:0.8228469967842102
step 101/334, epoch 247/501 --> loss:0.8255958676338195
step 151/334, epoch 247/501 --> loss:0.821569322347641
step 201/334, epoch 247/501 --> loss:0.8173636591434479
step 251/334, epoch 247/501 --> loss:0.8274608254432678
step 301/334, epoch 247/501 --> loss:0.8236306130886077
step 51/334, epoch 248/501 --> loss:0.8285473382472992
step 101/334, epoch 248/501 --> loss:0.8251841497421265
step 151/334, epoch 248/501 --> loss:0.8176889884471893
step 201/334, epoch 248/501 --> loss:0.8237074875831604
step 251/334, epoch 248/501 --> loss:0.8134005606174469
step 301/334, epoch 248/501 --> loss:0.8278128206729889
step 51/334, epoch 249/501 --> loss:0.8200857818126679
step 101/334, epoch 249/501 --> loss:0.8189436364173889
step 151/334, epoch 249/501 --> loss:0.8276641857624054
step 201/334, epoch 249/501 --> loss:0.8132427310943604
step 251/334, epoch 249/501 --> loss:0.8335884368419647
step 301/334, epoch 249/501 --> loss:0.8209978270530701
step 51/334, epoch 250/501 --> loss:0.8247956192493439
step 101/334, epoch 250/501 --> loss:0.8242302334308624
step 151/334, epoch 250/501 --> loss:0.8159063255786896
step 201/334, epoch 250/501 --> loss:0.825354722738266
step 251/334, epoch 250/501 --> loss:0.8284276986122131
step 301/334, epoch 250/501 --> loss:0.8226742458343506
step 51/334, epoch 251/501 --> loss:0.8228148484230041
step 101/334, epoch 251/501 --> loss:0.8305112838745117
step 151/334, epoch 251/501 --> loss:0.8197318720817566
step 201/334, epoch 251/501 --> loss:0.8248130512237549
step 251/334, epoch 251/501 --> loss:0.8182377803325653
step 301/334, epoch 251/501 --> loss:0.8182384359836579

##########train dataset##########
acc--> [99.51641310186284]
F1--> {'F1': [0.9451827055431744], 'precision': [0.9053467158323069], 'recall': [0.9886965976571805]}
##########eval dataset##########
acc--> [98.91193219682327]
F1--> {'F1': [0.875338665494699], 'precision': [0.8669732650441994], 'recall': [0.8838772686867317]}
save model!
step 51/334, epoch 252/501 --> loss:0.8192781698703766
step 101/334, epoch 252/501 --> loss:0.815966089963913
step 151/334, epoch 252/501 --> loss:0.827761869430542
step 201/334, epoch 252/501 --> loss:0.8261646699905395
step 251/334, epoch 252/501 --> loss:0.8280814898014068
step 301/334, epoch 252/501 --> loss:0.8037935888767243
step 51/334, epoch 253/501 --> loss:0.8280210554599762
step 101/334, epoch 253/501 --> loss:0.8384686779975891
step 151/334, epoch 253/501 --> loss:0.8270388460159301
step 201/334, epoch 253/501 --> loss:0.8022318518161774
step 251/334, epoch 253/501 --> loss:0.825592246055603
step 301/334, epoch 253/501 --> loss:0.8073568522930146
step 51/334, epoch 254/501 --> loss:0.8122511947154999
step 101/334, epoch 254/501 --> loss:0.8238721978664398
step 151/334, epoch 254/501 --> loss:0.8245606708526612
step 201/334, epoch 254/501 --> loss:0.8217547762393952
step 251/334, epoch 254/501 --> loss:0.8160883295536041
step 301/334, epoch 254/501 --> loss:0.8324060344696045
step 51/334, epoch 255/501 --> loss:0.8253494739532471
step 101/334, epoch 255/501 --> loss:0.8324563539028168
step 151/334, epoch 255/501 --> loss:0.8108725416660308
step 201/334, epoch 255/501 --> loss:0.8142254745960236
step 251/334, epoch 255/501 --> loss:0.8222207605838776
step 301/334, epoch 255/501 --> loss:0.82126025557518
step 51/334, epoch 256/501 --> loss:0.809554398059845
step 101/334, epoch 256/501 --> loss:0.8271701526641846
step 151/334, epoch 256/501 --> loss:0.822508761882782
step 201/334, epoch 256/501 --> loss:0.8183776247501373
step 251/334, epoch 256/501 --> loss:0.8249852180480957
step 301/334, epoch 256/501 --> loss:0.817294442653656
step 51/334, epoch 257/501 --> loss:0.8126393461227417
step 101/334, epoch 257/501 --> loss:0.8199409341812134
step 151/334, epoch 257/501 --> loss:0.8265376424789429
step 201/334, epoch 257/501 --> loss:0.8191788005828857
step 251/334, epoch 257/501 --> loss:0.8236115193367004
step 301/334, epoch 257/501 --> loss:0.8176115810871124
step 51/334, epoch 258/501 --> loss:0.8294832754135132
step 101/334, epoch 258/501 --> loss:0.815695071220398
step 151/334, epoch 258/501 --> loss:0.8144572567939758
step 201/334, epoch 258/501 --> loss:0.8267542612552643
step 251/334, epoch 258/501 --> loss:0.8273915493488312
step 301/334, epoch 258/501 --> loss:0.805376261472702
step 51/334, epoch 259/501 --> loss:0.8318419039249421
step 101/334, epoch 259/501 --> loss:0.8224964773654938
step 151/334, epoch 259/501 --> loss:0.8077056014537811
step 201/334, epoch 259/501 --> loss:0.8025610327720643
step 251/334, epoch 259/501 --> loss:0.830122367143631
step 301/334, epoch 259/501 --> loss:0.8175006771087646
step 51/334, epoch 260/501 --> loss:0.8300193881988526
step 101/334, epoch 260/501 --> loss:0.8216264951229095
step 151/334, epoch 260/501 --> loss:0.8063250088691711
step 201/334, epoch 260/501 --> loss:0.8107554471492767
step 251/334, epoch 260/501 --> loss:0.8295134198665619
step 301/334, epoch 260/501 --> loss:0.822140337228775
step 51/334, epoch 261/501 --> loss:0.8100779008865356
step 101/334, epoch 261/501 --> loss:0.8232397210597991
step 151/334, epoch 261/501 --> loss:0.8264960885047913
step 201/334, epoch 261/501 --> loss:0.824443564414978
step 251/334, epoch 261/501 --> loss:0.8168644714355469
step 301/334, epoch 261/501 --> loss:0.8288197886943817

##########train dataset##########
acc--> [99.50340136544537]
F1--> {'F1': [0.9438882218603629], 'precision': [0.9014621445184048], 'recall': [0.9905159655786046]}
##########eval dataset##########
acc--> [98.88401782418138]
F1--> {'F1': [0.8752177060294728], 'precision': [0.8468616902189547], 'recall': [0.9055491259122002]}
step 51/334, epoch 262/501 --> loss:0.8261900913715362
step 101/334, epoch 262/501 --> loss:0.8245504772663117
step 151/334, epoch 262/501 --> loss:0.8243130040168762
step 201/334, epoch 262/501 --> loss:0.8135540521144867
step 251/334, epoch 262/501 --> loss:0.8152019453048706
step 301/334, epoch 262/501 --> loss:0.8130756318569183
step 51/334, epoch 263/501 --> loss:0.825278388261795
step 101/334, epoch 263/501 --> loss:0.8084188270568847
step 151/334, epoch 263/501 --> loss:0.8191130268573761
step 201/334, epoch 263/501 --> loss:0.8320725107192993
step 251/334, epoch 263/501 --> loss:0.8173141932487488
step 301/334, epoch 263/501 --> loss:0.8274260258674622
step 51/334, epoch 264/501 --> loss:0.8142312633991241
step 101/334, epoch 264/501 --> loss:0.8123054158687592
step 151/334, epoch 264/501 --> loss:0.8375346183776855
step 201/334, epoch 264/501 --> loss:0.8149384462833404
step 251/334, epoch 264/501 --> loss:0.8191740214824677
step 301/334, epoch 264/501 --> loss:0.8250457847118378
step 51/334, epoch 265/501 --> loss:0.8416587746143341
step 101/334, epoch 265/501 --> loss:0.80776407122612
step 151/334, epoch 265/501 --> loss:0.8161370944976807
step 201/334, epoch 265/501 --> loss:0.8210289859771729
step 251/334, epoch 265/501 --> loss:0.8124353671073914
step 301/334, epoch 265/501 --> loss:0.8334889364242554
step 51/334, epoch 266/501 --> loss:0.8257779371738434
step 101/334, epoch 266/501 --> loss:0.8216957414150238
step 151/334, epoch 266/501 --> loss:0.8121473300457
step 201/334, epoch 266/501 --> loss:0.8147467923164368
step 251/334, epoch 266/501 --> loss:0.8173807370662689
step 301/334, epoch 266/501 --> loss:0.8364079904556274
step 51/334, epoch 267/501 --> loss:0.8228886353969574
step 101/334, epoch 267/501 --> loss:0.8334419357776642
step 151/334, epoch 267/501 --> loss:0.8139950430393219
step 201/334, epoch 267/501 --> loss:0.8107592236995697
step 251/334, epoch 267/501 --> loss:0.8180226266384125
step 301/334, epoch 267/501 --> loss:0.8289726912975312
step 51/334, epoch 268/501 --> loss:0.8246350884437561
step 101/334, epoch 268/501 --> loss:0.8155112016201019
step 151/334, epoch 268/501 --> loss:0.8220618283748626
step 201/334, epoch 268/501 --> loss:0.8214735984802246
step 251/334, epoch 268/501 --> loss:0.8220226907730103
step 301/334, epoch 268/501 --> loss:0.8244575834274293
step 51/334, epoch 269/501 --> loss:0.8196817851066589
step 101/334, epoch 269/501 --> loss:0.829370905160904
step 151/334, epoch 269/501 --> loss:0.8239910972118377
step 201/334, epoch 269/501 --> loss:0.8177586889266968
step 251/334, epoch 269/501 --> loss:0.8259482049942016
step 301/334, epoch 269/501 --> loss:0.8137347900867462
step 51/334, epoch 270/501 --> loss:0.8156705749034882
step 101/334, epoch 270/501 --> loss:0.8115883052349091
step 151/334, epoch 270/501 --> loss:0.8311203646659852
step 201/334, epoch 270/501 --> loss:0.823572005033493
step 251/334, epoch 270/501 --> loss:0.8203020143508911
step 301/334, epoch 270/501 --> loss:0.8241171264648437
step 51/334, epoch 271/501 --> loss:0.8141152548789978
step 101/334, epoch 271/501 --> loss:0.8153801023960113
step 151/334, epoch 271/501 --> loss:0.8252874982357025
step 201/334, epoch 271/501 --> loss:0.8307833337783813
step 251/334, epoch 271/501 --> loss:0.812441565990448
step 301/334, epoch 271/501 --> loss:0.815190691947937

##########train dataset##########
acc--> [99.54845341216217]
F1--> {'F1': [0.9486882217310662], 'precision': [0.9107592714469344], 'recall': [0.9899244556321142]}
##########eval dataset##########
acc--> [98.96078435141234]
F1--> {'F1': [0.8804220608663959], 'precision': [0.8757122646396576], 'recall': [0.885192900067789]}
save model!
step 51/334, epoch 272/501 --> loss:0.8170211565494537
step 101/334, epoch 272/501 --> loss:0.8238688838481903
step 151/334, epoch 272/501 --> loss:0.8196483969688415
step 201/334, epoch 272/501 --> loss:0.8241236662864685
step 251/334, epoch 272/501 --> loss:0.8144713175296784
step 301/334, epoch 272/501 --> loss:0.818544545173645
step 51/334, epoch 273/501 --> loss:0.8205917644500732
step 101/334, epoch 273/501 --> loss:0.8019391620159149
step 151/334, epoch 273/501 --> loss:0.8266603791713715
step 201/334, epoch 273/501 --> loss:0.8401958358287811
step 251/334, epoch 273/501 --> loss:0.8250893306732178
step 301/334, epoch 273/501 --> loss:0.8164444434642791
step 51/334, epoch 274/501 --> loss:0.8208663618564606
step 101/334, epoch 274/501 --> loss:0.8206871736049652
step 151/334, epoch 274/501 --> loss:0.8045810747146607
step 201/334, epoch 274/501 --> loss:0.8195892798900605
step 251/334, epoch 274/501 --> loss:0.8261541247367858
step 301/334, epoch 274/501 --> loss:0.8252531945705414
step 51/334, epoch 275/501 --> loss:0.8046841394901275
step 101/334, epoch 275/501 --> loss:0.821631053686142
step 151/334, epoch 275/501 --> loss:0.8283434188365937
step 201/334, epoch 275/501 --> loss:0.8298893988132476
step 251/334, epoch 275/501 --> loss:0.8264877450466156
step 301/334, epoch 275/501 --> loss:0.8141629242897034
step 51/334, epoch 276/501 --> loss:0.8210097849369049
step 101/334, epoch 276/501 --> loss:0.8291657960414887
step 151/334, epoch 276/501 --> loss:0.8108051669597626
step 201/334, epoch 276/501 --> loss:0.8194848811626434
step 251/334, epoch 276/501 --> loss:0.8206866765022278
step 301/334, epoch 276/501 --> loss:0.8250663101673126
step 51/334, epoch 277/501 --> loss:0.8199063158035278
step 101/334, epoch 277/501 --> loss:0.839048490524292
step 151/334, epoch 277/501 --> loss:0.8182685244083404
step 201/334, epoch 277/501 --> loss:0.8130212926864624
step 251/334, epoch 277/501 --> loss:0.811200065612793
step 301/334, epoch 277/501 --> loss:0.821794501543045
step 51/334, epoch 278/501 --> loss:0.8198494374752044
step 101/334, epoch 278/501 --> loss:0.8142579078674317
step 151/334, epoch 278/501 --> loss:0.8275591433048248
step 201/334, epoch 278/501 --> loss:0.8213287246227264
step 251/334, epoch 278/501 --> loss:0.8255913496017456
step 301/334, epoch 278/501 --> loss:0.8127398240566254
step 51/334, epoch 279/501 --> loss:0.8272913384437561
step 101/334, epoch 279/501 --> loss:0.8344587898254394
step 151/334, epoch 279/501 --> loss:0.8186657822132111
step 201/334, epoch 279/501 --> loss:0.8221176600456238
step 251/334, epoch 279/501 --> loss:0.8096065747737885
step 301/334, epoch 279/501 --> loss:0.810638610124588
step 51/334, epoch 280/501 --> loss:0.8236792778968811
step 101/334, epoch 280/501 --> loss:0.8247064614295959
step 151/334, epoch 280/501 --> loss:0.8181040811538697
step 201/334, epoch 280/501 --> loss:0.819463518857956
step 251/334, epoch 280/501 --> loss:0.8114893329143524
step 301/334, epoch 280/501 --> loss:0.8169048738479614
step 51/334, epoch 281/501 --> loss:0.8149664556980133
step 101/334, epoch 281/501 --> loss:0.8332776129245758
step 151/334, epoch 281/501 --> loss:0.8293899774551392
step 201/334, epoch 281/501 --> loss:0.8251214599609376
step 251/334, epoch 281/501 --> loss:0.8069158935546875
step 301/334, epoch 281/501 --> loss:0.8185299718379975

##########train dataset##########
acc--> [99.37424203810177]
F1--> {'F1': [0.930352679149956], 'precision': [0.8766058762558167], 'recall': [0.9911319634942098]}
##########eval dataset##########
acc--> [98.72139491016333]
F1--> {'F1': [0.8603418051111571], 'precision': [0.8148409869582726], 'recall': [0.9112358842576247]}
step 51/334, epoch 282/501 --> loss:0.820611582994461
step 101/334, epoch 282/501 --> loss:0.829760377407074
step 151/334, epoch 282/501 --> loss:0.8241156685352325
step 201/334, epoch 282/501 --> loss:0.8107053697109222
step 251/334, epoch 282/501 --> loss:0.8234984183311462
step 301/334, epoch 282/501 --> loss:0.8241993868350983
step 51/334, epoch 283/501 --> loss:0.8155140125751495
step 101/334, epoch 283/501 --> loss:0.816799088716507
step 151/334, epoch 283/501 --> loss:0.818470013141632
step 201/334, epoch 283/501 --> loss:0.8157141602039337
step 251/334, epoch 283/501 --> loss:0.8210695862770081
step 301/334, epoch 283/501 --> loss:0.8262758708000183
step 51/334, epoch 284/501 --> loss:0.825211136341095
step 101/334, epoch 284/501 --> loss:0.8182742869853974
step 151/334, epoch 284/501 --> loss:0.8262687504291535
step 201/334, epoch 284/501 --> loss:0.8050275611877441
step 251/334, epoch 284/501 --> loss:0.8135913145542145
step 301/334, epoch 284/501 --> loss:0.8358059060573578
step 51/334, epoch 285/501 --> loss:0.8165335845947266
step 101/334, epoch 285/501 --> loss:0.81747110247612
step 151/334, epoch 285/501 --> loss:0.8242167568206787
step 201/334, epoch 285/501 --> loss:0.8328084671497344
step 251/334, epoch 285/501 --> loss:0.826709748506546
step 301/334, epoch 285/501 --> loss:0.811989471912384
step 51/334, epoch 286/501 --> loss:0.8127192151546478
step 101/334, epoch 286/501 --> loss:0.8329068470001221
step 151/334, epoch 286/501 --> loss:0.807737009525299
step 201/334, epoch 286/501 --> loss:0.8263576805591584
step 251/334, epoch 286/501 --> loss:0.818011736869812
step 301/334, epoch 286/501 --> loss:0.8190413248538971
step 51/334, epoch 287/501 --> loss:0.8238126540184021
step 101/334, epoch 287/501 --> loss:0.8188338077068329
step 151/334, epoch 287/501 --> loss:0.8255843162536621
step 201/334, epoch 287/501 --> loss:0.8204784226417542
step 251/334, epoch 287/501 --> loss:0.8273720943927765
step 301/334, epoch 287/501 --> loss:0.8087032473087311
step 51/334, epoch 288/501 --> loss:0.8246384131908416
step 101/334, epoch 288/501 --> loss:0.8332445085048675
step 151/334, epoch 288/501 --> loss:0.8088594162464142
step 201/334, epoch 288/501 --> loss:0.818466784954071
step 251/334, epoch 288/501 --> loss:0.8155424320697784
step 301/334, epoch 288/501 --> loss:0.8173565340042114
step 51/334, epoch 289/501 --> loss:0.8288775146007538
step 101/334, epoch 289/501 --> loss:0.8120806992053986
step 151/334, epoch 289/501 --> loss:0.8187295794487
step 201/334, epoch 289/501 --> loss:0.8265173292160034
step 251/334, epoch 289/501 --> loss:0.8166800153255462
step 301/334, epoch 289/501 --> loss:0.8135676527023316
step 51/334, epoch 290/501 --> loss:0.8238503956794738
step 101/334, epoch 290/501 --> loss:0.8322861158847809
step 151/334, epoch 290/501 --> loss:0.8074017107486725
step 201/334, epoch 290/501 --> loss:0.8095805430412293
step 251/334, epoch 290/501 --> loss:0.8283052015304565
step 301/334, epoch 290/501 --> loss:0.8250119388103485
step 51/334, epoch 291/501 --> loss:0.8208434760570527
step 101/334, epoch 291/501 --> loss:0.8326207613945007
step 151/334, epoch 291/501 --> loss:0.8270347380638122
step 201/334, epoch 291/501 --> loss:0.8192625451087951
step 251/334, epoch 291/501 --> loss:0.8207481670379638
step 301/334, epoch 291/501 --> loss:0.8030084049701691

##########train dataset##########
acc--> [99.31244022353367]
F1--> {'F1': [0.9215116392574448], 'precision': [0.8884326168828858], 'recall': [0.9571599599584687]}
##########eval dataset##########
acc--> [98.7532080836326]
F1--> {'F1': [0.8573006196717785], 'precision': [0.8482546424410498], 'recall': [0.8665518288328415]}
step 51/334, epoch 292/501 --> loss:0.829200588464737
step 101/334, epoch 292/501 --> loss:0.8251782786846161
step 151/334, epoch 292/501 --> loss:0.8228123581409454
step 201/334, epoch 292/501 --> loss:0.820315123796463
step 251/334, epoch 292/501 --> loss:0.8172285997867584
step 301/334, epoch 292/501 --> loss:0.8211826634407043
step 51/334, epoch 293/501 --> loss:0.8165832388401032
step 101/334, epoch 293/501 --> loss:0.8254455649852752
step 151/334, epoch 293/501 --> loss:0.8018688666820526
step 201/334, epoch 293/501 --> loss:0.8166167235374451
step 251/334, epoch 293/501 --> loss:0.8340237176418305
step 301/334, epoch 293/501 --> loss:0.8265013253688812
step 51/334, epoch 294/501 --> loss:0.8060591685771942
step 101/334, epoch 294/501 --> loss:0.8132671546936036
step 151/334, epoch 294/501 --> loss:0.8158052432537078
step 201/334, epoch 294/501 --> loss:0.8281924557685852
step 251/334, epoch 294/501 --> loss:0.8234969091415405
step 301/334, epoch 294/501 --> loss:0.8233732283115387
step 51/334, epoch 295/501 --> loss:0.8157970750331879
step 101/334, epoch 295/501 --> loss:0.8126687788963318
step 151/334, epoch 295/501 --> loss:0.8133502578735352
step 201/334, epoch 295/501 --> loss:0.8295847988128662
step 251/334, epoch 295/501 --> loss:0.8343747341632843
step 301/334, epoch 295/501 --> loss:0.8254886531829834
step 51/334, epoch 296/501 --> loss:0.8343745636940002
step 101/334, epoch 296/501 --> loss:0.8179170656204223
step 151/334, epoch 296/501 --> loss:0.8230087506771088
step 201/334, epoch 296/501 --> loss:0.8136627197265625
step 251/334, epoch 296/501 --> loss:0.8251083147525787
step 301/334, epoch 296/501 --> loss:0.8165084326267242
step 51/334, epoch 297/501 --> loss:0.8221136116981507
step 101/334, epoch 297/501 --> loss:0.8362584984302521
step 151/334, epoch 297/501 --> loss:0.8193165302276612
step 201/334, epoch 297/501 --> loss:0.817705819606781
step 251/334, epoch 297/501 --> loss:0.8108504164218903
step 301/334, epoch 297/501 --> loss:0.8101416456699372
step 51/334, epoch 298/501 --> loss:0.8210114645957947
step 101/334, epoch 298/501 --> loss:0.8199042713642121
step 151/334, epoch 298/501 --> loss:0.811121791601181
step 201/334, epoch 298/501 --> loss:0.815114313364029
step 251/334, epoch 298/501 --> loss:0.8268259990215302
step 301/334, epoch 298/501 --> loss:0.8401524376869202
step 51/334, epoch 299/501 --> loss:0.8082264745235443
step 101/334, epoch 299/501 --> loss:0.8170098412036896
step 151/334, epoch 299/501 --> loss:0.8206917810440063
step 201/334, epoch 299/501 --> loss:0.8226422202587128
step 251/334, epoch 299/501 --> loss:0.8250290167331695
step 301/334, epoch 299/501 --> loss:0.8222956454753876
step 51/334, epoch 300/501 --> loss:0.8085660862922669
step 101/334, epoch 300/501 --> loss:0.8156645834445954
step 151/334, epoch 300/501 --> loss:0.8229567611217499
step 201/334, epoch 300/501 --> loss:0.8284539699554443
step 251/334, epoch 300/501 --> loss:0.8274958789348602
step 301/334, epoch 300/501 --> loss:0.8197120726108551
step 51/334, epoch 301/501 --> loss:0.8135384476184845
step 101/334, epoch 301/501 --> loss:0.8236530816555023
step 151/334, epoch 301/501 --> loss:0.8243985831737518
step 201/334, epoch 301/501 --> loss:0.8176897096633912
step 251/334, epoch 301/501 --> loss:0.837207727432251
step 301/334, epoch 301/501 --> loss:0.8107899630069733

##########train dataset##########
acc--> [99.50002778277918]
F1--> {'F1': [0.9431840097916324], 'precision': [0.9055025798066925], 'recall': [0.9841486187727321]}
##########eval dataset##########
acc--> [98.84844001509039]
F1--> {'F1': [0.8671393182741024], 'precision': [0.86480137392368], 'recall': [0.8694999921224745]}
step 51/334, epoch 302/501 --> loss:0.8154270577430726
step 101/334, epoch 302/501 --> loss:0.8300328171253204
step 151/334, epoch 302/501 --> loss:0.8202374792098999
step 201/334, epoch 302/501 --> loss:0.8180078995227814
step 251/334, epoch 302/501 --> loss:0.8363953614234925
step 301/334, epoch 302/501 --> loss:0.8144848716259002
step 51/334, epoch 303/501 --> loss:0.8165721070766448
step 101/334, epoch 303/501 --> loss:0.8269627749919891
step 151/334, epoch 303/501 --> loss:0.8275775933265686
step 201/334, epoch 303/501 --> loss:0.8249414730072021
step 251/334, epoch 303/501 --> loss:0.820826678276062
step 301/334, epoch 303/501 --> loss:0.8095867896080017
step 51/334, epoch 304/501 --> loss:0.8203847765922546
step 101/334, epoch 304/501 --> loss:0.8160939121246338
step 151/334, epoch 304/501 --> loss:0.8117333889007569
step 201/334, epoch 304/501 --> loss:0.827281482219696
step 251/334, epoch 304/501 --> loss:0.8291865825653076
step 301/334, epoch 304/501 --> loss:0.8233885443210602
step 51/334, epoch 305/501 --> loss:0.8167161560058593
step 101/334, epoch 305/501 --> loss:0.8392353963851928
step 151/334, epoch 305/501 --> loss:0.8137130272388459
step 201/334, epoch 305/501 --> loss:0.8155632209777832
step 251/334, epoch 305/501 --> loss:0.8145788097381592
step 301/334, epoch 305/501 --> loss:0.8172364127635956
step 51/334, epoch 306/501 --> loss:0.8207603871822358
step 101/334, epoch 306/501 --> loss:0.8159982252120972
step 151/334, epoch 306/501 --> loss:0.8144185364246368
step 201/334, epoch 306/501 --> loss:0.8219999122619629
step 251/334, epoch 306/501 --> loss:0.8277695310115815
step 301/334, epoch 306/501 --> loss:0.8153746938705444
step 51/334, epoch 307/501 --> loss:0.8283627700805664
step 101/334, epoch 307/501 --> loss:0.8110595834255219
step 151/334, epoch 307/501 --> loss:0.8254938280582428
step 201/334, epoch 307/501 --> loss:0.8250951254367829
step 251/334, epoch 307/501 --> loss:0.8159156215190887
step 301/334, epoch 307/501 --> loss:0.8055179703235626
step 51/334, epoch 308/501 --> loss:0.8431889152526856
step 101/334, epoch 308/501 --> loss:0.8224547255039215
step 151/334, epoch 308/501 --> loss:0.8148364210128785
step 201/334, epoch 308/501 --> loss:0.8239907443523407
step 251/334, epoch 308/501 --> loss:0.8235024893283844
step 301/334, epoch 308/501 --> loss:0.8090818226337433
step 51/334, epoch 309/501 --> loss:0.8150543475151062
step 101/334, epoch 309/501 --> loss:0.8158708107471466
step 151/334, epoch 309/501 --> loss:0.825531313419342
step 201/334, epoch 309/501 --> loss:0.8231244385242462
step 251/334, epoch 309/501 --> loss:0.8329790437221527
step 301/334, epoch 309/501 --> loss:0.8174703705310822
step 51/334, epoch 310/501 --> loss:0.817274112701416
step 101/334, epoch 310/501 --> loss:0.8268368697166443
step 151/334, epoch 310/501 --> loss:0.8104421019554138
step 201/334, epoch 310/501 --> loss:0.8142529237270355
step 251/334, epoch 310/501 --> loss:0.8168692517280579
step 301/334, epoch 310/501 --> loss:0.8322264516353607
step 51/334, epoch 311/501 --> loss:0.828366631269455
step 101/334, epoch 311/501 --> loss:0.8185162782669068
step 151/334, epoch 311/501 --> loss:0.8240600395202636
step 201/334, epoch 311/501 --> loss:0.8317899537086487
step 251/334, epoch 311/501 --> loss:0.8195988750457763
step 301/334, epoch 311/501 --> loss:0.8025240886211396

##########train dataset##########
acc--> [99.54129717168247]
F1--> {'F1': [0.9480357794944975], 'precision': [0.9075587428432074], 'recall': [0.9923028411531972]}
##########eval dataset##########
acc--> [98.87387333266031]
F1--> {'F1': [0.873101178245309], 'precision': [0.8510225757836087], 'recall': [0.8963664234920024]}
step 51/334, epoch 312/501 --> loss:0.8294597136974334
step 101/334, epoch 312/501 --> loss:0.8174187231063843
step 151/334, epoch 312/501 --> loss:0.8197688615322113
step 201/334, epoch 312/501 --> loss:0.8180969738960266
step 251/334, epoch 312/501 --> loss:0.832870272397995
step 301/334, epoch 312/501 --> loss:0.8210239350795746
step 51/334, epoch 313/501 --> loss:0.8198215591907502
step 101/334, epoch 313/501 --> loss:0.82970773935318
step 151/334, epoch 313/501 --> loss:0.8253616452217102
step 201/334, epoch 313/501 --> loss:0.8058906209468841
step 251/334, epoch 313/501 --> loss:0.8244242477416992
step 301/334, epoch 313/501 --> loss:0.818256846666336
step 51/334, epoch 314/501 --> loss:0.8109981489181518
step 101/334, epoch 314/501 --> loss:0.8171624052524566
step 151/334, epoch 314/501 --> loss:0.8263128757476806
step 201/334, epoch 314/501 --> loss:0.8309915113449097
step 251/334, epoch 314/501 --> loss:0.8093158352375031
step 301/334, epoch 314/501 --> loss:0.8161845600605011
step 51/334, epoch 315/501 --> loss:0.8174523198604584
step 101/334, epoch 315/501 --> loss:0.822664692401886
step 151/334, epoch 315/501 --> loss:0.8198372435569763
step 201/334, epoch 315/501 --> loss:0.8124473679065705
step 251/334, epoch 315/501 --> loss:0.8167775785923004
step 301/334, epoch 315/501 --> loss:0.8130868768692017
step 51/334, epoch 316/501 --> loss:0.8175479173660278
step 101/334, epoch 316/501 --> loss:0.8221013820171357
step 151/334, epoch 316/501 --> loss:0.8186364495754241
step 201/334, epoch 316/501 --> loss:0.8113263022899627
step 251/334, epoch 316/501 --> loss:0.8120249545574189
step 301/334, epoch 316/501 --> loss:0.8257459163665771
step 51/334, epoch 317/501 --> loss:0.844781676530838
step 101/334, epoch 317/501 --> loss:0.8181278705596924
step 151/334, epoch 317/501 --> loss:0.8106551599502564
step 201/334, epoch 317/501 --> loss:0.802719190120697
step 251/334, epoch 317/501 --> loss:0.8238519275188446
step 301/334, epoch 317/501 --> loss:0.8266656827926636
step 51/334, epoch 318/501 --> loss:0.8073121297359467
step 101/334, epoch 318/501 --> loss:0.8269102334976196
step 151/334, epoch 318/501 --> loss:0.8278908610343934
step 201/334, epoch 318/501 --> loss:0.8325641238689423
step 251/334, epoch 318/501 --> loss:0.8067913806438446
step 301/334, epoch 318/501 --> loss:0.8250776445865631
step 51/334, epoch 319/501 --> loss:0.8306134510040283
step 101/334, epoch 319/501 --> loss:0.8058752453327179
step 151/334, epoch 319/501 --> loss:0.822006151676178
step 201/334, epoch 319/501 --> loss:0.8215889942646026
step 251/334, epoch 319/501 --> loss:0.828891624212265
step 301/334, epoch 319/501 --> loss:0.8235887944698334
step 51/334, epoch 320/501 --> loss:0.8188978815078736
step 101/334, epoch 320/501 --> loss:0.8122667133808136
step 151/334, epoch 320/501 --> loss:0.8132366549968719
step 201/334, epoch 320/501 --> loss:0.821584587097168
step 251/334, epoch 320/501 --> loss:0.8208904910087585
step 301/334, epoch 320/501 --> loss:0.828809163570404
step 51/334, epoch 321/501 --> loss:0.8318060839176178
step 101/334, epoch 321/501 --> loss:0.8167591965198517
step 151/334, epoch 321/501 --> loss:0.812005170583725
step 201/334, epoch 321/501 --> loss:0.8143031287193299
step 251/334, epoch 321/501 --> loss:0.8265829014778138
step 301/334, epoch 321/501 --> loss:0.8220696496963501

##########train dataset##########
acc--> [99.50414571057361]
F1--> {'F1': [0.944038664327716], 'precision': [0.9006351484009244], 'recall': [0.9918484231335254]}
##########eval dataset##########
acc--> [98.80879786800281]
F1--> {'F1': [0.8674777609001582], 'precision': [0.8354401792619948], 'recall': [0.9020812889992244]}
step 51/334, epoch 322/501 --> loss:0.8194013619422913
step 101/334, epoch 322/501 --> loss:0.8147833526134491
step 151/334, epoch 322/501 --> loss:0.8276767385005951
step 201/334, epoch 322/501 --> loss:0.8272743999958039
step 251/334, epoch 322/501 --> loss:0.8159308743476867
step 301/334, epoch 322/501 --> loss:0.8197457373142243
step 51/334, epoch 323/501 --> loss:0.8159145140647888
step 101/334, epoch 323/501 --> loss:0.810560952425003
step 151/334, epoch 323/501 --> loss:0.8242790412902832
step 201/334, epoch 323/501 --> loss:0.8200906610488892
step 251/334, epoch 323/501 --> loss:0.830489091873169
step 301/334, epoch 323/501 --> loss:0.8292761659622192
step 51/334, epoch 324/501 --> loss:0.827229436635971
step 101/334, epoch 324/501 --> loss:0.8186750912666321
step 151/334, epoch 324/501 --> loss:0.8226620733737946
step 201/334, epoch 324/501 --> loss:0.8077616786956787
step 251/334, epoch 324/501 --> loss:0.8331662964820862
step 301/334, epoch 324/501 --> loss:0.813117401599884
step 51/334, epoch 325/501 --> loss:0.82739133477211
step 101/334, epoch 325/501 --> loss:0.8170599925518036
step 151/334, epoch 325/501 --> loss:0.8248616993427277
step 201/334, epoch 325/501 --> loss:0.8328851962089538
step 251/334, epoch 325/501 --> loss:0.8207211065292358
step 301/334, epoch 325/501 --> loss:0.8108576047420502
step 51/334, epoch 326/501 --> loss:0.8142010748386384
step 101/334, epoch 326/501 --> loss:0.8149953436851501
step 151/334, epoch 326/501 --> loss:0.8326928210258484
step 201/334, epoch 326/501 --> loss:0.8152899324893952
step 251/334, epoch 326/501 --> loss:0.8281313490867614
step 301/334, epoch 326/501 --> loss:0.8134306013584137
step 51/334, epoch 327/501 --> loss:0.8154183208942414
step 101/334, epoch 327/501 --> loss:0.8224284780025483
step 151/334, epoch 327/501 --> loss:0.8260643923282623
step 201/334, epoch 327/501 --> loss:0.8196140968799591
step 251/334, epoch 327/501 --> loss:0.8186052215099334
step 301/334, epoch 327/501 --> loss:0.8232855021953582
step 51/334, epoch 328/501 --> loss:0.8116269755363464
step 101/334, epoch 328/501 --> loss:0.8243320572376251
step 151/334, epoch 328/501 --> loss:0.8166962599754334
step 201/334, epoch 328/501 --> loss:0.8196066725254059
step 251/334, epoch 328/501 --> loss:0.827545838356018
step 301/334, epoch 328/501 --> loss:0.8195410418510437
step 51/334, epoch 329/501 --> loss:0.8286698353290558
step 101/334, epoch 329/501 --> loss:0.8124006342887878
step 151/334, epoch 329/501 --> loss:0.8233192372322082
step 201/334, epoch 329/501 --> loss:0.8222990918159485
step 251/334, epoch 329/501 --> loss:0.8144987487792968
step 301/334, epoch 329/501 --> loss:0.8148244047164916
step 51/334, epoch 330/501 --> loss:0.8070718896389008
step 101/334, epoch 330/501 --> loss:0.8224003624916076
step 151/334, epoch 330/501 --> loss:0.824312082529068
step 201/334, epoch 330/501 --> loss:0.8101617002487183
step 251/334, epoch 330/501 --> loss:0.8288261568546296
step 301/334, epoch 330/501 --> loss:0.8171860587596893
step 51/334, epoch 331/501 --> loss:0.8161190581321717
step 101/334, epoch 331/501 --> loss:0.8198566114902497
step 151/334, epoch 331/501 --> loss:0.8231115746498108
step 201/334, epoch 331/501 --> loss:0.8255943560600281
step 251/334, epoch 331/501 --> loss:0.8062479746341705
step 301/334, epoch 331/501 --> loss:0.8210858368873596

##########train dataset##########
acc--> [99.05975479108528]
F1--> {'F1': [0.8987763740507085], 'precision': [0.8230363933752736], 'recall': [0.9898811777254788]}
##########eval dataset##########
acc--> [98.27916970506814]
F1--> {'F1': [0.8218087988667651], 'precision': [0.7437802172221595], 'recall': [0.9181402077877253]}
step 51/334, epoch 332/501 --> loss:0.8231200456619263
step 101/334, epoch 332/501 --> loss:0.8168232917785645
step 151/334, epoch 332/501 --> loss:0.8305079793930054
step 201/334, epoch 332/501 --> loss:0.8200237286090851
step 251/334, epoch 332/501 --> loss:0.8167822563648224
step 301/334, epoch 332/501 --> loss:0.8077475941181183
step 51/334, epoch 333/501 --> loss:0.8231469941139221
step 101/334, epoch 333/501 --> loss:0.8132217979431152
step 151/334, epoch 333/501 --> loss:0.822781423330307
step 201/334, epoch 333/501 --> loss:0.8174726438522338
step 251/334, epoch 333/501 --> loss:0.8255501103401184
step 301/334, epoch 333/501 --> loss:0.8199660682678223
step 51/334, epoch 334/501 --> loss:0.8150791072845459
step 101/334, epoch 334/501 --> loss:0.8341209042072296
step 151/334, epoch 334/501 --> loss:0.8234190595149994
step 201/334, epoch 334/501 --> loss:0.824770210981369
step 251/334, epoch 334/501 --> loss:0.8188727271556854
step 301/334, epoch 334/501 --> loss:0.8131764149665832
step 51/334, epoch 335/501 --> loss:0.8136448967456817
step 101/334, epoch 335/501 --> loss:0.8206216323375702
step 151/334, epoch 335/501 --> loss:0.8333929860591889
step 201/334, epoch 335/501 --> loss:0.8057975018024445
step 251/334, epoch 335/501 --> loss:0.8227578222751617
step 301/334, epoch 335/501 --> loss:0.8178364276885987
step 51/334, epoch 336/501 --> loss:0.8319662380218505
step 101/334, epoch 336/501 --> loss:0.8131468379497528
step 151/334, epoch 336/501 --> loss:0.8305489134788513
step 201/334, epoch 336/501 --> loss:0.8269552195072174
step 251/334, epoch 336/501 --> loss:0.8096267652511596
step 301/334, epoch 336/501 --> loss:0.8150458335876465
step 51/334, epoch 337/501 --> loss:0.8241785275936127
step 101/334, epoch 337/501 --> loss:0.8298192739486694
step 151/334, epoch 337/501 --> loss:0.8217696344852448
step 201/334, epoch 337/501 --> loss:0.8090432322025299
step 251/334, epoch 337/501 --> loss:0.815385353565216
step 301/334, epoch 337/501 --> loss:0.8283925902843475
step 51/334, epoch 338/501 --> loss:0.82573810338974
step 101/334, epoch 338/501 --> loss:0.8214414513111115
step 151/334, epoch 338/501 --> loss:0.823191876411438
step 201/334, epoch 338/501 --> loss:0.8265054750442505
step 251/334, epoch 338/501 --> loss:0.813368890285492
step 301/334, epoch 338/501 --> loss:0.8158267629146576
step 51/334, epoch 339/501 --> loss:0.8270633232593536
step 101/334, epoch 339/501 --> loss:0.8166893625259399
step 151/334, epoch 339/501 --> loss:0.8150316274166107
step 201/334, epoch 339/501 --> loss:0.8234508752822876
step 251/334, epoch 339/501 --> loss:0.81703782081604
step 301/334, epoch 339/501 --> loss:0.8171730697154999
step 51/334, epoch 340/501 --> loss:0.8118338370323182
step 101/334, epoch 340/501 --> loss:0.8184447109699249
step 151/334, epoch 340/501 --> loss:0.8145866203308105
step 201/334, epoch 340/501 --> loss:0.8299686706066132
step 251/334, epoch 340/501 --> loss:0.8300760638713837
step 301/334, epoch 340/501 --> loss:0.8157100176811218
step 51/334, epoch 341/501 --> loss:0.8226684939861297
step 101/334, epoch 341/501 --> loss:0.83105473279953
step 151/334, epoch 341/501 --> loss:0.8368313407897949
step 201/334, epoch 341/501 --> loss:0.821409080028534
step 251/334, epoch 341/501 --> loss:0.8129668164253235
step 301/334, epoch 341/501 --> loss:0.8195097661018371

##########train dataset##########
acc--> [99.38604171060364]
F1--> {'F1': [0.9308424456902008], 'precision': [0.8865164137730217], 'recall': [0.9798454517457243]}
##########eval dataset##########
acc--> [98.7458697141246]
F1--> {'F1': [0.8589239889028624], 'precision': [0.8358199664280264], 'recall': [0.8833521898399663]}
step 51/334, epoch 342/501 --> loss:0.8188654887676239
step 101/334, epoch 342/501 --> loss:0.8125532782077789
step 151/334, epoch 342/501 --> loss:0.8249422180652618
step 201/334, epoch 342/501 --> loss:0.8167481911182404
step 251/334, epoch 342/501 --> loss:0.8239726543426513
step 301/334, epoch 342/501 --> loss:0.8280323684215546
step 51/334, epoch 343/501 --> loss:0.8202768397331238
step 101/334, epoch 343/501 --> loss:0.8097923457622528
step 151/334, epoch 343/501 --> loss:0.8235327672958374
step 201/334, epoch 343/501 --> loss:0.822646268606186
step 251/334, epoch 343/501 --> loss:0.8171675050258637
step 301/334, epoch 343/501 --> loss:0.8368385624885559
step 51/334, epoch 344/501 --> loss:0.8022964823246003
step 101/334, epoch 344/501 --> loss:0.8403251278400421
step 151/334, epoch 344/501 --> loss:0.8147247326374054
step 201/334, epoch 344/501 --> loss:0.8232359433174133
step 251/334, epoch 344/501 --> loss:0.8206742262840271
step 301/334, epoch 344/501 --> loss:0.8305987524986267
step 51/334, epoch 345/501 --> loss:0.8207437479496003
step 101/334, epoch 345/501 --> loss:0.8221499383449554
step 151/334, epoch 345/501 --> loss:0.8309788107872009
step 201/334, epoch 345/501 --> loss:0.8136678469181061
step 251/334, epoch 345/501 --> loss:0.8131065344810486
step 301/334, epoch 345/501 --> loss:0.8187445425987243
step 51/334, epoch 346/501 --> loss:0.8342396104335785
step 101/334, epoch 346/501 --> loss:0.8444845879077911
step 151/334, epoch 346/501 --> loss:0.8077063643932343
step 201/334, epoch 346/501 --> loss:0.8295328855514527
step 251/334, epoch 346/501 --> loss:0.8035243308544159
step 301/334, epoch 346/501 --> loss:0.818977187871933
step 51/334, epoch 347/501 --> loss:0.816078085899353
step 101/334, epoch 347/501 --> loss:0.8071136164665222
step 151/334, epoch 347/501 --> loss:0.831126583814621
step 201/334, epoch 347/501 --> loss:0.8066454792022705
step 251/334, epoch 347/501 --> loss:0.8276891756057739
step 301/334, epoch 347/501 --> loss:0.8213930201530456
step 51/334, epoch 348/501 --> loss:0.8174609661102294
step 101/334, epoch 348/501 --> loss:0.8116236901283265
step 151/334, epoch 348/501 --> loss:0.8146935307979584
step 201/334, epoch 348/501 --> loss:0.8154286193847656
step 251/334, epoch 348/501 --> loss:0.8149067771434784
step 301/334, epoch 348/501 --> loss:0.8251799607276916
step 51/334, epoch 349/501 --> loss:0.8253194165229797
step 101/334, epoch 349/501 --> loss:0.8251465570926666
step 151/334, epoch 349/501 --> loss:0.8363584601879119
step 201/334, epoch 349/501 --> loss:0.815034236907959
step 251/334, epoch 349/501 --> loss:0.8094830334186554
step 301/334, epoch 349/501 --> loss:0.8051955497264862
step 51/334, epoch 350/501 --> loss:0.8291470587253571
step 101/334, epoch 350/501 --> loss:0.8137883901596069
step 151/334, epoch 350/501 --> loss:0.8143138897418976
step 201/334, epoch 350/501 --> loss:0.8154548406600952
step 251/334, epoch 350/501 --> loss:0.8290294587612153
step 301/334, epoch 350/501 --> loss:0.8210713064670563
step 51/334, epoch 351/501 --> loss:0.8300223922729493
step 101/334, epoch 351/501 --> loss:0.8180631411075592
step 151/334, epoch 351/501 --> loss:0.8326642513275146
step 201/334, epoch 351/501 --> loss:0.8262831103801728
step 251/334, epoch 351/501 --> loss:0.8182087063789367
step 301/334, epoch 351/501 --> loss:0.8117109358310699

##########train dataset##########
acc--> [99.5386965407982]
F1--> {'F1': [0.947672839047683], 'precision': [0.908298009528761], 'recall': [0.9906270771946999]}
##########eval dataset##########
acc--> [98.87264782361748]
F1--> {'F1': [0.8724391978158449], 'precision': [0.8537200275143162], 'recall': [0.8920081146288944]}
step 51/334, epoch 352/501 --> loss:0.8184782755374909
step 101/334, epoch 352/501 --> loss:0.8210302853584289
step 151/334, epoch 352/501 --> loss:0.8226979184150696
step 201/334, epoch 352/501 --> loss:0.827363600730896
step 251/334, epoch 352/501 --> loss:0.8099676871299744
step 301/334, epoch 352/501 --> loss:0.8229940009117126
step 51/334, epoch 353/501 --> loss:0.8215321016311645
step 101/334, epoch 353/501 --> loss:0.8189970350265503
step 151/334, epoch 353/501 --> loss:0.8269730305671692
step 201/334, epoch 353/501 --> loss:0.8215454387664795
step 251/334, epoch 353/501 --> loss:0.8299668395519256
step 301/334, epoch 353/501 --> loss:0.8058019506931305
step 51/334, epoch 354/501 --> loss:0.8274635303020478
step 101/334, epoch 354/501 --> loss:0.8240071165561677
step 151/334, epoch 354/501 --> loss:0.81332732796669
step 201/334, epoch 354/501 --> loss:0.8149040496349335
step 251/334, epoch 354/501 --> loss:0.8219387137889862
step 301/334, epoch 354/501 --> loss:0.8216703033447266
step 51/334, epoch 355/501 --> loss:0.8028215420246124
step 101/334, epoch 355/501 --> loss:0.8178494715690613
step 151/334, epoch 355/501 --> loss:0.8327515614032746
step 201/334, epoch 355/501 --> loss:0.8212102365493774
step 251/334, epoch 355/501 --> loss:0.8312777781486511
step 301/334, epoch 355/501 --> loss:0.8155557012557983
step 51/334, epoch 356/501 --> loss:0.8310052239894867
step 101/334, epoch 356/501 --> loss:0.8239180684089661
step 151/334, epoch 356/501 --> loss:0.812916842699051
step 201/334, epoch 356/501 --> loss:0.8308765041828156
step 251/334, epoch 356/501 --> loss:0.818272043466568
step 301/334, epoch 356/501 --> loss:0.8151736104488373
step 51/334, epoch 357/501 --> loss:0.8165406608581542
step 101/334, epoch 357/501 --> loss:0.8214726865291595
step 151/334, epoch 357/501 --> loss:0.8226409351825714
step 201/334, epoch 357/501 --> loss:0.8198974299430847
step 251/334, epoch 357/501 --> loss:0.8211687517166137
step 301/334, epoch 357/501 --> loss:0.8155305182933807
step 51/334, epoch 358/501 --> loss:0.8044471204280853
step 101/334, epoch 358/501 --> loss:0.8207158589363098
step 151/334, epoch 358/501 --> loss:0.8142037200927734
step 201/334, epoch 358/501 --> loss:0.8269045329093934
step 251/334, epoch 358/501 --> loss:0.816228996515274
step 301/334, epoch 358/501 --> loss:0.8267192149162292
step 51/334, epoch 359/501 --> loss:0.8480855417251587
step 101/334, epoch 359/501 --> loss:0.8133462178707123
step 151/334, epoch 359/501 --> loss:0.8103551244735718
step 201/334, epoch 359/501 --> loss:0.8022864282131195
step 251/334, epoch 359/501 --> loss:0.8260075712203979
step 301/334, epoch 359/501 --> loss:0.827239773273468
step 51/334, epoch 360/501 --> loss:0.8134025382995606
step 101/334, epoch 360/501 --> loss:0.8256571531295777
step 151/334, epoch 360/501 --> loss:0.8160961318016052
step 201/334, epoch 360/501 --> loss:0.8167990052700043
step 251/334, epoch 360/501 --> loss:0.8239347112178802
step 301/334, epoch 360/501 --> loss:0.8246101677417755
step 51/334, epoch 361/501 --> loss:0.8316577792167663
step 101/334, epoch 361/501 --> loss:0.8209903764724732
step 151/334, epoch 361/501 --> loss:0.8270096778869629
step 201/334, epoch 361/501 --> loss:0.8154514193534851
step 251/334, epoch 361/501 --> loss:0.8168644988536835
step 301/334, epoch 361/501 --> loss:0.8264837503433228

##########train dataset##########
acc--> [99.56805697978264]
F1--> {'F1': [0.9508838633825841], 'precision': [0.9134117041299864], 'recall': [0.9915729504394087]}
##########eval dataset##########
acc--> [98.86388636844538]
F1--> {'F1': [0.87088767348744], 'precision': [0.8557710970341305], 'recall': [0.886558259504334]}
step 51/334, epoch 362/501 --> loss:0.8249909782409668
step 101/334, epoch 362/501 --> loss:0.8164155411720276
step 151/334, epoch 362/501 --> loss:0.8242744219303131
step 201/334, epoch 362/501 --> loss:0.8288300359249114
step 251/334, epoch 362/501 --> loss:0.8177072763442993
step 301/334, epoch 362/501 --> loss:0.8124528670310974
step 51/334, epoch 363/501 --> loss:0.7966535234451294
step 101/334, epoch 363/501 --> loss:0.831543710231781
step 151/334, epoch 363/501 --> loss:0.8142803728580474
step 201/334, epoch 363/501 --> loss:0.828643593788147
step 251/334, epoch 363/501 --> loss:0.8100927007198334
step 301/334, epoch 363/501 --> loss:0.8325524759292603
step 51/334, epoch 364/501 --> loss:0.8266037833690644
step 101/334, epoch 364/501 --> loss:0.8164875936508179
step 151/334, epoch 364/501 --> loss:0.8266800606250763
step 201/334, epoch 364/501 --> loss:0.8155083978176116
step 251/334, epoch 364/501 --> loss:0.820907598733902
step 301/334, epoch 364/501 --> loss:0.8175516402721406
step 51/334, epoch 365/501 --> loss:0.8104071223735809
step 101/334, epoch 365/501 --> loss:0.8105687689781189
step 151/334, epoch 365/501 --> loss:0.8403412687778473
step 201/334, epoch 365/501 --> loss:0.8245600640773774
step 251/334, epoch 365/501 --> loss:0.8205084216594696
step 301/334, epoch 365/501 --> loss:0.8255589556694031
step 51/334, epoch 366/501 --> loss:0.8262402296066285
step 101/334, epoch 366/501 --> loss:0.8100604224205017
step 151/334, epoch 366/501 --> loss:0.8135080850124359
step 201/334, epoch 366/501 --> loss:0.8179821097850799
step 251/334, epoch 366/501 --> loss:0.8203678131103516
step 301/334, epoch 366/501 --> loss:0.830273003578186
step 51/334, epoch 367/501 --> loss:0.8134181833267212
step 101/334, epoch 367/501 --> loss:0.8147270345687866
step 151/334, epoch 367/501 --> loss:0.8137722492218018
step 201/334, epoch 367/501 --> loss:0.8319232141971589
step 251/334, epoch 367/501 --> loss:0.8150006031990051
step 301/334, epoch 367/501 --> loss:0.8273802292346955
step 51/334, epoch 368/501 --> loss:0.8130184125900268
step 101/334, epoch 368/501 --> loss:0.8050667548179626
step 151/334, epoch 368/501 --> loss:0.832378305196762
step 201/334, epoch 368/501 --> loss:0.8115875613689423
step 251/334, epoch 368/501 --> loss:0.8236857008934021
step 301/334, epoch 368/501 --> loss:0.8242910993099213
step 51/334, epoch 369/501 --> loss:0.8281440973281861
step 101/334, epoch 369/501 --> loss:0.8068872809410095
step 151/334, epoch 369/501 --> loss:0.8262364995479584
step 201/334, epoch 369/501 --> loss:0.8164896893501282
step 251/334, epoch 369/501 --> loss:0.8170816087722779
step 301/334, epoch 369/501 --> loss:0.8243551123142242
step 51/334, epoch 370/501 --> loss:0.8374345326423644
step 101/334, epoch 370/501 --> loss:0.8186009490489959
step 151/334, epoch 370/501 --> loss:0.8336084163188935
step 201/334, epoch 370/501 --> loss:0.8221154618263244
step 251/334, epoch 370/501 --> loss:0.8150506353378296
step 301/334, epoch 370/501 --> loss:0.8113630700111389
step 51/334, epoch 371/501 --> loss:0.8129330515861511
step 101/334, epoch 371/501 --> loss:0.8216198098659515
step 151/334, epoch 371/501 --> loss:0.8204505813121795
step 201/334, epoch 371/501 --> loss:0.819558744430542
step 251/334, epoch 371/501 --> loss:0.8228920316696167
step 301/334, epoch 371/501 --> loss:0.8246619915962219

##########train dataset##########
acc--> [99.67075686888951]
F1--> {'F1': [0.9621689776307566], 'precision': [0.9332468943516268], 'recall': [0.9929516705841812]}
##########eval dataset##########
acc--> [99.0125614409827]
F1--> {'F1': [0.8862493551532438], 'precision': [0.8825089802868815], 'recall': [0.8900316560756873]}
save model!
step 51/334, epoch 372/501 --> loss:0.8293977749347686
step 101/334, epoch 372/501 --> loss:0.8165554463863373
step 151/334, epoch 372/501 --> loss:0.8170911538600921
step 201/334, epoch 372/501 --> loss:0.8245391201972961
step 251/334, epoch 372/501 --> loss:0.8118805932998657
step 301/334, epoch 372/501 --> loss:0.824010100364685
step 51/334, epoch 373/501 --> loss:0.8169109141826629
step 101/334, epoch 373/501 --> loss:0.8211898708343506
step 151/334, epoch 373/501 --> loss:0.8329888236522675
step 201/334, epoch 373/501 --> loss:0.828837263584137
step 251/334, epoch 373/501 --> loss:0.808886513710022
step 301/334, epoch 373/501 --> loss:0.8172090590000153
step 51/334, epoch 374/501 --> loss:0.820779994726181
step 101/334, epoch 374/501 --> loss:0.810811128616333
step 151/334, epoch 374/501 --> loss:0.8256014358997344
step 201/334, epoch 374/501 --> loss:0.820759824514389
step 251/334, epoch 374/501 --> loss:0.8219857203960419
step 301/334, epoch 374/501 --> loss:0.8109425449371338
step 51/334, epoch 375/501 --> loss:0.8358497309684754
step 101/334, epoch 375/501 --> loss:0.8205977690219879
step 151/334, epoch 375/501 --> loss:0.8134672546386719
step 201/334, epoch 375/501 --> loss:0.8043279421329498
step 251/334, epoch 375/501 --> loss:0.8144026267528534
step 301/334, epoch 375/501 --> loss:0.82027885556221
step 51/334, epoch 376/501 --> loss:0.8246960532665253
step 101/334, epoch 376/501 --> loss:0.8237039017677307
step 151/334, epoch 376/501 --> loss:0.8164413952827454
step 201/334, epoch 376/501 --> loss:0.8142917227745056
step 251/334, epoch 376/501 --> loss:0.8282485425472259
step 301/334, epoch 376/501 --> loss:0.8217701900005341
step 51/334, epoch 377/501 --> loss:0.8123655307292938
step 101/334, epoch 377/501 --> loss:0.8174050128459931
step 151/334, epoch 377/501 --> loss:0.8272547447681426
step 201/334, epoch 377/501 --> loss:0.8238450908660888
step 251/334, epoch 377/501 --> loss:0.8143262219429016
step 301/334, epoch 377/501 --> loss:0.8252187943458558
step 51/334, epoch 378/501 --> loss:0.8126848185062409
step 101/334, epoch 378/501 --> loss:0.8209187805652618
step 151/334, epoch 378/501 --> loss:0.8084626770019532
step 201/334, epoch 378/501 --> loss:0.8206223058700561
step 251/334, epoch 378/501 --> loss:0.8272233521938324
step 301/334, epoch 378/501 --> loss:0.8284672498703003
step 51/334, epoch 379/501 --> loss:0.8295888459682464
step 101/334, epoch 379/501 --> loss:0.826832240819931
step 151/334, epoch 379/501 --> loss:0.8247805809974671
step 201/334, epoch 379/501 --> loss:0.8074197697639466
step 251/334, epoch 379/501 --> loss:0.8142589926719666
step 301/334, epoch 379/501 --> loss:0.8094251275062561
step 51/334, epoch 380/501 --> loss:0.8234586656093598
step 101/334, epoch 380/501 --> loss:0.8193557262420654
step 151/334, epoch 380/501 --> loss:0.8092751145362854
step 201/334, epoch 380/501 --> loss:0.8240501534938812
step 251/334, epoch 380/501 --> loss:0.8271922290325164
step 301/334, epoch 380/501 --> loss:0.813898252248764
step 51/334, epoch 381/501 --> loss:0.83167977809906
step 101/334, epoch 381/501 --> loss:0.8129946362972259
step 151/334, epoch 381/501 --> loss:0.8227330029010773
step 201/334, epoch 381/501 --> loss:0.8295231962203979
step 251/334, epoch 381/501 --> loss:0.8144636833667755
step 301/334, epoch 381/501 --> loss:0.8091729319095612

##########train dataset##########
acc--> [99.6396437002353]
F1--> {'F1': [0.9587533624592867], 'precision': [0.9265912771756155], 'recall': [0.9932391498448723]}
##########eval dataset##########
acc--> [98.9428102187844]
F1--> {'F1': [0.8797082145548165], 'precision': [0.8654698790198657], 'recall': [0.894433206726176]}
step 51/334, epoch 382/501 --> loss:0.8156618297100067
step 101/334, epoch 382/501 --> loss:0.8246554982662201
step 151/334, epoch 382/501 --> loss:0.8327025210857392
step 201/334, epoch 382/501 --> loss:0.8030067026615143
step 251/334, epoch 382/501 --> loss:0.8295475578308106
step 301/334, epoch 382/501 --> loss:0.8134996569156647
step 51/334, epoch 383/501 --> loss:0.8102870047092438
step 101/334, epoch 383/501 --> loss:0.8080476951599121
step 151/334, epoch 383/501 --> loss:0.8188912320137024
step 201/334, epoch 383/501 --> loss:0.83722607254982
step 251/334, epoch 383/501 --> loss:0.8297198820114136
step 301/334, epoch 383/501 --> loss:0.8137594306468964
step 51/334, epoch 384/501 --> loss:0.8236070990562439
step 101/334, epoch 384/501 --> loss:0.8154775094985962
step 151/334, epoch 384/501 --> loss:0.8376182293891907
step 201/334, epoch 384/501 --> loss:0.8116616797447205
step 251/334, epoch 384/501 --> loss:0.8245595014095306
step 301/334, epoch 384/501 --> loss:0.8092750930786132
step 51/334, epoch 385/501 --> loss:0.8183967685699463
step 101/334, epoch 385/501 --> loss:0.8243236660957336
step 151/334, epoch 385/501 --> loss:0.8217737352848054
step 201/334, epoch 385/501 --> loss:0.8263212478160858
step 251/334, epoch 385/501 --> loss:0.8173227286338807
step 301/334, epoch 385/501 --> loss:0.8183950698375702
step 51/334, epoch 386/501 --> loss:0.8198445975780487
step 101/334, epoch 386/501 --> loss:0.8262654423713685
step 151/334, epoch 386/501 --> loss:0.8174744486808777
step 201/334, epoch 386/501 --> loss:0.8323989284038543
step 251/334, epoch 386/501 --> loss:0.8114474773406982
step 301/334, epoch 386/501 --> loss:0.8162976086139679
step 51/334, epoch 387/501 --> loss:0.8114197635650635
step 101/334, epoch 387/501 --> loss:0.8287139904499053
step 151/334, epoch 387/501 --> loss:0.8175863587856292
step 201/334, epoch 387/501 --> loss:0.826340276002884
step 251/334, epoch 387/501 --> loss:0.8146834862232208
step 301/334, epoch 387/501 --> loss:0.8256918144226074
step 51/334, epoch 388/501 --> loss:0.8350573098659515
step 101/334, epoch 388/501 --> loss:0.8317802524566651
step 151/334, epoch 388/501 --> loss:0.8087340784072876
step 201/334, epoch 388/501 --> loss:0.800926798582077
step 251/334, epoch 388/501 --> loss:0.8064526510238648
step 301/334, epoch 388/501 --> loss:0.8327261233329772
step 51/334, epoch 389/501 --> loss:0.8240318274497986
step 101/334, epoch 389/501 --> loss:0.8261712229251862
step 151/334, epoch 389/501 --> loss:0.8214320302009582
step 201/334, epoch 389/501 --> loss:0.8191698741912842
step 251/334, epoch 389/501 --> loss:0.8089832091331481
step 301/334, epoch 389/501 --> loss:0.8145117592811585
step 51/334, epoch 390/501 --> loss:0.8179344832897186
step 101/334, epoch 390/501 --> loss:0.8133415651321411
step 151/334, epoch 390/501 --> loss:0.8196400141716004
step 201/334, epoch 390/501 --> loss:0.81855548620224
step 251/334, epoch 390/501 --> loss:0.826074515581131
step 301/334, epoch 390/501 --> loss:0.8165893650054932
step 51/334, epoch 391/501 --> loss:0.822625287771225
step 101/334, epoch 391/501 --> loss:0.8105916953086854
step 151/334, epoch 391/501 --> loss:0.8208256483078002
step 201/334, epoch 391/501 --> loss:0.8273721909523011
step 251/334, epoch 391/501 --> loss:0.8147827827930451
step 301/334, epoch 391/501 --> loss:0.8203366756439209

##########train dataset##########
acc--> [99.58399603506918]
F1--> {'F1': [0.9525141357948123], 'precision': [0.9182318807365433], 'recall': [0.9894663067584221]}
##########eval dataset##########
acc--> [98.89016873149595]
F1--> {'F1': [0.8722203812359565], 'precision': [0.8680695952500789], 'recall': [0.8764211490626627]}
step 51/334, epoch 392/501 --> loss:0.8086660921573638
step 101/334, epoch 392/501 --> loss:0.815875734090805
step 151/334, epoch 392/501 --> loss:0.8304285395145417
step 201/334, epoch 392/501 --> loss:0.8206797802448272
step 251/334, epoch 392/501 --> loss:0.8330916559696198
step 301/334, epoch 392/501 --> loss:0.8100342273712158
step 51/334, epoch 393/501 --> loss:0.8330750131607055
step 101/334, epoch 393/501 --> loss:0.8008739125728607
step 151/334, epoch 393/501 --> loss:0.8229322624206543
step 201/334, epoch 393/501 --> loss:0.8282649385929107
step 251/334, epoch 393/501 --> loss:0.8286427772045135
step 301/334, epoch 393/501 --> loss:0.8266495084762573
step 51/334, epoch 394/501 --> loss:0.8154486918449402
step 101/334, epoch 394/501 --> loss:0.8373059678077698
step 151/334, epoch 394/501 --> loss:0.8124983787536622
step 201/334, epoch 394/501 --> loss:0.8240462625026703
step 251/334, epoch 394/501 --> loss:0.8161206984519959
step 301/334, epoch 394/501 --> loss:0.8266436922550201
step 51/334, epoch 395/501 --> loss:0.8173737311363221
step 101/334, epoch 395/501 --> loss:0.8150583982467652
step 151/334, epoch 395/501 --> loss:0.799306138753891
step 201/334, epoch 395/501 --> loss:0.8165519952774047
step 251/334, epoch 395/501 --> loss:0.8293119370937347
step 301/334, epoch 395/501 --> loss:0.8257311201095581
step 51/334, epoch 396/501 --> loss:0.8088063764572143
step 101/334, epoch 396/501 --> loss:0.8222376465797424
step 151/334, epoch 396/501 --> loss:0.8177062976360321
step 201/334, epoch 396/501 --> loss:0.8198694741725921
step 251/334, epoch 396/501 --> loss:0.8129995000362397
step 301/334, epoch 396/501 --> loss:0.8281848132610321
step 51/334, epoch 397/501 --> loss:0.8309728515148163
step 101/334, epoch 397/501 --> loss:0.8148013639450074
step 151/334, epoch 397/501 --> loss:0.8265741443634034
step 201/334, epoch 397/501 --> loss:0.807063022851944
step 251/334, epoch 397/501 --> loss:0.8132388138771057
step 301/334, epoch 397/501 --> loss:0.8324425005912781
step 51/334, epoch 398/501 --> loss:0.8398670756816864
step 101/334, epoch 398/501 --> loss:0.8328660559654236
step 151/334, epoch 398/501 --> loss:0.8166516268253327
step 201/334, epoch 398/501 --> loss:0.8065813946723938
step 251/334, epoch 398/501 --> loss:0.7975935113430023
step 301/334, epoch 398/501 --> loss:0.8152505803108215
step 51/334, epoch 399/501 --> loss:0.8236993980407715
step 101/334, epoch 399/501 --> loss:0.8319976425170899
step 151/334, epoch 399/501 --> loss:0.802785930633545
step 201/334, epoch 399/501 --> loss:0.8101557874679566
step 251/334, epoch 399/501 --> loss:0.8241469836235047
step 301/334, epoch 399/501 --> loss:0.8302025282382965
step 51/334, epoch 400/501 --> loss:0.8158295667171478
step 101/334, epoch 400/501 --> loss:0.8234957432746888
step 151/334, epoch 400/501 --> loss:0.8214397931098938
step 201/334, epoch 400/501 --> loss:0.815345470905304
step 251/334, epoch 400/501 --> loss:0.8177845096588134
step 301/334, epoch 400/501 --> loss:0.8243213212490081
step 51/334, epoch 401/501 --> loss:0.8072938370704651
step 101/334, epoch 401/501 --> loss:0.8133038699626922
step 151/334, epoch 401/501 --> loss:0.8177501952648163
step 201/334, epoch 401/501 --> loss:0.8328048872947693
step 251/334, epoch 401/501 --> loss:0.8284784233570099
step 301/334, epoch 401/501 --> loss:0.8064525902271271

##########train dataset##########
acc--> [99.6746064662726]
F1--> {'F1': [0.9626163710327443], 'precision': [0.9335606316228213], 'recall': [0.9935494890656513]}
##########eval dataset##########
acc--> [99.01645156445846]
F1--> {'F1': [0.8867885865586086], 'precision': [0.8823423201933893], 'recall': [0.8912899920884652]}
save model!
step 51/334, epoch 402/501 --> loss:0.8205259478092194
step 101/334, epoch 402/501 --> loss:0.823449342250824
step 151/334, epoch 402/501 --> loss:0.8209651923179626
step 201/334, epoch 402/501 --> loss:0.8225665140151978
step 251/334, epoch 402/501 --> loss:0.8168521308898926
step 301/334, epoch 402/501 --> loss:0.8110878682136535
step 51/334, epoch 403/501 --> loss:0.8242623245716095
step 101/334, epoch 403/501 --> loss:0.818204003572464
step 151/334, epoch 403/501 --> loss:0.819270737171173
step 201/334, epoch 403/501 --> loss:0.8204131853580475
step 251/334, epoch 403/501 --> loss:0.8220625460147858
step 301/334, epoch 403/501 --> loss:0.8159869253635407
step 51/334, epoch 404/501 --> loss:0.8260714519023895
step 101/334, epoch 404/501 --> loss:0.8389993619918823
step 151/334, epoch 404/501 --> loss:0.7986379992961884
step 201/334, epoch 404/501 --> loss:0.8377310264110566
step 251/334, epoch 404/501 --> loss:0.8201857912540436
step 301/334, epoch 404/501 --> loss:0.8102607333660126
step 51/334, epoch 405/501 --> loss:0.8214477324485778
step 101/334, epoch 405/501 --> loss:0.8349944972991943
step 151/334, epoch 405/501 --> loss:0.8138387501239777
step 201/334, epoch 405/501 --> loss:0.8236732268333435
step 251/334, epoch 405/501 --> loss:0.8103615462779998
step 301/334, epoch 405/501 --> loss:0.8218479168415069
step 51/334, epoch 406/501 --> loss:0.8258561873435974
step 101/334, epoch 406/501 --> loss:0.822684588432312
step 151/334, epoch 406/501 --> loss:0.8092382848262787
step 201/334, epoch 406/501 --> loss:0.8143492615222931
step 251/334, epoch 406/501 --> loss:0.8180338943004608
step 301/334, epoch 406/501 --> loss:0.8355119121074677
step 51/334, epoch 407/501 --> loss:0.809544335603714
step 101/334, epoch 407/501 --> loss:0.8202212262153625
step 151/334, epoch 407/501 --> loss:0.8251347517967225
step 201/334, epoch 407/501 --> loss:0.8162518465518951
step 251/334, epoch 407/501 --> loss:0.839214117527008
step 301/334, epoch 407/501 --> loss:0.809932062625885
step 51/334, epoch 408/501 --> loss:0.8231849241256713
step 101/334, epoch 408/501 --> loss:0.8281086444854736
step 151/334, epoch 408/501 --> loss:0.8197886860370636
step 201/334, epoch 408/501 --> loss:0.8245450496673584
step 251/334, epoch 408/501 --> loss:0.8147424829006195
step 301/334, epoch 408/501 --> loss:0.8193032228946686
step 51/334, epoch 409/501 --> loss:0.8284563291072845
step 101/334, epoch 409/501 --> loss:0.8119943141937256
step 151/334, epoch 409/501 --> loss:0.817594051361084
step 201/334, epoch 409/501 --> loss:0.8149113142490387
step 251/334, epoch 409/501 --> loss:0.8168711149692536
step 301/334, epoch 409/501 --> loss:0.8210304343700409
step 51/334, epoch 410/501 --> loss:0.8266012620925903
step 101/334, epoch 410/501 --> loss:0.8116817820072174
step 151/334, epoch 410/501 --> loss:0.8190791523456573
step 201/334, epoch 410/501 --> loss:0.8257735061645508
step 251/334, epoch 410/501 --> loss:0.8212022042274475
step 301/334, epoch 410/501 --> loss:0.8222814059257507
step 51/334, epoch 411/501 --> loss:0.8308386290073395
step 101/334, epoch 411/501 --> loss:0.8187493455410003
step 151/334, epoch 411/501 --> loss:0.8076782143115997
step 201/334, epoch 411/501 --> loss:0.8153014719486237
step 251/334, epoch 411/501 --> loss:0.8260509300231934
step 301/334, epoch 411/501 --> loss:0.818107717037201

##########train dataset##########
acc--> [99.68939353161623]
F1--> {'F1': [0.964255667471155], 'precision': [0.936613693623344], 'recall': [0.9935894431205232]}
##########eval dataset##########
acc--> [98.99702364169148]
F1--> {'F1': [0.8845859487800358], 'precision': [0.8798978465184606], 'recall': [0.8893343822541738]}
step 51/334, epoch 412/501 --> loss:0.8300537025928497
step 101/334, epoch 412/501 --> loss:0.8154634535312653
step 151/334, epoch 412/501 --> loss:0.8203188705444336
step 201/334, epoch 412/501 --> loss:0.8113115119934082
step 251/334, epoch 412/501 --> loss:0.8238440358638763
step 301/334, epoch 412/501 --> loss:0.8295890772342682
step 51/334, epoch 413/501 --> loss:0.826114478111267
step 101/334, epoch 413/501 --> loss:0.8160421478748322
step 151/334, epoch 413/501 --> loss:0.8222128808498382
step 201/334, epoch 413/501 --> loss:0.8130118405818939
step 251/334, epoch 413/501 --> loss:0.8126937806606293
step 301/334, epoch 413/501 --> loss:0.8246316361427307
step 51/334, epoch 414/501 --> loss:0.808022038936615
step 101/334, epoch 414/501 --> loss:0.824046368598938
step 151/334, epoch 414/501 --> loss:0.8157370233535767
step 201/334, epoch 414/501 --> loss:0.8241887629032135
step 251/334, epoch 414/501 --> loss:0.8232150244712829
step 301/334, epoch 414/501 --> loss:0.8163069152832031
step 51/334, epoch 415/501 --> loss:0.8236420834064484
step 101/334, epoch 415/501 --> loss:0.8173412370681763
step 151/334, epoch 415/501 --> loss:0.8283217716217041
step 201/334, epoch 415/501 --> loss:0.8196973025798797
step 251/334, epoch 415/501 --> loss:0.8131160926818848
step 301/334, epoch 415/501 --> loss:0.824941691160202
step 51/334, epoch 416/501 --> loss:0.8239582276344299
step 101/334, epoch 416/501 --> loss:0.8188268423080445
step 151/334, epoch 416/501 --> loss:0.8200918877124787
step 201/334, epoch 416/501 --> loss:0.8138590276241302
step 251/334, epoch 416/501 --> loss:0.8267136061191559
step 301/334, epoch 416/501 --> loss:0.8206992793083191
step 51/334, epoch 417/501 --> loss:0.8331579005718232
step 101/334, epoch 417/501 --> loss:0.8093033719062805
step 151/334, epoch 417/501 --> loss:0.822522087097168
step 201/334, epoch 417/501 --> loss:0.8291620934009551
step 251/334, epoch 417/501 --> loss:0.8132780325412751
step 301/334, epoch 417/501 --> loss:0.8138039743900299
step 51/334, epoch 418/501 --> loss:0.8103746426105499
step 101/334, epoch 418/501 --> loss:0.8231937289237976
step 151/334, epoch 418/501 --> loss:0.8335150921344757
step 201/334, epoch 418/501 --> loss:0.8171213304996491
step 251/334, epoch 418/501 --> loss:0.8233115959167481
step 301/334, epoch 418/501 --> loss:0.8215022695064544
step 51/334, epoch 419/501 --> loss:0.8191633093357086
step 101/334, epoch 419/501 --> loss:0.8245277690887451
step 151/334, epoch 419/501 --> loss:0.7979808211326599
step 201/334, epoch 419/501 --> loss:0.8162266004085541
step 251/334, epoch 419/501 --> loss:0.8308061003684998
step 301/334, epoch 419/501 --> loss:0.8247565376758575
step 51/334, epoch 420/501 --> loss:0.8230513608455658
step 101/334, epoch 420/501 --> loss:0.8104996085166931
step 151/334, epoch 420/501 --> loss:0.8188375818729401
step 201/334, epoch 420/501 --> loss:0.8267108058929443
step 251/334, epoch 420/501 --> loss:0.8242085492610931
step 301/334, epoch 420/501 --> loss:0.8213666069507599
step 51/334, epoch 421/501 --> loss:0.817510312795639
step 101/334, epoch 421/501 --> loss:0.8400562572479248
step 151/334, epoch 421/501 --> loss:0.8164870762825012
step 201/334, epoch 421/501 --> loss:0.8224244880676269
step 251/334, epoch 421/501 --> loss:0.8163391315937042
step 301/334, epoch 421/501 --> loss:0.8212062430381775

##########train dataset##########
acc--> [99.69056182735173]
F1--> {'F1': [0.9643631077137989], 'precision': [0.9373878172210303], 'recall': [0.9929475327279041]}
##########eval dataset##########
acc--> [98.90930896668262]
F1--> {'F1': [0.8770279212694548], 'precision': [0.8552874108135462], 'recall': [0.899913022231993]}
step 51/334, epoch 422/501 --> loss:0.8179355764389038
step 101/334, epoch 422/501 --> loss:0.8133044290542603
step 151/334, epoch 422/501 --> loss:0.8033048319816589
step 201/334, epoch 422/501 --> loss:0.8239670753479004
step 251/334, epoch 422/501 --> loss:0.8387700426578522
step 301/334, epoch 422/501 --> loss:0.8175722181797027
step 51/334, epoch 423/501 --> loss:0.8334510338306427
step 101/334, epoch 423/501 --> loss:0.8162940669059754
step 151/334, epoch 423/501 --> loss:0.8093879532814026
step 201/334, epoch 423/501 --> loss:0.8230596554279327
step 251/334, epoch 423/501 --> loss:0.8166005301475525
step 301/334, epoch 423/501 --> loss:0.827217767238617
step 51/334, epoch 424/501 --> loss:0.817658017873764
step 101/334, epoch 424/501 --> loss:0.8165449166297912
step 151/334, epoch 424/501 --> loss:0.8281250512599945
step 201/334, epoch 424/501 --> loss:0.8287462222576142
step 251/334, epoch 424/501 --> loss:0.8290685260295868
step 301/334, epoch 424/501 --> loss:0.7994577980041504
step 51/334, epoch 425/501 --> loss:0.8258564460277558
step 101/334, epoch 425/501 --> loss:0.8206978368759156
step 151/334, epoch 425/501 --> loss:0.8382227516174316
step 201/334, epoch 425/501 --> loss:0.8270505642890931
step 251/334, epoch 425/501 --> loss:0.81273805975914
step 301/334, epoch 425/501 --> loss:0.8083564305305481
step 51/334, epoch 426/501 --> loss:0.8191527843475341
step 101/334, epoch 426/501 --> loss:0.8203870868682861
step 151/334, epoch 426/501 --> loss:0.8066447794437408
step 201/334, epoch 426/501 --> loss:0.8291651904582977
step 251/334, epoch 426/501 --> loss:0.8403742969036102
step 301/334, epoch 426/501 --> loss:0.8133792781829834
step 51/334, epoch 427/501 --> loss:0.8239656090736389
step 101/334, epoch 427/501 --> loss:0.8191939401626587
step 151/334, epoch 427/501 --> loss:0.822429188489914
step 201/334, epoch 427/501 --> loss:0.815810626745224
step 251/334, epoch 427/501 --> loss:0.8264100408554077
step 301/334, epoch 427/501 --> loss:0.8089692854881286
step 51/334, epoch 428/501 --> loss:0.8064304339885712
step 101/334, epoch 428/501 --> loss:0.8241665458679199
step 151/334, epoch 428/501 --> loss:0.8280390214920044
step 201/334, epoch 428/501 --> loss:0.8136077308654786
step 251/334, epoch 428/501 --> loss:0.8238675439357758
step 301/334, epoch 428/501 --> loss:0.8209821391105652
step 51/334, epoch 429/501 --> loss:0.8147584271430969
step 101/334, epoch 429/501 --> loss:0.8282980418205261
step 151/334, epoch 429/501 --> loss:0.8165561425685882
step 201/334, epoch 429/501 --> loss:0.8176489663124085
step 251/334, epoch 429/501 --> loss:0.8145403373241424
step 301/334, epoch 429/501 --> loss:0.8255177485942841
step 51/334, epoch 430/501 --> loss:0.8103425240516663
step 101/334, epoch 430/501 --> loss:0.8107413172721862
step 151/334, epoch 430/501 --> loss:0.8374058890342713
step 201/334, epoch 430/501 --> loss:0.8292499387264252
step 251/334, epoch 430/501 --> loss:0.8160245525836944
step 301/334, epoch 430/501 --> loss:0.8251900517940521
step 51/334, epoch 431/501 --> loss:0.8018227636814117
step 101/334, epoch 431/501 --> loss:0.8219801640510559
step 151/334, epoch 431/501 --> loss:0.8249283611774445
step 201/334, epoch 431/501 --> loss:0.8339464330673217
step 251/334, epoch 431/501 --> loss:0.8160799884796143
step 301/334, epoch 431/501 --> loss:0.8233169388771057

##########train dataset##########
acc--> [99.45198004729104]
F1--> {'F1': [0.9353141011348383], 'precision': [0.9311075586081008], 'recall': [0.9395689154190705]}
##########eval dataset##########
acc--> [98.79607687054205]
F1--> {'F1': [0.8569765099523095], 'precision': [0.8806577072147169], 'recall': [0.8345450299884352]}
step 51/334, epoch 432/501 --> loss:0.8191433036327362
step 101/334, epoch 432/501 --> loss:0.8259776508808137
step 151/334, epoch 432/501 --> loss:0.8091313743591309
step 201/334, epoch 432/501 --> loss:0.8224307906627655
step 251/334, epoch 432/501 --> loss:0.8216825938224792
step 301/334, epoch 432/501 --> loss:0.8277157592773438
step 51/334, epoch 433/501 --> loss:0.8300876104831696
step 101/334, epoch 433/501 --> loss:0.8328999567031861
step 151/334, epoch 433/501 --> loss:0.8252760100364686
step 201/334, epoch 433/501 --> loss:0.8077284049987793
step 251/334, epoch 433/501 --> loss:0.8087060368061065
step 301/334, epoch 433/501 --> loss:0.8203254783153534
step 51/334, epoch 434/501 --> loss:0.8254153037071228
step 101/334, epoch 434/501 --> loss:0.8108600151538848
step 151/334, epoch 434/501 --> loss:0.8258532071113587
step 201/334, epoch 434/501 --> loss:0.8352615129947663
step 251/334, epoch 434/501 --> loss:0.8014342367649079
step 301/334, epoch 434/501 --> loss:0.8176139903068542
step 51/334, epoch 435/501 --> loss:0.8189568150043488
step 101/334, epoch 435/501 --> loss:0.8177142035961151
step 151/334, epoch 435/501 --> loss:0.82162810921669
step 201/334, epoch 435/501 --> loss:0.8269228422641755
step 251/334, epoch 435/501 --> loss:0.8150863075256347
step 301/334, epoch 435/501 --> loss:0.8245049178600311
step 51/334, epoch 436/501 --> loss:0.8200669956207275
step 101/334, epoch 436/501 --> loss:0.8180384814739228
step 151/334, epoch 436/501 --> loss:0.824229691028595
step 201/334, epoch 436/501 --> loss:0.8219403755664826
step 251/334, epoch 436/501 --> loss:0.8170396316051484
step 301/334, epoch 436/501 --> loss:0.8140877866744995
step 51/334, epoch 437/501 --> loss:0.8147075700759888
step 101/334, epoch 437/501 --> loss:0.8179051899909973
step 151/334, epoch 437/501 --> loss:0.8356911301612854
step 201/334, epoch 437/501 --> loss:0.8234676015377045
step 251/334, epoch 437/501 --> loss:0.8292041146755218
step 301/334, epoch 437/501 --> loss:0.803866732120514
step 51/334, epoch 438/501 --> loss:0.8148125517368316
step 101/334, epoch 438/501 --> loss:0.8375237846374511
step 151/334, epoch 438/501 --> loss:0.8338937652111054
step 201/334, epoch 438/501 --> loss:0.8169250416755677
step 251/334, epoch 438/501 --> loss:0.8094257307052612
step 301/334, epoch 438/501 --> loss:0.8073399496078492
step 51/334, epoch 439/501 --> loss:0.8207166028022767
step 101/334, epoch 439/501 --> loss:0.8268713462352753
step 151/334, epoch 439/501 --> loss:0.8171972227096558
step 201/334, epoch 439/501 --> loss:0.8072901558876038
step 251/334, epoch 439/501 --> loss:0.8269727730751038
step 301/334, epoch 439/501 --> loss:0.827309296131134
step 51/334, epoch 440/501 --> loss:0.8069733595848083
step 101/334, epoch 440/501 --> loss:0.8224594283103943
step 151/334, epoch 440/501 --> loss:0.8235525369644165
step 201/334, epoch 440/501 --> loss:0.817301617860794
step 251/334, epoch 440/501 --> loss:0.8159987246990204
step 301/334, epoch 440/501 --> loss:0.8247591924667358
step 51/334, epoch 441/501 --> loss:0.8302276432514191
step 101/334, epoch 441/501 --> loss:0.8087556040287018
step 151/334, epoch 441/501 --> loss:0.8165954220294952
step 201/334, epoch 441/501 --> loss:0.8231526982784271
step 251/334, epoch 441/501 --> loss:0.8322479462623597
step 301/334, epoch 441/501 --> loss:0.8177980136871338

##########train dataset##########
acc--> [99.73608588385395]
F1--> {'F1': [0.9694587582970104], 'precision': [0.9466477880969303], 'recall': [0.9934066991072382]}
##########eval dataset##########
acc--> [99.02233814629321]
F1--> {'F1': [0.886311655829855], 'precision': [0.8909162714863887], 'recall': [0.8817642896333656]}
step 51/334, epoch 442/501 --> loss:0.8107470083236694
step 101/334, epoch 442/501 --> loss:0.8139729619026184
step 151/334, epoch 442/501 --> loss:0.8344915962219238
step 201/334, epoch 442/501 --> loss:0.81619699716568
step 251/334, epoch 442/501 --> loss:0.836285057067871
step 301/334, epoch 442/501 --> loss:0.8139299499988556
step 51/334, epoch 443/501 --> loss:0.8214549887180328
step 101/334, epoch 443/501 --> loss:0.8320387876033783
step 151/334, epoch 443/501 --> loss:0.8258283770084381
step 201/334, epoch 443/501 --> loss:0.8042177367210388
step 251/334, epoch 443/501 --> loss:0.8171803689002991
step 301/334, epoch 443/501 --> loss:0.8214395081996918
step 51/334, epoch 444/501 --> loss:0.8134268605709076
step 101/334, epoch 444/501 --> loss:0.8088968074321747
step 151/334, epoch 444/501 --> loss:0.8168819272518157
step 201/334, epoch 444/501 --> loss:0.8228448581695557
step 251/334, epoch 444/501 --> loss:0.8269460952281952
step 301/334, epoch 444/501 --> loss:0.8252206087112427
step 51/334, epoch 445/501 --> loss:0.824039307832718
step 101/334, epoch 445/501 --> loss:0.8220383751392365
step 151/334, epoch 445/501 --> loss:0.8263807463645935
step 201/334, epoch 445/501 --> loss:0.8013646173477172
step 251/334, epoch 445/501 --> loss:0.8271173095703125
step 301/334, epoch 445/501 --> loss:0.8198307514190674
step 51/334, epoch 446/501 --> loss:0.8334441900253295
step 101/334, epoch 446/501 --> loss:0.8146968531608582
step 151/334, epoch 446/501 --> loss:0.8264965319633484
step 201/334, epoch 446/501 --> loss:0.8196703398227692
step 251/334, epoch 446/501 --> loss:0.8243225038051605
step 301/334, epoch 446/501 --> loss:0.8086873245239258
step 51/334, epoch 447/501 --> loss:0.8292972159385681
step 101/334, epoch 447/501 --> loss:0.8085780477523804
step 151/334, epoch 447/501 --> loss:0.8245838141441345
step 201/334, epoch 447/501 --> loss:0.8061547160148621
step 251/334, epoch 447/501 --> loss:0.8160673940181732
step 301/334, epoch 447/501 --> loss:0.8156602764129639
step 51/334, epoch 448/501 --> loss:0.8160995209217071
step 101/334, epoch 448/501 --> loss:0.8195687985420227
step 151/334, epoch 448/501 --> loss:0.8269397866725922
step 201/334, epoch 448/501 --> loss:0.8040660119056702
step 251/334, epoch 448/501 --> loss:0.838550683259964
step 301/334, epoch 448/501 --> loss:0.8123241937160492
step 51/334, epoch 449/501 --> loss:0.8295562636852264
step 101/334, epoch 449/501 --> loss:0.807000880241394
step 151/334, epoch 449/501 --> loss:0.821178594827652
step 201/334, epoch 449/501 --> loss:0.8221940886974335
step 251/334, epoch 449/501 --> loss:0.8066587781906128
step 301/334, epoch 449/501 --> loss:0.8297269928455353
step 51/334, epoch 450/501 --> loss:0.8104387080669403
step 101/334, epoch 450/501 --> loss:0.8220903706550599
step 151/334, epoch 450/501 --> loss:0.8263736367225647
step 201/334, epoch 450/501 --> loss:0.8153042101860046
step 251/334, epoch 450/501 --> loss:0.8278677427768707
step 301/334, epoch 450/501 --> loss:0.812974535226822
step 51/334, epoch 451/501 --> loss:0.8323976087570191
step 101/334, epoch 451/501 --> loss:0.8260301768779754
step 151/334, epoch 451/501 --> loss:0.8205739486217499
step 201/334, epoch 451/501 --> loss:0.8029743731021881
step 251/334, epoch 451/501 --> loss:0.8223172080516815
step 301/334, epoch 451/501 --> loss:0.8226258397102356

##########train dataset##########
acc--> [99.68518978386084]
F1--> {'F1': [0.9637965580460702], 'precision': [0.9355691400604421], 'recall': [0.993790909237619]}
##########eval dataset##########
acc--> [98.96441682429416]
F1--> {'F1': [0.8806819830674989], 'precision': [0.8771216441466854], 'recall': [0.8842814249637861]}
step 51/334, epoch 452/501 --> loss:0.8368960595130921
step 101/334, epoch 452/501 --> loss:0.8088566422462463
step 151/334, epoch 452/501 --> loss:0.8147026765346527
step 201/334, epoch 452/501 --> loss:0.8288737499713897
step 251/334, epoch 452/501 --> loss:0.8012647998332977
step 301/334, epoch 452/501 --> loss:0.8334293293952942
step 51/334, epoch 453/501 --> loss:0.8256092119216919
step 101/334, epoch 453/501 --> loss:0.8148734951019287
step 151/334, epoch 453/501 --> loss:0.8241288614273071
step 201/334, epoch 453/501 --> loss:0.815070025920868
step 251/334, epoch 453/501 --> loss:0.8240140402317047
step 301/334, epoch 453/501 --> loss:0.8293720674514771
step 51/334, epoch 454/501 --> loss:0.8206219172477722
step 101/334, epoch 454/501 --> loss:0.8267168486118317
step 151/334, epoch 454/501 --> loss:0.8155354404449463
step 201/334, epoch 454/501 --> loss:0.8197655606269837
step 251/334, epoch 454/501 --> loss:0.8283836007118225
step 301/334, epoch 454/501 --> loss:0.8116847169399262
step 51/334, epoch 455/501 --> loss:0.8230535387992859
step 101/334, epoch 455/501 --> loss:0.8103036653995513
step 151/334, epoch 455/501 --> loss:0.845578681230545
step 201/334, epoch 455/501 --> loss:0.8027112257480621
step 251/334, epoch 455/501 --> loss:0.8231919932365418
step 301/334, epoch 455/501 --> loss:0.8191118383407593
step 51/334, epoch 456/501 --> loss:0.8190736150741578
step 101/334, epoch 456/501 --> loss:0.8250239431858063
step 151/334, epoch 456/501 --> loss:0.8146103298664094
step 201/334, epoch 456/501 --> loss:0.8138645887374878
step 251/334, epoch 456/501 --> loss:0.8219193053245545
step 301/334, epoch 456/501 --> loss:0.8263328087329864
step 51/334, epoch 457/501 --> loss:0.8193629086017609
step 101/334, epoch 457/501 --> loss:0.8114300465583801
step 151/334, epoch 457/501 --> loss:0.8177025079727173
step 201/334, epoch 457/501 --> loss:0.8213478755950928
step 251/334, epoch 457/501 --> loss:0.8236748969554901
step 301/334, epoch 457/501 --> loss:0.8156301581859589
step 51/334, epoch 458/501 --> loss:0.8055515670776368
step 101/334, epoch 458/501 --> loss:0.8176059556007386
step 151/334, epoch 458/501 --> loss:0.8224302279949188
step 201/334, epoch 458/501 --> loss:0.8242680776119232
step 251/334, epoch 458/501 --> loss:0.8276957201957703
step 301/334, epoch 458/501 --> loss:0.8211305749416351
step 51/334, epoch 459/501 --> loss:0.8330712342262268
step 101/334, epoch 459/501 --> loss:0.8205945956707
step 151/334, epoch 459/501 --> loss:0.8233376765251159
step 201/334, epoch 459/501 --> loss:0.8162750184535981
step 251/334, epoch 459/501 --> loss:0.8247522282600402
step 301/334, epoch 459/501 --> loss:0.8105709683895111
step 51/334, epoch 460/501 --> loss:0.8235253345966339
step 101/334, epoch 460/501 --> loss:0.8176140105724334
step 151/334, epoch 460/501 --> loss:0.811019446849823
step 201/334, epoch 460/501 --> loss:0.8268194222450256
step 251/334, epoch 460/501 --> loss:0.8235814797878266
step 301/334, epoch 460/501 --> loss:0.8154046261310577
step 51/334, epoch 461/501 --> loss:0.8276826310157775
step 101/334, epoch 461/501 --> loss:0.8137729394435883
step 151/334, epoch 461/501 --> loss:0.8154008758068084
step 201/334, epoch 461/501 --> loss:0.8177799665927887
step 251/334, epoch 461/501 --> loss:0.8150811529159546
step 301/334, epoch 461/501 --> loss:0.8316855764389038

##########train dataset##########
acc--> [99.75272809068747]
F1--> {'F1': [0.9713327635632834], 'precision': [0.9501100986162181], 'recall': [0.9935356509889215]}
##########eval dataset##########
acc--> [99.05592416944535]
F1--> {'F1': [0.889816397299735], 'precision': [0.8977498362097024], 'recall': [0.8820317709753062]}
save model!
step 51/334, epoch 462/501 --> loss:0.8085134088993072
step 101/334, epoch 462/501 --> loss:0.8164029240608215
step 151/334, epoch 462/501 --> loss:0.822046936750412
step 201/334, epoch 462/501 --> loss:0.8174294114112854
step 251/334, epoch 462/501 --> loss:0.8187671864032745
step 301/334, epoch 462/501 --> loss:0.8280757153034211
step 51/334, epoch 463/501 --> loss:0.7959188079833984
step 101/334, epoch 463/501 --> loss:0.8130901503562927
step 151/334, epoch 463/501 --> loss:0.821150997877121
step 201/334, epoch 463/501 --> loss:0.8167210960388184
step 251/334, epoch 463/501 --> loss:0.8406152427196503
step 301/334, epoch 463/501 --> loss:0.8206070995330811
step 51/334, epoch 464/501 --> loss:0.8334324622154236
step 101/334, epoch 464/501 --> loss:0.831406729221344
step 151/334, epoch 464/501 --> loss:0.8093143343925476
step 201/334, epoch 464/501 --> loss:0.8201553905010224
step 251/334, epoch 464/501 --> loss:0.8286129772663117
step 301/334, epoch 464/501 --> loss:0.807198338508606
step 51/334, epoch 465/501 --> loss:0.8286330783367157
step 101/334, epoch 465/501 --> loss:0.8254596960544586
step 151/334, epoch 465/501 --> loss:0.823074152469635
step 201/334, epoch 465/501 --> loss:0.8263595116138458
step 251/334, epoch 465/501 --> loss:0.8125379514694214
step 301/334, epoch 465/501 --> loss:0.8278508925437927
step 51/334, epoch 466/501 --> loss:0.8192359042167664
step 101/334, epoch 466/501 --> loss:0.8156347155570984
step 151/334, epoch 466/501 --> loss:0.8207251965999603
step 201/334, epoch 466/501 --> loss:0.8162687635421753
step 251/334, epoch 466/501 --> loss:0.8167178785800934
step 301/334, epoch 466/501 --> loss:0.8239802289009094
step 51/334, epoch 467/501 --> loss:0.818489384651184
step 101/334, epoch 467/501 --> loss:0.8340156424045563
step 151/334, epoch 467/501 --> loss:0.8077778697013855
step 201/334, epoch 467/501 --> loss:0.8118869650363922
step 251/334, epoch 467/501 --> loss:0.8288182771205902
step 301/334, epoch 467/501 --> loss:0.8305875265598297
step 51/334, epoch 468/501 --> loss:0.8195795583724975
step 101/334, epoch 468/501 --> loss:0.8219032728672028
step 151/334, epoch 468/501 --> loss:0.8051760423183442
step 201/334, epoch 468/501 --> loss:0.8259130489826202
step 251/334, epoch 468/501 --> loss:0.8300975942611695
step 301/334, epoch 468/501 --> loss:0.8177830362319947
step 51/334, epoch 469/501 --> loss:0.817906106710434
step 101/334, epoch 469/501 --> loss:0.8094663751125336
step 151/334, epoch 469/501 --> loss:0.816994194984436
step 201/334, epoch 469/501 --> loss:0.8178377723693848
step 251/334, epoch 469/501 --> loss:0.8272096586227417
step 301/334, epoch 469/501 --> loss:0.8312329268455505
step 51/334, epoch 470/501 --> loss:0.8123293471336365
step 101/334, epoch 470/501 --> loss:0.8199795186519623
step 151/334, epoch 470/501 --> loss:0.8257159984111786
step 201/334, epoch 470/501 --> loss:0.8310601556301117
step 251/334, epoch 470/501 --> loss:0.8078912103176117
step 301/334, epoch 470/501 --> loss:0.8209810161590576
step 51/334, epoch 471/501 --> loss:0.8074968934059144
step 101/334, epoch 471/501 --> loss:0.8267430543899537
step 151/334, epoch 471/501 --> loss:0.8169740450382232
step 201/334, epoch 471/501 --> loss:0.8118819892406464
step 251/334, epoch 471/501 --> loss:0.8313320767879486
step 301/334, epoch 471/501 --> loss:0.8254071521759033

##########train dataset##########
acc--> [99.73471991614166]
F1--> {'F1': [0.9693299701751475], 'precision': [0.9456629516564334], 'recall': [0.9942225351309124]}
##########eval dataset##########
acc--> [99.00168204452908]
F1--> {'F1': [0.8836129909565219], 'precision': [0.8905036803712094], 'recall': [0.8768379690060214]}
step 51/334, epoch 472/501 --> loss:0.819618308544159
step 101/334, epoch 472/501 --> loss:0.813360458612442
step 151/334, epoch 472/501 --> loss:0.8236298787593842
step 201/334, epoch 472/501 --> loss:0.8137067997455597
step 251/334, epoch 472/501 --> loss:0.8319315099716187
step 301/334, epoch 472/501 --> loss:0.81080144405365
step 51/334, epoch 473/501 --> loss:0.8185714066028595
step 101/334, epoch 473/501 --> loss:0.8156961798667908
step 151/334, epoch 473/501 --> loss:0.8087238168716431
step 201/334, epoch 473/501 --> loss:0.8318240022659302
step 251/334, epoch 473/501 --> loss:0.8326274287700653
step 301/334, epoch 473/501 --> loss:0.8161077237129212
step 51/334, epoch 474/501 --> loss:0.8164916074275971
step 101/334, epoch 474/501 --> loss:0.824636892080307
step 151/334, epoch 474/501 --> loss:0.8191140651702881
step 201/334, epoch 474/501 --> loss:0.8159727525711059
step 251/334, epoch 474/501 --> loss:0.8292885208129883
step 301/334, epoch 474/501 --> loss:0.8160702681541443
step 51/334, epoch 475/501 --> loss:0.8031179249286652
step 101/334, epoch 475/501 --> loss:0.8274941754341125
step 151/334, epoch 475/501 --> loss:0.8220800828933715
step 201/334, epoch 475/501 --> loss:0.8281653797626496
step 251/334, epoch 475/501 --> loss:0.8030081498622894
step 301/334, epoch 475/501 --> loss:0.8287052094936371
step 51/334, epoch 476/501 --> loss:0.8109031689167022
step 101/334, epoch 476/501 --> loss:0.8101334702968598
step 151/334, epoch 476/501 --> loss:0.8320232844352722
step 201/334, epoch 476/501 --> loss:0.8268313920497894
step 251/334, epoch 476/501 --> loss:0.8153468322753906
step 301/334, epoch 476/501 --> loss:0.8195366108417511
step 51/334, epoch 477/501 --> loss:0.8310436427593231
step 101/334, epoch 477/501 --> loss:0.8269755780696869
step 151/334, epoch 477/501 --> loss:0.8000041460990905
step 201/334, epoch 477/501 --> loss:0.8165817022323608
step 251/334, epoch 477/501 --> loss:0.825872061252594
step 301/334, epoch 477/501 --> loss:0.8208307242393493
step 51/334, epoch 478/501 --> loss:0.8134558999538422
step 101/334, epoch 478/501 --> loss:0.8330717325210572
step 151/334, epoch 478/501 --> loss:0.8353354859352112
step 201/334, epoch 478/501 --> loss:0.8188902294635773
step 251/334, epoch 478/501 --> loss:0.8103240764141083
step 301/334, epoch 478/501 --> loss:0.8229900491237641
step 51/334, epoch 479/501 --> loss:0.8097028160095214
step 101/334, epoch 479/501 --> loss:0.8138568246364594
step 151/334, epoch 479/501 --> loss:0.8209775972366333
step 201/334, epoch 479/501 --> loss:0.8241244423389434
step 251/334, epoch 479/501 --> loss:0.8198180019855499
step 301/334, epoch 479/501 --> loss:0.8214820098876953
step 51/334, epoch 480/501 --> loss:0.8203722965717316
step 101/334, epoch 480/501 --> loss:0.8126988422870636
step 151/334, epoch 480/501 --> loss:0.8292021536827088
step 201/334, epoch 480/501 --> loss:0.8144269478321076
step 251/334, epoch 480/501 --> loss:0.8190011465549469
step 301/334, epoch 480/501 --> loss:0.8190245258808136
step 51/334, epoch 481/501 --> loss:0.8120794332027436
step 101/334, epoch 481/501 --> loss:0.8282330286502838
step 151/334, epoch 481/501 --> loss:0.8349883103370667
step 201/334, epoch 481/501 --> loss:0.8104130136966705
step 251/334, epoch 481/501 --> loss:0.8381181120872497
step 301/334, epoch 481/501 --> loss:0.7976678049564362

##########train dataset##########
acc--> [99.71220791004355]
F1--> {'F1': [0.9667663820832375], 'precision': [0.9421199184885788], 'recall': [0.9927475589524164]}
##########eval dataset##########
acc--> [99.00540863330143]
F1--> {'F1': [0.8846269239866503], 'precision': [0.8870231662615878], 'recall': [0.882253539570587]}
step 51/334, epoch 482/501 --> loss:0.812819151878357
step 101/334, epoch 482/501 --> loss:0.8316500759124756
step 151/334, epoch 482/501 --> loss:0.8073547887802124
step 201/334, epoch 482/501 --> loss:0.8240239405632019
step 251/334, epoch 482/501 --> loss:0.8466546678543091
step 301/334, epoch 482/501 --> loss:0.8065119779109955
step 51/334, epoch 483/501 --> loss:0.8124087846279144
step 101/334, epoch 483/501 --> loss:0.8246733129024506
step 151/334, epoch 483/501 --> loss:0.8198099863529206
step 201/334, epoch 483/501 --> loss:0.8111768925189972
step 251/334, epoch 483/501 --> loss:0.8212843799591064
step 301/334, epoch 483/501 --> loss:0.8209052777290344
step 51/334, epoch 484/501 --> loss:0.8103219509124756
step 101/334, epoch 484/501 --> loss:0.8148957443237305
step 151/334, epoch 484/501 --> loss:0.8317253768444062
step 201/334, epoch 484/501 --> loss:0.8143853187561035
step 251/334, epoch 484/501 --> loss:0.8306584692001343
step 301/334, epoch 484/501 --> loss:0.8162120449542999
step 51/334, epoch 485/501 --> loss:0.8122597253322601
step 101/334, epoch 485/501 --> loss:0.8436606049537658
step 151/334, epoch 485/501 --> loss:0.8032729303836823
step 201/334, epoch 485/501 --> loss:0.8203861856460571
step 251/334, epoch 485/501 --> loss:0.8266627442836761
step 301/334, epoch 485/501 --> loss:0.8142040145397186
step 51/334, epoch 486/501 --> loss:0.808173348903656
step 101/334, epoch 486/501 --> loss:0.8250690042972565
step 151/334, epoch 486/501 --> loss:0.8000603604316712
step 201/334, epoch 486/501 --> loss:0.8247917127609253
step 251/334, epoch 486/501 --> loss:0.833495420217514
step 301/334, epoch 486/501 --> loss:0.8227138245105743
step 51/334, epoch 487/501 --> loss:0.8211055731773377
step 101/334, epoch 487/501 --> loss:0.8415261507034302
step 151/334, epoch 487/501 --> loss:0.813939779996872
step 201/334, epoch 487/501 --> loss:0.804197211265564
step 251/334, epoch 487/501 --> loss:0.8192905509471893
step 301/334, epoch 487/501 --> loss:0.8249542403221131
step 51/334, epoch 488/501 --> loss:0.8187624764442444
step 101/334, epoch 488/501 --> loss:0.8205799305438995
step 151/334, epoch 488/501 --> loss:0.8127538633346557
step 201/334, epoch 488/501 --> loss:0.8233308780193329
step 251/334, epoch 488/501 --> loss:0.8203738617897034
step 301/334, epoch 488/501 --> loss:0.8195968317985535
step 51/334, epoch 489/501 --> loss:0.8201840329170227
step 101/334, epoch 489/501 --> loss:0.8214664840698243
step 151/334, epoch 489/501 --> loss:0.810012719631195
step 201/334, epoch 489/501 --> loss:0.8277784693241119
step 251/334, epoch 489/501 --> loss:0.8155776822566986
step 301/334, epoch 489/501 --> loss:0.8180788934230805
step 51/334, epoch 490/501 --> loss:0.8123335742950439
step 101/334, epoch 490/501 --> loss:0.8322776687145234
step 151/334, epoch 490/501 --> loss:0.8165520262718201
step 201/334, epoch 490/501 --> loss:0.8100924813747405
step 251/334, epoch 490/501 --> loss:0.8152735352516174
step 301/334, epoch 490/501 --> loss:0.8319692730903625
step 51/334, epoch 491/501 --> loss:0.8163232791423798
step 101/334, epoch 491/501 --> loss:0.8322515034675598
step 151/334, epoch 491/501 --> loss:0.8053925359249114
step 201/334, epoch 491/501 --> loss:0.821580947637558
step 251/334, epoch 491/501 --> loss:0.813956333398819
step 301/334, epoch 491/501 --> loss:0.8244286632537842

##########train dataset##########
acc--> [99.73691976780815]
F1--> {'F1': [0.9695718272622604], 'precision': [0.946266919823071], 'recall': [0.9940641434193236]}
##########eval dataset##########
acc--> [99.00313449969093]
F1--> {'F1': [0.8850203063957748], 'precision': [0.8823756877821309], 'recall': [0.887690885463798]}
step 51/334, epoch 492/501 --> loss:0.8015789294242859
step 101/334, epoch 492/501 --> loss:0.8243387734889984
step 151/334, epoch 492/501 --> loss:0.8262196362018586
step 201/334, epoch 492/501 --> loss:0.8091832923889161
step 251/334, epoch 492/501 --> loss:0.8155361413955688
step 301/334, epoch 492/501 --> loss:0.8318999946117401
step 51/334, epoch 493/501 --> loss:0.82378387093544
step 101/334, epoch 493/501 --> loss:0.8202891492843628
step 151/334, epoch 493/501 --> loss:0.8297315061092376
step 201/334, epoch 493/501 --> loss:0.8014838516712188
step 251/334, epoch 493/501 --> loss:0.8132530176639556
step 301/334, epoch 493/501 --> loss:0.8249698674678803
step 51/334, epoch 494/501 --> loss:0.8302964174747467
step 101/334, epoch 494/501 --> loss:0.8168276619911193
step 151/334, epoch 494/501 --> loss:0.8128297352790832
step 201/334, epoch 494/501 --> loss:0.815226571559906
step 251/334, epoch 494/501 --> loss:0.819631062746048
step 301/334, epoch 494/501 --> loss:0.8195003139972686
step 51/334, epoch 495/501 --> loss:0.8297069311141968
step 101/334, epoch 495/501 --> loss:0.8200569379329682
step 151/334, epoch 495/501 --> loss:0.8019662749767303
step 201/334, epoch 495/501 --> loss:0.8316374909877777
step 251/334, epoch 495/501 --> loss:0.8232985937595367
step 301/334, epoch 495/501 --> loss:0.8124624598026275
step 51/334, epoch 496/501 --> loss:0.8268415248394012
step 101/334, epoch 496/501 --> loss:0.8196604013442993
step 151/334, epoch 496/501 --> loss:0.8058391296863556
step 201/334, epoch 496/501 --> loss:0.8228064894676208
step 251/334, epoch 496/501 --> loss:0.818759446144104
step 301/334, epoch 496/501 --> loss:0.8192267072200775
step 51/334, epoch 497/501 --> loss:0.8215235376358032
step 101/334, epoch 497/501 --> loss:0.8294003677368164
step 151/334, epoch 497/501 --> loss:0.8157861411571503
step 201/334, epoch 497/501 --> loss:0.8252891385555268
step 251/334, epoch 497/501 --> loss:0.8129822206497193
step 301/334, epoch 497/501 --> loss:0.8305493807792663
step 51/334, epoch 498/501 --> loss:0.8269486558437348
step 101/334, epoch 498/501 --> loss:0.8189488363265991
step 151/334, epoch 498/501 --> loss:0.8289970064163208
step 201/334, epoch 498/501 --> loss:0.8154228699207305
step 251/334, epoch 498/501 --> loss:0.8166773092746734
step 301/334, epoch 498/501 --> loss:0.8212849020957946
step 51/334, epoch 499/501 --> loss:0.8223699367046357
step 101/334, epoch 499/501 --> loss:0.8320866119861603
step 151/334, epoch 499/501 --> loss:0.8157766318321228
step 201/334, epoch 499/501 --> loss:0.8078497350215912
step 251/334, epoch 499/501 --> loss:0.8203916943073273
step 301/334, epoch 499/501 --> loss:0.8179851484298706
step 51/334, epoch 500/501 --> loss:0.8158561861515046
step 101/334, epoch 500/501 --> loss:0.8226044714450836
step 151/334, epoch 500/501 --> loss:0.8312993836402893
step 201/334, epoch 500/501 --> loss:0.8095355987548828
step 251/334, epoch 500/501 --> loss:0.8161747622489929
step 301/334, epoch 500/501 --> loss:0.8234202182292938
step 51/334, epoch 501/501 --> loss:0.8264021217823029
step 101/334, epoch 501/501 --> loss:0.8080491101741791
step 151/334, epoch 501/501 --> loss:0.8220208585262299
step 201/334, epoch 501/501 --> loss:0.822606703042984
step 251/334, epoch 501/501 --> loss:0.8183654010295868
step 301/334, epoch 501/501 --> loss:0.8165326595306397

##########train dataset##########
acc--> [99.78436676356942]
F1--> {'F1': [0.9749022012161345], 'precision': [0.9571875818317634], 'recall': [0.9932952483225956]}
##########eval dataset##########
acc--> [99.07104278590491]
F1--> {'F1': [0.8908618216143399], 'precision': [0.9049128650415692], 'recall': [0.8772501559007323]}
save model!
