##########Config##########
{'device': 'cuda:1', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 256, 'num_parallel_workers': 4, 'batch_size': 16, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/RefineNet', 'log_path': 'Logs/RefineNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (backbone): ResNet(
    (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (final): Sequential(
    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (ResidualConvUnit): ResidualConvUnit(
    (0): Sequential(
      (0): ReLU()
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (1): Sequential(
      (0): ReLU()
      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): Sequential(
      (0): ReLU()
      (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): Sequential(
      (0): ReLU()
      (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (MultiResolutionFusion): MultiResolutionFusion(
    (0): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): Sequential(
      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (ChainedResidualPool): ChainedResidualPool(
    (0): ReLU()
    (1-4): 4 x Sequential(
      (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.9043441045284272
step 101/334, epoch 1/501 --> loss:0.850682612657547
step 151/334, epoch 1/501 --> loss:0.8676758110523224
step 201/334, epoch 1/501 --> loss:0.8516728067398072
step 251/334, epoch 1/501 --> loss:0.8652367877960205
step 301/334, epoch 1/501 --> loss:0.8417288076877594

##########train dataset##########
acc--> [91.20115288017935]
F1--> {'F1': [0.46116759456600803], 'precision': [0.3118348296829206], 'recall': [0.8849901701856201]}
##########eval dataset##########
acc--> [91.60111928757317]
F1--> {'F1': [0.47028620115948655], 'precision': [0.31884639058643244], 'recall': [0.8957459177072686]}
save model!
step 51/334, epoch 2/501 --> loss:0.833774471282959
step 101/334, epoch 2/501 --> loss:0.8434963977336883
step 151/334, epoch 2/501 --> loss:0.8686070251464844
step 201/334, epoch 2/501 --> loss:0.8568026936054229
step 251/334, epoch 2/501 --> loss:0.8643623650074005
step 301/334, epoch 2/501 --> loss:0.844585452079773
step 51/334, epoch 3/501 --> loss:0.8429450476169587
step 101/334, epoch 3/501 --> loss:0.8349754965305328
step 151/334, epoch 3/501 --> loss:0.8489811563491821
step 201/334, epoch 3/501 --> loss:0.8808209431171418
step 251/334, epoch 3/501 --> loss:0.8391713821887969
step 301/334, epoch 3/501 --> loss:0.8418935930728912
step 51/334, epoch 4/501 --> loss:0.8494379758834839
step 101/334, epoch 4/501 --> loss:0.8319702506065368
step 151/334, epoch 4/501 --> loss:0.8498906743526459
step 201/334, epoch 4/501 --> loss:0.8479847586154938
step 251/334, epoch 4/501 --> loss:0.8555155038833618
step 301/334, epoch 4/501 --> loss:0.8598470687866211
step 51/334, epoch 5/501 --> loss:0.8540266466140747
step 101/334, epoch 5/501 --> loss:0.8304335510730744
step 151/334, epoch 5/501 --> loss:0.8657291877269745
step 201/334, epoch 5/501 --> loss:0.8751162767410279
step 251/334, epoch 5/501 --> loss:0.825553869009018
step 301/334, epoch 5/501 --> loss:0.8497505557537078
step 51/334, epoch 6/501 --> loss:0.845832736492157
step 101/334, epoch 6/501 --> loss:0.8449341964721679
step 151/334, epoch 6/501 --> loss:0.8501710188388825
step 201/334, epoch 6/501 --> loss:0.8387655949592591
step 251/334, epoch 6/501 --> loss:0.8547368943691254
step 301/334, epoch 6/501 --> loss:0.8503400468826294
step 51/334, epoch 7/501 --> loss:0.8399049293994904
step 101/334, epoch 7/501 --> loss:0.8416622364521027
step 151/334, epoch 7/501 --> loss:0.8379888558387756
step 201/334, epoch 7/501 --> loss:0.8563896322250366
step 251/334, epoch 7/501 --> loss:0.8473073709011077
step 301/334, epoch 7/501 --> loss:0.8412234354019165
step 51/334, epoch 8/501 --> loss:0.8716742813587188
step 101/334, epoch 8/501 --> loss:0.8385810112953186
step 151/334, epoch 8/501 --> loss:0.8446931004524231
step 201/334, epoch 8/501 --> loss:0.8529342341423035
step 251/334, epoch 8/501 --> loss:0.8292295384407044
step 301/334, epoch 8/501 --> loss:0.8530932188034057
step 51/334, epoch 9/501 --> loss:0.847548508644104
step 101/334, epoch 9/501 --> loss:0.8357670712471008
step 151/334, epoch 9/501 --> loss:0.8482528233528137
step 201/334, epoch 9/501 --> loss:0.8388365650177002
step 251/334, epoch 9/501 --> loss:0.8402805340290069
step 301/334, epoch 9/501 --> loss:0.8533472847938538
step 51/334, epoch 10/501 --> loss:0.8181706976890564
step 101/334, epoch 10/501 --> loss:0.8487740302085877
step 151/334, epoch 10/501 --> loss:0.8337370908260345
step 201/334, epoch 10/501 --> loss:0.8543492829799653
step 251/334, epoch 10/501 --> loss:0.8613667809963226
step 301/334, epoch 10/501 --> loss:0.8645969247817993
step 51/334, epoch 11/501 --> loss:0.8257313394546508
step 101/334, epoch 11/501 --> loss:0.838957486152649
step 151/334, epoch 11/501 --> loss:0.8405557453632355
step 201/334, epoch 11/501 --> loss:0.8541478741168976
step 251/334, epoch 11/501 --> loss:0.8424610006809234
step 301/334, epoch 11/501 --> loss:0.8597673857212067

##########train dataset##########
acc--> [88.0461765399417]
F1--> {'F1': [0.39240505125619424], 'precision': [0.2503431631445027], 'recall': [0.9072653215996997]}
##########eval dataset##########
acc--> [88.585569393389]
F1--> {'F1': [0.39839562336279866], 'precision': [0.2551797671292758], 'recall': [0.9080290741713613]}
step 51/334, epoch 12/501 --> loss:0.8525221252441406
step 101/334, epoch 12/501 --> loss:0.8189201855659485
step 151/334, epoch 12/501 --> loss:0.8537841129302979
step 201/334, epoch 12/501 --> loss:0.8450361943244934
step 251/334, epoch 12/501 --> loss:0.8584440636634827
step 301/334, epoch 12/501 --> loss:0.8316204071044921
step 51/334, epoch 13/501 --> loss:0.8492362511157989
step 101/334, epoch 13/501 --> loss:0.8403399395942688
step 151/334, epoch 13/501 --> loss:0.8384578204154969
step 201/334, epoch 13/501 --> loss:0.8469260108470916
step 251/334, epoch 13/501 --> loss:0.832023411989212
step 301/334, epoch 13/501 --> loss:0.8522757172584534
step 51/334, epoch 14/501 --> loss:0.8653821277618409
step 101/334, epoch 14/501 --> loss:0.8465103268623352
step 151/334, epoch 14/501 --> loss:0.8531180250644684
step 201/334, epoch 14/501 --> loss:0.8354784047603607
step 251/334, epoch 14/501 --> loss:0.8389546966552734
step 301/334, epoch 14/501 --> loss:0.8416542017459869
step 51/334, epoch 15/501 --> loss:0.8572534060478211
step 101/334, epoch 15/501 --> loss:0.8332769548892975
step 151/334, epoch 15/501 --> loss:0.8571966028213501
step 201/334, epoch 15/501 --> loss:0.8443695580959321
step 251/334, epoch 15/501 --> loss:0.8385741913318634
step 301/334, epoch 15/501 --> loss:0.8220018517971038
step 51/334, epoch 16/501 --> loss:0.8285496425628662
step 101/334, epoch 16/501 --> loss:0.854738531112671
step 151/334, epoch 16/501 --> loss:0.8432090413570404
step 201/334, epoch 16/501 --> loss:0.8489840304851533
step 251/334, epoch 16/501 --> loss:0.8470711827278137
step 301/334, epoch 16/501 --> loss:0.8450167989730835
step 51/334, epoch 17/501 --> loss:0.8313833808898926
step 101/334, epoch 17/501 --> loss:0.8532713294029236
step 151/334, epoch 17/501 --> loss:0.8525156056880951
step 201/334, epoch 17/501 --> loss:0.8528750216960908
step 251/334, epoch 17/501 --> loss:0.8439231145381928
step 301/334, epoch 17/501 --> loss:0.8510385286808014
step 51/334, epoch 18/501 --> loss:0.8616435289382934
step 101/334, epoch 18/501 --> loss:0.8353384506702423
step 151/334, epoch 18/501 --> loss:0.8373974108695984
step 201/334, epoch 18/501 --> loss:0.8414861392974854
step 251/334, epoch 18/501 --> loss:0.8297529292106628
step 301/334, epoch 18/501 --> loss:0.8599164617061615
step 51/334, epoch 19/501 --> loss:0.8511401355266571
step 101/334, epoch 19/501 --> loss:0.8582976734638215
step 151/334, epoch 19/501 --> loss:0.8497109985351563
step 201/334, epoch 19/501 --> loss:0.8422997367382049
step 251/334, epoch 19/501 --> loss:0.8328783810138702
step 301/334, epoch 19/501 --> loss:0.8402051150798797
step 51/334, epoch 20/501 --> loss:0.8590061509609223
step 101/334, epoch 20/501 --> loss:0.8396000504493714
step 151/334, epoch 20/501 --> loss:0.8410431706905365
step 201/334, epoch 20/501 --> loss:0.8304377007484436
step 251/334, epoch 20/501 --> loss:0.8308553552627563
step 301/334, epoch 20/501 --> loss:0.846344393491745
step 51/334, epoch 21/501 --> loss:0.8654776048660279
step 101/334, epoch 21/501 --> loss:0.8368523156642914
step 151/334, epoch 21/501 --> loss:0.8536017549037933
step 201/334, epoch 21/501 --> loss:0.839289118051529
step 251/334, epoch 21/501 --> loss:0.8251082992553711
step 301/334, epoch 21/501 --> loss:0.8448996663093566

##########train dataset##########
acc--> [96.4363741794211]
F1--> {'F1': [0.6844825663788962], 'precision': [0.5490820326196002], 'recall': [0.9085345240266275]}
##########eval dataset##########
acc--> [96.56371269325552]
F1--> {'F1': [0.6871781670386502], 'precision': [0.5532065932690845], 'recall': [0.9067905753480413]}
save model!
step 51/334, epoch 22/501 --> loss:0.8442427265644074
step 101/334, epoch 22/501 --> loss:0.8432242584228515
step 151/334, epoch 22/501 --> loss:0.8484140753746032
step 201/334, epoch 22/501 --> loss:0.8399460685253143
step 251/334, epoch 22/501 --> loss:0.8251044070720672
step 301/334, epoch 22/501 --> loss:0.855734406709671
step 51/334, epoch 23/501 --> loss:0.8555580353736878
step 101/334, epoch 23/501 --> loss:0.8334001731872559
step 151/334, epoch 23/501 --> loss:0.8447743618488311
step 201/334, epoch 23/501 --> loss:0.8529977452754974
step 251/334, epoch 23/501 --> loss:0.8383720588684082
step 301/334, epoch 23/501 --> loss:0.8316773092746734
step 51/334, epoch 24/501 --> loss:0.8424277293682099
step 101/334, epoch 24/501 --> loss:0.8592123234272003
step 151/334, epoch 24/501 --> loss:0.8729945123195648
step 201/334, epoch 24/501 --> loss:0.8198078572750092
step 251/334, epoch 24/501 --> loss:0.8505093705654144
step 301/334, epoch 24/501 --> loss:0.8198847365379334
step 51/334, epoch 25/501 --> loss:0.8438128483295441
step 101/334, epoch 25/501 --> loss:0.8343158340454102
step 151/334, epoch 25/501 --> loss:0.8409060168266297
step 201/334, epoch 25/501 --> loss:0.8285086143016815
step 251/334, epoch 25/501 --> loss:0.8521753931045533
step 301/334, epoch 25/501 --> loss:0.8548093664646149
step 51/334, epoch 26/501 --> loss:0.840663206577301
step 101/334, epoch 26/501 --> loss:0.8602813994884491
step 151/334, epoch 26/501 --> loss:0.82861701130867
step 201/334, epoch 26/501 --> loss:0.8400640189647675
step 251/334, epoch 26/501 --> loss:0.8492174994945526
step 301/334, epoch 26/501 --> loss:0.8323911225795746
step 51/334, epoch 27/501 --> loss:0.8348795044422149
step 101/334, epoch 27/501 --> loss:0.8293582034111023
step 151/334, epoch 27/501 --> loss:0.8174025356769562
step 201/334, epoch 27/501 --> loss:0.8736379289627075
step 251/334, epoch 27/501 --> loss:0.8508741283416748
step 301/334, epoch 27/501 --> loss:0.8482366621494293
step 51/334, epoch 28/501 --> loss:0.8355728459358215
step 101/334, epoch 28/501 --> loss:0.8418237030506134
step 151/334, epoch 28/501 --> loss:0.8699770617485046
step 201/334, epoch 28/501 --> loss:0.8111869621276856
step 251/334, epoch 28/501 --> loss:0.8597267997264862
step 301/334, epoch 28/501 --> loss:0.8329841423034668
step 51/334, epoch 29/501 --> loss:0.8471103036403655
step 101/334, epoch 29/501 --> loss:0.8415873599052429
step 151/334, epoch 29/501 --> loss:0.8312374258041382
step 201/334, epoch 29/501 --> loss:0.8596365618705749
step 251/334, epoch 29/501 --> loss:0.8279433465003967
step 301/334, epoch 29/501 --> loss:0.8348153805732728
step 51/334, epoch 30/501 --> loss:0.8641673243045807
step 101/334, epoch 30/501 --> loss:0.8359449148178101
step 151/334, epoch 30/501 --> loss:0.8393486356735229
step 201/334, epoch 30/501 --> loss:0.8269591867923737
step 251/334, epoch 30/501 --> loss:0.8377766346931458
step 301/334, epoch 30/501 --> loss:0.8343935430049896
step 51/334, epoch 31/501 --> loss:0.8194146621227264
step 101/334, epoch 31/501 --> loss:0.8327322292327881
step 151/334, epoch 31/501 --> loss:0.8378450620174408
step 201/334, epoch 31/501 --> loss:0.8502332043647766
step 251/334, epoch 31/501 --> loss:0.8333655345439911
step 301/334, epoch 31/501 --> loss:0.8630491054058075

##########train dataset##########
acc--> [97.35691083429356]
F1--> {'F1': [0.7449235477607331], 'precision': [0.6319384070925147], 'recall': [0.9071206982792399]}
##########eval dataset##########
acc--> [97.38859842454956]
F1--> {'F1': [0.744169051017509], 'precision': [0.6282714096302726], 'recall': [0.9125127189468631]}
save model!
step 51/334, epoch 32/501 --> loss:0.8318171405792236
step 101/334, epoch 32/501 --> loss:0.8448909556865692
step 151/334, epoch 32/501 --> loss:0.8566480982303619
step 201/334, epoch 32/501 --> loss:0.8477729082107544
step 251/334, epoch 32/501 --> loss:0.8140549910068512
step 301/334, epoch 32/501 --> loss:0.8520836734771728
step 51/334, epoch 33/501 --> loss:0.8638648962974549
step 101/334, epoch 33/501 --> loss:0.8344609141349792
step 151/334, epoch 33/501 --> loss:0.8365661060810089
step 201/334, epoch 33/501 --> loss:0.8208438432216645
step 251/334, epoch 33/501 --> loss:0.8363500595092773
step 301/334, epoch 33/501 --> loss:0.8559482538700104
step 51/334, epoch 34/501 --> loss:0.8572379350662231
step 101/334, epoch 34/501 --> loss:0.8259186923503876
step 151/334, epoch 34/501 --> loss:0.8552468192577362
step 201/334, epoch 34/501 --> loss:0.8432210516929627
step 251/334, epoch 34/501 --> loss:0.8273069775104522
step 301/334, epoch 34/501 --> loss:0.8402939283847809
step 51/334, epoch 35/501 --> loss:0.8338562703132629
step 101/334, epoch 35/501 --> loss:0.8533361625671386
step 151/334, epoch 35/501 --> loss:0.8515538346767425
step 201/334, epoch 35/501 --> loss:0.8534111726284027
step 251/334, epoch 35/501 --> loss:0.8368287527561188
step 301/334, epoch 35/501 --> loss:0.8366267728805542
step 51/334, epoch 36/501 --> loss:0.860309112071991
step 101/334, epoch 36/501 --> loss:0.8335907256603241
step 151/334, epoch 36/501 --> loss:0.8475173532962799
step 201/334, epoch 36/501 --> loss:0.8453704643249512
step 251/334, epoch 36/501 --> loss:0.8300234580039978
step 301/334, epoch 36/501 --> loss:0.8426771795749665
step 51/334, epoch 37/501 --> loss:0.8336782443523407
step 101/334, epoch 37/501 --> loss:0.8205621290206909
step 151/334, epoch 37/501 --> loss:0.8350582730770111
step 201/334, epoch 37/501 --> loss:0.8518587529659272
step 251/334, epoch 37/501 --> loss:0.8396818292140961
step 301/334, epoch 37/501 --> loss:0.8440461373329162
step 51/334, epoch 38/501 --> loss:0.8442484652996063
step 101/334, epoch 38/501 --> loss:0.8518362176418305
step 151/334, epoch 38/501 --> loss:0.8332297396659851
step 201/334, epoch 38/501 --> loss:0.8498407888412476
step 251/334, epoch 38/501 --> loss:0.8401273274421692
step 301/334, epoch 38/501 --> loss:0.8250934875011444
step 51/334, epoch 39/501 --> loss:0.8659112143516541
step 101/334, epoch 39/501 --> loss:0.8474394237995148
step 151/334, epoch 39/501 --> loss:0.8358618462085724
step 201/334, epoch 39/501 --> loss:0.8516469359397888
step 251/334, epoch 39/501 --> loss:0.8163556921482086
step 301/334, epoch 39/501 --> loss:0.8365042543411255
step 51/334, epoch 40/501 --> loss:0.828065847158432
step 101/334, epoch 40/501 --> loss:0.8291620707511902
step 151/334, epoch 40/501 --> loss:0.8399200546741485
step 201/334, epoch 40/501 --> loss:0.8410408580303192
step 251/334, epoch 40/501 --> loss:0.8470700335502624
step 301/334, epoch 40/501 --> loss:0.8440744173526764
step 51/334, epoch 41/501 --> loss:0.8226668155193329
step 101/334, epoch 41/501 --> loss:0.8387518191337585
step 151/334, epoch 41/501 --> loss:0.8606464540958405
step 201/334, epoch 41/501 --> loss:0.8349985826015472
step 251/334, epoch 41/501 --> loss:0.8439917206764221
step 301/334, epoch 41/501 --> loss:0.8532020843029022

##########train dataset##########
acc--> [97.6546182764724]
F1--> {'F1': [0.7697839319420168], 'precision': [0.6608999065607527], 'recall': [0.9216362135174585]}
##########eval dataset##########
acc--> [97.58002947842537]
F1--> {'F1': [0.7587893281209633], 'precision': [0.6483978951882808], 'recall': [0.9144964338820524]}
save model!
step 51/334, epoch 42/501 --> loss:0.8178583812713623
step 101/334, epoch 42/501 --> loss:0.8355800342559815
step 151/334, epoch 42/501 --> loss:0.848774516582489
step 201/334, epoch 42/501 --> loss:0.847456728219986
step 251/334, epoch 42/501 --> loss:0.8282312047481537
step 301/334, epoch 42/501 --> loss:0.8630584836006164
step 51/334, epoch 43/501 --> loss:0.837116470336914
step 101/334, epoch 43/501 --> loss:0.8453607952594757
step 151/334, epoch 43/501 --> loss:0.8093688344955444
step 201/334, epoch 43/501 --> loss:0.8498552799224853
step 251/334, epoch 43/501 --> loss:0.8332743346691132
step 301/334, epoch 43/501 --> loss:0.8624007606506348
step 51/334, epoch 44/501 --> loss:0.8315846085548401
step 101/334, epoch 44/501 --> loss:0.8458145034313201
step 151/334, epoch 44/501 --> loss:0.8273072028160096
step 201/334, epoch 44/501 --> loss:0.8427440309524536
step 251/334, epoch 44/501 --> loss:0.8435101222991943
step 301/334, epoch 44/501 --> loss:0.8493106818199158
step 51/334, epoch 45/501 --> loss:0.8526922345161438
step 101/334, epoch 45/501 --> loss:0.8190528333187104
step 151/334, epoch 45/501 --> loss:0.8521044993400574
step 201/334, epoch 45/501 --> loss:0.8336025619506836
step 251/334, epoch 45/501 --> loss:0.8240750873088837
step 301/334, epoch 45/501 --> loss:0.8433016204833984
step 51/334, epoch 46/501 --> loss:0.838414101600647
step 101/334, epoch 46/501 --> loss:0.8621607065200806
step 151/334, epoch 46/501 --> loss:0.8244757890701294
step 201/334, epoch 46/501 --> loss:0.8540369319915772
step 251/334, epoch 46/501 --> loss:0.817183256149292
step 301/334, epoch 46/501 --> loss:0.8383966994285583
step 51/334, epoch 47/501 --> loss:0.8218470823764801
step 101/334, epoch 47/501 --> loss:0.8452710449695587
step 151/334, epoch 47/501 --> loss:0.8281426119804383
step 201/334, epoch 47/501 --> loss:0.8570271062850953
step 251/334, epoch 47/501 --> loss:0.8485180795192718
step 301/334, epoch 47/501 --> loss:0.8270941901206971
step 51/334, epoch 48/501 --> loss:0.8575388264656066
step 101/334, epoch 48/501 --> loss:0.8432752156257629
step 151/334, epoch 48/501 --> loss:0.8275718295574188
step 201/334, epoch 48/501 --> loss:0.8499454236030579
step 251/334, epoch 48/501 --> loss:0.8498089540004731
step 301/334, epoch 48/501 --> loss:0.8157741904258728
step 51/334, epoch 49/501 --> loss:0.8383630561828613
step 101/334, epoch 49/501 --> loss:0.83317351937294
step 151/334, epoch 49/501 --> loss:0.8375969862937928
step 201/334, epoch 49/501 --> loss:0.8308759129047394
step 251/334, epoch 49/501 --> loss:0.8400093197822571
step 301/334, epoch 49/501 --> loss:0.8377922594547271
step 51/334, epoch 50/501 --> loss:0.8471647274494171
step 101/334, epoch 50/501 --> loss:0.830000011920929
step 151/334, epoch 50/501 --> loss:0.84496222615242
step 201/334, epoch 50/501 --> loss:0.8445207488536834
step 251/334, epoch 50/501 --> loss:0.8528877353668213
step 301/334, epoch 50/501 --> loss:0.822540602684021
step 51/334, epoch 51/501 --> loss:0.8418339145183563
step 101/334, epoch 51/501 --> loss:0.8545671379566193
step 151/334, epoch 51/501 --> loss:0.8293049144744873
step 201/334, epoch 51/501 --> loss:0.8393812870979309
step 251/334, epoch 51/501 --> loss:0.8497473573684693
step 301/334, epoch 51/501 --> loss:0.8291954076290131

##########train dataset##########
acc--> [95.54820652529854]
F1--> {'F1': [0.6488369710829832], 'precision': [0.48830003809678535], 'recall': [0.966654272942754]}
##########eval dataset##########
acc--> [95.45606744987538]
F1--> {'F1': [0.63452546197194], 'precision': [0.4769294686911273], 'recall': [0.9476934302457231]}
step 51/334, epoch 52/501 --> loss:0.8464985179901123
step 101/334, epoch 52/501 --> loss:0.8365398144721985
step 151/334, epoch 52/501 --> loss:0.8716914582252503
step 201/334, epoch 52/501 --> loss:0.8420636510848999
step 251/334, epoch 52/501 --> loss:0.8280050122737884
step 301/334, epoch 52/501 --> loss:0.8113034725189209
step 51/334, epoch 53/501 --> loss:0.8501778268814086
step 101/334, epoch 53/501 --> loss:0.8425500214099884
step 151/334, epoch 53/501 --> loss:0.8276261699199676
step 201/334, epoch 53/501 --> loss:0.838652970790863
step 251/334, epoch 53/501 --> loss:0.8044582426548004
step 301/334, epoch 53/501 --> loss:0.843006100654602
step 51/334, epoch 54/501 --> loss:0.8464040839672089
step 101/334, epoch 54/501 --> loss:0.8373058390617371
step 151/334, epoch 54/501 --> loss:0.8372416794300079
step 201/334, epoch 54/501 --> loss:0.8310106754302978
step 251/334, epoch 54/501 --> loss:0.8193594014644623
step 301/334, epoch 54/501 --> loss:0.8537455236911774
step 51/334, epoch 55/501 --> loss:0.8375889766216278
step 101/334, epoch 55/501 --> loss:0.836733295917511
step 151/334, epoch 55/501 --> loss:0.8243111097812652
step 201/334, epoch 55/501 --> loss:0.846821460723877
step 251/334, epoch 55/501 --> loss:0.8424939358234406
step 301/334, epoch 55/501 --> loss:0.8543672251701355
step 51/334, epoch 56/501 --> loss:0.8148549997806549
step 101/334, epoch 56/501 --> loss:0.8533432614803315
step 151/334, epoch 56/501 --> loss:0.8211703538894654
step 201/334, epoch 56/501 --> loss:0.8537989723682403
step 251/334, epoch 56/501 --> loss:0.8335572552680969
step 301/334, epoch 56/501 --> loss:0.8353290295600891
step 51/334, epoch 57/501 --> loss:0.8099947583675384
step 101/334, epoch 57/501 --> loss:0.8518589425086975
step 151/334, epoch 57/501 --> loss:0.8107983863353729
step 201/334, epoch 57/501 --> loss:0.8490323662757874
step 251/334, epoch 57/501 --> loss:0.8570176661014557
step 301/334, epoch 57/501 --> loss:0.8487154805660247
step 51/334, epoch 58/501 --> loss:0.8405129909515381
step 101/334, epoch 58/501 --> loss:0.8329332172870636
step 151/334, epoch 58/501 --> loss:0.841550954580307
step 201/334, epoch 58/501 --> loss:0.8287315917015076
step 251/334, epoch 58/501 --> loss:0.8269060206413269
step 301/334, epoch 58/501 --> loss:0.8382284450531006
step 51/334, epoch 59/501 --> loss:0.828234384059906
step 101/334, epoch 59/501 --> loss:0.8355278861522675
step 151/334, epoch 59/501 --> loss:0.8500239503383636
step 201/334, epoch 59/501 --> loss:0.8357857048511506
step 251/334, epoch 59/501 --> loss:0.8321189248561859
step 301/334, epoch 59/501 --> loss:0.8333732998371124
step 51/334, epoch 60/501 --> loss:0.8563895738124847
step 101/334, epoch 60/501 --> loss:0.8225634181499482
step 151/334, epoch 60/501 --> loss:0.8330059778690339
step 201/334, epoch 60/501 --> loss:0.8426071381568909
step 251/334, epoch 60/501 --> loss:0.8342839848995208
step 301/334, epoch 60/501 --> loss:0.8105083155632019
step 51/334, epoch 61/501 --> loss:0.8275213968753815
step 101/334, epoch 61/501 --> loss:0.827527619600296
step 151/334, epoch 61/501 --> loss:0.8447271978855133
step 201/334, epoch 61/501 --> loss:0.8552281045913697
step 251/334, epoch 61/501 --> loss:0.8379955649375915
step 301/334, epoch 61/501 --> loss:0.8236257362365723

##########train dataset##########
acc--> [96.11661644253421]
F1--> {'F1': [0.6810912165024849], 'precision': [0.5234342124978025], 'recall': [0.9746732360116944]}
##########eval dataset##########
acc--> [96.06170988040526]
F1--> {'F1': [0.6699351880095474], 'precision': [0.5144161166487394], 'recall': [0.9602535370487867]}
step 51/334, epoch 62/501 --> loss:0.8398986601829529
step 101/334, epoch 62/501 --> loss:0.8271266603469849
step 151/334, epoch 62/501 --> loss:0.8479597568511963
step 201/334, epoch 62/501 --> loss:0.8054707181453705
step 251/334, epoch 62/501 --> loss:0.8372825717926026
step 301/334, epoch 62/501 --> loss:0.8514342665672302
step 51/334, epoch 63/501 --> loss:0.8318965637683868
step 101/334, epoch 63/501 --> loss:0.8294514894485474
step 151/334, epoch 63/501 --> loss:0.8413370323181152
step 201/334, epoch 63/501 --> loss:0.8367208087444306
step 251/334, epoch 63/501 --> loss:0.8514029777050018
step 301/334, epoch 63/501 --> loss:0.8350953459739685
step 51/334, epoch 64/501 --> loss:0.8383932495117188
step 101/334, epoch 64/501 --> loss:0.8226488173007965
step 151/334, epoch 64/501 --> loss:0.8313323152065277
step 201/334, epoch 64/501 --> loss:0.8311623179912567
step 251/334, epoch 64/501 --> loss:0.8386782658100128
step 301/334, epoch 64/501 --> loss:0.8442123198509216
step 51/334, epoch 65/501 --> loss:0.8338682329654694
step 101/334, epoch 65/501 --> loss:0.8393466460704804
step 151/334, epoch 65/501 --> loss:0.8094420802593231
step 201/334, epoch 65/501 --> loss:0.8405085027217865
step 251/334, epoch 65/501 --> loss:0.8451290893554687
step 301/334, epoch 65/501 --> loss:0.8311598563194275
step 51/334, epoch 66/501 --> loss:0.8648188054561615
step 101/334, epoch 66/501 --> loss:0.8369958364963531
step 151/334, epoch 66/501 --> loss:0.8439082622528076
step 201/334, epoch 66/501 --> loss:0.8186677289009094
step 251/334, epoch 66/501 --> loss:0.8121415293216705
step 301/334, epoch 66/501 --> loss:0.8303854918479919
step 51/334, epoch 67/501 --> loss:0.8514457082748413
step 101/334, epoch 67/501 --> loss:0.8336634457111358
step 151/334, epoch 67/501 --> loss:0.8238114762306213
step 201/334, epoch 67/501 --> loss:0.8256422924995422
step 251/334, epoch 67/501 --> loss:0.8457335364818573
step 301/334, epoch 67/501 --> loss:0.8299940371513367
step 51/334, epoch 68/501 --> loss:0.8284555113315583
step 101/334, epoch 68/501 --> loss:0.8341586291790009
step 151/334, epoch 68/501 --> loss:0.832876683473587
step 201/334, epoch 68/501 --> loss:0.8629205751419068
step 251/334, epoch 68/501 --> loss:0.8231052279472351
step 301/334, epoch 68/501 --> loss:0.8324818313121796
step 51/334, epoch 69/501 --> loss:0.8496627342700959
step 101/334, epoch 69/501 --> loss:0.803816111087799
step 151/334, epoch 69/501 --> loss:0.8363771557807922
step 201/334, epoch 69/501 --> loss:0.8453267693519593
step 251/334, epoch 69/501 --> loss:0.8213942968845367
step 301/334, epoch 69/501 --> loss:0.8357711088657379
step 51/334, epoch 70/501 --> loss:0.8316880142688752
step 101/334, epoch 70/501 --> loss:0.8529197347164154
step 151/334, epoch 70/501 --> loss:0.8513387620449067
step 201/334, epoch 70/501 --> loss:0.7999627482891083
step 251/334, epoch 70/501 --> loss:0.8390391421318054
step 301/334, epoch 70/501 --> loss:0.8381099152565002
step 51/334, epoch 71/501 --> loss:0.8318997144699096
step 101/334, epoch 71/501 --> loss:0.8528871536254883
step 151/334, epoch 71/501 --> loss:0.8106864261627197
step 201/334, epoch 71/501 --> loss:0.8347511303424835
step 251/334, epoch 71/501 --> loss:0.8578194010257721
step 301/334, epoch 71/501 --> loss:0.8204959607124329

##########train dataset##########
acc--> [98.43093277051635]
F1--> {'F1': [0.8369284928741756], 'precision': [0.7501733571887446], 'recall': [0.9463861651079623]}
##########eval dataset##########
acc--> [98.29389383577373]
F1--> {'F1': [0.8176965089183788], 'precision': [0.7363301638671506], 'recall': [0.9192916678345205]}
save model!
step 51/334, epoch 72/501 --> loss:0.8422740042209625
step 101/334, epoch 72/501 --> loss:0.8174196016788483
step 151/334, epoch 72/501 --> loss:0.831816793680191
step 201/334, epoch 72/501 --> loss:0.8394183588027954
step 251/334, epoch 72/501 --> loss:0.8280704379081726
step 301/334, epoch 72/501 --> loss:0.8485656404495239
step 51/334, epoch 73/501 --> loss:0.8277155220508575
step 101/334, epoch 73/501 --> loss:0.8429171180725098
step 151/334, epoch 73/501 --> loss:0.8366138827800751
step 201/334, epoch 73/501 --> loss:0.8270025765895843
step 251/334, epoch 73/501 --> loss:0.8146389961242676
step 301/334, epoch 73/501 --> loss:0.8385256123542786
step 51/334, epoch 74/501 --> loss:0.8233855366706848
step 101/334, epoch 74/501 --> loss:0.8472938442230225
step 151/334, epoch 74/501 --> loss:0.8388292980194092
step 201/334, epoch 74/501 --> loss:0.8066561722755432
step 251/334, epoch 74/501 --> loss:0.8608908402919769
step 301/334, epoch 74/501 --> loss:0.8417762506008148
step 51/334, epoch 75/501 --> loss:0.8444381022453308
step 101/334, epoch 75/501 --> loss:0.8330010569095612
step 151/334, epoch 75/501 --> loss:0.819152170419693
step 201/334, epoch 75/501 --> loss:0.8474639427661895
step 251/334, epoch 75/501 --> loss:0.8403999423980713
step 301/334, epoch 75/501 --> loss:0.8108865106105805
step 51/334, epoch 76/501 --> loss:0.8779285800457001
step 101/334, epoch 76/501 --> loss:0.8306066191196442
step 151/334, epoch 76/501 --> loss:0.8180768585205078
step 201/334, epoch 76/501 --> loss:0.844406988620758
step 251/334, epoch 76/501 --> loss:0.8428728890419006
step 301/334, epoch 76/501 --> loss:0.83007159948349
step 51/334, epoch 77/501 --> loss:0.8303837847709655
step 101/334, epoch 77/501 --> loss:0.815833933353424
step 151/334, epoch 77/501 --> loss:0.8550445377826691
step 201/334, epoch 77/501 --> loss:0.8344417333602905
step 251/334, epoch 77/501 --> loss:0.8426693332195282
step 301/334, epoch 77/501 --> loss:0.8151011300086975
step 51/334, epoch 78/501 --> loss:0.8506755065917969
step 101/334, epoch 78/501 --> loss:0.8487893891334534
step 151/334, epoch 78/501 --> loss:0.8563184225559235
step 201/334, epoch 78/501 --> loss:0.8246911680698394
step 251/334, epoch 78/501 --> loss:0.8225267612934113
step 301/334, epoch 78/501 --> loss:0.8191693484783172
step 51/334, epoch 79/501 --> loss:0.8454902267456055
step 101/334, epoch 79/501 --> loss:0.8137551808357238
step 151/334, epoch 79/501 --> loss:0.8553896009922027
step 201/334, epoch 79/501 --> loss:0.8499709284305572
step 251/334, epoch 79/501 --> loss:0.8312347280979157
step 301/334, epoch 79/501 --> loss:0.8355848336219788
step 51/334, epoch 80/501 --> loss:0.8467582964897156
step 101/334, epoch 80/501 --> loss:0.8334921944141388
step 151/334, epoch 80/501 --> loss:0.8423493134975434
step 201/334, epoch 80/501 --> loss:0.8391446697711945
step 251/334, epoch 80/501 --> loss:0.8305551564693451
step 301/334, epoch 80/501 --> loss:0.820040249824524
step 51/334, epoch 81/501 --> loss:0.8357825815677643
step 101/334, epoch 81/501 --> loss:0.8366492676734925
step 151/334, epoch 81/501 --> loss:0.8088885688781738
step 201/334, epoch 81/501 --> loss:0.8670531332492828
step 251/334, epoch 81/501 --> loss:0.8475070261955261
step 301/334, epoch 81/501 --> loss:0.8174923443794251

##########train dataset##########
acc--> [98.40017661289981]
F1--> {'F1': [0.8336460326642258], 'precision': [0.747538031437241], 'recall': [0.9421865082773875]}
##########eval dataset##########
acc--> [98.22530537869122]
F1--> {'F1': [0.812435198048683], 'precision': [0.7252590218879995], 'recall': [0.9234444796341699]}
step 51/334, epoch 82/501 --> loss:0.8369323134422302
step 101/334, epoch 82/501 --> loss:0.8320060563087464
step 151/334, epoch 82/501 --> loss:0.826613484621048
step 201/334, epoch 82/501 --> loss:0.8366655075550079
step 251/334, epoch 82/501 --> loss:0.853293912410736
step 301/334, epoch 82/501 --> loss:0.8171032416820526
step 51/334, epoch 83/501 --> loss:0.8224812483787537
step 101/334, epoch 83/501 --> loss:0.8473923766613006
step 151/334, epoch 83/501 --> loss:0.8446326518058777
step 201/334, epoch 83/501 --> loss:0.8220020604133605
step 251/334, epoch 83/501 --> loss:0.8322792339324951
step 301/334, epoch 83/501 --> loss:0.8569464921951294
step 51/334, epoch 84/501 --> loss:0.8293449056148529
step 101/334, epoch 84/501 --> loss:0.8214486920833588
step 151/334, epoch 84/501 --> loss:0.8522884249687195
step 201/334, epoch 84/501 --> loss:0.8441118156909942
step 251/334, epoch 84/501 --> loss:0.8422633922100067
step 301/334, epoch 84/501 --> loss:0.8327604806423188
step 51/334, epoch 85/501 --> loss:0.8393851470947266
step 101/334, epoch 85/501 --> loss:0.8264149951934815
step 151/334, epoch 85/501 --> loss:0.8355711722373962
step 201/334, epoch 85/501 --> loss:0.8311225092411041
step 251/334, epoch 85/501 --> loss:0.8411496865749359
step 301/334, epoch 85/501 --> loss:0.8220030319690704
step 51/334, epoch 86/501 --> loss:0.8246037840843201
step 101/334, epoch 86/501 --> loss:0.833955397605896
step 151/334, epoch 86/501 --> loss:0.8186727893352509
step 201/334, epoch 86/501 --> loss:0.8491081702709198
step 251/334, epoch 86/501 --> loss:0.829118971824646
step 301/334, epoch 86/501 --> loss:0.8352486073970795
step 51/334, epoch 87/501 --> loss:0.8329796528816223
step 101/334, epoch 87/501 --> loss:0.8052048182487488
step 151/334, epoch 87/501 --> loss:0.8429599440097809
step 201/334, epoch 87/501 --> loss:0.8424038767814637
step 251/334, epoch 87/501 --> loss:0.8458218407630921
step 301/334, epoch 87/501 --> loss:0.8157091128826142
step 51/334, epoch 88/501 --> loss:0.8201958167552948
step 101/334, epoch 88/501 --> loss:0.8231348025798798
step 151/334, epoch 88/501 --> loss:0.8355563700199127
step 201/334, epoch 88/501 --> loss:0.8554195010662079
step 251/334, epoch 88/501 --> loss:0.8464544713497162
step 301/334, epoch 88/501 --> loss:0.8214124751091003
step 51/334, epoch 89/501 --> loss:0.853812255859375
step 101/334, epoch 89/501 --> loss:0.8052313899993897
step 151/334, epoch 89/501 --> loss:0.8211439979076386
step 201/334, epoch 89/501 --> loss:0.8216457545757294
step 251/334, epoch 89/501 --> loss:0.8459571957588196
step 301/334, epoch 89/501 --> loss:0.834797238111496
step 51/334, epoch 90/501 --> loss:0.8425002253055572
step 101/334, epoch 90/501 --> loss:0.84554123878479
step 151/334, epoch 90/501 --> loss:0.8304900360107422
step 201/334, epoch 90/501 --> loss:0.8037572014331817
step 251/334, epoch 90/501 --> loss:0.8354534327983856
step 301/334, epoch 90/501 --> loss:0.8398521590232849
step 51/334, epoch 91/501 --> loss:0.8457549941539765
step 101/334, epoch 91/501 --> loss:0.826583286523819
step 151/334, epoch 91/501 --> loss:0.8257066488265992
step 201/334, epoch 91/501 --> loss:0.8199940812587738
step 251/334, epoch 91/501 --> loss:0.8306067276000977
step 301/334, epoch 91/501 --> loss:0.8468506681919098

##########train dataset##########
acc--> [98.30704164451086]
F1--> {'F1': [0.8296819446020303], 'precision': [0.7252848506580711], 'recall': [0.9691994013354499]}
##########eval dataset##########
acc--> [98.12999334860318]
F1--> {'F1': [0.8078870728474923], 'precision': [0.7057045037779701], 'recall': [0.9446841811948882]}
step 51/334, epoch 92/501 --> loss:0.8261251652240753
step 101/334, epoch 92/501 --> loss:0.8305523443222046
step 151/334, epoch 92/501 --> loss:0.8300582945346833
step 201/334, epoch 92/501 --> loss:0.8213395464420319
step 251/334, epoch 92/501 --> loss:0.8504300451278687
step 301/334, epoch 92/501 --> loss:0.8228209590911866
step 51/334, epoch 93/501 --> loss:0.8169008529186249
step 101/334, epoch 93/501 --> loss:0.8245819342136383
step 151/334, epoch 93/501 --> loss:0.8442969512939453
step 201/334, epoch 93/501 --> loss:0.8360729992389679
step 251/334, epoch 93/501 --> loss:0.8396998763084411
step 301/334, epoch 93/501 --> loss:0.8334850633144378
step 51/334, epoch 94/501 --> loss:0.8430019915103912
step 101/334, epoch 94/501 --> loss:0.8149767601490021
step 151/334, epoch 94/501 --> loss:0.8245852816104889
step 201/334, epoch 94/501 --> loss:0.8763026678562165
step 251/334, epoch 94/501 --> loss:0.8352707207202912
step 301/334, epoch 94/501 --> loss:0.8330848157405853
step 51/334, epoch 95/501 --> loss:0.8406997919082642
step 101/334, epoch 95/501 --> loss:0.8232367837429047
step 151/334, epoch 95/501 --> loss:0.8393149220943451
step 201/334, epoch 95/501 --> loss:0.8003050768375397
step 251/334, epoch 95/501 --> loss:0.8426986300945282
step 301/334, epoch 95/501 --> loss:0.8373971247673034
step 51/334, epoch 96/501 --> loss:0.8568466544151306
step 101/334, epoch 96/501 --> loss:0.8127521371841431
step 151/334, epoch 96/501 --> loss:0.8197798085212707
step 201/334, epoch 96/501 --> loss:0.8240738606452942
step 251/334, epoch 96/501 --> loss:0.8461978352069854
step 301/334, epoch 96/501 --> loss:0.823870769739151
step 51/334, epoch 97/501 --> loss:0.8590506768226623
step 101/334, epoch 97/501 --> loss:0.8270751821994782
step 151/334, epoch 97/501 --> loss:0.8390068352222443
step 201/334, epoch 97/501 --> loss:0.8213176214694977
step 251/334, epoch 97/501 --> loss:0.8148689913749695
step 301/334, epoch 97/501 --> loss:0.8131303322315216
step 51/334, epoch 98/501 --> loss:0.8307076346874237
step 101/334, epoch 98/501 --> loss:0.8201927769184113
step 151/334, epoch 98/501 --> loss:0.8253680193424224
step 201/334, epoch 98/501 --> loss:0.8288722383975983
step 251/334, epoch 98/501 --> loss:0.8278822863101959
step 301/334, epoch 98/501 --> loss:0.836913571357727
step 51/334, epoch 99/501 --> loss:0.8367044603824616
step 101/334, epoch 99/501 --> loss:0.8199530959129333
step 151/334, epoch 99/501 --> loss:0.8350699913501739
step 201/334, epoch 99/501 --> loss:0.840840585231781
step 251/334, epoch 99/501 --> loss:0.8291224360466003
step 301/334, epoch 99/501 --> loss:0.8534609317779541
step 51/334, epoch 100/501 --> loss:0.8192209708690643
step 101/334, epoch 100/501 --> loss:0.8445624709129333
step 151/334, epoch 100/501 --> loss:0.8305877864360809
step 201/334, epoch 100/501 --> loss:0.8451250886917114
step 251/334, epoch 100/501 --> loss:0.846989735364914
step 301/334, epoch 100/501 --> loss:0.812264142036438
step 51/334, epoch 101/501 --> loss:0.8141418123245239
step 101/334, epoch 101/501 --> loss:0.8253948032855988
step 151/334, epoch 101/501 --> loss:0.8278735411167145
step 201/334, epoch 101/501 --> loss:0.8506428515911102
step 251/334, epoch 101/501 --> loss:0.8467785823345184
step 301/334, epoch 101/501 --> loss:0.8237981081008912

##########train dataset##########
acc--> [98.42856328139365]
F1--> {'F1': [0.8409324823396106], 'precision': [0.7385269905591081], 'recall': [0.9763225200892776]}
##########eval dataset##########
acc--> [98.17634776090564]
F1--> {'F1': [0.8123113334919009], 'precision': [0.7105261481763766], 'recall': [0.9481480649944103]}
step 51/334, epoch 102/501 --> loss:0.8322879242897033
step 101/334, epoch 102/501 --> loss:0.8297062242031097
step 151/334, epoch 102/501 --> loss:0.8345465171337128
step 201/334, epoch 102/501 --> loss:0.8182209920883179
step 251/334, epoch 102/501 --> loss:0.8282312512397766
step 301/334, epoch 102/501 --> loss:0.8482715797424316
step 51/334, epoch 103/501 --> loss:0.8180833828449249
step 101/334, epoch 103/501 --> loss:0.8349591958522796
step 151/334, epoch 103/501 --> loss:0.8510684442520141
step 201/334, epoch 103/501 --> loss:0.8157449340820313
step 251/334, epoch 103/501 --> loss:0.8107543814182282
step 301/334, epoch 103/501 --> loss:0.8363784217834472
step 51/334, epoch 104/501 --> loss:0.814045261144638
step 101/334, epoch 104/501 --> loss:0.8500479531288146
step 151/334, epoch 104/501 --> loss:0.8357711684703827
step 201/334, epoch 104/501 --> loss:0.8367639470100403
step 251/334, epoch 104/501 --> loss:0.8430751919746399
step 301/334, epoch 104/501 --> loss:0.8262058115005493
step 51/334, epoch 105/501 --> loss:0.8273948907852173
step 101/334, epoch 105/501 --> loss:0.8251738464832306
step 151/334, epoch 105/501 --> loss:0.8387706029415131
step 201/334, epoch 105/501 --> loss:0.8477570736408233
step 251/334, epoch 105/501 --> loss:0.8314222526550293
step 301/334, epoch 105/501 --> loss:0.8313669562339783
step 51/334, epoch 106/501 --> loss:0.8286765336990356
step 101/334, epoch 106/501 --> loss:0.8197703444957734
step 151/334, epoch 106/501 --> loss:0.8146711480617523
step 201/334, epoch 106/501 --> loss:0.843200763463974
step 251/334, epoch 106/501 --> loss:0.8361776077747345
step 301/334, epoch 106/501 --> loss:0.833856303691864
step 51/334, epoch 107/501 --> loss:0.8297809708118439
step 101/334, epoch 107/501 --> loss:0.834072835445404
step 151/334, epoch 107/501 --> loss:0.8483936500549316
step 201/334, epoch 107/501 --> loss:0.8158400237560273
step 251/334, epoch 107/501 --> loss:0.8482159328460693
step 301/334, epoch 107/501 --> loss:0.8121260523796081
step 51/334, epoch 108/501 --> loss:0.8347159945964813
step 101/334, epoch 108/501 --> loss:0.831938978433609
step 151/334, epoch 108/501 --> loss:0.8199686217308044
step 201/334, epoch 108/501 --> loss:0.8138943684101104
step 251/334, epoch 108/501 --> loss:0.8304343461990357
step 301/334, epoch 108/501 --> loss:0.8449514579772949
step 51/334, epoch 109/501 --> loss:0.8427487564086914
step 101/334, epoch 109/501 --> loss:0.8153103947639465
step 151/334, epoch 109/501 --> loss:0.8384771227836609
step 201/334, epoch 109/501 --> loss:0.8265682494640351
step 251/334, epoch 109/501 --> loss:0.8317274725437165
step 301/334, epoch 109/501 --> loss:0.8276379334926606
step 51/334, epoch 110/501 --> loss:0.8514327096939087
step 101/334, epoch 110/501 --> loss:0.8208050167560578
step 151/334, epoch 110/501 --> loss:0.8253319656848908
step 201/334, epoch 110/501 --> loss:0.8212224316596984
step 251/334, epoch 110/501 --> loss:0.8308312892913818
step 301/334, epoch 110/501 --> loss:0.835374528169632
step 51/334, epoch 111/501 --> loss:0.8288794708251953
step 101/334, epoch 111/501 --> loss:0.8099105274677276
step 151/334, epoch 111/501 --> loss:0.8134399843215943
step 201/334, epoch 111/501 --> loss:0.8467440438270569
step 251/334, epoch 111/501 --> loss:0.8287958025932312
step 301/334, epoch 111/501 --> loss:0.8508807802200318

##########train dataset##########
acc--> [97.62953424614368]
F1--> {'F1': [0.7777711051181986], 'precision': [0.6469234939432578], 'recall': [0.9749848720369532]}
##########eval dataset##########
acc--> [97.33784592995781]
F1--> {'F1': [0.7486343125807147], 'precision': [0.6166771898592585], 'recall': [0.9524532235352423]}
step 51/334, epoch 112/501 --> loss:0.8255911564826965
step 101/334, epoch 112/501 --> loss:0.8223592019081116
step 151/334, epoch 112/501 --> loss:0.8247458577156067
step 201/334, epoch 112/501 --> loss:0.80541867852211
step 251/334, epoch 112/501 --> loss:0.8407898175716401
step 301/334, epoch 112/501 --> loss:0.8585747039318085
step 51/334, epoch 113/501 --> loss:0.8263541448116303
step 101/334, epoch 113/501 --> loss:0.8317394101619721
step 151/334, epoch 113/501 --> loss:0.8322869145870209
step 201/334, epoch 113/501 --> loss:0.8133491671085358
step 251/334, epoch 113/501 --> loss:0.8384000873565673
step 301/334, epoch 113/501 --> loss:0.828914657831192
step 51/334, epoch 114/501 --> loss:0.8504082822799682
step 101/334, epoch 114/501 --> loss:0.8528418135643006
step 151/334, epoch 114/501 --> loss:0.8248118138313294
step 201/334, epoch 114/501 --> loss:0.8226966738700867
step 251/334, epoch 114/501 --> loss:0.8261462759971618
step 301/334, epoch 114/501 --> loss:0.8346305108070373
step 51/334, epoch 115/501 --> loss:0.8510631191730499
step 101/334, epoch 115/501 --> loss:0.814644775390625
step 151/334, epoch 115/501 --> loss:0.8583803403377533
step 201/334, epoch 115/501 --> loss:0.8120327484607697
step 251/334, epoch 115/501 --> loss:0.8312216579914093
step 301/334, epoch 115/501 --> loss:0.8338063824176788
step 51/334, epoch 116/501 --> loss:0.8411233687400818
step 101/334, epoch 116/501 --> loss:0.840559401512146
step 151/334, epoch 116/501 --> loss:0.7942731630802154
step 201/334, epoch 116/501 --> loss:0.8479379987716675
step 251/334, epoch 116/501 --> loss:0.8281271076202392
step 301/334, epoch 116/501 --> loss:0.8467725145816803
step 51/334, epoch 117/501 --> loss:0.8130107009410859
step 101/334, epoch 117/501 --> loss:0.849341881275177
step 151/334, epoch 117/501 --> loss:0.8416951739788056
step 201/334, epoch 117/501 --> loss:0.8387569677829743
step 251/334, epoch 117/501 --> loss:0.8289829409122467
step 301/334, epoch 117/501 --> loss:0.8135047328472137
step 51/334, epoch 118/501 --> loss:0.8482581114768982
step 101/334, epoch 118/501 --> loss:0.8195007717609406
step 151/334, epoch 118/501 --> loss:0.8373868715763092
step 201/334, epoch 118/501 --> loss:0.8553614354133606
step 251/334, epoch 118/501 --> loss:0.8086435902118683
step 301/334, epoch 118/501 --> loss:0.8302783131599426
step 51/334, epoch 119/501 --> loss:0.8226603245735169
step 101/334, epoch 119/501 --> loss:0.8400669610500335
step 151/334, epoch 119/501 --> loss:0.8406519186496735
step 201/334, epoch 119/501 --> loss:0.8366929054260254
step 251/334, epoch 119/501 --> loss:0.8162762069702149
step 301/334, epoch 119/501 --> loss:0.839896389245987
step 51/334, epoch 120/501 --> loss:0.8147909331321717
step 101/334, epoch 120/501 --> loss:0.8174177205562592
step 151/334, epoch 120/501 --> loss:0.857061105966568
step 201/334, epoch 120/501 --> loss:0.8262969720363617
step 251/334, epoch 120/501 --> loss:0.8366602420806885
step 301/334, epoch 120/501 --> loss:0.8178338015079498
step 51/334, epoch 121/501 --> loss:0.8231546783447266
step 101/334, epoch 121/501 --> loss:0.8206423711776734
step 151/334, epoch 121/501 --> loss:0.8690135562419892
step 201/334, epoch 121/501 --> loss:0.806448986530304
step 251/334, epoch 121/501 --> loss:0.851215569972992
step 301/334, epoch 121/501 --> loss:0.8437430047988892

##########train dataset##########
acc--> [98.60994577139367]
F1--> {'F1': [0.8558761313509053], 'precision': [0.7657133089449901], 'recall': [0.9701189125072814]}
##########eval dataset##########
acc--> [98.41052611876837]
F1--> {'F1': [0.8290465668311382], 'precision': [0.7504871974355746], 'recall': [0.925988028751958]}
save model!
step 51/334, epoch 122/501 --> loss:0.8376688373088836
step 101/334, epoch 122/501 --> loss:0.8292463004589081
step 151/334, epoch 122/501 --> loss:0.8241180992126464
step 201/334, epoch 122/501 --> loss:0.8336313331127166
step 251/334, epoch 122/501 --> loss:0.8414203763008118
step 301/334, epoch 122/501 --> loss:0.8189039003849029
step 51/334, epoch 123/501 --> loss:0.8399681305885315
step 101/334, epoch 123/501 --> loss:0.8452284407615661
step 151/334, epoch 123/501 --> loss:0.8318385982513428
step 201/334, epoch 123/501 --> loss:0.8087902247905732
step 251/334, epoch 123/501 --> loss:0.829215407371521
step 301/334, epoch 123/501 --> loss:0.8242246723175048
step 51/334, epoch 124/501 --> loss:0.8132895648479461
step 101/334, epoch 124/501 --> loss:0.8437389183044434
step 151/334, epoch 124/501 --> loss:0.8317730844020843
step 201/334, epoch 124/501 --> loss:0.8245544219017029
step 251/334, epoch 124/501 --> loss:0.8193003618717194
step 301/334, epoch 124/501 --> loss:0.8425102829933167
step 51/334, epoch 125/501 --> loss:0.8187171518802643
step 101/334, epoch 125/501 --> loss:0.833767215013504
step 151/334, epoch 125/501 --> loss:0.8408284854888916
step 201/334, epoch 125/501 --> loss:0.8386917662620544
step 251/334, epoch 125/501 --> loss:0.8497437560558319
step 301/334, epoch 125/501 --> loss:0.8157541000843048
step 51/334, epoch 126/501 --> loss:0.8415635311603546
step 101/334, epoch 126/501 --> loss:0.8219402086734772
step 151/334, epoch 126/501 --> loss:0.8351949584484101
step 201/334, epoch 126/501 --> loss:0.8265778946876526
step 251/334, epoch 126/501 --> loss:0.8228367722034454
step 301/334, epoch 126/501 --> loss:0.8422355937957764
step 51/334, epoch 127/501 --> loss:0.8324100112915039
step 101/334, epoch 127/501 --> loss:0.8232030308246613
step 151/334, epoch 127/501 --> loss:0.8093752014636993
step 201/334, epoch 127/501 --> loss:0.8276793611049652
step 251/334, epoch 127/501 --> loss:0.8454586088657379
step 301/334, epoch 127/501 --> loss:0.8406701266765595
step 51/334, epoch 128/501 --> loss:0.8261367034912109
step 101/334, epoch 128/501 --> loss:0.8170334601402283
step 151/334, epoch 128/501 --> loss:0.8320808696746826
step 201/334, epoch 128/501 --> loss:0.8396515607833862
step 251/334, epoch 128/501 --> loss:0.8466424357891082
step 301/334, epoch 128/501 --> loss:0.8365668225288391
step 51/334, epoch 129/501 --> loss:0.8623627543449401
step 101/334, epoch 129/501 --> loss:0.8058321225643158
step 151/334, epoch 129/501 --> loss:0.83327103972435
step 201/334, epoch 129/501 --> loss:0.8324744844436646
step 251/334, epoch 129/501 --> loss:0.8140374767780304
step 301/334, epoch 129/501 --> loss:0.8448881852626801
step 51/334, epoch 130/501 --> loss:0.8337976717948914
step 101/334, epoch 130/501 --> loss:0.8284474575519561
step 151/334, epoch 130/501 --> loss:0.8407430469989776
step 201/334, epoch 130/501 --> loss:0.8291042792797089
step 251/334, epoch 130/501 --> loss:0.8613416624069213
step 301/334, epoch 130/501 --> loss:0.8123124647140503
step 51/334, epoch 131/501 --> loss:0.8379422914981842
step 101/334, epoch 131/501 --> loss:0.840820734500885
step 151/334, epoch 131/501 --> loss:0.8338442289829254
step 201/334, epoch 131/501 --> loss:0.8120411574840546
step 251/334, epoch 131/501 --> loss:0.8280315554141998
step 301/334, epoch 131/501 --> loss:0.8501965916156768

##########train dataset##########
acc--> [97.0630678649577]
F1--> {'F1': [0.7398838513151836], 'precision': [0.5936401424193145], 'recall': [0.9817515759968442]}
##########eval dataset##########
acc--> [96.77507428880314]
F1--> {'F1': [0.7111685430886838], 'precision': [0.5669244088519492], 'recall': [0.9538791890327123]}
step 51/334, epoch 132/501 --> loss:0.8388017892837525
step 101/334, epoch 132/501 --> loss:0.8406889760494232
step 151/334, epoch 132/501 --> loss:0.8277737498283386
step 201/334, epoch 132/501 --> loss:0.8227584552764893
step 251/334, epoch 132/501 --> loss:0.8037470090389252
step 301/334, epoch 132/501 --> loss:0.8354572355747223
step 51/334, epoch 133/501 --> loss:0.8276819837093353
step 101/334, epoch 133/501 --> loss:0.8594530498981476
step 151/334, epoch 133/501 --> loss:0.8354782080650329
step 201/334, epoch 133/501 --> loss:0.8182304120063781
step 251/334, epoch 133/501 --> loss:0.8385289275646209
step 301/334, epoch 133/501 --> loss:0.8060421252250671
step 51/334, epoch 134/501 --> loss:0.8512765574455261
step 101/334, epoch 134/501 --> loss:0.8202902901172638
step 151/334, epoch 134/501 --> loss:0.845815063714981
step 201/334, epoch 134/501 --> loss:0.8402764129638672
step 251/334, epoch 134/501 --> loss:0.8156513667106629
step 301/334, epoch 134/501 --> loss:0.8242648422718049
step 51/334, epoch 135/501 --> loss:0.822035276889801
step 101/334, epoch 135/501 --> loss:0.8139511382579804
step 151/334, epoch 135/501 --> loss:0.8284912014007568
step 201/334, epoch 135/501 --> loss:0.8271047580242157
step 251/334, epoch 135/501 --> loss:0.8396738684177398
step 301/334, epoch 135/501 --> loss:0.8366556537151336
step 51/334, epoch 136/501 --> loss:0.8267320621013642
step 101/334, epoch 136/501 --> loss:0.8273664677143097
step 151/334, epoch 136/501 --> loss:0.8388902246952057
step 201/334, epoch 136/501 --> loss:0.8364458775520325
step 251/334, epoch 136/501 --> loss:0.8256437635421753
step 301/334, epoch 136/501 --> loss:0.8284886467456818
step 51/334, epoch 137/501 --> loss:0.8295913922786713
step 101/334, epoch 137/501 --> loss:0.8127232992649078
step 151/334, epoch 137/501 --> loss:0.8340435993671417
step 201/334, epoch 137/501 --> loss:0.8098229610919953
step 251/334, epoch 137/501 --> loss:0.854892303943634
step 301/334, epoch 137/501 --> loss:0.8288467407226563
step 51/334, epoch 138/501 --> loss:0.8315898263454438
step 101/334, epoch 138/501 --> loss:0.8336257588863373
step 151/334, epoch 138/501 --> loss:0.8260903096199036
step 201/334, epoch 138/501 --> loss:0.8519653427600861
step 251/334, epoch 138/501 --> loss:0.8309833788871765
step 301/334, epoch 138/501 --> loss:0.8240810608863831
step 51/334, epoch 139/501 --> loss:0.8327267324924469
step 101/334, epoch 139/501 --> loss:0.8149314224720001
step 151/334, epoch 139/501 --> loss:0.8707393503189087
step 201/334, epoch 139/501 --> loss:0.8086796367168426
step 251/334, epoch 139/501 --> loss:0.8211105823516845
step 301/334, epoch 139/501 --> loss:0.8345539247989655
step 51/334, epoch 140/501 --> loss:0.8112514197826386
step 101/334, epoch 140/501 --> loss:0.8535836148262024
step 151/334, epoch 140/501 --> loss:0.8155321455001832
step 201/334, epoch 140/501 --> loss:0.8342679262161254
step 251/334, epoch 140/501 --> loss:0.8333637022972107
step 301/334, epoch 140/501 --> loss:0.8439734840393066
step 51/334, epoch 141/501 --> loss:0.8304364693164825
step 101/334, epoch 141/501 --> loss:0.8212637376785278
step 151/334, epoch 141/501 --> loss:0.8328386807441711
step 201/334, epoch 141/501 --> loss:0.8369162356853486
step 251/334, epoch 141/501 --> loss:0.823865385055542
step 301/334, epoch 141/501 --> loss:0.829447809457779

##########train dataset##########
acc--> [97.99674241159028]
F1--> {'F1': [0.8075042379409761], 'precision': [0.6829751876280442], 'recall': [0.9875857251195528]}
##########eval dataset##########
acc--> [97.56113888396965]
F1--> {'F1': [0.7659139288247089], 'precision': [0.637731325155791], 'recall': [0.9586030605643107]}
step 51/334, epoch 142/501 --> loss:0.8181606590747833
step 101/334, epoch 142/501 --> loss:0.829983696937561
step 151/334, epoch 142/501 --> loss:0.8066229975223541
step 201/334, epoch 142/501 --> loss:0.8410242056846619
step 251/334, epoch 142/501 --> loss:0.8431568026542664
step 301/334, epoch 142/501 --> loss:0.8321804440021515
step 51/334, epoch 143/501 --> loss:0.81289235830307
step 101/334, epoch 143/501 --> loss:0.8506824231147766
step 151/334, epoch 143/501 --> loss:0.8369735980033874
step 201/334, epoch 143/501 --> loss:0.8294606518745422
step 251/334, epoch 143/501 --> loss:0.8386522495746612
step 301/334, epoch 143/501 --> loss:0.8200486934185028
step 51/334, epoch 144/501 --> loss:0.8367881751060486
step 101/334, epoch 144/501 --> loss:0.8264361226558685
step 151/334, epoch 144/501 --> loss:0.8430965924263001
step 201/334, epoch 144/501 --> loss:0.8075493478775024
step 251/334, epoch 144/501 --> loss:0.8147117054462433
step 301/334, epoch 144/501 --> loss:0.829090793132782
step 51/334, epoch 145/501 --> loss:0.8245936381816864
step 101/334, epoch 145/501 --> loss:0.8379319393634796
step 151/334, epoch 145/501 --> loss:0.8307966840267181
step 201/334, epoch 145/501 --> loss:0.8203034770488739
step 251/334, epoch 145/501 --> loss:0.8144970750808715
step 301/334, epoch 145/501 --> loss:0.8432988893985748
step 51/334, epoch 146/501 --> loss:0.8264528751373291
step 101/334, epoch 146/501 --> loss:0.8385366523265838
step 151/334, epoch 146/501 --> loss:0.8302456402778625
step 201/334, epoch 146/501 --> loss:0.8332532870769501
step 251/334, epoch 146/501 --> loss:0.8142286896705627
step 301/334, epoch 146/501 --> loss:0.8302080976963043
step 51/334, epoch 147/501 --> loss:0.8511467063426972
step 101/334, epoch 147/501 --> loss:0.8226355183124542
step 151/334, epoch 147/501 --> loss:0.8369993638992309
step 201/334, epoch 147/501 --> loss:0.8246630072593689
step 251/334, epoch 147/501 --> loss:0.8518408024311066
step 301/334, epoch 147/501 --> loss:0.8043535268306732
step 51/334, epoch 148/501 --> loss:0.8176168239116669
step 101/334, epoch 148/501 --> loss:0.8476347732543945
step 151/334, epoch 148/501 --> loss:0.8257044076919555
step 201/334, epoch 148/501 --> loss:0.832329763174057
step 251/334, epoch 148/501 --> loss:0.8255990374088288
step 301/334, epoch 148/501 --> loss:0.8301028370857239
step 51/334, epoch 149/501 --> loss:0.8329593634605408
step 101/334, epoch 149/501 --> loss:0.8166274154186248
step 151/334, epoch 149/501 --> loss:0.8246786451339722
step 201/334, epoch 149/501 --> loss:0.8233441460132599
step 251/334, epoch 149/501 --> loss:0.8301261246204377
step 301/334, epoch 149/501 --> loss:0.8375437831878663
step 51/334, epoch 150/501 --> loss:0.849862699508667
step 101/334, epoch 150/501 --> loss:0.8299574625492095
step 151/334, epoch 150/501 --> loss:0.8346498847007752
step 201/334, epoch 150/501 --> loss:0.8200848686695099
step 251/334, epoch 150/501 --> loss:0.8440562975406647
step 301/334, epoch 150/501 --> loss:0.8148900103569031
step 51/334, epoch 151/501 --> loss:0.8209051966667176
step 101/334, epoch 151/501 --> loss:0.8119639229774475
step 151/334, epoch 151/501 --> loss:0.8177227556705475
step 201/334, epoch 151/501 --> loss:0.8561410486698151
step 251/334, epoch 151/501 --> loss:0.8296963155269623
step 301/334, epoch 151/501 --> loss:0.8343719661235809

##########train dataset##########
acc--> [98.77635668313394]
F1--> {'F1': [0.8728667302796285], 'precision': [0.782192137044982], 'recall': [0.9873331890007023]}
##########eval dataset##########
acc--> [98.36792699724744]
F1--> {'F1': [0.8283877749800225], 'precision': [0.7365501376353317], 'recall': [0.9464023317054474]}
step 51/334, epoch 152/501 --> loss:0.8391666412353516
step 101/334, epoch 152/501 --> loss:0.8367991113662719
step 151/334, epoch 152/501 --> loss:0.8096950316429138
step 201/334, epoch 152/501 --> loss:0.846473652124405
step 251/334, epoch 152/501 --> loss:0.821641719341278
step 301/334, epoch 152/501 --> loss:0.8123337018489838
step 51/334, epoch 153/501 --> loss:0.8468183422088623
step 101/334, epoch 153/501 --> loss:0.8208300840854644
step 151/334, epoch 153/501 --> loss:0.8289370512962342
step 201/334, epoch 153/501 --> loss:0.8247875297069549
step 251/334, epoch 153/501 --> loss:0.8351309788227081
step 301/334, epoch 153/501 --> loss:0.8270665419101715
step 51/334, epoch 154/501 --> loss:0.8338985025882721
step 101/334, epoch 154/501 --> loss:0.829114670753479
step 151/334, epoch 154/501 --> loss:0.8271864414215088
step 201/334, epoch 154/501 --> loss:0.8291070067882538
step 251/334, epoch 154/501 --> loss:0.8274009203910828
step 301/334, epoch 154/501 --> loss:0.8353355550765991
step 51/334, epoch 155/501 --> loss:0.8395492088794708
step 101/334, epoch 155/501 --> loss:0.8290294575691223
step 151/334, epoch 155/501 --> loss:0.8531118750572204
step 201/334, epoch 155/501 --> loss:0.8284181380271911
step 251/334, epoch 155/501 --> loss:0.8260228252410888
step 301/334, epoch 155/501 --> loss:0.8084137153625488
step 51/334, epoch 156/501 --> loss:0.8402421486377716
step 101/334, epoch 156/501 --> loss:0.8255925309658051
step 151/334, epoch 156/501 --> loss:0.8521538281440735
step 201/334, epoch 156/501 --> loss:0.8075616347789765
step 251/334, epoch 156/501 --> loss:0.8246405529975891
step 301/334, epoch 156/501 --> loss:0.8406087648868561
step 51/334, epoch 157/501 --> loss:0.8305700635910034
step 101/334, epoch 157/501 --> loss:0.8435267245769501
step 151/334, epoch 157/501 --> loss:0.8393826842308044
step 201/334, epoch 157/501 --> loss:0.8292006587982178
step 251/334, epoch 157/501 --> loss:0.8140246295928955
step 301/334, epoch 157/501 --> loss:0.8183002316951752
step 51/334, epoch 158/501 --> loss:0.8193917810916901
step 101/334, epoch 158/501 --> loss:0.8258584964275361
step 151/334, epoch 158/501 --> loss:0.8349447607994079
step 201/334, epoch 158/501 --> loss:0.8239492011070252
step 251/334, epoch 158/501 --> loss:0.8258983707427978
step 301/334, epoch 158/501 --> loss:0.8259304535388946
step 51/334, epoch 159/501 --> loss:0.817127491235733
step 101/334, epoch 159/501 --> loss:0.8348611557483673
step 151/334, epoch 159/501 --> loss:0.8276443111896515
step 201/334, epoch 159/501 --> loss:0.8280841135978698
step 251/334, epoch 159/501 --> loss:0.8328352653980255
step 301/334, epoch 159/501 --> loss:0.8311981642246247
step 51/334, epoch 160/501 --> loss:0.8252955770492554
step 101/334, epoch 160/501 --> loss:0.8262141525745392
step 151/334, epoch 160/501 --> loss:0.8382503855228424
step 201/334, epoch 160/501 --> loss:0.8237841236591339
step 251/334, epoch 160/501 --> loss:0.8324378454685211
step 301/334, epoch 160/501 --> loss:0.8468631017208099
step 51/334, epoch 161/501 --> loss:0.8230067193508148
step 101/334, epoch 161/501 --> loss:0.8166594612598419
step 151/334, epoch 161/501 --> loss:0.8243714892864227
step 201/334, epoch 161/501 --> loss:0.8313697969913483
step 251/334, epoch 161/501 --> loss:0.8268272662162781
step 301/334, epoch 161/501 --> loss:0.836688414812088

##########train dataset##########
acc--> [99.06316470420161]
F1--> {'F1': [0.8990464547488475], 'precision': [0.8300952697540416], 'recall': [0.9805018718325539]}
##########eval dataset##########
acc--> [98.73469996013633]
F1--> {'F1': [0.8584908906125334], 'precision': [0.8030673460324476], 'recall': [0.9221431177344883]}
save model!
step 51/334, epoch 162/501 --> loss:0.8425267672538758
step 101/334, epoch 162/501 --> loss:0.8238668537139893
step 151/334, epoch 162/501 --> loss:0.8261482644081116
step 201/334, epoch 162/501 --> loss:0.8283180010318756
step 251/334, epoch 162/501 --> loss:0.8200304889678955
step 301/334, epoch 162/501 --> loss:0.8361224603652954
step 51/334, epoch 163/501 --> loss:0.808194340467453
step 101/334, epoch 163/501 --> loss:0.8338563239574432
step 151/334, epoch 163/501 --> loss:0.8319765889644622
step 201/334, epoch 163/501 --> loss:0.8158625328540802
step 251/334, epoch 163/501 --> loss:0.8353630590438843
step 301/334, epoch 163/501 --> loss:0.8408592903614044
step 51/334, epoch 164/501 --> loss:0.8124099349975586
step 101/334, epoch 164/501 --> loss:0.8470315754413604
step 151/334, epoch 164/501 --> loss:0.8110892510414124
step 201/334, epoch 164/501 --> loss:0.838462519645691
step 251/334, epoch 164/501 --> loss:0.8435824620723724
step 301/334, epoch 164/501 --> loss:0.8400122225284576
step 51/334, epoch 165/501 --> loss:0.8570758724212646
step 101/334, epoch 165/501 --> loss:0.8573765408992767
step 151/334, epoch 165/501 --> loss:0.8263674461841584
step 201/334, epoch 165/501 --> loss:0.8115395331382751
step 251/334, epoch 165/501 --> loss:0.8118336462974548
step 301/334, epoch 165/501 --> loss:0.847848904132843
step 51/334, epoch 166/501 --> loss:0.8322025585174561
step 101/334, epoch 166/501 --> loss:0.8443532121181488
step 151/334, epoch 166/501 --> loss:0.8341397297382355
step 201/334, epoch 166/501 --> loss:0.8111316406726837
step 251/334, epoch 166/501 --> loss:0.8332129001617432
step 301/334, epoch 166/501 --> loss:0.8443352603912353
step 51/334, epoch 167/501 --> loss:0.8379981100559235
step 101/334, epoch 167/501 --> loss:0.8077827429771424
step 151/334, epoch 167/501 --> loss:0.8289269495010376
step 201/334, epoch 167/501 --> loss:0.8329709506034851
step 251/334, epoch 167/501 --> loss:0.8310679042339325
step 301/334, epoch 167/501 --> loss:0.8372776246070862
step 51/334, epoch 168/501 --> loss:0.8216707885265351
step 101/334, epoch 168/501 --> loss:0.8321816885471344
step 151/334, epoch 168/501 --> loss:0.8289805221557617
step 201/334, epoch 168/501 --> loss:0.8166269111633301
step 251/334, epoch 168/501 --> loss:0.8494438803195954
step 301/334, epoch 168/501 --> loss:0.8424953532218933
step 51/334, epoch 169/501 --> loss:0.8170722770690918
step 101/334, epoch 169/501 --> loss:0.8493584263324737
step 151/334, epoch 169/501 --> loss:0.8428577947616577
step 201/334, epoch 169/501 --> loss:0.8164639508724213
step 251/334, epoch 169/501 --> loss:0.8326371622085571
step 301/334, epoch 169/501 --> loss:0.8311830198764801
step 51/334, epoch 170/501 --> loss:0.8330425214767456
step 101/334, epoch 170/501 --> loss:0.8221503710746765
step 151/334, epoch 170/501 --> loss:0.8442619037628174
step 201/334, epoch 170/501 --> loss:0.8189184606075287
step 251/334, epoch 170/501 --> loss:0.840477888584137
step 301/334, epoch 170/501 --> loss:0.8275314450263977
step 51/334, epoch 171/501 --> loss:0.8178917849063874
step 101/334, epoch 171/501 --> loss:0.8300907015800476
step 151/334, epoch 171/501 --> loss:0.8318162953853607
step 201/334, epoch 171/501 --> loss:0.8457558798789978
step 251/334, epoch 171/501 --> loss:0.8392485225200653
step 301/334, epoch 171/501 --> loss:0.8293035948276519

##########train dataset##########
acc--> [98.84420050902168]
F1--> {'F1': [0.8791551819590345], 'precision': [0.7917977823176477], 'recall': [0.9881913814983082]}
##########eval dataset##########
acc--> [98.41973746124704]
F1--> {'F1': [0.8316791448375839], 'precision': [0.7470214133321613], 'recall': [0.9379899050223302]}
step 51/334, epoch 172/501 --> loss:0.8217441689968109
step 101/334, epoch 172/501 --> loss:0.8134684789180756
step 151/334, epoch 172/501 --> loss:0.8337878108024597
step 201/334, epoch 172/501 --> loss:0.8220820796489715
step 251/334, epoch 172/501 --> loss:0.8463477218151092
step 301/334, epoch 172/501 --> loss:0.8155246472358704
step 51/334, epoch 173/501 --> loss:0.8289510023593902
step 101/334, epoch 173/501 --> loss:0.8153045916557312
step 151/334, epoch 173/501 --> loss:0.8320759475231171
step 201/334, epoch 173/501 --> loss:0.807812557220459
step 251/334, epoch 173/501 --> loss:0.8446660375595093
step 301/334, epoch 173/501 --> loss:0.8476438057422638
step 51/334, epoch 174/501 --> loss:0.8257176792621612
step 101/334, epoch 174/501 --> loss:0.8271078407764435
step 151/334, epoch 174/501 --> loss:0.828502357006073
step 201/334, epoch 174/501 --> loss:0.8502101564407348
step 251/334, epoch 174/501 --> loss:0.8183689832687377
step 301/334, epoch 174/501 --> loss:0.818114550113678
step 51/334, epoch 175/501 --> loss:0.8221271133422852
step 101/334, epoch 175/501 --> loss:0.8360489356517792
step 151/334, epoch 175/501 --> loss:0.8564243066310883
step 201/334, epoch 175/501 --> loss:0.8160956823825836
step 251/334, epoch 175/501 --> loss:0.8190504765510559
step 301/334, epoch 175/501 --> loss:0.8522569811344147
step 51/334, epoch 176/501 --> loss:0.8134988069534301
step 101/334, epoch 176/501 --> loss:0.8292654228210449
step 151/334, epoch 176/501 --> loss:0.8431708312034607
step 201/334, epoch 176/501 --> loss:0.827566305398941
step 251/334, epoch 176/501 --> loss:0.8212034118175506
step 301/334, epoch 176/501 --> loss:0.8377569675445556
step 51/334, epoch 177/501 --> loss:0.8485369598865509
step 101/334, epoch 177/501 --> loss:0.850613168478012
step 151/334, epoch 177/501 --> loss:0.8181835043430329
step 201/334, epoch 177/501 --> loss:0.8370823788642884
step 251/334, epoch 177/501 --> loss:0.8278036391735077
step 301/334, epoch 177/501 --> loss:0.8160371196269989
step 51/334, epoch 178/501 --> loss:0.8433627998828888
step 101/334, epoch 178/501 --> loss:0.825822627544403
step 151/334, epoch 178/501 --> loss:0.8408967530727387
step 201/334, epoch 178/501 --> loss:0.8236377251148224
step 251/334, epoch 178/501 --> loss:0.8309469437599182
step 301/334, epoch 178/501 --> loss:0.8327286434173584
step 51/334, epoch 179/501 --> loss:0.8389842259883881
step 101/334, epoch 179/501 --> loss:0.8570646524429322
step 151/334, epoch 179/501 --> loss:0.8360276472568512
step 201/334, epoch 179/501 --> loss:0.8261198651790619
step 251/334, epoch 179/501 --> loss:0.8102891457080841
step 301/334, epoch 179/501 --> loss:0.8279340767860413
step 51/334, epoch 180/501 --> loss:0.8595368313789368
step 101/334, epoch 180/501 --> loss:0.8315147912502289
step 151/334, epoch 180/501 --> loss:0.8262640035152435
step 201/334, epoch 180/501 --> loss:0.8445557951927185
step 251/334, epoch 180/501 --> loss:0.8178005182743072
step 301/334, epoch 180/501 --> loss:0.8205641806125641
step 51/334, epoch 181/501 --> loss:0.8311988830566406
step 101/334, epoch 181/501 --> loss:0.8439068937301636
step 151/334, epoch 181/501 --> loss:0.8019292342662812
step 201/334, epoch 181/501 --> loss:0.8163099384307861
step 251/334, epoch 181/501 --> loss:0.8280304646492005
step 301/334, epoch 181/501 --> loss:0.8465818476676941

##########train dataset##########
acc--> [99.10231977533286]
F1--> {'F1': [0.9034394657790786], 'precision': [0.8328839701116728], 'recall': [0.9870670040980056]}
##########eval dataset##########
acc--> [98.68100384088292]
F1--> {'F1': [0.8540162156078681], 'precision': [0.7917281493758429], 'recall': [0.9269537467260655]}
step 51/334, epoch 182/501 --> loss:0.8307913339138031
step 101/334, epoch 182/501 --> loss:0.8369965589046479
step 151/334, epoch 182/501 --> loss:0.8152433216571808
step 201/334, epoch 182/501 --> loss:0.8111222279071808
step 251/334, epoch 182/501 --> loss:0.8217706024646759
step 301/334, epoch 182/501 --> loss:0.8475791871547699
step 51/334, epoch 183/501 --> loss:0.8219291126728058
step 101/334, epoch 183/501 --> loss:0.8382853829860687
step 151/334, epoch 183/501 --> loss:0.830620551109314
step 201/334, epoch 183/501 --> loss:0.8228259301185608
step 251/334, epoch 183/501 --> loss:0.8201350116729736
step 301/334, epoch 183/501 --> loss:0.8415146040916442
step 51/334, epoch 184/501 --> loss:0.8365544831752777
step 101/334, epoch 184/501 --> loss:0.7979175674915314
step 151/334, epoch 184/501 --> loss:0.833071802854538
step 201/334, epoch 184/501 --> loss:0.8454470837116241
step 251/334, epoch 184/501 --> loss:0.8255564892292022
step 301/334, epoch 184/501 --> loss:0.8302351212501526
step 51/334, epoch 185/501 --> loss:0.8134135794639588
step 101/334, epoch 185/501 --> loss:0.8374633908271789
step 151/334, epoch 185/501 --> loss:0.8189588129520416
step 201/334, epoch 185/501 --> loss:0.8379367697238922
step 251/334, epoch 185/501 --> loss:0.8298702156543731
step 301/334, epoch 185/501 --> loss:0.8330768597126007
step 51/334, epoch 186/501 --> loss:0.8399195325374603
step 101/334, epoch 186/501 --> loss:0.7980902516841888
step 151/334, epoch 186/501 --> loss:0.8342980659008026
step 201/334, epoch 186/501 --> loss:0.8592523944377899
step 251/334, epoch 186/501 --> loss:0.8180160558223725
step 301/334, epoch 186/501 --> loss:0.8294462442398072
step 51/334, epoch 187/501 --> loss:0.8126238131523132
step 101/334, epoch 187/501 --> loss:0.8252400135993958
step 151/334, epoch 187/501 --> loss:0.8420702016353607
step 201/334, epoch 187/501 --> loss:0.8434767818450928
step 251/334, epoch 187/501 --> loss:0.818310911655426
step 301/334, epoch 187/501 --> loss:0.8419623744487762
step 51/334, epoch 188/501 --> loss:0.82280526638031
step 101/334, epoch 188/501 --> loss:0.8358720910549163
step 151/334, epoch 188/501 --> loss:0.8460160803794861
step 201/334, epoch 188/501 --> loss:0.8130480778217316
step 251/334, epoch 188/501 --> loss:0.8353318893909454
step 301/334, epoch 188/501 --> loss:0.8156175005435944
step 51/334, epoch 189/501 --> loss:0.827203962802887
step 101/334, epoch 189/501 --> loss:0.8308431732654572
step 151/334, epoch 189/501 --> loss:0.8165038585662842
step 201/334, epoch 189/501 --> loss:0.8314504039287567
step 251/334, epoch 189/501 --> loss:0.845717865228653
step 301/334, epoch 189/501 --> loss:0.8175404179096222
step 51/334, epoch 190/501 --> loss:0.8455633199214936
step 101/334, epoch 190/501 --> loss:0.8322031962871551
step 151/334, epoch 190/501 --> loss:0.8455302929878235
step 201/334, epoch 190/501 --> loss:0.8127846813201904
step 251/334, epoch 190/501 --> loss:0.8367474222183228
step 301/334, epoch 190/501 --> loss:0.7943638384342193
step 51/334, epoch 191/501 --> loss:0.8533137321472168
step 101/334, epoch 191/501 --> loss:0.8237628769874573
step 151/334, epoch 191/501 --> loss:0.8237973153591156
step 201/334, epoch 191/501 --> loss:0.8212969148159027
step 251/334, epoch 191/501 --> loss:0.8091370105743408
step 301/334, epoch 191/501 --> loss:0.8404271352291107

##########train dataset##########
acc--> [99.07965558183727]
F1--> {'F1': [0.9013830293895476], 'precision': [0.8282957429550872], 'recall': [0.9886286804644966]}
##########eval dataset##########
acc--> [98.59912969099639]
F1--> {'F1': [0.8476866274879618], 'precision': [0.7742089641025521], 'recall': [0.9365859095285886]}
step 51/334, epoch 192/501 --> loss:0.8141878283023835
step 101/334, epoch 192/501 --> loss:0.84356809258461
step 151/334, epoch 192/501 --> loss:0.8317631578445435
step 201/334, epoch 192/501 --> loss:0.8337818908691407
step 251/334, epoch 192/501 --> loss:0.829683985710144
step 301/334, epoch 192/501 --> loss:0.8274424207210541
step 51/334, epoch 193/501 --> loss:0.8417122054100037
step 101/334, epoch 193/501 --> loss:0.8331564950942993
step 151/334, epoch 193/501 --> loss:0.8308045744895936
step 201/334, epoch 193/501 --> loss:0.8061166894435883
step 251/334, epoch 193/501 --> loss:0.8419407999515534
step 301/334, epoch 193/501 --> loss:0.8390235865116119
step 51/334, epoch 194/501 --> loss:0.8164565420150757
step 101/334, epoch 194/501 --> loss:0.8224661993980408
step 151/334, epoch 194/501 --> loss:0.8483585774898529
step 201/334, epoch 194/501 --> loss:0.7988450729846954
step 251/334, epoch 194/501 --> loss:0.8544990932941436
step 301/334, epoch 194/501 --> loss:0.8172996091842651
step 51/334, epoch 195/501 --> loss:0.7994333243370056
step 101/334, epoch 195/501 --> loss:0.8591171312332153
step 151/334, epoch 195/501 --> loss:0.8423154258728027
step 201/334, epoch 195/501 --> loss:0.8176871919631958
step 251/334, epoch 195/501 --> loss:0.8158591246604919
step 301/334, epoch 195/501 --> loss:0.8354305529594421
step 51/334, epoch 196/501 --> loss:0.830934898853302
step 101/334, epoch 196/501 --> loss:0.8285190987586976
step 151/334, epoch 196/501 --> loss:0.8021270275115967
step 201/334, epoch 196/501 --> loss:0.8308359706401824
step 251/334, epoch 196/501 --> loss:0.8332198691368103
step 301/334, epoch 196/501 --> loss:0.8223719346523285
step 51/334, epoch 197/501 --> loss:0.8395851683616639
step 101/334, epoch 197/501 --> loss:0.8253212928771972
step 151/334, epoch 197/501 --> loss:0.8326744341850281
step 201/334, epoch 197/501 --> loss:0.7896180427074433
step 251/334, epoch 197/501 --> loss:0.8223994934558868
step 301/334, epoch 197/501 --> loss:0.8538144290447235
step 51/334, epoch 198/501 --> loss:0.8294144642353057
step 101/334, epoch 198/501 --> loss:0.8091717267036438
step 151/334, epoch 198/501 --> loss:0.838025529384613
step 201/334, epoch 198/501 --> loss:0.8284407234191895
step 251/334, epoch 198/501 --> loss:0.8436672985553741
step 301/334, epoch 198/501 --> loss:0.8029445922374725
step 51/334, epoch 199/501 --> loss:0.8241717696189881
step 101/334, epoch 199/501 --> loss:0.8419597923755646
step 151/334, epoch 199/501 --> loss:0.8360835325717926
step 201/334, epoch 199/501 --> loss:0.8161284244060516
step 251/334, epoch 199/501 --> loss:0.829438580274582
step 301/334, epoch 199/501 --> loss:0.8165472173690795
step 51/334, epoch 200/501 --> loss:0.8144195139408111
step 101/334, epoch 200/501 --> loss:0.8429626214504242
step 151/334, epoch 200/501 --> loss:0.8188449168205261
step 201/334, epoch 200/501 --> loss:0.8350857186317444
step 251/334, epoch 200/501 --> loss:0.8310919976234437
step 301/334, epoch 200/501 --> loss:0.8393918776512146
step 51/334, epoch 201/501 --> loss:0.8434842550754547
step 101/334, epoch 201/501 --> loss:0.8253854870796203
step 151/334, epoch 201/501 --> loss:0.8178363609313964
step 201/334, epoch 201/501 --> loss:0.8259634137153625
step 251/334, epoch 201/501 --> loss:0.8371320950984955
step 301/334, epoch 201/501 --> loss:0.8363854551315307

##########train dataset##########
acc--> [99.06876073779068]
F1--> {'F1': [0.9003049821280842], 'precision': [0.8266852367056758], 'recall': [0.9883308276938609]}
##########eval dataset##########
acc--> [98.63323502026797]
F1--> {'F1': [0.8502879536503657], 'precision': [0.7813954882131533], 'recall': [0.9325148838742058]}
step 51/334, epoch 202/501 --> loss:0.8295793616771698
step 101/334, epoch 202/501 --> loss:0.8531658959388733
step 151/334, epoch 202/501 --> loss:0.8051329159736633
step 201/334, epoch 202/501 --> loss:0.8050364673137664
step 251/334, epoch 202/501 --> loss:0.8354495811462402
step 301/334, epoch 202/501 --> loss:0.8291120409965516
step 51/334, epoch 203/501 --> loss:0.801251357793808
step 101/334, epoch 203/501 --> loss:0.8413549399375916
step 151/334, epoch 203/501 --> loss:0.8231914937496185
step 201/334, epoch 203/501 --> loss:0.8136248683929443
step 251/334, epoch 203/501 --> loss:0.8398738622665405
step 301/334, epoch 203/501 --> loss:0.8427458882331849
step 51/334, epoch 204/501 --> loss:0.843229329586029
step 101/334, epoch 204/501 --> loss:0.8159364366531372
step 151/334, epoch 204/501 --> loss:0.8228243601322174
step 201/334, epoch 204/501 --> loss:0.8430259084701538
step 251/334, epoch 204/501 --> loss:0.8207733476161957
step 301/334, epoch 204/501 --> loss:0.8189559710025788
step 51/334, epoch 205/501 --> loss:0.8239643049240112
step 101/334, epoch 205/501 --> loss:0.8316720271110535
step 151/334, epoch 205/501 --> loss:0.8424322807788849
step 201/334, epoch 205/501 --> loss:0.8163354408740997
step 251/334, epoch 205/501 --> loss:0.8320567071437835
step 301/334, epoch 205/501 --> loss:0.8320579469203949
step 51/334, epoch 206/501 --> loss:0.8286456060409546
step 101/334, epoch 206/501 --> loss:0.8252299153804779
step 151/334, epoch 206/501 --> loss:0.8250032794475556
step 201/334, epoch 206/501 --> loss:0.8570805501937866
step 251/334, epoch 206/501 --> loss:0.8230799317359925
step 301/334, epoch 206/501 --> loss:0.8326890790462493
step 51/334, epoch 207/501 --> loss:0.815937271118164
step 101/334, epoch 207/501 --> loss:0.825525233745575
step 151/334, epoch 207/501 --> loss:0.8367702245712281
step 201/334, epoch 207/501 --> loss:0.8285343813896179
step 251/334, epoch 207/501 --> loss:0.8551113414764404
step 301/334, epoch 207/501 --> loss:0.8065858042240143
step 51/334, epoch 208/501 --> loss:0.845186904668808
step 101/334, epoch 208/501 --> loss:0.8336657953262329
step 151/334, epoch 208/501 --> loss:0.82580197930336
step 201/334, epoch 208/501 --> loss:0.8087033462524414
step 251/334, epoch 208/501 --> loss:0.8336982011795044
step 301/334, epoch 208/501 --> loss:0.8265671694278717
step 51/334, epoch 209/501 --> loss:0.8341361558437348
step 101/334, epoch 209/501 --> loss:0.8532880198955536
step 151/334, epoch 209/501 --> loss:0.8368676829338074
step 201/334, epoch 209/501 --> loss:0.8087829697132111
step 251/334, epoch 209/501 --> loss:0.8094980096817017
step 301/334, epoch 209/501 --> loss:0.8355049622058869
step 51/334, epoch 210/501 --> loss:0.8290058028697967
step 101/334, epoch 210/501 --> loss:0.8224296355247498
step 151/334, epoch 210/501 --> loss:0.8181843459606171
step 201/334, epoch 210/501 --> loss:0.8300271844863891
step 251/334, epoch 210/501 --> loss:0.8406253349781037
step 301/334, epoch 210/501 --> loss:0.8211806225776672
step 51/334, epoch 211/501 --> loss:0.8151940143108368
step 101/334, epoch 211/501 --> loss:0.860217057466507
step 151/334, epoch 211/501 --> loss:0.8279213356971741
step 201/334, epoch 211/501 --> loss:0.8251906192302704
step 251/334, epoch 211/501 --> loss:0.8104549300670624
step 301/334, epoch 211/501 --> loss:0.8285441410541534

##########train dataset##########
acc--> [99.27188085177423]
F1--> {'F1': [0.9203106244399779], 'precision': [0.861120405153226], 'recall': [0.9882499435216092]}
##########eval dataset##########
acc--> [98.80576880480221]
F1--> {'F1': [0.8649945967410209], 'precision': [0.8168436158618445], 'recall': [0.9191891946054513]}
save model!
step 51/334, epoch 212/501 --> loss:0.8235457992553711
step 101/334, epoch 212/501 --> loss:0.8054240620136262
step 151/334, epoch 212/501 --> loss:0.8396046340465546
step 201/334, epoch 212/501 --> loss:0.8567159008979798
step 251/334, epoch 212/501 --> loss:0.8195018017292023
step 301/334, epoch 212/501 --> loss:0.8183805906772613
step 51/334, epoch 213/501 --> loss:0.8085830581188201
step 101/334, epoch 213/501 --> loss:0.8453543281555176
step 151/334, epoch 213/501 --> loss:0.8229948151111602
step 201/334, epoch 213/501 --> loss:0.8456757295131684
step 251/334, epoch 213/501 --> loss:0.8145299744606018
step 301/334, epoch 213/501 --> loss:0.8300306451320648
step 51/334, epoch 214/501 --> loss:0.8352128338813781
step 101/334, epoch 214/501 --> loss:0.838075703382492
step 151/334, epoch 214/501 --> loss:0.818257714509964
step 201/334, epoch 214/501 --> loss:0.8167730140686035
step 251/334, epoch 214/501 --> loss:0.8220984935760498
step 301/334, epoch 214/501 --> loss:0.8367614936828613
step 51/334, epoch 215/501 --> loss:0.8190539133548737
step 101/334, epoch 215/501 --> loss:0.8366192734241485
step 151/334, epoch 215/501 --> loss:0.8281945359706878
step 201/334, epoch 215/501 --> loss:0.8268589353561402
step 251/334, epoch 215/501 --> loss:0.823192538022995
step 301/334, epoch 215/501 --> loss:0.8331880569458008
step 51/334, epoch 216/501 --> loss:0.8523421943187713
step 101/334, epoch 216/501 --> loss:0.8398468458652496
step 151/334, epoch 216/501 --> loss:0.8242832398414612
step 201/334, epoch 216/501 --> loss:0.8200235569477081
step 251/334, epoch 216/501 --> loss:0.8423346066474915
step 301/334, epoch 216/501 --> loss:0.805455127954483
step 51/334, epoch 217/501 --> loss:0.8111016345024109
step 101/334, epoch 217/501 --> loss:0.823351981639862
step 151/334, epoch 217/501 --> loss:0.8336944651603698
step 201/334, epoch 217/501 --> loss:0.8265457153320312
step 251/334, epoch 217/501 --> loss:0.8321526670455932
step 301/334, epoch 217/501 --> loss:0.8379491364955902
step 51/334, epoch 218/501 --> loss:0.8163775467872619
step 101/334, epoch 218/501 --> loss:0.8365867960453034
step 151/334, epoch 218/501 --> loss:0.8242887842655182
step 201/334, epoch 218/501 --> loss:0.8433323860168457
step 251/334, epoch 218/501 --> loss:0.8391417694091797
step 301/334, epoch 218/501 --> loss:0.8041966211795807
step 51/334, epoch 219/501 --> loss:0.8101936531066894
step 101/334, epoch 219/501 --> loss:0.8346702182292938
step 151/334, epoch 219/501 --> loss:0.8443352675437927
step 201/334, epoch 219/501 --> loss:0.8320536124706268
step 251/334, epoch 219/501 --> loss:0.8319343876838684
step 301/334, epoch 219/501 --> loss:0.8279156947135925
step 51/334, epoch 220/501 --> loss:0.8463068687915802
step 101/334, epoch 220/501 --> loss:0.8211108148097992
step 151/334, epoch 220/501 --> loss:0.8240334498882294
step 201/334, epoch 220/501 --> loss:0.8211951315402984
step 251/334, epoch 220/501 --> loss:0.8058569633960724
step 301/334, epoch 220/501 --> loss:0.8576107203960419
step 51/334, epoch 221/501 --> loss:0.8291040074825287
step 101/334, epoch 221/501 --> loss:0.8271391379833222
step 151/334, epoch 221/501 --> loss:0.8245922219753266
step 201/334, epoch 221/501 --> loss:0.8359715914726258
step 251/334, epoch 221/501 --> loss:0.8071631193161011
step 301/334, epoch 221/501 --> loss:0.823362842798233

##########train dataset##########
acc--> [99.25140506728367]
F1--> {'F1': [0.9183528732115738], 'precision': [0.8567090711346523], 'recall': [0.98956708478047]}
##########eval dataset##########
acc--> [98.74536042033392]
F1--> {'F1': [0.8583706297088614], 'precision': [0.8095494657865059], 'recall': [0.9134694564814904]}
step 51/334, epoch 222/501 --> loss:0.8314936399459839
step 101/334, epoch 222/501 --> loss:0.817956006526947
step 151/334, epoch 222/501 --> loss:0.8340381312370301
step 201/334, epoch 222/501 --> loss:0.8226798176765442
step 251/334, epoch 222/501 --> loss:0.8269886696338653
step 301/334, epoch 222/501 --> loss:0.8454840171337128
step 51/334, epoch 223/501 --> loss:0.8411598014831543
step 101/334, epoch 223/501 --> loss:0.827730450630188
step 151/334, epoch 223/501 --> loss:0.8333663225173951
step 201/334, epoch 223/501 --> loss:0.8490091323852539
step 251/334, epoch 223/501 --> loss:0.8134621214866639
step 301/334, epoch 223/501 --> loss:0.8045108926296234
step 51/334, epoch 224/501 --> loss:0.8156839323043823
step 101/334, epoch 224/501 --> loss:0.8187546670436859
step 151/334, epoch 224/501 --> loss:0.8301496934890747
step 201/334, epoch 224/501 --> loss:0.8131768655776977
step 251/334, epoch 224/501 --> loss:0.820393624305725
step 301/334, epoch 224/501 --> loss:0.8450703382492065
step 51/334, epoch 225/501 --> loss:0.810678893327713
step 101/334, epoch 225/501 --> loss:0.8248747646808624
step 151/334, epoch 225/501 --> loss:0.8198009395599365
step 201/334, epoch 225/501 --> loss:0.8293033611774444
step 251/334, epoch 225/501 --> loss:0.8378567862510681
step 301/334, epoch 225/501 --> loss:0.8294196212291718
step 51/334, epoch 226/501 --> loss:0.8154161560535431
step 101/334, epoch 226/501 --> loss:0.8367608165740967
step 151/334, epoch 226/501 --> loss:0.8470956158638
step 201/334, epoch 226/501 --> loss:0.8456702566146851
step 251/334, epoch 226/501 --> loss:0.8256024730205536
step 301/334, epoch 226/501 --> loss:0.8121165096759796
step 51/334, epoch 227/501 --> loss:0.8248980844020843
step 101/334, epoch 227/501 --> loss:0.8202383947372437
step 151/334, epoch 227/501 --> loss:0.8306263029575348
step 201/334, epoch 227/501 --> loss:0.8420702040195465
step 251/334, epoch 227/501 --> loss:0.8262181627750397
step 301/334, epoch 227/501 --> loss:0.8130669105052948
step 51/334, epoch 228/501 --> loss:0.8263194060325623
step 101/334, epoch 228/501 --> loss:0.8268816351890564
step 151/334, epoch 228/501 --> loss:0.8513039970397949
step 201/334, epoch 228/501 --> loss:0.8408477199077606
step 251/334, epoch 228/501 --> loss:0.800693109035492
step 301/334, epoch 228/501 --> loss:0.8328252768516541
step 51/334, epoch 229/501 --> loss:0.8322349488735199
step 101/334, epoch 229/501 --> loss:0.822897320985794
step 151/334, epoch 229/501 --> loss:0.8141574621200561
step 201/334, epoch 229/501 --> loss:0.8391637551784515
step 251/334, epoch 229/501 --> loss:0.840359206199646
step 301/334, epoch 229/501 --> loss:0.8386556267738342
step 51/334, epoch 230/501 --> loss:0.8185625350475312
step 101/334, epoch 230/501 --> loss:0.832746775150299
step 151/334, epoch 230/501 --> loss:0.8357251715660096
step 201/334, epoch 230/501 --> loss:0.8104675471782684
step 251/334, epoch 230/501 --> loss:0.8384533202648163
step 301/334, epoch 230/501 --> loss:0.8261763644218445
step 51/334, epoch 231/501 --> loss:0.8341108226776123
step 101/334, epoch 231/501 --> loss:0.8187616121768951
step 151/334, epoch 231/501 --> loss:0.8218011319637298
step 201/334, epoch 231/501 --> loss:0.8269702005386352
step 251/334, epoch 231/501 --> loss:0.8381736934185028
step 301/334, epoch 231/501 --> loss:0.8389799094200134

##########train dataset##########
acc--> [95.61054671708854]
F1--> {'F1': [0.6575025334620644], 'precision': [0.492130370985009], 'recall': [0.9902843519039782]}
##########eval dataset##########
acc--> [95.25273574201026]
F1--> {'F1': [0.6275798052498252], 'precision': [0.46593115905624255], 'recall': [0.9609992342556283]}
step 51/334, epoch 232/501 --> loss:0.8194220161437988
step 101/334, epoch 232/501 --> loss:0.8303922128677368
step 151/334, epoch 232/501 --> loss:0.8279174470901489
step 201/334, epoch 232/501 --> loss:0.8446568322181701
step 251/334, epoch 232/501 --> loss:0.8214881873130798
step 301/334, epoch 232/501 --> loss:0.8255937540531159
step 51/334, epoch 233/501 --> loss:0.8033918941020965
step 101/334, epoch 233/501 --> loss:0.8500871074199676
step 151/334, epoch 233/501 --> loss:0.8364106976985931
step 201/334, epoch 233/501 --> loss:0.8293720066547394
step 251/334, epoch 233/501 --> loss:0.808547956943512
step 301/334, epoch 233/501 --> loss:0.833954644203186
step 51/334, epoch 234/501 --> loss:0.8202016508579254
step 101/334, epoch 234/501 --> loss:0.8525074863433838
step 151/334, epoch 234/501 --> loss:0.8175649690628052
step 201/334, epoch 234/501 --> loss:0.8401070713996888
step 251/334, epoch 234/501 --> loss:0.8161324310302734
step 301/334, epoch 234/501 --> loss:0.8229411625862122
step 51/334, epoch 235/501 --> loss:0.8354326951503753
step 101/334, epoch 235/501 --> loss:0.8298683142662049
step 151/334, epoch 235/501 --> loss:0.8260610020160675
step 201/334, epoch 235/501 --> loss:0.8252392792701722
step 251/334, epoch 235/501 --> loss:0.8345152366161347
step 301/334, epoch 235/501 --> loss:0.8186062633991241
step 51/334, epoch 236/501 --> loss:0.8415033042430877
step 101/334, epoch 236/501 --> loss:0.8280264043807983
step 151/334, epoch 236/501 --> loss:0.8340076959133148
step 201/334, epoch 236/501 --> loss:0.8247544038295745
step 251/334, epoch 236/501 --> loss:0.8236597871780396
step 301/334, epoch 236/501 --> loss:0.822851756811142
step 51/334, epoch 237/501 --> loss:0.821110303401947
step 101/334, epoch 237/501 --> loss:0.8252243208885193
step 151/334, epoch 237/501 --> loss:0.8291991746425629
step 201/334, epoch 237/501 --> loss:0.841572368144989
step 251/334, epoch 237/501 --> loss:0.8108518052101136
step 301/334, epoch 237/501 --> loss:0.8321606636047363
step 51/334, epoch 238/501 --> loss:0.8071056342124939
step 101/334, epoch 238/501 --> loss:0.818440545797348
step 151/334, epoch 238/501 --> loss:0.8269993829727172
step 201/334, epoch 238/501 --> loss:0.8374160289764404
step 251/334, epoch 238/501 --> loss:0.8416543591022492
step 301/334, epoch 238/501 --> loss:0.8289346671104432
step 51/334, epoch 239/501 --> loss:0.8427683818340301
step 101/334, epoch 239/501 --> loss:0.8333410239219665
step 151/334, epoch 239/501 --> loss:0.8168207442760468
step 201/334, epoch 239/501 --> loss:0.8403603112697602
step 251/334, epoch 239/501 --> loss:0.8286830377578736
step 301/334, epoch 239/501 --> loss:0.8239013314247131
step 51/334, epoch 240/501 --> loss:0.8227427482604981
step 101/334, epoch 240/501 --> loss:0.8368404734134675
step 151/334, epoch 240/501 --> loss:0.8320476019382477
step 201/334, epoch 240/501 --> loss:0.8401901471614838
step 251/334, epoch 240/501 --> loss:0.830705543756485
step 301/334, epoch 240/501 --> loss:0.8064673042297363
step 51/334, epoch 241/501 --> loss:0.8106441342830658
step 101/334, epoch 241/501 --> loss:0.8383826458454132
step 151/334, epoch 241/501 --> loss:0.8225201058387757
step 201/334, epoch 241/501 --> loss:0.8227161800861359
step 251/334, epoch 241/501 --> loss:0.8285907363891601
step 301/334, epoch 241/501 --> loss:0.8276149868965149

##########train dataset##########
acc--> [99.33371928285901]
F1--> {'F1': [0.9265807434973302], 'precision': [0.8721712969377884], 'recall': [0.9882417408042238]}
##########eval dataset##########
acc--> [98.80437175119296]
F1--> {'F1': [0.8607947846944086], 'precision': [0.8350696179950062], 'recall': [0.8881659456909716]}
step 51/334, epoch 242/501 --> loss:0.8322751629352569
step 101/334, epoch 242/501 --> loss:0.8315835750102997
step 151/334, epoch 242/501 --> loss:0.8382031881809234
step 201/334, epoch 242/501 --> loss:0.7988255143165588
step 251/334, epoch 242/501 --> loss:0.8306688141822814
step 301/334, epoch 242/501 --> loss:0.8340300929546356
step 51/334, epoch 243/501 --> loss:0.8469764006137848
step 101/334, epoch 243/501 --> loss:0.8358587515354157
step 151/334, epoch 243/501 --> loss:0.828302161693573
step 201/334, epoch 243/501 --> loss:0.8208714973926544
step 251/334, epoch 243/501 --> loss:0.8272220587730408
step 301/334, epoch 243/501 --> loss:0.8080569624900817
step 51/334, epoch 244/501 --> loss:0.8296027541160583
step 101/334, epoch 244/501 --> loss:0.828123323917389
step 151/334, epoch 244/501 --> loss:0.8284098970890045
step 201/334, epoch 244/501 --> loss:0.812965532541275
step 251/334, epoch 244/501 --> loss:0.8364526879787445
step 301/334, epoch 244/501 --> loss:0.8197333264350891
step 51/334, epoch 245/501 --> loss:0.8303262388706207
step 101/334, epoch 245/501 --> loss:0.8269131529331207
step 151/334, epoch 245/501 --> loss:0.8295742440223693
step 201/334, epoch 245/501 --> loss:0.8401587200164795
step 251/334, epoch 245/501 --> loss:0.8245612096786499
step 301/334, epoch 245/501 --> loss:0.8214158606529236
step 51/334, epoch 246/501 --> loss:0.8051922154426575
step 101/334, epoch 246/501 --> loss:0.8306229472160339
step 151/334, epoch 246/501 --> loss:0.8599724805355072
step 201/334, epoch 246/501 --> loss:0.8374172914028167
step 251/334, epoch 246/501 --> loss:0.8171365225315094
step 301/334, epoch 246/501 --> loss:0.8278170168399811
step 51/334, epoch 247/501 --> loss:0.8203435230255127
step 101/334, epoch 247/501 --> loss:0.8421369361877441
step 151/334, epoch 247/501 --> loss:0.8313078451156616
step 201/334, epoch 247/501 --> loss:0.8291605341434479
step 251/334, epoch 247/501 --> loss:0.8251674580574035
step 301/334, epoch 247/501 --> loss:0.8197007429599762
step 51/334, epoch 248/501 --> loss:0.8312680649757386
step 101/334, epoch 248/501 --> loss:0.810208797454834
step 151/334, epoch 248/501 --> loss:0.8427615630626678
step 201/334, epoch 248/501 --> loss:0.8143848872184754
step 251/334, epoch 248/501 --> loss:0.8259116077423095
step 301/334, epoch 248/501 --> loss:0.8434560537338257
step 51/334, epoch 249/501 --> loss:0.804669258594513
step 101/334, epoch 249/501 --> loss:0.8102742004394531
step 151/334, epoch 249/501 --> loss:0.8353401529788971
step 201/334, epoch 249/501 --> loss:0.8197792077064514
step 251/334, epoch 249/501 --> loss:0.839708434343338
step 301/334, epoch 249/501 --> loss:0.843321157693863
step 51/334, epoch 250/501 --> loss:0.8157677936553955
step 101/334, epoch 250/501 --> loss:0.8198059606552124
step 151/334, epoch 250/501 --> loss:0.8391463088989258
step 201/334, epoch 250/501 --> loss:0.8366868698596954
step 251/334, epoch 250/501 --> loss:0.8211854791641235
step 301/334, epoch 250/501 --> loss:0.8461428117752076
step 51/334, epoch 251/501 --> loss:0.8470054042339324
step 101/334, epoch 251/501 --> loss:0.8199989664554596
step 151/334, epoch 251/501 --> loss:0.8129350399971008
step 201/334, epoch 251/501 --> loss:0.8451162672042847
step 251/334, epoch 251/501 --> loss:0.8273451519012451
step 301/334, epoch 251/501 --> loss:0.8224490189552307

##########train dataset##########
acc--> [98.88302374297045]
F1--> {'F1': [0.8830415840226574], 'precision': [0.7962409422952135], 'recall': [0.9910949417465931]}
##########eval dataset##########
acc--> [98.34271128095635]
F1--> {'F1': [0.8232327773386342], 'precision': [0.7402437580129859], 'recall': [0.9271917283722919]}
step 51/334, epoch 252/501 --> loss:0.8200859165191651
step 101/334, epoch 252/501 --> loss:0.8361632859706879
step 151/334, epoch 252/501 --> loss:0.8342295324802399
step 201/334, epoch 252/501 --> loss:0.8282940435409546
step 251/334, epoch 252/501 --> loss:0.8250380253791809
step 301/334, epoch 252/501 --> loss:0.8487943637371064
step 51/334, epoch 253/501 --> loss:0.823025975227356
step 101/334, epoch 253/501 --> loss:0.8250101470947265
step 151/334, epoch 253/501 --> loss:0.8270551311969757
step 201/334, epoch 253/501 --> loss:0.834399186372757
step 251/334, epoch 253/501 --> loss:0.8156146705150604
step 301/334, epoch 253/501 --> loss:0.8426025915145874
step 51/334, epoch 254/501 --> loss:0.8427428567409515
step 101/334, epoch 254/501 --> loss:0.8362300407886505
step 151/334, epoch 254/501 --> loss:0.8098393893241882
step 201/334, epoch 254/501 --> loss:0.8223371112346649
step 251/334, epoch 254/501 --> loss:0.8122768759727478
step 301/334, epoch 254/501 --> loss:0.8309576380252838
step 51/334, epoch 255/501 --> loss:0.815393921136856
step 101/334, epoch 255/501 --> loss:0.823420581817627
step 151/334, epoch 255/501 --> loss:0.8423029839992523
step 201/334, epoch 255/501 --> loss:0.8216408050060272
step 251/334, epoch 255/501 --> loss:0.8128162121772766
step 301/334, epoch 255/501 --> loss:0.8196004486083984
step 51/334, epoch 256/501 --> loss:0.8005164968967438
step 101/334, epoch 256/501 --> loss:0.8162905597686767
step 151/334, epoch 256/501 --> loss:0.8397717118263245
step 201/334, epoch 256/501 --> loss:0.8306069207191468
step 251/334, epoch 256/501 --> loss:0.8491036641597748
step 301/334, epoch 256/501 --> loss:0.8129165136814117
step 51/334, epoch 257/501 --> loss:0.8465134143829346
step 101/334, epoch 257/501 --> loss:0.7979937088489533
step 151/334, epoch 257/501 --> loss:0.8180964696407318
step 201/334, epoch 257/501 --> loss:0.8597526705265045
step 251/334, epoch 257/501 --> loss:0.8485717177391052
step 301/334, epoch 257/501 --> loss:0.8131913232803345
step 51/334, epoch 258/501 --> loss:0.8270190966129303
step 101/334, epoch 258/501 --> loss:0.8210679888725281
step 151/334, epoch 258/501 --> loss:0.8219934296607971
step 201/334, epoch 258/501 --> loss:0.8387971138954162
step 251/334, epoch 258/501 --> loss:0.8258070456981659
step 301/334, epoch 258/501 --> loss:0.8295688617229462
step 51/334, epoch 259/501 --> loss:0.8428032803535461
step 101/334, epoch 259/501 --> loss:0.7976035130023956
step 151/334, epoch 259/501 --> loss:0.8259105968475342
step 201/334, epoch 259/501 --> loss:0.8373357701301575
step 251/334, epoch 259/501 --> loss:0.8447167766094208
step 301/334, epoch 259/501 --> loss:0.8277815735340118
step 51/334, epoch 260/501 --> loss:0.8328270089626312
step 101/334, epoch 260/501 --> loss:0.8346540701389312
step 151/334, epoch 260/501 --> loss:0.816327588558197
step 201/334, epoch 260/501 --> loss:0.8073301219940185
step 251/334, epoch 260/501 --> loss:0.837394369840622
step 301/334, epoch 260/501 --> loss:0.8292925369739532
step 51/334, epoch 261/501 --> loss:0.8241134262084961
step 101/334, epoch 261/501 --> loss:0.8114908623695374
step 151/334, epoch 261/501 --> loss:0.828951643705368
step 201/334, epoch 261/501 --> loss:0.839489130973816
step 251/334, epoch 261/501 --> loss:0.8051015198230743
step 301/334, epoch 261/501 --> loss:0.8440820038318634

##########train dataset##########
acc--> [97.88708710071239]
F1--> {'F1': [0.7948738569025358], 'precision': [0.6771216672482329], 'recall': [0.9622159977187287]}
##########eval dataset##########
acc--> [97.51882077514317]
F1--> {'F1': [0.7545062264267154], 'precision': [0.6413953433529791], 'recall': [0.9160676900611134]}
step 51/334, epoch 262/501 --> loss:0.8325173830986023
step 101/334, epoch 262/501 --> loss:0.8318213582038879
step 151/334, epoch 262/501 --> loss:0.8349629890918732
step 201/334, epoch 262/501 --> loss:0.8278840947151184
step 251/334, epoch 262/501 --> loss:0.8360255646705628
step 301/334, epoch 262/501 --> loss:0.8139862418174744
step 51/334, epoch 263/501 --> loss:0.8405679595470429
step 101/334, epoch 263/501 --> loss:0.8059287905693054
step 151/334, epoch 263/501 --> loss:0.8336080753803253
step 201/334, epoch 263/501 --> loss:0.8355616354942321
step 251/334, epoch 263/501 --> loss:0.8230672812461853
step 301/334, epoch 263/501 --> loss:0.8357309687137604
step 51/334, epoch 264/501 --> loss:0.8159751582145691
step 101/334, epoch 264/501 --> loss:0.8329507601261139
step 151/334, epoch 264/501 --> loss:0.824944748878479
step 201/334, epoch 264/501 --> loss:0.8404472887516021
step 251/334, epoch 264/501 --> loss:0.8204226434230805
step 301/334, epoch 264/501 --> loss:0.8307323694229126
step 51/334, epoch 265/501 --> loss:0.8227046823501587
step 101/334, epoch 265/501 --> loss:0.8093650031089783
step 151/334, epoch 265/501 --> loss:0.8178346180915832
step 201/334, epoch 265/501 --> loss:0.8298170554637909
step 251/334, epoch 265/501 --> loss:0.8299840319156647
step 301/334, epoch 265/501 --> loss:0.8458729815483094
step 51/334, epoch 266/501 --> loss:0.8525306153297424
step 101/334, epoch 266/501 --> loss:0.8114338111877442
step 151/334, epoch 266/501 --> loss:0.8232438087463378
step 201/334, epoch 266/501 --> loss:0.8210270082950593
step 251/334, epoch 266/501 --> loss:0.8240457880496979
step 301/334, epoch 266/501 --> loss:0.8334354531764984
step 51/334, epoch 267/501 --> loss:0.8376358139514923
step 101/334, epoch 267/501 --> loss:0.8312866067886353
step 151/334, epoch 267/501 --> loss:0.8167715799808503
step 201/334, epoch 267/501 --> loss:0.8332563734054566
step 251/334, epoch 267/501 --> loss:0.8453721511363983
step 301/334, epoch 267/501 --> loss:0.8210678923130036
step 51/334, epoch 268/501 --> loss:0.8382903921604157
step 101/334, epoch 268/501 --> loss:0.8224135267734528
step 151/334, epoch 268/501 --> loss:0.8276345765590668
step 201/334, epoch 268/501 --> loss:0.8040770936012268
step 251/334, epoch 268/501 --> loss:0.8284055805206298
step 301/334, epoch 268/501 --> loss:0.8342766976356506
step 51/334, epoch 269/501 --> loss:0.8389180934429169
step 101/334, epoch 269/501 --> loss:0.8210931921005249
step 151/334, epoch 269/501 --> loss:0.8340583002567291
step 201/334, epoch 269/501 --> loss:0.8339508640766143
step 251/334, epoch 269/501 --> loss:0.828773226737976
step 301/334, epoch 269/501 --> loss:0.8235705065727233
step 51/334, epoch 270/501 --> loss:0.8362877798080445
step 101/334, epoch 270/501 --> loss:0.8074095737934113
step 151/334, epoch 270/501 --> loss:0.8238298618793487
step 201/334, epoch 270/501 --> loss:0.8262458312511444
step 251/334, epoch 270/501 --> loss:0.8246189296245575
step 301/334, epoch 270/501 --> loss:0.8504066359996796
step 51/334, epoch 271/501 --> loss:0.8297187030315399
step 101/334, epoch 271/501 --> loss:0.8520805382728577
step 151/334, epoch 271/501 --> loss:0.8063737797737122
step 201/334, epoch 271/501 --> loss:0.8342259740829467
step 251/334, epoch 271/501 --> loss:0.8269396817684174
step 301/334, epoch 271/501 --> loss:0.8257309317588806

##########train dataset##########
acc--> [99.53287708924405]
F1--> {'F1': [0.9475994741267894], 'precision': [0.9063388066080318], 'recall': [0.9928070236769468]}
##########eval dataset##########
acc--> [98.97122454037677]
F1--> {'F1': [0.8779419966987005], 'precision': [0.8672153593470331], 'recall': [0.8889475646557342]}
save model!
step 51/334, epoch 272/501 --> loss:0.8296028733253479
step 101/334, epoch 272/501 --> loss:0.8242749691009521
step 151/334, epoch 272/501 --> loss:0.8234570431709289
step 201/334, epoch 272/501 --> loss:0.8095070683956146
step 251/334, epoch 272/501 --> loss:0.8350457406044006
step 301/334, epoch 272/501 --> loss:0.833624438047409
step 51/334, epoch 273/501 --> loss:0.8300786483287811
step 101/334, epoch 273/501 --> loss:0.8437421631813049
step 151/334, epoch 273/501 --> loss:0.8319046866893768
step 201/334, epoch 273/501 --> loss:0.8199692285060882
step 251/334, epoch 273/501 --> loss:0.8169534075260162
step 301/334, epoch 273/501 --> loss:0.8385102689266205
step 51/334, epoch 274/501 --> loss:0.8198465192317963
step 101/334, epoch 274/501 --> loss:0.8287805569171905
step 151/334, epoch 274/501 --> loss:0.8358312439918518
step 201/334, epoch 274/501 --> loss:0.8481636273860932
step 251/334, epoch 274/501 --> loss:0.8223848235607147
step 301/334, epoch 274/501 --> loss:0.821811398267746
step 51/334, epoch 275/501 --> loss:0.8348745501041412
step 101/334, epoch 275/501 --> loss:0.8167208874225617
step 151/334, epoch 275/501 --> loss:0.8442674255371094
step 201/334, epoch 275/501 --> loss:0.8221553933620452
step 251/334, epoch 275/501 --> loss:0.8200663244724273
step 301/334, epoch 275/501 --> loss:0.8310886120796204
step 51/334, epoch 276/501 --> loss:0.8506035375595092
step 101/334, epoch 276/501 --> loss:0.8109468078613281
step 151/334, epoch 276/501 --> loss:0.8278473556041718
step 201/334, epoch 276/501 --> loss:0.8310534858703613
step 251/334, epoch 276/501 --> loss:0.828913003206253
step 301/334, epoch 276/501 --> loss:0.8216665697097778
step 51/334, epoch 277/501 --> loss:0.8189271128177643
step 101/334, epoch 277/501 --> loss:0.8101063573360443
step 151/334, epoch 277/501 --> loss:0.8302333188056946
step 201/334, epoch 277/501 --> loss:0.8535726380348205
step 251/334, epoch 277/501 --> loss:0.824330769777298
step 301/334, epoch 277/501 --> loss:0.8240810847282409
step 51/334, epoch 278/501 --> loss:0.8351378202438354
step 101/334, epoch 278/501 --> loss:0.8388136088848114
step 151/334, epoch 278/501 --> loss:0.8245508337020874
step 201/334, epoch 278/501 --> loss:0.8198849189281464
step 251/334, epoch 278/501 --> loss:0.818513935804367
step 301/334, epoch 278/501 --> loss:0.8123832583427429
step 51/334, epoch 279/501 --> loss:0.8281759345531463
step 101/334, epoch 279/501 --> loss:0.8217759478092194
step 151/334, epoch 279/501 --> loss:0.8117577064037323
step 201/334, epoch 279/501 --> loss:0.8318926179409027
step 251/334, epoch 279/501 --> loss:0.8510049891471863
step 301/334, epoch 279/501 --> loss:0.8169934904575348
step 51/334, epoch 280/501 --> loss:0.8442742717266083
step 101/334, epoch 280/501 --> loss:0.8217900502681732
step 151/334, epoch 280/501 --> loss:0.8258845138549805
step 201/334, epoch 280/501 --> loss:0.8286499035358429
step 251/334, epoch 280/501 --> loss:0.8262038004398345
step 301/334, epoch 280/501 --> loss:0.8071281754970551
step 51/334, epoch 281/501 --> loss:0.8399297368526458
step 101/334, epoch 281/501 --> loss:0.8251456880569458
step 151/334, epoch 281/501 --> loss:0.8093089365959167
step 201/334, epoch 281/501 --> loss:0.8306858944892883
step 251/334, epoch 281/501 --> loss:0.8021359491348267
step 301/334, epoch 281/501 --> loss:0.8475284504890442

##########train dataset##########
acc--> [99.51306669551764]
F1--> {'F1': [0.9455576398230072], 'precision': [0.9016697029110002], 'recall': [0.9939476048058551]}
##########eval dataset##########
acc--> [98.98652137628864]
F1--> {'F1': [0.8813184810737463], 'precision': [0.8596581111335365], 'recall': [0.9041091123382353]}
save model!
step 51/334, epoch 282/501 --> loss:0.8064454209804535
step 101/334, epoch 282/501 --> loss:0.8272665619850159
step 151/334, epoch 282/501 --> loss:0.8144529139995575
step 201/334, epoch 282/501 --> loss:0.8185749983787537
step 251/334, epoch 282/501 --> loss:0.8240115225315094
step 301/334, epoch 282/501 --> loss:0.8629582095146179
step 51/334, epoch 283/501 --> loss:0.815470130443573
step 101/334, epoch 283/501 --> loss:0.846841367483139
step 151/334, epoch 283/501 --> loss:0.8197922873497009
step 201/334, epoch 283/501 --> loss:0.8287567162513733
step 251/334, epoch 283/501 --> loss:0.8300456607341766
step 301/334, epoch 283/501 --> loss:0.838818883895874
step 51/334, epoch 284/501 --> loss:0.8291753447055816
step 101/334, epoch 284/501 --> loss:0.8439554631710052
step 151/334, epoch 284/501 --> loss:0.8336246275901794
step 201/334, epoch 284/501 --> loss:0.8163332951068878
step 251/334, epoch 284/501 --> loss:0.822734934091568
step 301/334, epoch 284/501 --> loss:0.8259579515457154
step 51/334, epoch 285/501 --> loss:0.8437167203426361
step 101/334, epoch 285/501 --> loss:0.8203220558166504
step 151/334, epoch 285/501 --> loss:0.8254845976829529
step 201/334, epoch 285/501 --> loss:0.8231513178348542
step 251/334, epoch 285/501 --> loss:0.8101102602481842
step 301/334, epoch 285/501 --> loss:0.8479784107208252
step 51/334, epoch 286/501 --> loss:0.8066795134544372
step 101/334, epoch 286/501 --> loss:0.8306752336025238
step 151/334, epoch 286/501 --> loss:0.8314268577098847
step 201/334, epoch 286/501 --> loss:0.8314297366142273
step 251/334, epoch 286/501 --> loss:0.8207652401924134
step 301/334, epoch 286/501 --> loss:0.8242675626277923
step 51/334, epoch 287/501 --> loss:0.8184996914863586
step 101/334, epoch 287/501 --> loss:0.81442702293396
step 151/334, epoch 287/501 --> loss:0.8356736791133881
step 201/334, epoch 287/501 --> loss:0.8315378999710084
step 251/334, epoch 287/501 --> loss:0.8434879541397095
step 301/334, epoch 287/501 --> loss:0.8182363951206207
step 51/334, epoch 288/501 --> loss:0.8266343426704407
step 101/334, epoch 288/501 --> loss:0.8252896010875702
step 151/334, epoch 288/501 --> loss:0.8490684461593628
step 201/334, epoch 288/501 --> loss:0.8148608875274658
step 251/334, epoch 288/501 --> loss:0.8240493094921112
step 301/334, epoch 288/501 --> loss:0.8094700074195862
step 51/334, epoch 289/501 --> loss:0.8155179572105408
step 101/334, epoch 289/501 --> loss:0.8306147050857544
step 151/334, epoch 289/501 --> loss:0.8382683491706848
step 201/334, epoch 289/501 --> loss:0.8250196850299836
step 251/334, epoch 289/501 --> loss:0.8338035833835602
step 301/334, epoch 289/501 --> loss:0.821493068933487
step 51/334, epoch 290/501 --> loss:0.8213182604312896
step 101/334, epoch 290/501 --> loss:0.8166853272914887
step 151/334, epoch 290/501 --> loss:0.8274653267860412
step 201/334, epoch 290/501 --> loss:0.8371622383594512
step 251/334, epoch 290/501 --> loss:0.8452145278453826
step 301/334, epoch 290/501 --> loss:0.8116076171398163
step 51/334, epoch 291/501 --> loss:0.8370943963527679
step 101/334, epoch 291/501 --> loss:0.7984701275825501
step 151/334, epoch 291/501 --> loss:0.846797205209732
step 201/334, epoch 291/501 --> loss:0.8319652283191681
step 251/334, epoch 291/501 --> loss:0.8283756399154663
step 301/334, epoch 291/501 --> loss:0.8201144790649414

##########train dataset##########
acc--> [99.52280811928864]
F1--> {'F1': [0.9465690805956108], 'precision': [0.9038270557841964], 'recall': [0.9935653043873823]}
##########eval dataset##########
acc--> [98.95563067204439]
F1--> {'F1': [0.8798113334036815], 'precision': [0.8443358534170874], 'recall': [0.9184095000205774]}
step 51/334, epoch 292/501 --> loss:0.8440404307842254
step 101/334, epoch 292/501 --> loss:0.8310705578327179
step 151/334, epoch 292/501 --> loss:0.8239395630359649
step 201/334, epoch 292/501 --> loss:0.8121290755271912
step 251/334, epoch 292/501 --> loss:0.8117503237724304
step 301/334, epoch 292/501 --> loss:0.8309108412265778
step 51/334, epoch 293/501 --> loss:0.8138100409507751
step 101/334, epoch 293/501 --> loss:0.8406968557834625
step 151/334, epoch 293/501 --> loss:0.8254767847061157
step 201/334, epoch 293/501 --> loss:0.8315124344825745
step 251/334, epoch 293/501 --> loss:0.8409825706481934
step 301/334, epoch 293/501 --> loss:0.827487462759018
step 51/334, epoch 294/501 --> loss:0.821618869304657
step 101/334, epoch 294/501 --> loss:0.8160885977745056
step 151/334, epoch 294/501 --> loss:0.8210643780231476
step 201/334, epoch 294/501 --> loss:0.8369282746315002
step 251/334, epoch 294/501 --> loss:0.838147531747818
step 301/334, epoch 294/501 --> loss:0.8354412400722504
step 51/334, epoch 295/501 --> loss:0.8076718771457672
step 101/334, epoch 295/501 --> loss:0.8220521092414856
step 151/334, epoch 295/501 --> loss:0.8151302516460419
step 201/334, epoch 295/501 --> loss:0.8269674003124237
step 251/334, epoch 295/501 --> loss:0.8421773827075958
step 301/334, epoch 295/501 --> loss:0.8383100533485413
step 51/334, epoch 296/501 --> loss:0.8224634075164795
step 101/334, epoch 296/501 --> loss:0.8531067502498627
step 151/334, epoch 296/501 --> loss:0.8487265503406525
step 201/334, epoch 296/501 --> loss:0.8238079357147217
step 251/334, epoch 296/501 --> loss:0.7984340333938599
step 301/334, epoch 296/501 --> loss:0.8267124760150909
step 51/334, epoch 297/501 --> loss:0.8306991493701935
step 101/334, epoch 297/501 --> loss:0.8328653836250305
step 151/334, epoch 297/501 --> loss:0.8106039309501648
step 201/334, epoch 297/501 --> loss:0.8211330115795136
step 251/334, epoch 297/501 --> loss:0.8259584367275238
step 301/334, epoch 297/501 --> loss:0.8288961708545685
step 51/334, epoch 298/501 --> loss:0.8537016701698303
step 101/334, epoch 298/501 --> loss:0.8166437542438507
step 151/334, epoch 298/501 --> loss:0.8304454529285431
step 201/334, epoch 298/501 --> loss:0.815219556093216
step 251/334, epoch 298/501 --> loss:0.8134579980373382
step 301/334, epoch 298/501 --> loss:0.8250263702869415
step 51/334, epoch 299/501 --> loss:0.8052097260951996
step 101/334, epoch 299/501 --> loss:0.8380128455162048
step 151/334, epoch 299/501 --> loss:0.8202426648139953
step 201/334, epoch 299/501 --> loss:0.8354626524448395
step 251/334, epoch 299/501 --> loss:0.8508784317970276
step 301/334, epoch 299/501 --> loss:0.811828281879425
step 51/334, epoch 300/501 --> loss:0.842087150812149
step 101/334, epoch 300/501 --> loss:0.8388675951957703
step 151/334, epoch 300/501 --> loss:0.8312067329883576
step 201/334, epoch 300/501 --> loss:0.8219941675662994
step 251/334, epoch 300/501 --> loss:0.8120311057567596
step 301/334, epoch 300/501 --> loss:0.819155695438385
step 51/334, epoch 301/501 --> loss:0.8299254357814789
step 101/334, epoch 301/501 --> loss:0.8386093473434448
step 151/334, epoch 301/501 --> loss:0.8264967000484467
step 201/334, epoch 301/501 --> loss:0.8302234423160553
step 251/334, epoch 301/501 --> loss:0.8331963169574738
step 301/334, epoch 301/501 --> loss:0.8158478236198425

##########train dataset##########
acc--> [99.27715420231]
F1--> {'F1': [0.9213430916776935], 'precision': [0.8577840820864302], 'recall': [0.9950865050500532]}
##########eval dataset##########
acc--> [98.6532603453189]
F1--> {'F1': [0.8522679064619431], 'precision': [0.7841660191525744], 'recall': [0.9333354715317129]}
step 51/334, epoch 302/501 --> loss:0.8094364428520202
step 101/334, epoch 302/501 --> loss:0.8477134740352631
step 151/334, epoch 302/501 --> loss:0.8225269103050232
step 201/334, epoch 302/501 --> loss:0.8391408193111419
step 251/334, epoch 302/501 --> loss:0.8238096594810486
step 301/334, epoch 302/501 --> loss:0.828931725025177
step 51/334, epoch 303/501 --> loss:0.82282261967659
step 101/334, epoch 303/501 --> loss:0.822382526397705
step 151/334, epoch 303/501 --> loss:0.8134982931613922
step 201/334, epoch 303/501 --> loss:0.8296807706356049
step 251/334, epoch 303/501 --> loss:0.8547861540317535
step 301/334, epoch 303/501 --> loss:0.8182558977603912
step 51/334, epoch 304/501 --> loss:0.7910124778747558
step 101/334, epoch 304/501 --> loss:0.8504316234588623
step 151/334, epoch 304/501 --> loss:0.8458265113830566
step 201/334, epoch 304/501 --> loss:0.8314290511608123
step 251/334, epoch 304/501 --> loss:0.8268186461925506
step 301/334, epoch 304/501 --> loss:0.8183675277233123
step 51/334, epoch 305/501 --> loss:0.833032056093216
step 101/334, epoch 305/501 --> loss:0.8151288259029389
step 151/334, epoch 305/501 --> loss:0.830791677236557
step 201/334, epoch 305/501 --> loss:0.8252017235755921
step 251/334, epoch 305/501 --> loss:0.8142902648448944
step 301/334, epoch 305/501 --> loss:0.8400114762783051
step 51/334, epoch 306/501 --> loss:0.8560963237285614
step 101/334, epoch 306/501 --> loss:0.8265715730190277
step 151/334, epoch 306/501 --> loss:0.8198055016994477
step 201/334, epoch 306/501 --> loss:0.8250292551517486
step 251/334, epoch 306/501 --> loss:0.8069736433029174
step 301/334, epoch 306/501 --> loss:0.8175854527950287
step 51/334, epoch 307/501 --> loss:0.8149498772621154
step 101/334, epoch 307/501 --> loss:0.8258333051204682
step 151/334, epoch 307/501 --> loss:0.8517633247375488
step 201/334, epoch 307/501 --> loss:0.8272177422046662
step 251/334, epoch 307/501 --> loss:0.8314127969741821
step 301/334, epoch 307/501 --> loss:0.8019860136508942
step 51/334, epoch 308/501 --> loss:0.8345929324626923
step 101/334, epoch 308/501 --> loss:0.8168098092079162
step 151/334, epoch 308/501 --> loss:0.8450078856945038
step 201/334, epoch 308/501 --> loss:0.8082247638702392
step 251/334, epoch 308/501 --> loss:0.8187840032577515
step 301/334, epoch 308/501 --> loss:0.8282203161716462
step 51/334, epoch 309/501 --> loss:0.8068247103691101
step 101/334, epoch 309/501 --> loss:0.840050095319748
step 151/334, epoch 309/501 --> loss:0.8327219307422637
step 201/334, epoch 309/501 --> loss:0.8138385844230652
step 251/334, epoch 309/501 --> loss:0.8365590643882751
step 301/334, epoch 309/501 --> loss:0.8416655373573303
step 51/334, epoch 310/501 --> loss:0.8303608953952789
step 101/334, epoch 310/501 --> loss:0.8087218141555786
step 151/334, epoch 310/501 --> loss:0.8338266038894653
step 201/334, epoch 310/501 --> loss:0.8422003316879273
step 251/334, epoch 310/501 --> loss:0.8262276077270507
step 301/334, epoch 310/501 --> loss:0.8153487205505371
step 51/334, epoch 311/501 --> loss:0.8256897974014282
step 101/334, epoch 311/501 --> loss:0.8372244560718536
step 151/334, epoch 311/501 --> loss:0.8378005957603455
step 201/334, epoch 311/501 --> loss:0.8192207074165344
step 251/334, epoch 311/501 --> loss:0.8340227627754211
step 301/334, epoch 311/501 --> loss:0.8197124230861664

##########train dataset##########
acc--> [99.33333109056885]
F1--> {'F1': [0.9269500961985133], 'precision': [0.868220019135837], 'recall': [0.9942135880023867]}
##########eval dataset##########
acc--> [98.764710914425]
F1--> {'F1': [0.8622849991090655], 'precision': [0.8043962329495281], 'recall': [0.9291634159331773]}
step 51/334, epoch 312/501 --> loss:0.8180435323715209
step 101/334, epoch 312/501 --> loss:0.8308191800117493
step 151/334, epoch 312/501 --> loss:0.8247173571586609
step 201/334, epoch 312/501 --> loss:0.8261880946159362
step 251/334, epoch 312/501 --> loss:0.8256963908672332
step 301/334, epoch 312/501 --> loss:0.821472498178482
step 51/334, epoch 313/501 --> loss:0.8403801035881042
step 101/334, epoch 313/501 --> loss:0.8179705047607422
step 151/334, epoch 313/501 --> loss:0.7985874271392822
step 201/334, epoch 313/501 --> loss:0.8406172800064087
step 251/334, epoch 313/501 --> loss:0.8105159127712249
step 301/334, epoch 313/501 --> loss:0.8285724258422852
step 51/334, epoch 314/501 --> loss:0.8302611255645752
step 101/334, epoch 314/501 --> loss:0.8213061666488648
step 151/334, epoch 314/501 --> loss:0.8234475386142731
step 201/334, epoch 314/501 --> loss:0.8437891113758087
step 251/334, epoch 314/501 --> loss:0.8234846138954163
step 301/334, epoch 314/501 --> loss:0.8340833580493927
step 51/334, epoch 315/501 --> loss:0.8576231646537781
step 101/334, epoch 315/501 --> loss:0.8228593993186951
step 151/334, epoch 315/501 --> loss:0.8189996647834777
step 201/334, epoch 315/501 --> loss:0.8132885730266571
step 251/334, epoch 315/501 --> loss:0.8236965596675873
step 301/334, epoch 315/501 --> loss:0.8218485939502717
step 51/334, epoch 316/501 --> loss:0.8101258790493011
step 101/334, epoch 316/501 --> loss:0.8381344819068909
step 151/334, epoch 316/501 --> loss:0.8138381314277648
step 201/334, epoch 316/501 --> loss:0.8090852904319763
step 251/334, epoch 316/501 --> loss:0.8558077764511108
step 301/334, epoch 316/501 --> loss:0.83154567360878
step 51/334, epoch 317/501 --> loss:0.8238657295703888
step 101/334, epoch 317/501 --> loss:0.8253842079639435
step 151/334, epoch 317/501 --> loss:0.8420992791652679
step 201/334, epoch 317/501 --> loss:0.8235499668121338
step 251/334, epoch 317/501 --> loss:0.8295555090904236
step 301/334, epoch 317/501 --> loss:0.818037623167038
step 51/334, epoch 318/501 --> loss:0.8191443431377411
step 101/334, epoch 318/501 --> loss:0.8310136365890503
step 151/334, epoch 318/501 --> loss:0.8249578714370728
step 201/334, epoch 318/501 --> loss:0.832458336353302
step 251/334, epoch 318/501 --> loss:0.8474447977542877
step 301/334, epoch 318/501 --> loss:0.8133689689636231
step 51/334, epoch 319/501 --> loss:0.8427398335933686
step 101/334, epoch 319/501 --> loss:0.8038801538944245
step 151/334, epoch 319/501 --> loss:0.8402262568473816
step 201/334, epoch 319/501 --> loss:0.8241935110092163
step 251/334, epoch 319/501 --> loss:0.8207464182376861
step 301/334, epoch 319/501 --> loss:0.8306947529315949
step 51/334, epoch 320/501 --> loss:0.8129425513744354
step 101/334, epoch 320/501 --> loss:0.8411617982387543
step 151/334, epoch 320/501 --> loss:0.8531625020503998
step 201/334, epoch 320/501 --> loss:0.787828449010849
step 251/334, epoch 320/501 --> loss:0.8223644268512725
step 301/334, epoch 320/501 --> loss:0.8404292798042298
step 51/334, epoch 321/501 --> loss:0.815402684211731
step 101/334, epoch 321/501 --> loss:0.8310807228088379
step 151/334, epoch 321/501 --> loss:0.8328384554386139
step 201/334, epoch 321/501 --> loss:0.8327889335155487
step 251/334, epoch 321/501 --> loss:0.82341756939888
step 301/334, epoch 321/501 --> loss:0.8199180769920349

##########train dataset##########
acc--> [99.49323398860136]
F1--> {'F1': [0.9433659026900757], 'precision': [0.8992127342222186], 'recall': [0.9920900254949923]}
##########eval dataset##########
acc--> [98.82319492930155]
F1--> {'F1': [0.8675003073732936], 'precision': [0.8162882090561729], 'recall': [0.9255797394855884]}
step 51/334, epoch 322/501 --> loss:0.8252144408226013
step 101/334, epoch 322/501 --> loss:0.8135747587680817
step 151/334, epoch 322/501 --> loss:0.8153740906715393
step 201/334, epoch 322/501 --> loss:0.8386437678337098
step 251/334, epoch 322/501 --> loss:0.8413728260993958
step 301/334, epoch 322/501 --> loss:0.8251095247268677
step 51/334, epoch 323/501 --> loss:0.8213608109951019
step 101/334, epoch 323/501 --> loss:0.8175432932376862
step 151/334, epoch 323/501 --> loss:0.8391780579090118
step 201/334, epoch 323/501 --> loss:0.8253249108791352
step 251/334, epoch 323/501 --> loss:0.8395767772197723
step 301/334, epoch 323/501 --> loss:0.8124744808673858
step 51/334, epoch 324/501 --> loss:0.8410648143291474
step 101/334, epoch 324/501 --> loss:0.8280321776866912
step 151/334, epoch 324/501 --> loss:0.841268459558487
step 201/334, epoch 324/501 --> loss:0.8279656481742859
step 251/334, epoch 324/501 --> loss:0.8101681411266327
step 301/334, epoch 324/501 --> loss:0.8283788442611695
step 51/334, epoch 325/501 --> loss:0.8079778563976288
step 101/334, epoch 325/501 --> loss:0.8459521639347076
step 151/334, epoch 325/501 --> loss:0.8529369843006134
step 201/334, epoch 325/501 --> loss:0.8238326179981231
step 251/334, epoch 325/501 --> loss:0.8302534520626068
step 301/334, epoch 325/501 --> loss:0.806191588640213
step 51/334, epoch 326/501 --> loss:0.8195948004722595
step 101/334, epoch 326/501 --> loss:0.8249849963188172
step 151/334, epoch 326/501 --> loss:0.8238028287887573
step 201/334, epoch 326/501 --> loss:0.8136303436756134
step 251/334, epoch 326/501 --> loss:0.8497702538967132
step 301/334, epoch 326/501 --> loss:0.8258251130580903
step 51/334, epoch 327/501 --> loss:0.8055649518966674
step 101/334, epoch 327/501 --> loss:0.8056358909606933
step 151/334, epoch 327/501 --> loss:0.8157071220874786
step 201/334, epoch 327/501 --> loss:0.8447178471088409
step 251/334, epoch 327/501 --> loss:0.8718375635147094
step 301/334, epoch 327/501 --> loss:0.8209790968894959
step 51/334, epoch 328/501 --> loss:0.8312922036647796
step 101/334, epoch 328/501 --> loss:0.8505540084838867
step 151/334, epoch 328/501 --> loss:0.8341680383682251
step 201/334, epoch 328/501 --> loss:0.8110142028331757
step 251/334, epoch 328/501 --> loss:0.8147630786895752
step 301/334, epoch 328/501 --> loss:0.819863885641098
step 51/334, epoch 329/501 --> loss:0.8214735293388367
step 101/334, epoch 329/501 --> loss:0.8379624831676483
step 151/334, epoch 329/501 --> loss:0.8313766241073608
step 201/334, epoch 329/501 --> loss:0.8200213360786438
step 251/334, epoch 329/501 --> loss:0.825530618429184
step 301/334, epoch 329/501 --> loss:0.8050118446350097
step 51/334, epoch 330/501 --> loss:0.8201774215698242
step 101/334, epoch 330/501 --> loss:0.8124355041980743
step 151/334, epoch 330/501 --> loss:0.822137451171875
step 201/334, epoch 330/501 --> loss:0.8100311827659606
step 251/334, epoch 330/501 --> loss:0.8559853053092956
step 301/334, epoch 330/501 --> loss:0.8341542148590088
step 51/334, epoch 331/501 --> loss:0.8339434123039245
step 101/334, epoch 331/501 --> loss:0.8341356217861176
step 151/334, epoch 331/501 --> loss:0.816631817817688
step 201/334, epoch 331/501 --> loss:0.8285767221450806
step 251/334, epoch 331/501 --> loss:0.8251064777374267
step 301/334, epoch 331/501 --> loss:0.816667605638504

##########train dataset##########
acc--> [99.63442441627494]
F1--> {'F1': [0.9586102601498028], 'precision': [0.9246808875647252], 'recall': [0.9951351834712587]}
##########eval dataset##########
acc--> [99.07843188454541]
F1--> {'F1': [0.8928675722890937], 'precision': [0.8649291724899271], 'recall': [0.9226817837383121]}
save model!
step 51/334, epoch 332/501 --> loss:0.8178235602378845
step 101/334, epoch 332/501 --> loss:0.8310492408275604
step 151/334, epoch 332/501 --> loss:0.8233128380775452
step 201/334, epoch 332/501 --> loss:0.8233754503726959
step 251/334, epoch 332/501 --> loss:0.8287456774711609
step 301/334, epoch 332/501 --> loss:0.8374918031692505
step 51/334, epoch 333/501 --> loss:0.833093775510788
step 101/334, epoch 333/501 --> loss:0.8368657922744751
step 151/334, epoch 333/501 --> loss:0.812526662349701
step 201/334, epoch 333/501 --> loss:0.8171782314777374
step 251/334, epoch 333/501 --> loss:0.8078011977672577
step 301/334, epoch 333/501 --> loss:0.8303154253959656
step 51/334, epoch 334/501 --> loss:0.8142898893356323
step 101/334, epoch 334/501 --> loss:0.8196188354492188
step 151/334, epoch 334/501 --> loss:0.8264775848388672
step 201/334, epoch 334/501 --> loss:0.8409103989601135
step 251/334, epoch 334/501 --> loss:0.8342020785808564
step 301/334, epoch 334/501 --> loss:0.8610208654403686
step 51/334, epoch 335/501 --> loss:0.8284010899066925
step 101/334, epoch 335/501 --> loss:0.8390516436100006
step 151/334, epoch 335/501 --> loss:0.8429003405570984
step 201/334, epoch 335/501 --> loss:0.805481835603714
step 251/334, epoch 335/501 --> loss:0.8173653841018677
step 301/334, epoch 335/501 --> loss:0.8313288068771363
step 51/334, epoch 336/501 --> loss:0.8248722195625305
step 101/334, epoch 336/501 --> loss:0.821495224237442
step 151/334, epoch 336/501 --> loss:0.8225660181045532
step 201/334, epoch 336/501 --> loss:0.8151787841320037
step 251/334, epoch 336/501 --> loss:0.84150315284729
step 301/334, epoch 336/501 --> loss:0.8410127139091492
step 51/334, epoch 337/501 --> loss:0.8381910252571106
step 101/334, epoch 337/501 --> loss:0.8327057540416718
step 151/334, epoch 337/501 --> loss:0.8082557249069214
step 201/334, epoch 337/501 --> loss:0.8314728319644928
step 251/334, epoch 337/501 --> loss:0.8311688673496246
step 301/334, epoch 337/501 --> loss:0.8132736659049988
step 51/334, epoch 338/501 --> loss:0.8313201868534088
step 101/334, epoch 338/501 --> loss:0.8326119005680084
step 151/334, epoch 338/501 --> loss:0.8249420237541198
step 201/334, epoch 338/501 --> loss:0.8293602240085601
step 251/334, epoch 338/501 --> loss:0.8113625967502593
step 301/334, epoch 338/501 --> loss:0.8308665263652801
step 51/334, epoch 339/501 --> loss:0.8227531850337982
step 101/334, epoch 339/501 --> loss:0.8425873434543609
step 151/334, epoch 339/501 --> loss:0.8145006906986236
step 201/334, epoch 339/501 --> loss:0.8260821187496186
step 251/334, epoch 339/501 --> loss:0.8402527916431427
step 301/334, epoch 339/501 --> loss:0.815398120880127
step 51/334, epoch 340/501 --> loss:0.8236330044269562
step 101/334, epoch 340/501 --> loss:0.802246379852295
step 151/334, epoch 340/501 --> loss:0.8391597938537597
step 201/334, epoch 340/501 --> loss:0.8135569143295288
step 251/334, epoch 340/501 --> loss:0.8280332028865814
step 301/334, epoch 340/501 --> loss:0.8256096959114074
step 51/334, epoch 341/501 --> loss:0.8528381633758545
step 101/334, epoch 341/501 --> loss:0.836239162683487
step 151/334, epoch 341/501 --> loss:0.798023579120636
step 201/334, epoch 341/501 --> loss:0.8235394537448884
step 251/334, epoch 341/501 --> loss:0.8163395833969116
step 301/334, epoch 341/501 --> loss:0.83021728515625

##########train dataset##########
acc--> [99.4756732221188]
F1--> {'F1': [0.9415968706543858], 'precision': [0.8948504918398266], 'recall': [0.9935075491887422]}
##########eval dataset##########
acc--> [98.83175947534096]
F1--> {'F1': [0.8648336022821276], 'precision': [0.8340854822047366], 'recall': [0.8979462857449187]}
step 51/334, epoch 342/501 --> loss:0.8237553751468658
step 101/334, epoch 342/501 --> loss:0.8315151417255402
step 151/334, epoch 342/501 --> loss:0.8503395819664001
step 201/334, epoch 342/501 --> loss:0.8094996595382691
step 251/334, epoch 342/501 --> loss:0.8293575787544251
step 301/334, epoch 342/501 --> loss:0.8183674657344818
step 51/334, epoch 343/501 --> loss:0.8552461504936218
step 101/334, epoch 343/501 --> loss:0.8079601633548736
step 151/334, epoch 343/501 --> loss:0.841058782339096
step 201/334, epoch 343/501 --> loss:0.8268514919281006
step 251/334, epoch 343/501 --> loss:0.8264198970794677
step 301/334, epoch 343/501 --> loss:0.8323351073265076
step 51/334, epoch 344/501 --> loss:0.81380894780159
step 101/334, epoch 344/501 --> loss:0.8203413653373718
step 151/334, epoch 344/501 --> loss:0.8050744438171387
step 201/334, epoch 344/501 --> loss:0.8209756553173065
step 251/334, epoch 344/501 --> loss:0.8441393661499024
step 301/334, epoch 344/501 --> loss:0.8392630767822266
step 51/334, epoch 345/501 --> loss:0.8529937660694122
step 101/334, epoch 345/501 --> loss:0.8269928622245789
step 151/334, epoch 345/501 --> loss:0.8192198514938355
step 201/334, epoch 345/501 --> loss:0.8140204000473023
step 251/334, epoch 345/501 --> loss:0.818819271326065
step 301/334, epoch 345/501 --> loss:0.8289183032512665
step 51/334, epoch 346/501 --> loss:0.8170026123523713
step 101/334, epoch 346/501 --> loss:0.8293809795379639
step 151/334, epoch 346/501 --> loss:0.8205178260803223
step 201/334, epoch 346/501 --> loss:0.8312408375740051
step 251/334, epoch 346/501 --> loss:0.8328678703308106
step 301/334, epoch 346/501 --> loss:0.8157398116588592
step 51/334, epoch 347/501 --> loss:0.8129888546466827
step 101/334, epoch 347/501 --> loss:0.8405979371070862
step 151/334, epoch 347/501 --> loss:0.817733633518219
step 201/334, epoch 347/501 --> loss:0.8313590705394744
step 251/334, epoch 347/501 --> loss:0.8201420164108276
step 301/334, epoch 347/501 --> loss:0.8433652400970459
step 51/334, epoch 348/501 --> loss:0.8338373625278472
step 101/334, epoch 348/501 --> loss:0.8264825737476349
step 151/334, epoch 348/501 --> loss:0.8363096332550048
step 201/334, epoch 348/501 --> loss:0.8129855680465699
step 251/334, epoch 348/501 --> loss:0.8232965338230133
step 301/334, epoch 348/501 --> loss:0.8322451758384705
step 51/334, epoch 349/501 --> loss:0.814744951725006
step 101/334, epoch 349/501 --> loss:0.8160232257843018
step 151/334, epoch 349/501 --> loss:0.8511301803588868
step 201/334, epoch 349/501 --> loss:0.818589323759079
step 251/334, epoch 349/501 --> loss:0.82203138589859
step 301/334, epoch 349/501 --> loss:0.8429360842704773
step 51/334, epoch 350/501 --> loss:0.8300810503959656
step 101/334, epoch 350/501 --> loss:0.8220062530040741
step 151/334, epoch 350/501 --> loss:0.8317129063606262
step 201/334, epoch 350/501 --> loss:0.8175079798698426
step 251/334, epoch 350/501 --> loss:0.8242339718341828
step 301/334, epoch 350/501 --> loss:0.8391166508197785
step 51/334, epoch 351/501 --> loss:0.8410224628448486
step 101/334, epoch 351/501 --> loss:0.8255327999591827
step 151/334, epoch 351/501 --> loss:0.816021888256073
step 201/334, epoch 351/501 --> loss:0.8514980280399322
step 251/334, epoch 351/501 --> loss:0.8101618564128876
step 301/334, epoch 351/501 --> loss:0.8357628488540649

##########train dataset##########
acc--> [99.66668957487207]
F1--> {'F1': [0.9621294091143543], 'precision': [0.931134508976769], 'recall': [0.9952695197772926]}
##########eval dataset##########
acc--> [99.08656389798146]
F1--> {'F1': [0.8912042665797446], 'precision': [0.8836684103923597], 'recall': [0.8988799307208826]}
step 51/334, epoch 352/501 --> loss:0.8226712906360626
step 101/334, epoch 352/501 --> loss:0.8138651371002197
step 151/334, epoch 352/501 --> loss:0.8119947123527527
step 201/334, epoch 352/501 --> loss:0.8453056073188782
step 251/334, epoch 352/501 --> loss:0.8372670090198517
step 301/334, epoch 352/501 --> loss:0.851055635213852
step 51/334, epoch 353/501 --> loss:0.7985621535778046
step 101/334, epoch 353/501 --> loss:0.8547666656970978
step 151/334, epoch 353/501 --> loss:0.8176981794834137
step 201/334, epoch 353/501 --> loss:0.8352026414871215
step 251/334, epoch 353/501 --> loss:0.8271450412273407
step 301/334, epoch 353/501 --> loss:0.8316239535808563
step 51/334, epoch 354/501 --> loss:0.8009298110008239
step 101/334, epoch 354/501 --> loss:0.8273020827770233
step 151/334, epoch 354/501 --> loss:0.8107727134227752
step 201/334, epoch 354/501 --> loss:0.8189619815349579
step 251/334, epoch 354/501 --> loss:0.8383144855499267
step 301/334, epoch 354/501 --> loss:0.8399258852005005
step 51/334, epoch 355/501 --> loss:0.8235494494438171
step 101/334, epoch 355/501 --> loss:0.8396023178100586
step 151/334, epoch 355/501 --> loss:0.8354817879199982
step 201/334, epoch 355/501 --> loss:0.8500029730796814
step 251/334, epoch 355/501 --> loss:0.808484275341034
step 301/334, epoch 355/501 --> loss:0.8075095224380493
step 51/334, epoch 356/501 --> loss:0.848025928735733
step 101/334, epoch 356/501 --> loss:0.8213362920284272
step 151/334, epoch 356/501 --> loss:0.8366600215435028
step 201/334, epoch 356/501 --> loss:0.8291398906707763
step 251/334, epoch 356/501 --> loss:0.817144695520401
step 301/334, epoch 356/501 --> loss:0.8254296398162841
step 51/334, epoch 357/501 --> loss:0.8285983288288117
step 101/334, epoch 357/501 --> loss:0.8188920092582702
step 151/334, epoch 357/501 --> loss:0.836913366317749
step 201/334, epoch 357/501 --> loss:0.8263728201389313
step 251/334, epoch 357/501 --> loss:0.8214237582683563
step 301/334, epoch 357/501 --> loss:0.8287313449382782
step 51/334, epoch 358/501 --> loss:0.8549097168445587
step 101/334, epoch 358/501 --> loss:0.8150114405155182
step 151/334, epoch 358/501 --> loss:0.805675802230835
step 201/334, epoch 358/501 --> loss:0.8359397089481354
step 251/334, epoch 358/501 --> loss:0.8150192701816559
step 301/334, epoch 358/501 --> loss:0.8052902591228485
step 51/334, epoch 359/501 --> loss:0.8391808998584748
step 101/334, epoch 359/501 --> loss:0.8469035971164703
step 151/334, epoch 359/501 --> loss:0.8225886702537537
step 201/334, epoch 359/501 --> loss:0.8272109389305115
step 251/334, epoch 359/501 --> loss:0.8126534855365753
step 301/334, epoch 359/501 --> loss:0.8389012098312378
step 51/334, epoch 360/501 --> loss:0.8240537166595459
step 101/334, epoch 360/501 --> loss:0.817342529296875
step 151/334, epoch 360/501 --> loss:0.8160221827030182
step 201/334, epoch 360/501 --> loss:0.8518676495552063
step 251/334, epoch 360/501 --> loss:0.8312503874301911
step 301/334, epoch 360/501 --> loss:0.8287068736553193
step 51/334, epoch 361/501 --> loss:0.8344442856311798
step 101/334, epoch 361/501 --> loss:0.8273707497119903
step 151/334, epoch 361/501 --> loss:0.8123448288440704
step 201/334, epoch 361/501 --> loss:0.8262616217136383
step 251/334, epoch 361/501 --> loss:0.8175898277759552
step 301/334, epoch 361/501 --> loss:0.823249546289444

##########train dataset##########
acc--> [99.64704595793648]
F1--> {'F1': [0.9600384434187673], 'precision': [0.9260720853391551], 'recall': [0.9966020579400982]}
##########eval dataset##########
acc--> [99.0435362487887]
F1--> {'F1': [0.8885833253807387], 'precision': [0.8624355058414699], 'recall': [0.916376873763219]}
step 51/334, epoch 362/501 --> loss:0.8100900137424469
step 101/334, epoch 362/501 --> loss:0.8239188051223755
step 151/334, epoch 362/501 --> loss:0.8433873438835144
step 201/334, epoch 362/501 --> loss:0.8066804707050323
step 251/334, epoch 362/501 --> loss:0.83264972448349
step 301/334, epoch 362/501 --> loss:0.8302940213680268
step 51/334, epoch 363/501 --> loss:0.8268452620506287
step 101/334, epoch 363/501 --> loss:0.8027012538909912
step 151/334, epoch 363/501 --> loss:0.8321867990493774
step 201/334, epoch 363/501 --> loss:0.8123775577545166
step 251/334, epoch 363/501 --> loss:0.8264819574356079
step 301/334, epoch 363/501 --> loss:0.8394532704353332
step 51/334, epoch 364/501 --> loss:0.8289416706562043
step 101/334, epoch 364/501 --> loss:0.8462854731082916
step 151/334, epoch 364/501 --> loss:0.8249830043315888
step 201/334, epoch 364/501 --> loss:0.8196911072731018
step 251/334, epoch 364/501 --> loss:0.8246426022052765
step 301/334, epoch 364/501 --> loss:0.8249821960926056
step 51/334, epoch 365/501 --> loss:0.8386651659011841
step 101/334, epoch 365/501 --> loss:0.8220234036445617
step 151/334, epoch 365/501 --> loss:0.8260054063796997
step 201/334, epoch 365/501 --> loss:0.849984005689621
step 251/334, epoch 365/501 --> loss:0.803443466424942
step 301/334, epoch 365/501 --> loss:0.8128398501873016
step 51/334, epoch 366/501 --> loss:0.8316901016235352
step 101/334, epoch 366/501 --> loss:0.8344375741481781
step 151/334, epoch 366/501 --> loss:0.8311864376068115
step 201/334, epoch 366/501 --> loss:0.8355587184429168
step 251/334, epoch 366/501 --> loss:0.8343456351757049
step 301/334, epoch 366/501 --> loss:0.8082581949234009
step 51/334, epoch 367/501 --> loss:0.8207949876785279
step 101/334, epoch 367/501 --> loss:0.8235756528377532
step 151/334, epoch 367/501 --> loss:0.8073198461532592
step 201/334, epoch 367/501 --> loss:0.8408805084228516
step 251/334, epoch 367/501 --> loss:0.8437472379207611
step 301/334, epoch 367/501 --> loss:0.8273313856124878
step 51/334, epoch 368/501 --> loss:0.8203905308246613
step 101/334, epoch 368/501 --> loss:0.8417229807376861
step 151/334, epoch 368/501 --> loss:0.8330062890052795
step 201/334, epoch 368/501 --> loss:0.8326814115047455
step 251/334, epoch 368/501 --> loss:0.8107796406745911
step 301/334, epoch 368/501 --> loss:0.7951223409175873
step 51/334, epoch 369/501 --> loss:0.8091864085197449
step 101/334, epoch 369/501 --> loss:0.8288652694225311
step 151/334, epoch 369/501 --> loss:0.813458811044693
step 201/334, epoch 369/501 --> loss:0.8522445917129516
step 251/334, epoch 369/501 --> loss:0.8396604001522064
step 301/334, epoch 369/501 --> loss:0.8296027255058288
step 51/334, epoch 370/501 --> loss:0.8069972157478332
step 101/334, epoch 370/501 --> loss:0.8444939041137696
step 151/334, epoch 370/501 --> loss:0.8555616319179535
step 201/334, epoch 370/501 --> loss:0.8148026657104492
step 251/334, epoch 370/501 --> loss:0.8268206417560577
step 301/334, epoch 370/501 --> loss:0.8274077308177948
step 51/334, epoch 371/501 --> loss:0.8319506025314332
step 101/334, epoch 371/501 --> loss:0.8125961136817932
step 151/334, epoch 371/501 --> loss:0.832311338186264
step 201/334, epoch 371/501 --> loss:0.8168890643119812
step 251/334, epoch 371/501 --> loss:0.8294180166721344
step 301/334, epoch 371/501 --> loss:0.8169874179363251

##########train dataset##########
acc--> [99.41745782372011]
F1--> {'F1': [0.9356508482654651], 'precision': [0.8826096782078182], 'recall': [0.9954860177279579]}
##########eval dataset##########
acc--> [98.690964773043]
F1--> {'F1': [0.8559831281587683], 'precision': [0.7895276840212917], 'recall': [0.9346658594947146]}
step 51/334, epoch 372/501 --> loss:0.8373492646217346
step 101/334, epoch 372/501 --> loss:0.8397359597682953
step 151/334, epoch 372/501 --> loss:0.820522381067276
step 201/334, epoch 372/501 --> loss:0.8372432518005372
step 251/334, epoch 372/501 --> loss:0.8115088057518005
step 301/334, epoch 372/501 --> loss:0.8317234826087951
step 51/334, epoch 373/501 --> loss:0.8443158566951752
step 101/334, epoch 373/501 --> loss:0.8122792840003967
step 151/334, epoch 373/501 --> loss:0.8229201972484589
step 201/334, epoch 373/501 --> loss:0.8147762441635131
step 251/334, epoch 373/501 --> loss:0.8240744173526764
step 301/334, epoch 373/501 --> loss:0.8057336616516113
step 51/334, epoch 374/501 --> loss:0.8079999017715455
step 101/334, epoch 374/501 --> loss:0.8450765430927276
step 151/334, epoch 374/501 --> loss:0.8226100540161133
step 201/334, epoch 374/501 --> loss:0.8040882468223571
step 251/334, epoch 374/501 --> loss:0.8341737616062165
step 301/334, epoch 374/501 --> loss:0.8326239931583405
step 51/334, epoch 375/501 --> loss:0.8448742556571961
step 101/334, epoch 375/501 --> loss:0.8007810580730438
step 151/334, epoch 375/501 --> loss:0.8065892899036408
step 201/334, epoch 375/501 --> loss:0.8399655592441558
step 251/334, epoch 375/501 --> loss:0.8480823361873626
step 301/334, epoch 375/501 --> loss:0.8160115468502045
step 51/334, epoch 376/501 --> loss:0.810544672012329
step 101/334, epoch 376/501 --> loss:0.8404903471469879
step 151/334, epoch 376/501 --> loss:0.8274729228019715
step 201/334, epoch 376/501 --> loss:0.8383107340335846
step 251/334, epoch 376/501 --> loss:0.8372655653953552
step 301/334, epoch 376/501 --> loss:0.7956708300113678
step 51/334, epoch 377/501 --> loss:0.825245337486267
step 101/334, epoch 377/501 --> loss:0.8013787508010864
step 151/334, epoch 377/501 --> loss:0.846229453086853
step 201/334, epoch 377/501 --> loss:0.8116764533519745
step 251/334, epoch 377/501 --> loss:0.8447872507572174
step 301/334, epoch 377/501 --> loss:0.8240372824668885
step 51/334, epoch 378/501 --> loss:0.835980931520462
step 101/334, epoch 378/501 --> loss:0.8451175177097321
step 151/334, epoch 378/501 --> loss:0.8156280648708344
step 201/334, epoch 378/501 --> loss:0.8204599630832672
step 251/334, epoch 378/501 --> loss:0.8267983305454254
step 301/334, epoch 378/501 --> loss:0.8274768614768981
step 51/334, epoch 379/501 --> loss:0.8291146218776703
step 101/334, epoch 379/501 --> loss:0.8440688014030456
step 151/334, epoch 379/501 --> loss:0.8310392570495605
step 201/334, epoch 379/501 --> loss:0.8087147295475006
step 251/334, epoch 379/501 --> loss:0.8182935833930969
step 301/334, epoch 379/501 --> loss:0.8244076836109161
step 51/334, epoch 380/501 --> loss:0.8405428218841553
step 101/334, epoch 380/501 --> loss:0.8166154706478119
step 151/334, epoch 380/501 --> loss:0.8249400556087494
step 201/334, epoch 380/501 --> loss:0.8338062465190887
step 251/334, epoch 380/501 --> loss:0.8127856516838073
step 301/334, epoch 380/501 --> loss:0.82281791806221
step 51/334, epoch 381/501 --> loss:0.8139818215370178
step 101/334, epoch 381/501 --> loss:0.8379569041728974
step 151/334, epoch 381/501 --> loss:0.8260439348220825
step 201/334, epoch 381/501 --> loss:0.8105255889892579
step 251/334, epoch 381/501 --> loss:0.82430135846138
step 301/334, epoch 381/501 --> loss:0.8303441858291626

##########train dataset##########
acc--> [99.67531991621489]
F1--> {'F1': [0.9631245759804751], 'precision': [0.9317497264737392], 'recall': [0.996696725366973]}
##########eval dataset##########
acc--> [99.1129577316258]
F1--> {'F1': [0.8939304723875514], 'precision': [0.8898269455463516], 'recall': [0.8980821148920575]}
save model!
step 51/334, epoch 382/501 --> loss:0.8438662254810333
step 101/334, epoch 382/501 --> loss:0.8274658215045929
step 151/334, epoch 382/501 --> loss:0.8240768253803253
step 201/334, epoch 382/501 --> loss:0.8123913431167602
step 251/334, epoch 382/501 --> loss:0.8310630965232849
step 301/334, epoch 382/501 --> loss:0.823094984292984
step 51/334, epoch 383/501 --> loss:0.7974377393722534
step 101/334, epoch 383/501 --> loss:0.8241098964214325
step 151/334, epoch 383/501 --> loss:0.8579615294933319
step 201/334, epoch 383/501 --> loss:0.8439190912246705
step 251/334, epoch 383/501 --> loss:0.8266237854957581
step 301/334, epoch 383/501 --> loss:0.8052759850025177
step 51/334, epoch 384/501 --> loss:0.8219198942184448
step 101/334, epoch 384/501 --> loss:0.8373263299465179
step 151/334, epoch 384/501 --> loss:0.8250017285346984
step 201/334, epoch 384/501 --> loss:0.8223205065727234
step 251/334, epoch 384/501 --> loss:0.8279974246025086
step 301/334, epoch 384/501 --> loss:0.8236850500106812
step 51/334, epoch 385/501 --> loss:0.8204911172389984
step 101/334, epoch 385/501 --> loss:0.8133131527900695
step 151/334, epoch 385/501 --> loss:0.8605587542057037
step 201/334, epoch 385/501 --> loss:0.8521002733707428
step 251/334, epoch 385/501 --> loss:0.817746946811676
step 301/334, epoch 385/501 --> loss:0.8065314769744873
step 51/334, epoch 386/501 --> loss:0.8225254964828491
step 101/334, epoch 386/501 --> loss:0.8142114400863647
step 151/334, epoch 386/501 --> loss:0.821648885011673
step 201/334, epoch 386/501 --> loss:0.8321266841888427
step 251/334, epoch 386/501 --> loss:0.8446986603736878
step 301/334, epoch 386/501 --> loss:0.8156490552425385
step 51/334, epoch 387/501 --> loss:0.8159165549278259
step 101/334, epoch 387/501 --> loss:0.8447272777557373
step 151/334, epoch 387/501 --> loss:0.8322431290149689
step 201/334, epoch 387/501 --> loss:0.8126432418823242
step 251/334, epoch 387/501 --> loss:0.8197757267951965
step 301/334, epoch 387/501 --> loss:0.8501241278648376
step 51/334, epoch 388/501 --> loss:0.8110324001312256
step 101/334, epoch 388/501 --> loss:0.827102986574173
step 151/334, epoch 388/501 --> loss:0.8335052013397217
step 201/334, epoch 388/501 --> loss:0.8261146354675293
step 251/334, epoch 388/501 --> loss:0.8272715103626251
step 301/334, epoch 388/501 --> loss:0.8438626527786255
step 51/334, epoch 389/501 --> loss:0.8344206273555755
step 101/334, epoch 389/501 --> loss:0.8298838007450103
step 151/334, epoch 389/501 --> loss:0.8160870540142059
step 201/334, epoch 389/501 --> loss:0.8117021346092224
step 251/334, epoch 389/501 --> loss:0.8309668374061584
step 301/334, epoch 389/501 --> loss:0.8238419997692108
step 51/334, epoch 390/501 --> loss:0.8271093249320984
step 101/334, epoch 390/501 --> loss:0.8106120157241822
step 151/334, epoch 390/501 --> loss:0.8294537115097046
step 201/334, epoch 390/501 --> loss:0.8089628851413727
step 251/334, epoch 390/501 --> loss:0.8415521621704102
step 301/334, epoch 390/501 --> loss:0.8419749009609222
step 51/334, epoch 391/501 --> loss:0.8168169987201691
step 101/334, epoch 391/501 --> loss:0.8212230396270752
step 151/334, epoch 391/501 --> loss:0.837160416841507
step 201/334, epoch 391/501 --> loss:0.7988631093502044
step 251/334, epoch 391/501 --> loss:0.8397244882583618
step 301/334, epoch 391/501 --> loss:0.8339303803443908

##########train dataset##########
acc--> [99.69913838818569]
F1--> {'F1': [0.9657382319747766], 'precision': [0.9366142123542369], 'recall': [0.9967422437249234]}
##########eval dataset##########
acc--> [99.08276789039624]
F1--> {'F1': [0.8919817066575024], 'precision': [0.8747603220123952], 'recall': [0.9099051841977783]}
step 51/334, epoch 392/501 --> loss:0.8237352466583252
step 101/334, epoch 392/501 --> loss:0.8161762869358062
step 151/334, epoch 392/501 --> loss:0.8306948959827423
step 201/334, epoch 392/501 --> loss:0.8449259531497956
step 251/334, epoch 392/501 --> loss:0.8283867967128754
step 301/334, epoch 392/501 --> loss:0.8133417129516601
step 51/334, epoch 393/501 --> loss:0.8142468333244324
step 101/334, epoch 393/501 --> loss:0.832329273223877
step 151/334, epoch 393/501 --> loss:0.8063080537319184
step 201/334, epoch 393/501 --> loss:0.8245883965492249
step 251/334, epoch 393/501 --> loss:0.8232180297374725
step 301/334, epoch 393/501 --> loss:0.842657344341278
step 51/334, epoch 394/501 --> loss:0.8181757271289826
step 101/334, epoch 394/501 --> loss:0.835119457244873
step 151/334, epoch 394/501 --> loss:0.8164036536216736
step 201/334, epoch 394/501 --> loss:0.8420796108245849
step 251/334, epoch 394/501 --> loss:0.827938231229782
step 301/334, epoch 394/501 --> loss:0.8077283251285553
step 51/334, epoch 395/501 --> loss:0.8219274306297302
step 101/334, epoch 395/501 --> loss:0.8230835855007171
step 151/334, epoch 395/501 --> loss:0.8283920669555664
step 201/334, epoch 395/501 --> loss:0.8055763900279999
step 251/334, epoch 395/501 --> loss:0.8515778636932373
step 301/334, epoch 395/501 --> loss:0.8433213889598846
step 51/334, epoch 396/501 --> loss:0.8304955101013184
step 101/334, epoch 396/501 --> loss:0.8040127778053283
step 151/334, epoch 396/501 --> loss:0.8293247210979462
step 201/334, epoch 396/501 --> loss:0.8143559062480926
step 251/334, epoch 396/501 --> loss:0.8457945358753204
step 301/334, epoch 396/501 --> loss:0.8080007243156433
step 51/334, epoch 397/501 --> loss:0.8517043006420135
step 101/334, epoch 397/501 --> loss:0.8049925649166108
step 151/334, epoch 397/501 --> loss:0.8176493322849274
step 201/334, epoch 397/501 --> loss:0.826604391336441
step 251/334, epoch 397/501 --> loss:0.823107213973999
step 301/334, epoch 397/501 --> loss:0.8276718175411224
step 51/334, epoch 398/501 --> loss:0.8309859716892243
step 101/334, epoch 398/501 --> loss:0.816548479795456
step 151/334, epoch 398/501 --> loss:0.8146634268760681
step 201/334, epoch 398/501 --> loss:0.8227436184883118
step 251/334, epoch 398/501 --> loss:0.8546314656734466
step 301/334, epoch 398/501 --> loss:0.8194913482666015
step 51/334, epoch 399/501 --> loss:0.8113468146324158
step 101/334, epoch 399/501 --> loss:0.82647745013237
step 151/334, epoch 399/501 --> loss:0.8376539242267609
step 201/334, epoch 399/501 --> loss:0.827699203491211
step 251/334, epoch 399/501 --> loss:0.8251256442070007
step 301/334, epoch 399/501 --> loss:0.820427531003952
step 51/334, epoch 400/501 --> loss:0.8600421738624573
step 101/334, epoch 400/501 --> loss:0.808122615814209
step 151/334, epoch 400/501 --> loss:0.8306959104537964
step 201/334, epoch 400/501 --> loss:0.8176553666591644
step 251/334, epoch 400/501 --> loss:0.8205143630504608
step 301/334, epoch 400/501 --> loss:0.8198261332511901
step 51/334, epoch 401/501 --> loss:0.8094682669639588
step 101/334, epoch 401/501 --> loss:0.8306904113292695
step 151/334, epoch 401/501 --> loss:0.8164313566684723
step 201/334, epoch 401/501 --> loss:0.8567528808116913
step 251/334, epoch 401/501 --> loss:0.835909948348999
step 301/334, epoch 401/501 --> loss:0.8217885816097259

##########train dataset##########
acc--> [99.63354476167488]
F1--> {'F1': [0.9584983954107054], 'precision': [0.924827444888555], 'recall': [0.9947245097188789]}
##########eval dataset##########
acc--> [99.0304688247521]
F1--> {'F1': [0.8851932835416574], 'precision': [0.8727303726753232], 'recall': [0.8980275907952131]}
step 51/334, epoch 402/501 --> loss:0.8175412893295289
step 101/334, epoch 402/501 --> loss:0.8564179956912994
step 151/334, epoch 402/501 --> loss:0.8202739453315735
step 201/334, epoch 402/501 --> loss:0.8117966604232788
step 251/334, epoch 402/501 --> loss:0.8087259447574615
step 301/334, epoch 402/501 --> loss:0.8423017072677612
step 51/334, epoch 403/501 --> loss:0.8299516606330871
step 101/334, epoch 403/501 --> loss:0.8148840045928956
step 151/334, epoch 403/501 --> loss:0.8181392133235932
step 201/334, epoch 403/501 --> loss:0.8388994228839874
step 251/334, epoch 403/501 --> loss:0.8224900257587433
step 301/334, epoch 403/501 --> loss:0.8234377324581146
step 51/334, epoch 404/501 --> loss:0.8240154242515564
step 101/334, epoch 404/501 --> loss:0.8156261944770813
step 151/334, epoch 404/501 --> loss:0.8328520822525024
step 201/334, epoch 404/501 --> loss:0.8455584847927093
step 251/334, epoch 404/501 --> loss:0.8385798847675323
step 301/334, epoch 404/501 --> loss:0.811558005809784
step 51/334, epoch 405/501 --> loss:0.8307680082321167
step 101/334, epoch 405/501 --> loss:0.8166966545581817
step 151/334, epoch 405/501 --> loss:0.8229740583896636
step 201/334, epoch 405/501 --> loss:0.8424785506725311
step 251/334, epoch 405/501 --> loss:0.8259430825710297
step 301/334, epoch 405/501 --> loss:0.8116221046447754
step 51/334, epoch 406/501 --> loss:0.8375529921054841
step 101/334, epoch 406/501 --> loss:0.8234883642196655
step 151/334, epoch 406/501 --> loss:0.8195795261859894
step 201/334, epoch 406/501 --> loss:0.8237955951690674
step 251/334, epoch 406/501 --> loss:0.8140625536441803
step 301/334, epoch 406/501 --> loss:0.8351406073570251
step 51/334, epoch 407/501 --> loss:0.8192163872718811
step 101/334, epoch 407/501 --> loss:0.8305179572105408
step 151/334, epoch 407/501 --> loss:0.8382676386833191
step 201/334, epoch 407/501 --> loss:0.8109085178375244
step 251/334, epoch 407/501 --> loss:0.831044374704361
step 301/334, epoch 407/501 --> loss:0.8384814119338989
step 51/334, epoch 408/501 --> loss:0.8438099086284637
step 101/334, epoch 408/501 --> loss:0.7933378159999848
step 151/334, epoch 408/501 --> loss:0.8180001211166382
step 201/334, epoch 408/501 --> loss:0.8317053866386414
step 251/334, epoch 408/501 --> loss:0.8567032384872436
step 301/334, epoch 408/501 --> loss:0.8357655537128449
step 51/334, epoch 409/501 --> loss:0.8230931448936463
step 101/334, epoch 409/501 --> loss:0.8220867419242859
step 151/334, epoch 409/501 --> loss:0.8275320076942444
step 201/334, epoch 409/501 --> loss:0.8352916300296783
step 251/334, epoch 409/501 --> loss:0.8330286395549774
step 301/334, epoch 409/501 --> loss:0.8229916000366211
step 51/334, epoch 410/501 --> loss:0.8211324405670166
step 101/334, epoch 410/501 --> loss:0.82249835729599
step 151/334, epoch 410/501 --> loss:0.8375700771808624
step 201/334, epoch 410/501 --> loss:0.8554013645648957
step 251/334, epoch 410/501 --> loss:0.8083044004440307
step 301/334, epoch 410/501 --> loss:0.8030734980106353
step 51/334, epoch 411/501 --> loss:0.8477144491672516
step 101/334, epoch 411/501 --> loss:0.8072259736061096
step 151/334, epoch 411/501 --> loss:0.8539529728889466
step 201/334, epoch 411/501 --> loss:0.8290505933761597
step 251/334, epoch 411/501 --> loss:0.8207457888126374
step 301/334, epoch 411/501 --> loss:0.8064197373390197

##########train dataset##########
acc--> [99.72744409984965]
F1--> {'F1': [0.9688148046206115], 'precision': [0.9437809893702811], 'recall': [0.9952233963008466]}
##########eval dataset##########
acc--> [99.04941148331748]
F1--> {'F1': [0.8884268301297475], 'precision': [0.8684906706517395], 'recall': [0.9093102300822122]}
step 51/334, epoch 412/501 --> loss:0.8369744193553924
step 101/334, epoch 412/501 --> loss:0.8333421170711517
step 151/334, epoch 412/501 --> loss:0.8164629817008973
step 201/334, epoch 412/501 --> loss:0.8105427229404449
step 251/334, epoch 412/501 --> loss:0.835857343673706
step 301/334, epoch 412/501 --> loss:0.8400406110286712
step 51/334, epoch 413/501 --> loss:0.8166835761070251
step 101/334, epoch 413/501 --> loss:0.8396250891685486
step 151/334, epoch 413/501 --> loss:0.8238103926181793
step 201/334, epoch 413/501 --> loss:0.833047046661377
step 251/334, epoch 413/501 --> loss:0.8139577054977417
step 301/334, epoch 413/501 --> loss:0.824731011390686
step 51/334, epoch 414/501 --> loss:0.800253176689148
step 101/334, epoch 414/501 --> loss:0.8524774158000946
step 151/334, epoch 414/501 --> loss:0.8430995643138885
step 201/334, epoch 414/501 --> loss:0.8161414778232574
step 251/334, epoch 414/501 --> loss:0.8250333344936371
step 301/334, epoch 414/501 --> loss:0.8175135684013367
step 51/334, epoch 415/501 --> loss:0.8361459517478943
step 101/334, epoch 415/501 --> loss:0.8215705454349518
step 151/334, epoch 415/501 --> loss:0.8099984157085419
step 201/334, epoch 415/501 --> loss:0.8133358132839202
step 251/334, epoch 415/501 --> loss:0.8451692676544189
step 301/334, epoch 415/501 --> loss:0.8173409569263458
step 51/334, epoch 416/501 --> loss:0.830398508310318
step 101/334, epoch 416/501 --> loss:0.8507270693778992
step 151/334, epoch 416/501 --> loss:0.8285669946670532
step 201/334, epoch 416/501 --> loss:0.8124080383777619
step 251/334, epoch 416/501 --> loss:0.8383303022384644
step 301/334, epoch 416/501 --> loss:0.8123271906375885
step 51/334, epoch 417/501 --> loss:0.8220109558105468
step 101/334, epoch 417/501 --> loss:0.8268458735942841
step 151/334, epoch 417/501 --> loss:0.8399036061763764
step 201/334, epoch 417/501 --> loss:0.8297019004821777
step 251/334, epoch 417/501 --> loss:0.8093837785720825
step 301/334, epoch 417/501 --> loss:0.8238044703006744
step 51/334, epoch 418/501 --> loss:0.8208608078956604
step 101/334, epoch 418/501 --> loss:0.8194869422912597
step 151/334, epoch 418/501 --> loss:0.8414696788787842
step 201/334, epoch 418/501 --> loss:0.8371442687511444
step 251/334, epoch 418/501 --> loss:0.8437674236297608
step 301/334, epoch 418/501 --> loss:0.8196138596534729
step 51/334, epoch 419/501 --> loss:0.8272425043582916
step 101/334, epoch 419/501 --> loss:0.8250815379619598
step 151/334, epoch 419/501 --> loss:0.8098188316822053
step 201/334, epoch 419/501 --> loss:0.831190402507782
step 251/334, epoch 419/501 --> loss:0.8394968795776367
step 301/334, epoch 419/501 --> loss:0.821047385931015
step 51/334, epoch 420/501 --> loss:0.843243305683136
step 101/334, epoch 420/501 --> loss:0.8196240985393524
step 151/334, epoch 420/501 --> loss:0.8320148766040802
step 201/334, epoch 420/501 --> loss:0.8132607090473175
step 251/334, epoch 420/501 --> loss:0.8148297691345214
step 301/334, epoch 420/501 --> loss:0.8364745628833771
step 51/334, epoch 421/501 --> loss:0.8113619661331177
step 101/334, epoch 421/501 --> loss:0.8348305547237396
step 151/334, epoch 421/501 --> loss:0.8395016181468964
step 201/334, epoch 421/501 --> loss:0.831202358007431
step 251/334, epoch 421/501 --> loss:0.8301608240604401
step 301/334, epoch 421/501 --> loss:0.810923455953598

##########train dataset##########
acc--> [99.73341459453513]
F1--> {'F1': [0.9695124423831061], 'precision': [0.9440315093361755], 'recall': [0.9964176312697023]}
##########eval dataset##########
acc--> [99.1492857978868]
F1--> {'F1': [0.8980926686341412], 'precision': [0.8955540152033964], 'recall': [0.9006558126281006]}
save model!
step 51/334, epoch 422/501 --> loss:0.8128952252864837
step 101/334, epoch 422/501 --> loss:0.8365355265140534
step 151/334, epoch 422/501 --> loss:0.8156959676742553
step 201/334, epoch 422/501 --> loss:0.8335431301593781
step 251/334, epoch 422/501 --> loss:0.8203962934017182
step 301/334, epoch 422/501 --> loss:0.8457902276515961
step 51/334, epoch 423/501 --> loss:0.8234647846221924
step 101/334, epoch 423/501 --> loss:0.8263396048545837
step 151/334, epoch 423/501 --> loss:0.830339390039444
step 201/334, epoch 423/501 --> loss:0.8410195231437683
step 251/334, epoch 423/501 --> loss:0.8330418765544891
step 301/334, epoch 423/501 --> loss:0.8223544919490814
step 51/334, epoch 424/501 --> loss:0.8176949083805084
step 101/334, epoch 424/501 --> loss:0.826182382106781
step 151/334, epoch 424/501 --> loss:0.8157394182682037
step 201/334, epoch 424/501 --> loss:0.8141794157028198
step 251/334, epoch 424/501 --> loss:0.8411493599414825
step 301/334, epoch 424/501 --> loss:0.8425279033184051
step 51/334, epoch 425/501 --> loss:0.8215417468547821
step 101/334, epoch 425/501 --> loss:0.8287672376632691
step 151/334, epoch 425/501 --> loss:0.8042545425891876
step 201/334, epoch 425/501 --> loss:0.8203301024436951
step 251/334, epoch 425/501 --> loss:0.8377937495708465
step 301/334, epoch 425/501 --> loss:0.8237904489040375
step 51/334, epoch 426/501 --> loss:0.8197382891178131
step 101/334, epoch 426/501 --> loss:0.8360672008991241
step 151/334, epoch 426/501 --> loss:0.8261641204357147
step 201/334, epoch 426/501 --> loss:0.8399756228923798
step 251/334, epoch 426/501 --> loss:0.8232030153274537
step 301/334, epoch 426/501 --> loss:0.8088639545440673
step 51/334, epoch 427/501 --> loss:0.8507269787788391
step 101/334, epoch 427/501 --> loss:0.8204441583156585
step 151/334, epoch 427/501 --> loss:0.8144622898101807
step 201/334, epoch 427/501 --> loss:0.8228825449943542
step 251/334, epoch 427/501 --> loss:0.8292054438591003
step 301/334, epoch 427/501 --> loss:0.8162425184249877
step 51/334, epoch 428/501 --> loss:0.8315231370925903
step 101/334, epoch 428/501 --> loss:0.8291066074371338
step 151/334, epoch 428/501 --> loss:0.8169528794288635
step 201/334, epoch 428/501 --> loss:0.826749175786972
step 251/334, epoch 428/501 --> loss:0.8399634802341461
step 301/334, epoch 428/501 --> loss:0.8012780547142029
step 51/334, epoch 429/501 --> loss:0.8393126583099365
step 101/334, epoch 429/501 --> loss:0.8376226246356964
step 151/334, epoch 429/501 --> loss:0.8370650637149811
step 201/334, epoch 429/501 --> loss:0.8054358720779419
step 251/334, epoch 429/501 --> loss:0.8092428147792816
step 301/334, epoch 429/501 --> loss:0.8401739370822906
step 51/334, epoch 430/501 --> loss:0.8260058641433716
step 101/334, epoch 430/501 --> loss:0.8255277287960052
step 151/334, epoch 430/501 --> loss:0.83205375790596
step 201/334, epoch 430/501 --> loss:0.8134029293060303
step 251/334, epoch 430/501 --> loss:0.8297625339031219
step 301/334, epoch 430/501 --> loss:0.8387075269222259
step 51/334, epoch 431/501 --> loss:0.814965866804123
step 101/334, epoch 431/501 --> loss:0.8350471389293671
step 151/334, epoch 431/501 --> loss:0.8139618980884552
step 201/334, epoch 431/501 --> loss:0.8181220698356628
step 251/334, epoch 431/501 --> loss:0.8444505977630615
step 301/334, epoch 431/501 --> loss:0.812286593914032

##########train dataset##########
acc--> [99.68348997650033]
F1--> {'F1': [0.964013760902922], 'precision': [0.9335471965187143], 'recall': [0.9965466559800522]}
##########eval dataset##########
acc--> [98.94964663687618]
F1--> {'F1': [0.8787269374819132], 'precision': [0.8458413083100458], 'recall': [0.9142839502693503]}
step 51/334, epoch 432/501 --> loss:0.827422081232071
step 101/334, epoch 432/501 --> loss:0.8759600496292115
step 151/334, epoch 432/501 --> loss:0.7887808096408844
step 201/334, epoch 432/501 --> loss:0.8401576781272888
step 251/334, epoch 432/501 --> loss:0.8079996132850646
step 301/334, epoch 432/501 --> loss:0.8260794079303742
step 51/334, epoch 433/501 --> loss:0.8405306088924408
step 101/334, epoch 433/501 --> loss:0.834271695613861
step 151/334, epoch 433/501 --> loss:0.8188568627834321
step 201/334, epoch 433/501 --> loss:0.8208935987949372
step 251/334, epoch 433/501 --> loss:0.8126823997497559
step 301/334, epoch 433/501 --> loss:0.8344976007938385
step 51/334, epoch 434/501 --> loss:0.829313805103302
step 101/334, epoch 434/501 --> loss:0.8261245536804199
step 151/334, epoch 434/501 --> loss:0.8170869386196137
step 201/334, epoch 434/501 --> loss:0.8166533207893372
step 251/334, epoch 434/501 --> loss:0.8165215182304383
step 301/334, epoch 434/501 --> loss:0.840726124048233
step 51/334, epoch 435/501 --> loss:0.8262673127651214
step 101/334, epoch 435/501 --> loss:0.8442263007164001
step 151/334, epoch 435/501 --> loss:0.82145143866539
step 201/334, epoch 435/501 --> loss:0.8248881959915161
step 251/334, epoch 435/501 --> loss:0.8296182250976563
step 301/334, epoch 435/501 --> loss:0.8160432779788971
step 51/334, epoch 436/501 --> loss:0.8410031616687774
step 101/334, epoch 436/501 --> loss:0.83113041639328
step 151/334, epoch 436/501 --> loss:0.8218309032917023
step 201/334, epoch 436/501 --> loss:0.8170204663276672
step 251/334, epoch 436/501 --> loss:0.816551501750946
step 301/334, epoch 436/501 --> loss:0.8443088936805725
step 51/334, epoch 437/501 --> loss:0.8141757440567017
step 101/334, epoch 437/501 --> loss:0.8258398604393006
step 151/334, epoch 437/501 --> loss:0.8251057052612305
step 201/334, epoch 437/501 --> loss:0.8264686775207519
step 251/334, epoch 437/501 --> loss:0.8455987167358399
step 301/334, epoch 437/501 --> loss:0.8264256823062897
step 51/334, epoch 438/501 --> loss:0.8325801575183869
step 101/334, epoch 438/501 --> loss:0.8313693082332612
step 151/334, epoch 438/501 --> loss:0.8264564955234528
step 201/334, epoch 438/501 --> loss:0.8172267210483551
step 251/334, epoch 438/501 --> loss:0.8335805201530456
step 301/334, epoch 438/501 --> loss:0.8185224103927612
step 51/334, epoch 439/501 --> loss:0.8265276324748992
step 101/334, epoch 439/501 --> loss:0.832430557012558
step 151/334, epoch 439/501 --> loss:0.8171678447723388
step 201/334, epoch 439/501 --> loss:0.8232227277755737
step 251/334, epoch 439/501 --> loss:0.8311915957927704
step 301/334, epoch 439/501 --> loss:0.8333936786651611
step 51/334, epoch 440/501 --> loss:0.8119486737251281
step 101/334, epoch 440/501 --> loss:0.8154717314243317
step 151/334, epoch 440/501 --> loss:0.8340098440647126
step 201/334, epoch 440/501 --> loss:0.8261195969581604
step 251/334, epoch 440/501 --> loss:0.8352289545536041
step 301/334, epoch 440/501 --> loss:0.8250479567050933
step 51/334, epoch 441/501 --> loss:0.8033913064002991
step 101/334, epoch 441/501 --> loss:0.8099562978744507
step 151/334, epoch 441/501 --> loss:0.8617092144489288
step 201/334, epoch 441/501 --> loss:0.83001504778862
step 251/334, epoch 441/501 --> loss:0.8172987365722656
step 301/334, epoch 441/501 --> loss:0.8225251924991608

##########train dataset##########
acc--> [99.75979107347624]
F1--> {'F1': [0.9724559524360259], 'precision': [0.9492713591288879], 'recall': [0.9968118995873113]}
##########eval dataset##########
acc--> [99.13988622362704]
F1--> {'F1': [0.8974811727849697], 'precision': [0.8905229993824018], 'recall': [0.9045590965021918]}
step 51/334, epoch 442/501 --> loss:0.8094882488250732
step 101/334, epoch 442/501 --> loss:0.8375292003154755
step 151/334, epoch 442/501 --> loss:0.8247821128368378
step 201/334, epoch 442/501 --> loss:0.8209586489200592
step 251/334, epoch 442/501 --> loss:0.8181510782241821
step 301/334, epoch 442/501 --> loss:0.8361782622337341
step 51/334, epoch 443/501 --> loss:0.8260862946510314
step 101/334, epoch 443/501 --> loss:0.8189989566802979
step 151/334, epoch 443/501 --> loss:0.8447438752651215
step 201/334, epoch 443/501 --> loss:0.8304097902774811
step 251/334, epoch 443/501 --> loss:0.8347113144397735
step 301/334, epoch 443/501 --> loss:0.8039963698387146
step 51/334, epoch 444/501 --> loss:0.8243161749839782
step 101/334, epoch 444/501 --> loss:0.8291761636734009
step 151/334, epoch 444/501 --> loss:0.827104434967041
step 201/334, epoch 444/501 --> loss:0.8323966574668884
step 251/334, epoch 444/501 --> loss:0.8385096144676208
step 301/334, epoch 444/501 --> loss:0.8067888259887696
step 51/334, epoch 445/501 --> loss:0.8357932102680207
step 101/334, epoch 445/501 --> loss:0.8327524769306183
step 151/334, epoch 445/501 --> loss:0.8337799143791199
step 201/334, epoch 445/501 --> loss:0.8254618120193481
step 251/334, epoch 445/501 --> loss:0.8104729998111725
step 301/334, epoch 445/501 --> loss:0.8158553838729858
step 51/334, epoch 446/501 --> loss:0.8259014201164245
step 101/334, epoch 446/501 --> loss:0.8340739905834198
step 151/334, epoch 446/501 --> loss:0.8092234098911285
step 201/334, epoch 446/501 --> loss:0.8206095623970032
step 251/334, epoch 446/501 --> loss:0.8236224138736725
step 301/334, epoch 446/501 --> loss:0.8494455409049988
step 51/334, epoch 447/501 --> loss:0.809903336763382
step 101/334, epoch 447/501 --> loss:0.8138250589370728
step 151/334, epoch 447/501 --> loss:0.8236035037040711
step 201/334, epoch 447/501 --> loss:0.8123931729793549
step 251/334, epoch 447/501 --> loss:0.8378830540180207
step 301/334, epoch 447/501 --> loss:0.8211799609661102
step 51/334, epoch 448/501 --> loss:0.8362017142772674
step 101/334, epoch 448/501 --> loss:0.8364232110977173
step 151/334, epoch 448/501 --> loss:0.8303194296360016
step 201/334, epoch 448/501 --> loss:0.8315035200119019
step 251/334, epoch 448/501 --> loss:0.8283830523490906
step 301/334, epoch 448/501 --> loss:0.8061104667186737
step 51/334, epoch 449/501 --> loss:0.8214813828468323
step 101/334, epoch 449/501 --> loss:0.8480319857597352
step 151/334, epoch 449/501 --> loss:0.8108478236198425
step 201/334, epoch 449/501 --> loss:0.829438544511795
step 251/334, epoch 449/501 --> loss:0.8329196405410767
step 301/334, epoch 449/501 --> loss:0.8136177492141724
step 51/334, epoch 450/501 --> loss:0.8063166105747223
step 101/334, epoch 450/501 --> loss:0.8282905530929565
step 151/334, epoch 450/501 --> loss:0.845479645729065
step 201/334, epoch 450/501 --> loss:0.826104679107666
step 251/334, epoch 450/501 --> loss:0.8147449016571044
step 301/334, epoch 450/501 --> loss:0.818148056268692
step 51/334, epoch 451/501 --> loss:0.8376122796535492
step 101/334, epoch 451/501 --> loss:0.817938141822815
step 151/334, epoch 451/501 --> loss:0.811776385307312
step 201/334, epoch 451/501 --> loss:0.8179976105690002
step 251/334, epoch 451/501 --> loss:0.8327144932746887
step 301/334, epoch 451/501 --> loss:0.8426269519329072

##########train dataset##########
acc--> [99.36971703762575]
F1--> {'F1': [0.930739002318584], 'precision': [0.8739547586567158], 'recall': [0.9954263799384423]}
##########eval dataset##########
acc--> [98.58910200997776]
F1--> {'F1': [0.8459946837456603], 'precision': [0.7751770945587013], 'recall': [0.9310645428981466]}
step 51/334, epoch 452/501 --> loss:0.8534175610542297
step 101/334, epoch 452/501 --> loss:0.8152276659011841
step 151/334, epoch 452/501 --> loss:0.8320994186401367
step 201/334, epoch 452/501 --> loss:0.812535707950592
step 251/334, epoch 452/501 --> loss:0.8280383765697479
step 301/334, epoch 452/501 --> loss:0.8277878737449647
step 51/334, epoch 453/501 --> loss:0.8301653134822845
step 101/334, epoch 453/501 --> loss:0.8325995337963105
step 151/334, epoch 453/501 --> loss:0.8290621244907379
step 201/334, epoch 453/501 --> loss:0.8053020417690278
step 251/334, epoch 453/501 --> loss:0.8405120015144348
step 301/334, epoch 453/501 --> loss:0.8295306158065796
step 51/334, epoch 454/501 --> loss:0.8181857192516326
step 101/334, epoch 454/501 --> loss:0.8103299045562744
step 151/334, epoch 454/501 --> loss:0.8234387075901032
step 201/334, epoch 454/501 --> loss:0.8276675593852997
step 251/334, epoch 454/501 --> loss:0.833920875787735
step 301/334, epoch 454/501 --> loss:0.8292005431652069
step 51/334, epoch 455/501 --> loss:0.8122587025165557
step 101/334, epoch 455/501 --> loss:0.8185378313064575
step 151/334, epoch 455/501 --> loss:0.8494448637962342
step 201/334, epoch 455/501 --> loss:0.8344297909736633
step 251/334, epoch 455/501 --> loss:0.836545615196228
step 301/334, epoch 455/501 --> loss:0.8267568576335907
step 51/334, epoch 456/501 --> loss:0.8267420828342438
step 101/334, epoch 456/501 --> loss:0.8354482734203339
step 151/334, epoch 456/501 --> loss:0.8550182282924652
step 201/334, epoch 456/501 --> loss:0.8287023842334748
step 251/334, epoch 456/501 --> loss:0.7923879277706146
step 301/334, epoch 456/501 --> loss:0.8274854469299316
step 51/334, epoch 457/501 --> loss:0.8233487164974213
step 101/334, epoch 457/501 --> loss:0.8380502223968506
step 151/334, epoch 457/501 --> loss:0.8350733494758606
step 201/334, epoch 457/501 --> loss:0.8228688871860504
step 251/334, epoch 457/501 --> loss:0.8301056694984436
step 301/334, epoch 457/501 --> loss:0.8199526619911194
step 51/334, epoch 458/501 --> loss:0.8254337751865387
step 101/334, epoch 458/501 --> loss:0.8384488797187806
step 151/334, epoch 458/501 --> loss:0.827930612564087
step 201/334, epoch 458/501 --> loss:0.8304626882076264
step 251/334, epoch 458/501 --> loss:0.8054593873023986
step 301/334, epoch 458/501 --> loss:0.8288708806037903
step 51/334, epoch 459/501 --> loss:0.8230696058273316
step 101/334, epoch 459/501 --> loss:0.8229177701473236
step 151/334, epoch 459/501 --> loss:0.7974558746814728
step 201/334, epoch 459/501 --> loss:0.8539470851421356
step 251/334, epoch 459/501 --> loss:0.8347033309936523
step 301/334, epoch 459/501 --> loss:0.824368085861206
step 51/334, epoch 460/501 --> loss:0.8179227805137634
step 101/334, epoch 460/501 --> loss:0.8310553026199341
step 151/334, epoch 460/501 --> loss:0.831494232416153
step 201/334, epoch 460/501 --> loss:0.8448167979717255
step 251/334, epoch 460/501 --> loss:0.8120929074287414
step 301/334, epoch 460/501 --> loss:0.8181302809715271
step 51/334, epoch 461/501 --> loss:0.8169100868701935
step 101/334, epoch 461/501 --> loss:0.8200486123561859
step 151/334, epoch 461/501 --> loss:0.8344026362895965
step 201/334, epoch 461/501 --> loss:0.831292816400528
step 251/334, epoch 461/501 --> loss:0.8268318104743958
step 301/334, epoch 461/501 --> loss:0.8133217144012451

##########train dataset##########
acc--> [99.73808663321095]
F1--> {'F1': [0.9700501334915622], 'precision': [0.9444514074634158], 'recall': [0.9970857493242863]}
##########eval dataset##########
acc--> [99.09622713023246]
F1--> {'F1': [0.8924818737143737], 'precision': [0.8839122967901979], 'recall': [0.9012294381999011]}
step 51/334, epoch 462/501 --> loss:0.8331768298149109
step 101/334, epoch 462/501 --> loss:0.8128007555007934
step 151/334, epoch 462/501 --> loss:0.829158992767334
step 201/334, epoch 462/501 --> loss:0.8369818770885468
step 251/334, epoch 462/501 --> loss:0.8234223663806916
step 301/334, epoch 462/501 --> loss:0.8293242609500885
step 51/334, epoch 463/501 --> loss:0.8341729700565338
step 101/334, epoch 463/501 --> loss:0.8225237309932709
step 151/334, epoch 463/501 --> loss:0.8082618594169617
step 201/334, epoch 463/501 --> loss:0.8260345137119294
step 251/334, epoch 463/501 --> loss:0.8199480473995209
step 301/334, epoch 463/501 --> loss:0.8290282344818115
step 51/334, epoch 464/501 --> loss:0.8334760212898255
step 101/334, epoch 464/501 --> loss:0.8161142992973328
step 151/334, epoch 464/501 --> loss:0.8348786926269531
step 201/334, epoch 464/501 --> loss:0.8103332281112671
step 251/334, epoch 464/501 --> loss:0.8244705247879028
step 301/334, epoch 464/501 --> loss:0.832854083776474
step 51/334, epoch 465/501 --> loss:0.8140746235847474
step 101/334, epoch 465/501 --> loss:0.8426075148582458
step 151/334, epoch 465/501 --> loss:0.8264514219760895
step 201/334, epoch 465/501 --> loss:0.8204932022094726
step 251/334, epoch 465/501 --> loss:0.8430292665958404
step 301/334, epoch 465/501 --> loss:0.8224490129947662
step 51/334, epoch 466/501 --> loss:0.8274659180641174
step 101/334, epoch 466/501 --> loss:0.8357301235198975
step 151/334, epoch 466/501 --> loss:0.8243845474720001
step 201/334, epoch 466/501 --> loss:0.8154594373703002
step 251/334, epoch 466/501 --> loss:0.8287819588184356
step 301/334, epoch 466/501 --> loss:0.8361558210849762
step 51/334, epoch 467/501 --> loss:0.8405388522148133
step 101/334, epoch 467/501 --> loss:0.8177521097660064
step 151/334, epoch 467/501 --> loss:0.8259046936035156
step 201/334, epoch 467/501 --> loss:0.8262877643108368
step 251/334, epoch 467/501 --> loss:0.8224569380283355
step 301/334, epoch 467/501 --> loss:0.8254143500328064
step 51/334, epoch 468/501 --> loss:0.8358222186565399
step 101/334, epoch 468/501 --> loss:0.8219378626346588
step 151/334, epoch 468/501 --> loss:0.8459748208522797
step 201/334, epoch 468/501 --> loss:0.8229453253746033
step 251/334, epoch 468/501 --> loss:0.8015450704097747
step 301/334, epoch 468/501 --> loss:0.8366925549507142
step 51/334, epoch 469/501 --> loss:0.8313555133342743
step 101/334, epoch 469/501 --> loss:0.8110193979740142
step 151/334, epoch 469/501 --> loss:0.8272870421409607
step 201/334, epoch 469/501 --> loss:0.8187602543830872
step 251/334, epoch 469/501 --> loss:0.8086784565448761
step 301/334, epoch 469/501 --> loss:0.8559888541698456
step 51/334, epoch 470/501 --> loss:0.8440891873836517
step 101/334, epoch 470/501 --> loss:0.8038503098487854
step 151/334, epoch 470/501 --> loss:0.8243711376190186
step 201/334, epoch 470/501 --> loss:0.8279092335700988
step 251/334, epoch 470/501 --> loss:0.8213384783267975
step 301/334, epoch 470/501 --> loss:0.821378663778305
step 51/334, epoch 471/501 --> loss:0.827406358718872
step 101/334, epoch 471/501 --> loss:0.8329792833328247
step 151/334, epoch 471/501 --> loss:0.8121760010719299
step 201/334, epoch 471/501 --> loss:0.8320976948738098
step 251/334, epoch 471/501 --> loss:0.7931073987483979
step 301/334, epoch 471/501 --> loss:0.8408887827396393

##########train dataset##########
acc--> [99.68569898229599]
F1--> {'F1': [0.9642633890155057], 'precision': [0.9338255205442049], 'recall': [0.9967630194599406]}
##########eval dataset##########
acc--> [98.97603379563465]
F1--> {'F1': [0.8823455236140826], 'precision': [0.8455392257649825], 'recall': [0.9225129194030854]}
step 51/334, epoch 472/501 --> loss:0.8385235464572907
step 101/334, epoch 472/501 --> loss:0.8077739107608796
step 151/334, epoch 472/501 --> loss:0.8000903606414795
step 201/334, epoch 472/501 --> loss:0.8488835620880127
step 251/334, epoch 472/501 --> loss:0.8221167528629303
step 301/334, epoch 472/501 --> loss:0.8388727068901062
step 51/334, epoch 473/501 --> loss:0.811727420091629
step 101/334, epoch 473/501 --> loss:0.8263705229759216
step 151/334, epoch 473/501 --> loss:0.8211819040775299
step 201/334, epoch 473/501 --> loss:0.8453929984569549
step 251/334, epoch 473/501 --> loss:0.8289906227588654
step 301/334, epoch 473/501 --> loss:0.8214688575267792
step 51/334, epoch 474/501 --> loss:0.8251646316051483
step 101/334, epoch 474/501 --> loss:0.8052412307262421
step 151/334, epoch 474/501 --> loss:0.8412765753269196
step 201/334, epoch 474/501 --> loss:0.8342845356464386
step 251/334, epoch 474/501 --> loss:0.8352561390399933
step 301/334, epoch 474/501 --> loss:0.8178090691566468
step 51/334, epoch 475/501 --> loss:0.821335598230362
step 101/334, epoch 475/501 --> loss:0.8070818269252777
step 151/334, epoch 475/501 --> loss:0.8375197660923004
step 201/334, epoch 475/501 --> loss:0.8190529048442841
step 251/334, epoch 475/501 --> loss:0.8282619154453278
step 301/334, epoch 475/501 --> loss:0.8250778865814209
step 51/334, epoch 476/501 --> loss:0.814488309621811
step 101/334, epoch 476/501 --> loss:0.8343318092823029
step 151/334, epoch 476/501 --> loss:0.8169560372829437
step 201/334, epoch 476/501 --> loss:0.8243064463138581
step 251/334, epoch 476/501 --> loss:0.8205418181419373
step 301/334, epoch 476/501 --> loss:0.8314613604545593
step 51/334, epoch 477/501 --> loss:0.8103804171085358
step 101/334, epoch 477/501 --> loss:0.8424521648883819
step 151/334, epoch 477/501 --> loss:0.8242426097393036
step 201/334, epoch 477/501 --> loss:0.834107699394226
step 251/334, epoch 477/501 --> loss:0.8271501886844635
step 301/334, epoch 477/501 --> loss:0.8342286789417267
step 51/334, epoch 478/501 --> loss:0.8072547614574432
step 101/334, epoch 478/501 --> loss:0.8233145606517792
step 151/334, epoch 478/501 --> loss:0.8320782923698425
step 201/334, epoch 478/501 --> loss:0.8399747502803803
step 251/334, epoch 478/501 --> loss:0.8484795033931732
step 301/334, epoch 478/501 --> loss:0.8295880627632141
step 51/334, epoch 479/501 --> loss:0.8175077915191651
step 101/334, epoch 479/501 --> loss:0.8396525120735169
step 151/334, epoch 479/501 --> loss:0.8054192888736725
step 201/334, epoch 479/501 --> loss:0.8407137537002564
step 251/334, epoch 479/501 --> loss:0.8269067358970642
step 301/334, epoch 479/501 --> loss:0.8274863195419312
step 51/334, epoch 480/501 --> loss:0.8375947165489197
step 101/334, epoch 480/501 --> loss:0.7982781529426575
step 151/334, epoch 480/501 --> loss:0.8155550622940063
step 201/334, epoch 480/501 --> loss:0.827117280960083
step 251/334, epoch 480/501 --> loss:0.8489288222789765
step 301/334, epoch 480/501 --> loss:0.8257267212867737
step 51/334, epoch 481/501 --> loss:0.8225819778442383
step 101/334, epoch 481/501 --> loss:0.8139315295219421
step 151/334, epoch 481/501 --> loss:0.8418919229507447
step 201/334, epoch 481/501 --> loss:0.8178514194488525
step 251/334, epoch 481/501 --> loss:0.8455427396297455
step 301/334, epoch 481/501 --> loss:0.8235681593418122

##########train dataset##########
acc--> [99.78011352630318]
F1--> {'F1': [0.9747168466512761], 'precision': [0.95396483562119], 'recall': [0.9964022343657575]}
##########eval dataset##########
acc--> [99.15292494565446]
F1--> {'F1': [0.8976196594577673], 'precision': [0.9031266065670587], 'recall': [0.892189342943111]}
step 51/334, epoch 482/501 --> loss:0.8290233862400055
step 101/334, epoch 482/501 --> loss:0.8273950517177582
step 151/334, epoch 482/501 --> loss:0.843588936328888
step 201/334, epoch 482/501 --> loss:0.8219900202751159
step 251/334, epoch 482/501 --> loss:0.8118596827983856
step 301/334, epoch 482/501 --> loss:0.8286118721961975
step 51/334, epoch 483/501 --> loss:0.8223850858211518
step 101/334, epoch 483/501 --> loss:0.8339488744735718
step 151/334, epoch 483/501 --> loss:0.8108125758171082
step 201/334, epoch 483/501 --> loss:0.8346247780323028
step 251/334, epoch 483/501 --> loss:0.8290198397636414
step 301/334, epoch 483/501 --> loss:0.8278830528259278
step 51/334, epoch 484/501 --> loss:0.8350510358810425
step 101/334, epoch 484/501 --> loss:0.8335208892822266
step 151/334, epoch 484/501 --> loss:0.8284685957431793
step 201/334, epoch 484/501 --> loss:0.8099565899372101
step 251/334, epoch 484/501 --> loss:0.832915893793106
step 301/334, epoch 484/501 --> loss:0.8073347830772399
step 51/334, epoch 485/501 --> loss:0.8139874148368835
step 101/334, epoch 485/501 --> loss:0.8270054793357849
step 151/334, epoch 485/501 --> loss:0.8216269159317017
step 201/334, epoch 485/501 --> loss:0.820214684009552
step 251/334, epoch 485/501 --> loss:0.8407696080207825
step 301/334, epoch 485/501 --> loss:0.8182658934593201
step 51/334, epoch 486/501 --> loss:0.8221591031551361
step 101/334, epoch 486/501 --> loss:0.8215097296237945
step 151/334, epoch 486/501 --> loss:0.8345898187160492
step 201/334, epoch 486/501 --> loss:0.8134518527984619
step 251/334, epoch 486/501 --> loss:0.8253203320503235
step 301/334, epoch 486/501 --> loss:0.8392809462547303
step 51/334, epoch 487/501 --> loss:0.8185278570652008
step 101/334, epoch 487/501 --> loss:0.8245474541187287
step 151/334, epoch 487/501 --> loss:0.8374632382392884
step 201/334, epoch 487/501 --> loss:0.8387878215312958
step 251/334, epoch 487/501 --> loss:0.8194895505905151
step 301/334, epoch 487/501 --> loss:0.8236192917823791
step 51/334, epoch 488/501 --> loss:0.835196441411972
step 101/334, epoch 488/501 --> loss:0.8220060741901398
step 151/334, epoch 488/501 --> loss:0.8248923289775848
step 201/334, epoch 488/501 --> loss:0.8186617231369019
step 251/334, epoch 488/501 --> loss:0.8272604835033417
step 301/334, epoch 488/501 --> loss:0.839650673866272
step 51/334, epoch 489/501 --> loss:0.8172707736492157
step 101/334, epoch 489/501 --> loss:0.8157652056217194
step 151/334, epoch 489/501 --> loss:0.8324455034732818
step 201/334, epoch 489/501 --> loss:0.829366854429245
step 251/334, epoch 489/501 --> loss:0.8503169119358063
step 301/334, epoch 489/501 --> loss:0.7983177816867828
step 51/334, epoch 490/501 --> loss:0.8251059675216674
step 101/334, epoch 490/501 --> loss:0.8251792132854462
step 151/334, epoch 490/501 --> loss:0.8211095762252808
step 201/334, epoch 490/501 --> loss:0.8387189078330993
step 251/334, epoch 490/501 --> loss:0.8276060152053833
step 301/334, epoch 490/501 --> loss:0.8296717834472657
step 51/334, epoch 491/501 --> loss:0.8200501453876495
step 101/334, epoch 491/501 --> loss:0.8260717475414276
step 151/334, epoch 491/501 --> loss:0.8153981518745422
step 201/334, epoch 491/501 --> loss:0.8249976301193237
step 251/334, epoch 491/501 --> loss:0.8355927634239196
step 301/334, epoch 491/501 --> loss:0.8375209760665894

##########train dataset##########
acc--> [99.74354278026362]
F1--> {'F1': [0.9706659440408267], 'precision': [0.9452951806082841], 'recall': [0.9974466688892462]}
##########eval dataset##########
acc--> [99.09363193461239]
F1--> {'F1': [0.8922746955526674], 'precision': [0.882891575342639], 'recall': [0.9018696152428498]}
step 51/334, epoch 492/501 --> loss:0.7821688008308411
step 101/334, epoch 492/501 --> loss:0.8304100704193115
step 151/334, epoch 492/501 --> loss:0.8478971695899964
step 201/334, epoch 492/501 --> loss:0.8299508738517761
step 251/334, epoch 492/501 --> loss:0.8384522998332977
step 301/334, epoch 492/501 --> loss:0.8258543944358826
step 51/334, epoch 493/501 --> loss:0.8176006650924683
step 101/334, epoch 493/501 --> loss:0.8333359062671661
step 151/334, epoch 493/501 --> loss:0.8256288957595825
step 201/334, epoch 493/501 --> loss:0.8240638899803162
step 251/334, epoch 493/501 --> loss:0.8113675332069397
step 301/334, epoch 493/501 --> loss:0.8299563539028167
step 51/334, epoch 494/501 --> loss:0.823463568687439
step 101/334, epoch 494/501 --> loss:0.8216652739048004
step 151/334, epoch 494/501 --> loss:0.7976417970657349
step 201/334, epoch 494/501 --> loss:0.8485618531703949
step 251/334, epoch 494/501 --> loss:0.8301596128940583
step 301/334, epoch 494/501 --> loss:0.8350446450710297
step 51/334, epoch 495/501 --> loss:0.8388346910476685
step 101/334, epoch 495/501 --> loss:0.8418256962299346
step 151/334, epoch 495/501 --> loss:0.8092347764968872
step 201/334, epoch 495/501 --> loss:0.822615876197815
step 251/334, epoch 495/501 --> loss:0.8285995650291443
step 301/334, epoch 495/501 --> loss:0.8390051460266114
step 51/334, epoch 496/501 --> loss:0.8110207903385163
step 101/334, epoch 496/501 --> loss:0.8172550857067108
step 151/334, epoch 496/501 --> loss:0.8496041822433472
step 201/334, epoch 496/501 --> loss:0.8250581741333007
step 251/334, epoch 496/501 --> loss:0.8191298902034759
step 301/334, epoch 496/501 --> loss:0.8320312774181366
step 51/334, epoch 497/501 --> loss:0.8216084098815918
step 101/334, epoch 497/501 --> loss:0.8368933773040772
step 151/334, epoch 497/501 --> loss:0.8291634774208069
step 201/334, epoch 497/501 --> loss:0.8186741781234741
step 251/334, epoch 497/501 --> loss:0.8073373532295227
step 301/334, epoch 497/501 --> loss:0.8359638321399688
step 51/334, epoch 498/501 --> loss:0.8044498026371002
step 101/334, epoch 498/501 --> loss:0.827840371131897
step 151/334, epoch 498/501 --> loss:0.8444139766693115
step 201/334, epoch 498/501 --> loss:0.8206907653808594
step 251/334, epoch 498/501 --> loss:0.8273374795913696
step 301/334, epoch 498/501 --> loss:0.8403071355819702
step 51/334, epoch 499/501 --> loss:0.8270190107822418
step 101/334, epoch 499/501 --> loss:0.8220034277439118
step 151/334, epoch 499/501 --> loss:0.8402918827533722
step 201/334, epoch 499/501 --> loss:0.8189954113960266
step 251/334, epoch 499/501 --> loss:0.8293000018596649
step 301/334, epoch 499/501 --> loss:0.8148104178905488
step 51/334, epoch 500/501 --> loss:0.8132848215103149
step 101/334, epoch 500/501 --> loss:0.8124044954776763
step 151/334, epoch 500/501 --> loss:0.841516170501709
step 201/334, epoch 500/501 --> loss:0.8153899896144867
step 251/334, epoch 500/501 --> loss:0.8311483573913574
step 301/334, epoch 500/501 --> loss:0.8162395322322845
step 51/334, epoch 501/501 --> loss:0.8339945602416993
step 101/334, epoch 501/501 --> loss:0.8081494843959809
step 151/334, epoch 501/501 --> loss:0.8455619513988495
step 201/334, epoch 501/501 --> loss:0.8204313945770264
step 251/334, epoch 501/501 --> loss:0.8269152283668518
step 301/334, epoch 501/501 --> loss:0.8275372457504272

##########train dataset##########
acc--> [99.7506855756161]
F1--> {'F1': [0.971460073027963], 'precision': [0.9467841141139584], 'recall': [0.9974672429180982]}
##########eval dataset##########
acc--> [99.1338347721117]
F1--> {'F1': [0.8967015446943651], 'precision': [0.8902450261620394], 'recall': [0.9032625455522317]}
