##########Config##########
{'device': 'cuda:1', 'class_nums': 2, 'data_path': '/Code/T1/Datasets/WHU-BCD', 'image_size': 256, 'num_parallel_workers': 4, 'batch_size': 16, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': '/Code/T1/Models/DeepLabv3', 'log_path': '/Code/T1/Logs/DeepLabv3', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (model): DeepLabV3(
    (backbone): IntermediateLayerGetter(
      (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (classifier): DeepLabHead(
      (0): ASPP(
        (convs): ModuleList(
          (0): Sequential(
            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (3): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (4): ASPPPooling(
            (0): AdaptiveAvgPool2d(output_size=1)
            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
          )
        )
        (project): Sequential(
          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.5, inplace=False)
        )
      )
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.7476975309848786
step 101/334, epoch 1/501 --> loss:0.5579006743431091
step 151/334, epoch 1/501 --> loss:0.4821378397941589
step 201/334, epoch 1/501 --> loss:0.46604905486106873
step 251/334, epoch 1/501 --> loss:0.48767969131469724
step 301/334, epoch 1/501 --> loss:0.44612851142883303

##########train dataset##########
acc--> [96.38977107994282]
F1--> {'F1': [0.2847478912025802], 'precision': [0.9064494921689505], 'recall': [0.16890504480554927]}
##########eval dataset##########
acc--> [96.58876554233176]
F1--> {'F1': [0.326787357607604], 'precision': [0.9150445650614951], 'recall': [0.19891432901251424]}
save model!
step 51/334, epoch 2/501 --> loss:0.42153236508369446
step 101/334, epoch 2/501 --> loss:0.38472784757614137
step 151/334, epoch 2/501 --> loss:0.35024062275886536
step 201/334, epoch 2/501 --> loss:0.3706527233123779
step 251/334, epoch 2/501 --> loss:0.43130829334259035
step 301/334, epoch 2/501 --> loss:0.4377564215660095
step 51/334, epoch 3/501 --> loss:0.3720284152030945
step 101/334, epoch 3/501 --> loss:0.41422823786735535
step 151/334, epoch 3/501 --> loss:0.37271639466285705
step 201/334, epoch 3/501 --> loss:0.38934268355369567
step 251/334, epoch 3/501 --> loss:0.3783608102798462
step 301/334, epoch 3/501 --> loss:0.35187345147132876
step 51/334, epoch 4/501 --> loss:0.3710263478755951
step 101/334, epoch 4/501 --> loss:0.3444834327697754
step 151/334, epoch 4/501 --> loss:0.3166697466373444
step 201/334, epoch 4/501 --> loss:0.4021047627925873
step 251/334, epoch 4/501 --> loss:0.427801605463028
step 301/334, epoch 4/501 --> loss:0.3932067346572876
step 51/334, epoch 5/501 --> loss:0.3454243791103363
step 101/334, epoch 5/501 --> loss:0.30503746747970584
step 151/334, epoch 5/501 --> loss:0.36904014825820924
step 201/334, epoch 5/501 --> loss:0.3485030233860016
step 251/334, epoch 5/501 --> loss:0.31470927596092224
step 301/334, epoch 5/501 --> loss:0.3384714961051941
step 51/334, epoch 6/501 --> loss:0.33517972826957704
step 101/334, epoch 6/501 --> loss:0.32654045343399046
step 151/334, epoch 6/501 --> loss:0.4098322522640228
step 201/334, epoch 6/501 --> loss:0.29527917861938474
step 251/334, epoch 6/501 --> loss:0.28654961585998534
step 301/334, epoch 6/501 --> loss:0.322920925617218
step 51/334, epoch 7/501 --> loss:0.3397967946529388
step 101/334, epoch 7/501 --> loss:0.33776825428009033
step 151/334, epoch 7/501 --> loss:0.3536353075504303
step 201/334, epoch 7/501 --> loss:0.34480602502822877
step 251/334, epoch 7/501 --> loss:0.3285378897190094
step 301/334, epoch 7/501 --> loss:0.3625829386711121
step 51/334, epoch 8/501 --> loss:0.3363452744483948
step 101/334, epoch 8/501 --> loss:0.378303679227829
step 151/334, epoch 8/501 --> loss:0.31754726290702817
step 201/334, epoch 8/501 --> loss:0.3631222009658813
step 251/334, epoch 8/501 --> loss:0.26818480253219606
step 301/334, epoch 8/501 --> loss:0.3258960425853729
step 51/334, epoch 9/501 --> loss:0.33057469725608823
step 101/334, epoch 9/501 --> loss:0.33623335003852844
step 151/334, epoch 9/501 --> loss:0.34958014130592346
step 201/334, epoch 9/501 --> loss:0.298129677772522
step 251/334, epoch 9/501 --> loss:0.2959302055835724
step 301/334, epoch 9/501 --> loss:0.40609200835227965
step 51/334, epoch 10/501 --> loss:0.37876141428947446
step 101/334, epoch 10/501 --> loss:0.2968622899055481
step 151/334, epoch 10/501 --> loss:0.3017762804031372
step 201/334, epoch 10/501 --> loss:0.2977243554592133
step 251/334, epoch 10/501 --> loss:0.3370184576511383
step 301/334, epoch 10/501 --> loss:0.25763004302978515
step 51/334, epoch 11/501 --> loss:0.29878111720085143
step 101/334, epoch 11/501 --> loss:0.3074550974369049
step 151/334, epoch 11/501 --> loss:0.3270847249031067
step 201/334, epoch 11/501 --> loss:0.3115033054351807
step 251/334, epoch 11/501 --> loss:0.35018444776535035
step 301/334, epoch 11/501 --> loss:0.31347630023956297

##########train dataset##########
acc--> [98.33737728431858]
F1--> {'F1': [0.7837426647771342], 'precision': [0.8774533556249036], 'recall': [0.7081249260405966]}
##########eval dataset##########
acc--> [98.37015774409987]
F1--> {'F1': [0.7830162897627587], 'precision': [0.8780680733808974], 'recall': [0.7065413681527853]}
save model!
step 51/334, epoch 12/501 --> loss:0.3222909939289093
step 101/334, epoch 12/501 --> loss:0.23502588987350465
step 151/334, epoch 12/501 --> loss:0.25819061636924745
step 201/334, epoch 12/501 --> loss:0.2709153389930725
step 251/334, epoch 12/501 --> loss:0.2598011887073517
step 301/334, epoch 12/501 --> loss:0.30592244029045107
step 51/334, epoch 13/501 --> loss:0.27973068594932554
step 101/334, epoch 13/501 --> loss:0.2306107485294342
step 151/334, epoch 13/501 --> loss:0.26446887612342834
step 201/334, epoch 13/501 --> loss:0.24850077509880067
step 251/334, epoch 13/501 --> loss:0.35357529401779175
step 301/334, epoch 13/501 --> loss:0.28751273155212403
step 51/334, epoch 14/501 --> loss:0.27763659834861754
step 101/334, epoch 14/501 --> loss:0.29164199113845823
step 151/334, epoch 14/501 --> loss:0.33024481058120725
step 201/334, epoch 14/501 --> loss:0.31638504028320313
step 251/334, epoch 14/501 --> loss:0.2626556384563446
step 301/334, epoch 14/501 --> loss:0.303182635307312
step 51/334, epoch 15/501 --> loss:0.28232556104660034
step 101/334, epoch 15/501 --> loss:0.31868523836135865
step 151/334, epoch 15/501 --> loss:0.2407773733139038
step 201/334, epoch 15/501 --> loss:0.2788954710960388
step 251/334, epoch 15/501 --> loss:0.2781001031398773
step 301/334, epoch 15/501 --> loss:0.30009847760200503
step 51/334, epoch 16/501 --> loss:0.41351796984672545
step 101/334, epoch 16/501 --> loss:0.28140741944313047
step 151/334, epoch 16/501 --> loss:0.32680890560150144
step 201/334, epoch 16/501 --> loss:0.25296875834465027
step 251/334, epoch 16/501 --> loss:0.24900103211402894
step 301/334, epoch 16/501 --> loss:0.2981995236873627
step 51/334, epoch 17/501 --> loss:0.2584579086303711
step 101/334, epoch 17/501 --> loss:0.28828234553337095
step 151/334, epoch 17/501 --> loss:0.2618756568431854
step 201/334, epoch 17/501 --> loss:0.23899246573448182
step 251/334, epoch 17/501 --> loss:0.24489058256149293
step 301/334, epoch 17/501 --> loss:0.24086779594421387
step 51/334, epoch 18/501 --> loss:0.2581174552440643
step 101/334, epoch 18/501 --> loss:0.2509991753101349
step 151/334, epoch 18/501 --> loss:0.27362124919891356
step 201/334, epoch 18/501 --> loss:0.21055437326431276
step 251/334, epoch 18/501 --> loss:0.23735954523086547
step 301/334, epoch 18/501 --> loss:0.2912841582298279
step 51/334, epoch 19/501 --> loss:0.25401576519012453
step 101/334, epoch 19/501 --> loss:0.22630765080451964
step 151/334, epoch 19/501 --> loss:0.29895859718322754
step 201/334, epoch 19/501 --> loss:0.25053591132164
step 251/334, epoch 19/501 --> loss:0.3643160653114319
step 301/334, epoch 19/501 --> loss:0.25161985397338865
step 51/334, epoch 20/501 --> loss:0.25101144790649416
step 101/334, epoch 20/501 --> loss:0.2682466471195221
step 151/334, epoch 20/501 --> loss:0.3212718641757965
step 201/334, epoch 20/501 --> loss:0.27282472252845763
step 251/334, epoch 20/501 --> loss:0.2442302668094635
step 301/334, epoch 20/501 --> loss:0.2312459397315979
step 51/334, epoch 21/501 --> loss:0.29310317993164064
step 101/334, epoch 21/501 --> loss:0.2864026081562042
step 151/334, epoch 21/501 --> loss:0.2556310153007507
step 201/334, epoch 21/501 --> loss:0.2707524502277374
step 251/334, epoch 21/501 --> loss:0.17788594484329223
step 301/334, epoch 21/501 --> loss:0.24811749339103698

##########train dataset##########
acc--> [98.4256448305872]
F1--> {'F1': [0.7853271329061395], 'precision': [0.9352335221231696], 'recall': [0.6768455441758845]}
##########eval dataset##########
acc--> [98.53364906076756]
F1--> {'F1': [0.7965895864598639], 'precision': [0.9424399150538253], 'recall': [0.6898396750994226]}
save model!
step 51/334, epoch 22/501 --> loss:0.2138188898563385
step 101/334, epoch 22/501 --> loss:0.16799660086631774
step 151/334, epoch 22/501 --> loss:0.30932334303855896
step 201/334, epoch 22/501 --> loss:0.24803539037704467
step 251/334, epoch 22/501 --> loss:0.2770391631126404
step 301/334, epoch 22/501 --> loss:0.32155120611190796
step 51/334, epoch 23/501 --> loss:0.2844113528728485
step 101/334, epoch 23/501 --> loss:0.2260182762145996
step 151/334, epoch 23/501 --> loss:0.24967895746231078
step 201/334, epoch 23/501 --> loss:0.23122344732284547
step 251/334, epoch 23/501 --> loss:0.2139375114440918
step 301/334, epoch 23/501 --> loss:0.23387878775596618
step 51/334, epoch 24/501 --> loss:0.23658524870872497
step 101/334, epoch 24/501 --> loss:0.2684341549873352
step 151/334, epoch 24/501 --> loss:0.16657466769218446
step 201/334, epoch 24/501 --> loss:0.2315379023551941
step 251/334, epoch 24/501 --> loss:0.3081736958026886
step 301/334, epoch 24/501 --> loss:0.19820569157600404
step 51/334, epoch 25/501 --> loss:0.28343949556350706
step 101/334, epoch 25/501 --> loss:0.20405824542045592
step 151/334, epoch 25/501 --> loss:0.22050556063652038
step 201/334, epoch 25/501 --> loss:0.2754915046691895
step 251/334, epoch 25/501 --> loss:0.23347426772117616
step 301/334, epoch 25/501 --> loss:0.2931479525566101
step 51/334, epoch 26/501 --> loss:0.21292321443557738
step 101/334, epoch 26/501 --> loss:0.24001550197601318
step 151/334, epoch 26/501 --> loss:0.2685584783554077
step 201/334, epoch 26/501 --> loss:0.20012996077537537
step 251/334, epoch 26/501 --> loss:0.22099973917007446
step 301/334, epoch 26/501 --> loss:0.2612177813053131
step 51/334, epoch 27/501 --> loss:0.21992894887924194
step 101/334, epoch 27/501 --> loss:0.24736808180809022
step 151/334, epoch 27/501 --> loss:0.29062440514564514
step 201/334, epoch 27/501 --> loss:0.32941506505012513
step 251/334, epoch 27/501 --> loss:0.2962493419647217
step 301/334, epoch 27/501 --> loss:0.18068501949310303
step 51/334, epoch 28/501 --> loss:0.24271273732185364
step 101/334, epoch 28/501 --> loss:0.19464457631111146
step 151/334, epoch 28/501 --> loss:0.22431620478630065
step 201/334, epoch 28/501 --> loss:0.2923994278907776
step 251/334, epoch 28/501 --> loss:0.31536197900772095
step 301/334, epoch 28/501 --> loss:0.27948702216148374
step 51/334, epoch 29/501 --> loss:0.2918361866474152
step 101/334, epoch 29/501 --> loss:0.20216877579689027
step 151/334, epoch 29/501 --> loss:0.25968278288841246
step 201/334, epoch 29/501 --> loss:0.3212719464302063
step 251/334, epoch 29/501 --> loss:0.2769495809078217
step 301/334, epoch 29/501 --> loss:0.23375393986701964
step 51/334, epoch 30/501 --> loss:0.21292800068855286
step 101/334, epoch 30/501 --> loss:0.2307613444328308
step 151/334, epoch 30/501 --> loss:0.25950467586517334
step 201/334, epoch 30/501 --> loss:0.2544778847694397
step 251/334, epoch 30/501 --> loss:0.23678593158721925
step 301/334, epoch 30/501 --> loss:0.27052727460861203
step 51/334, epoch 31/501 --> loss:0.245749591588974
step 101/334, epoch 31/501 --> loss:0.23832203269004823
step 151/334, epoch 31/501 --> loss:0.2452637791633606
step 201/334, epoch 31/501 --> loss:0.24662503838539124
step 251/334, epoch 31/501 --> loss:0.25892067074775693
step 301/334, epoch 31/501 --> loss:0.20468687891960144

##########train dataset##########
acc--> [98.57989877272139]
F1--> {'F1': [0.8192175357931413], 'precision': [0.893605116376502], 'recall': [0.7562713808530044]}
##########eval dataset##########
acc--> [98.54131383219384]
F1--> {'F1': [0.8078504036902514], 'precision': [0.8941949661401037], 'recall': [0.7367207764861037]}
save model!
step 51/334, epoch 32/501 --> loss:0.22263306140899658
step 101/334, epoch 32/501 --> loss:0.24137011051177978
step 151/334, epoch 32/501 --> loss:0.2483056640625
step 201/334, epoch 32/501 --> loss:0.20888927698135376
step 251/334, epoch 32/501 --> loss:0.2392365276813507
step 301/334, epoch 32/501 --> loss:0.20477645874023437
step 51/334, epoch 33/501 --> loss:0.26275792717933655
step 101/334, epoch 33/501 --> loss:0.2504925918579102
step 151/334, epoch 33/501 --> loss:0.2667413735389709
step 201/334, epoch 33/501 --> loss:0.21069531679153441
step 251/334, epoch 33/501 --> loss:0.19579189896583557
step 301/334, epoch 33/501 --> loss:0.24252742648124695
step 51/334, epoch 34/501 --> loss:0.32722811818122866
step 101/334, epoch 34/501 --> loss:0.2507438409328461
step 151/334, epoch 34/501 --> loss:0.2413484847545624
step 201/334, epoch 34/501 --> loss:0.24439504027366638
step 251/334, epoch 34/501 --> loss:0.2663736295700073
step 301/334, epoch 34/501 --> loss:0.21820898294448854
step 51/334, epoch 35/501 --> loss:0.20756134629249573
step 101/334, epoch 35/501 --> loss:0.24806548476219178
step 151/334, epoch 35/501 --> loss:0.1948782992362976
step 201/334, epoch 35/501 --> loss:0.3783018732070923
step 251/334, epoch 35/501 --> loss:0.2702387320995331
step 301/334, epoch 35/501 --> loss:0.21875515103340148
step 51/334, epoch 36/501 --> loss:0.2550094139575958
step 101/334, epoch 36/501 --> loss:0.19283952713012695
step 151/334, epoch 36/501 --> loss:0.20270987510681152
step 201/334, epoch 36/501 --> loss:0.2915260350704193
step 251/334, epoch 36/501 --> loss:0.2729708993434906
step 301/334, epoch 36/501 --> loss:0.298473664522171
step 51/334, epoch 37/501 --> loss:0.26213042974472045
step 101/334, epoch 37/501 --> loss:0.24754246354103088
step 151/334, epoch 37/501 --> loss:0.24141770601272583
step 201/334, epoch 37/501 --> loss:0.21458983302116394
step 251/334, epoch 37/501 --> loss:0.2386706840991974
step 301/334, epoch 37/501 --> loss:0.2274981677532196
step 51/334, epoch 38/501 --> loss:0.2521812736988068
step 101/334, epoch 38/501 --> loss:0.3168587946891785
step 151/334, epoch 38/501 --> loss:0.2091517400741577
step 201/334, epoch 38/501 --> loss:0.23080108046531678
step 251/334, epoch 38/501 --> loss:0.204710910320282
step 301/334, epoch 38/501 --> loss:0.1854877781867981
step 51/334, epoch 39/501 --> loss:0.22467167019844056
step 101/334, epoch 39/501 --> loss:0.22046322345733643
step 151/334, epoch 39/501 --> loss:0.2365647053718567
step 201/334, epoch 39/501 --> loss:0.1986615252494812
step 251/334, epoch 39/501 --> loss:0.20597544312477112
step 301/334, epoch 39/501 --> loss:0.24458019971847533
step 51/334, epoch 40/501 --> loss:0.2489564001560211
step 101/334, epoch 40/501 --> loss:0.21616897821426392
step 151/334, epoch 40/501 --> loss:0.2323315393924713
step 201/334, epoch 40/501 --> loss:0.19276342153549195
step 251/334, epoch 40/501 --> loss:0.24505746722221375
step 301/334, epoch 40/501 --> loss:0.213533593416214
step 51/334, epoch 41/501 --> loss:0.16885141849517823
step 101/334, epoch 41/501 --> loss:0.18830058455467225
step 151/334, epoch 41/501 --> loss:0.23331275463104248
step 201/334, epoch 41/501 --> loss:0.1925569188594818
step 251/334, epoch 41/501 --> loss:0.30581589579582213
step 301/334, epoch 41/501 --> loss:0.25572089791297914

##########train dataset##########
acc--> [98.5652933596285]
F1--> {'F1': [0.8364839033975006], 'precision': [0.8119694007737203], 'recall': [0.8625353658137213]}
##########eval dataset##########
acc--> [98.43118422299825]
F1--> {'F1': [0.8196594752168803], 'precision': [0.7858083882449365], 'recall': [0.8565692315696821]}
save model!
step 51/334, epoch 42/501 --> loss:0.1859411883354187
step 101/334, epoch 42/501 --> loss:0.1847783350944519
step 151/334, epoch 42/501 --> loss:0.21590843081474304
step 201/334, epoch 42/501 --> loss:0.2751374328136444
step 251/334, epoch 42/501 --> loss:0.18361209750175475
step 301/334, epoch 42/501 --> loss:0.22441431760787964
step 51/334, epoch 43/501 --> loss:0.2078172743320465
step 101/334, epoch 43/501 --> loss:0.1811268675327301
step 151/334, epoch 43/501 --> loss:0.23809244632720947
step 201/334, epoch 43/501 --> loss:0.246918683052063
step 251/334, epoch 43/501 --> loss:0.17781145691871644
step 301/334, epoch 43/501 --> loss:0.25073236107826236
step 51/334, epoch 44/501 --> loss:0.2862095534801483
step 101/334, epoch 44/501 --> loss:0.27177286744117735
step 151/334, epoch 44/501 --> loss:0.23469449043273927
step 201/334, epoch 44/501 --> loss:0.1944681203365326
step 251/334, epoch 44/501 --> loss:0.22903075337409973
step 301/334, epoch 44/501 --> loss:0.16818684220314026
step 51/334, epoch 45/501 --> loss:0.22597123622894288
step 101/334, epoch 45/501 --> loss:0.24472882628440856
step 151/334, epoch 45/501 --> loss:0.27257899165153504
step 201/334, epoch 45/501 --> loss:0.2526885449886322
step 251/334, epoch 45/501 --> loss:0.16634567379951476
step 301/334, epoch 45/501 --> loss:0.15993232488632203
step 51/334, epoch 46/501 --> loss:0.211108021736145
step 101/334, epoch 46/501 --> loss:0.16054603457450867
step 151/334, epoch 46/501 --> loss:0.26419156432151797
step 201/334, epoch 46/501 --> loss:0.19290109992027282
step 251/334, epoch 46/501 --> loss:0.2344251585006714
step 301/334, epoch 46/501 --> loss:0.2863147735595703
step 51/334, epoch 47/501 --> loss:0.18301358938217163
step 101/334, epoch 47/501 --> loss:0.2258167326450348
step 151/334, epoch 47/501 --> loss:0.2984159195423126
step 201/334, epoch 47/501 --> loss:0.21389829635620117
step 251/334, epoch 47/501 --> loss:0.1650582480430603
step 301/334, epoch 47/501 --> loss:0.24602604031562805
step 51/334, epoch 48/501 --> loss:0.26023394346237183
step 101/334, epoch 48/501 --> loss:0.239844970703125
step 151/334, epoch 48/501 --> loss:0.2356526470184326
step 201/334, epoch 48/501 --> loss:0.22546099066734315
step 251/334, epoch 48/501 --> loss:0.19358814001083374
step 301/334, epoch 48/501 --> loss:0.19695971488952638
step 51/334, epoch 49/501 --> loss:0.21346451997756957
step 101/334, epoch 49/501 --> loss:0.23607369899749756
step 151/334, epoch 49/501 --> loss:0.18544859409332276
step 201/334, epoch 49/501 --> loss:0.18069855213165284
step 251/334, epoch 49/501 --> loss:0.17347295641899108
step 301/334, epoch 49/501 --> loss:0.38662755250930786
step 51/334, epoch 50/501 --> loss:0.22679383397102357
step 101/334, epoch 50/501 --> loss:0.23836870312690736
step 151/334, epoch 50/501 --> loss:0.22393871903419493
step 201/334, epoch 50/501 --> loss:0.21799129009246826
step 251/334, epoch 50/501 --> loss:0.25200246095657347
step 301/334, epoch 50/501 --> loss:0.22561706185340882
step 51/334, epoch 51/501 --> loss:0.29891396760940553
step 101/334, epoch 51/501 --> loss:0.25512760877609253
step 151/334, epoch 51/501 --> loss:0.16677846789360046
step 201/334, epoch 51/501 --> loss:0.2547877371311188
step 251/334, epoch 51/501 --> loss:0.17129201531410218
step 301/334, epoch 51/501 --> loss:0.1955600368976593

##########train dataset##########
acc--> [98.64045705605487]
F1--> {'F1': [0.8436378037604643], 'precision': [0.8260015001648107], 'recall': [0.8620540949035158]}
##########eval dataset##########
acc--> [98.65990185674373]
F1--> {'F1': [0.8402027768632017], 'precision': [0.8340530151645408], 'recall': [0.8464540494151145]}
save model!
step 51/334, epoch 52/501 --> loss:0.17803068995475768
step 101/334, epoch 52/501 --> loss:0.17506761074066163
step 151/334, epoch 52/501 --> loss:0.2665903317928314
step 201/334, epoch 52/501 --> loss:0.28919938921928406
step 251/334, epoch 52/501 --> loss:0.2135755455493927
step 301/334, epoch 52/501 --> loss:0.20384804487228395
step 51/334, epoch 53/501 --> loss:0.13062259435653686
step 101/334, epoch 53/501 --> loss:0.23711376309394835
step 151/334, epoch 53/501 --> loss:0.19789675116539002
step 201/334, epoch 53/501 --> loss:0.21595481038093567
step 251/334, epoch 53/501 --> loss:0.1722743856906891
step 301/334, epoch 53/501 --> loss:0.2050584840774536
step 51/334, epoch 54/501 --> loss:0.2418623673915863
step 101/334, epoch 54/501 --> loss:0.21006634593009949
step 151/334, epoch 54/501 --> loss:0.20996171355247498
step 201/334, epoch 54/501 --> loss:0.23551763653755187
step 251/334, epoch 54/501 --> loss:0.19683757305145264
step 301/334, epoch 54/501 --> loss:0.18677323937416077
step 51/334, epoch 55/501 --> loss:0.1977125608921051
step 101/334, epoch 55/501 --> loss:0.2236580204963684
step 151/334, epoch 55/501 --> loss:0.265573011636734
step 201/334, epoch 55/501 --> loss:0.24599863529205324
step 251/334, epoch 55/501 --> loss:0.16826274514198303
step 301/334, epoch 55/501 --> loss:0.17678116202354432
step 51/334, epoch 56/501 --> loss:0.1550364911556244
step 101/334, epoch 56/501 --> loss:0.14924529790878296
step 151/334, epoch 56/501 --> loss:0.22590509414672852
step 201/334, epoch 56/501 --> loss:0.18307462215423584
step 251/334, epoch 56/501 --> loss:0.15013338685035704
step 301/334, epoch 56/501 --> loss:0.27870840191841123
step 51/334, epoch 57/501 --> loss:0.21851097345352172
step 101/334, epoch 57/501 --> loss:0.18664149403572083
step 151/334, epoch 57/501 --> loss:0.15926342725753784
step 201/334, epoch 57/501 --> loss:0.2321925163269043
step 251/334, epoch 57/501 --> loss:0.20319441318511963
step 301/334, epoch 57/501 --> loss:0.15675709009170533
step 51/334, epoch 58/501 --> loss:0.15006497621536255
step 101/334, epoch 58/501 --> loss:0.14269261479377746
step 151/334, epoch 58/501 --> loss:0.19262123703956605
step 201/334, epoch 58/501 --> loss:0.1534283399581909
step 251/334, epoch 58/501 --> loss:0.17954573631286622
step 301/334, epoch 58/501 --> loss:0.170390282869339
step 51/334, epoch 59/501 --> loss:0.20423763155937194
step 101/334, epoch 59/501 --> loss:0.15220164895057678
step 151/334, epoch 59/501 --> loss:0.20622382998466493
step 201/334, epoch 59/501 --> loss:0.16966285943984985
step 251/334, epoch 59/501 --> loss:0.18305941224098204
step 301/334, epoch 59/501 --> loss:0.24360184073448182
step 51/334, epoch 60/501 --> loss:0.15580896377563477
step 101/334, epoch 60/501 --> loss:0.18790292382240295
step 151/334, epoch 60/501 --> loss:0.17671902298927308
step 201/334, epoch 60/501 --> loss:0.22383567333221435
step 251/334, epoch 60/501 --> loss:0.19886441111564637
step 301/334, epoch 60/501 --> loss:0.21521623969078063
step 51/334, epoch 61/501 --> loss:0.15894176602363586
step 101/334, epoch 61/501 --> loss:0.1905635678768158
step 151/334, epoch 61/501 --> loss:0.14267028927803038
step 201/334, epoch 61/501 --> loss:0.18898901581764221
step 251/334, epoch 61/501 --> loss:0.1638626801967621
step 301/334, epoch 61/501 --> loss:0.13487215757369994

##########train dataset##########
acc--> [98.953456757081]
F1--> {'F1': [0.872338358685306], 'precision': [0.9067748824017449], 'recall': [0.8404309922861911]}
##########eval dataset##########
acc--> [98.92026779427789]
F1--> {'F1': [0.8642315489821004], 'precision': [0.9065939203325415], 'recall': [0.8256604832585756]}
save model!
step 51/334, epoch 62/501 --> loss:0.26395129799842837
step 101/334, epoch 62/501 --> loss:0.18997705101966858
step 151/334, epoch 62/501 --> loss:0.12818482279777527
step 201/334, epoch 62/501 --> loss:0.18798923134803772
step 251/334, epoch 62/501 --> loss:0.17953742384910584
step 301/334, epoch 62/501 --> loss:0.1708325958251953
step 51/334, epoch 63/501 --> loss:0.215581556558609
step 101/334, epoch 63/501 --> loss:0.12890350341796875
step 151/334, epoch 63/501 --> loss:0.20990898609161376
step 201/334, epoch 63/501 --> loss:0.21466599345207216
step 251/334, epoch 63/501 --> loss:0.2329338276386261
step 301/334, epoch 63/501 --> loss:0.2003963315486908
step 51/334, epoch 64/501 --> loss:0.21929312825202943
step 101/334, epoch 64/501 --> loss:0.17635428309440612
step 151/334, epoch 64/501 --> loss:0.13856342911720276
step 201/334, epoch 64/501 --> loss:0.17331411600112914
step 251/334, epoch 64/501 --> loss:0.171876220703125
step 301/334, epoch 64/501 --> loss:0.20281323552131653
step 51/334, epoch 65/501 --> loss:0.17173702359199525
step 101/334, epoch 65/501 --> loss:0.18640104055404663
step 151/334, epoch 65/501 --> loss:0.17584606766700744
step 201/334, epoch 65/501 --> loss:0.18013788223266602
step 251/334, epoch 65/501 --> loss:0.17762142062187194
step 301/334, epoch 65/501 --> loss:0.2542885982990265
step 51/334, epoch 66/501 --> loss:0.18427172303199768
step 101/334, epoch 66/501 --> loss:0.24569642901420594
step 151/334, epoch 66/501 --> loss:0.18074238657951355
step 201/334, epoch 66/501 --> loss:0.22380752086639405
step 251/334, epoch 66/501 --> loss:0.17172993898391722
step 301/334, epoch 66/501 --> loss:0.2449024188518524
step 51/334, epoch 67/501 --> loss:0.1712847411632538
step 101/334, epoch 67/501 --> loss:0.19973597168922425
step 151/334, epoch 67/501 --> loss:0.19553662180900575
step 201/334, epoch 67/501 --> loss:0.2002047896385193
step 251/334, epoch 67/501 --> loss:0.21330750346183777
step 301/334, epoch 67/501 --> loss:0.14946142435073853
step 51/334, epoch 68/501 --> loss:0.20500248074531555
step 101/334, epoch 68/501 --> loss:0.18482041358947754
step 151/334, epoch 68/501 --> loss:0.2534194540977478
step 201/334, epoch 68/501 --> loss:0.1600211524963379
step 251/334, epoch 68/501 --> loss:0.1428154706954956
step 301/334, epoch 68/501 --> loss:0.17834897994995116
step 51/334, epoch 69/501 --> loss:0.15489511966705322
step 101/334, epoch 69/501 --> loss:0.17645066380500793
step 151/334, epoch 69/501 --> loss:0.15018584370613097
step 201/334, epoch 69/501 --> loss:0.2176857817173004
step 251/334, epoch 69/501 --> loss:0.20974727749824523
step 301/334, epoch 69/501 --> loss:0.1530558931827545
step 51/334, epoch 70/501 --> loss:0.18309393525123596
step 101/334, epoch 70/501 --> loss:0.18458084940910338
step 151/334, epoch 70/501 --> loss:0.1935193145275116
step 201/334, epoch 70/501 --> loss:0.17816869497299195
step 251/334, epoch 70/501 --> loss:0.19908053517341615
step 301/334, epoch 70/501 --> loss:0.20214881300926207
step 51/334, epoch 71/501 --> loss:0.1663108456134796
step 101/334, epoch 71/501 --> loss:0.18088152766227722
step 151/334, epoch 71/501 --> loss:0.16530738830566405
step 201/334, epoch 71/501 --> loss:0.13694076538085936
step 251/334, epoch 71/501 --> loss:0.1380262529850006
step 301/334, epoch 71/501 --> loss:0.15114067673683165

##########train dataset##########
acc--> [99.19224050011653]
F1--> {'F1': [0.9023479348576594], 'precision': [0.928987441016179], 'recall': [0.8772031019712897]}
##########eval dataset##########
acc--> [99.08980455506364]
F1--> {'F1': [0.886867953257747], 'precision': [0.918712073748407], 'recall': [0.8571667515250997]}
save model!
step 51/334, epoch 72/501 --> loss:0.1793038535118103
step 101/334, epoch 72/501 --> loss:0.10938571333885193
step 151/334, epoch 72/501 --> loss:0.17673368334770204
step 201/334, epoch 72/501 --> loss:0.12675958156585693
step 251/334, epoch 72/501 --> loss:0.18684363961219788
step 301/334, epoch 72/501 --> loss:0.16547176837921143
step 51/334, epoch 73/501 --> loss:0.15300921559333802
step 101/334, epoch 73/501 --> loss:0.15433674931526184
step 151/334, epoch 73/501 --> loss:0.13759815335273742
step 201/334, epoch 73/501 --> loss:0.15138461709022522
step 251/334, epoch 73/501 --> loss:0.15842450737953187
step 301/334, epoch 73/501 --> loss:0.19835202217102052
step 51/334, epoch 74/501 --> loss:0.15711398243904115
step 101/334, epoch 74/501 --> loss:0.1687372326850891
step 151/334, epoch 74/501 --> loss:0.16320922136306762
step 201/334, epoch 74/501 --> loss:0.15126827597618103
step 251/334, epoch 74/501 --> loss:0.21589712500572206
step 301/334, epoch 74/501 --> loss:0.27645756721496584
step 51/334, epoch 75/501 --> loss:0.17883096694946288
step 101/334, epoch 75/501 --> loss:0.10416142344474792
step 151/334, epoch 75/501 --> loss:0.16036787986755371
step 201/334, epoch 75/501 --> loss:0.18407274961471556
step 251/334, epoch 75/501 --> loss:0.18137521505355836
step 301/334, epoch 75/501 --> loss:0.241652535200119
step 51/334, epoch 76/501 --> loss:0.21401506304740905
step 101/334, epoch 76/501 --> loss:0.14812482357025147
step 151/334, epoch 76/501 --> loss:0.17697468280792236
step 201/334, epoch 76/501 --> loss:0.1790384340286255
step 251/334, epoch 76/501 --> loss:0.12133561849594116
step 301/334, epoch 76/501 --> loss:0.21228383660316466
step 51/334, epoch 77/501 --> loss:0.18953026413917543
step 101/334, epoch 77/501 --> loss:0.21851784467697144
step 151/334, epoch 77/501 --> loss:0.23745071053504943
step 201/334, epoch 77/501 --> loss:0.15542666912078856
step 251/334, epoch 77/501 --> loss:0.1562082886695862
step 301/334, epoch 77/501 --> loss:0.18843074917793273
step 51/334, epoch 78/501 --> loss:0.18028513312339783
step 101/334, epoch 78/501 --> loss:0.22567293643951417
step 151/334, epoch 78/501 --> loss:0.20874180316925048
step 201/334, epoch 78/501 --> loss:0.2073957097530365
step 251/334, epoch 78/501 --> loss:0.18063674569129945
step 301/334, epoch 78/501 --> loss:0.1569903886318207
step 51/334, epoch 79/501 --> loss:0.2330706799030304
step 101/334, epoch 79/501 --> loss:0.15839693307876587
step 151/334, epoch 79/501 --> loss:0.1769816505908966
step 201/334, epoch 79/501 --> loss:0.1721516764163971
step 251/334, epoch 79/501 --> loss:0.17316193580627443
step 301/334, epoch 79/501 --> loss:0.16915544271469116
step 51/334, epoch 80/501 --> loss:0.17547318696975708
step 101/334, epoch 80/501 --> loss:0.18424056529998778
step 151/334, epoch 80/501 --> loss:0.20840916514396668
step 201/334, epoch 80/501 --> loss:0.11489965200424195
step 251/334, epoch 80/501 --> loss:0.15429619789123536
step 301/334, epoch 80/501 --> loss:0.2014043891429901
step 51/334, epoch 81/501 --> loss:0.17014700174331665
step 101/334, epoch 81/501 --> loss:0.1559254789352417
step 151/334, epoch 81/501 --> loss:0.16681775212287903
step 201/334, epoch 81/501 --> loss:0.17566959381103517
step 251/334, epoch 81/501 --> loss:0.13400251269340516
step 301/334, epoch 81/501 --> loss:0.1456548261642456

##########train dataset##########
acc--> [99.1034477356867]
F1--> {'F1': [0.8945752065630994], 'precision': [0.8950867096956293], 'recall': [0.894074276277678]}
##########eval dataset##########
acc--> [98.96351771716961]
F1--> {'F1': [0.8750297560528119], 'precision': [0.8782593706893171], 'recall': [0.8718337335762246]}
step 51/334, epoch 82/501 --> loss:0.12367725014686584
step 101/334, epoch 82/501 --> loss:0.15998226523399353
step 151/334, epoch 82/501 --> loss:0.13753331422805787
step 201/334, epoch 82/501 --> loss:0.1835404109954834
step 251/334, epoch 82/501 --> loss:0.13032370924949646
step 301/334, epoch 82/501 --> loss:0.14540842771530152
step 51/334, epoch 83/501 --> loss:0.18859244108200074
step 101/334, epoch 83/501 --> loss:0.19315590858459472
step 151/334, epoch 83/501 --> loss:0.21910441398620606
step 201/334, epoch 83/501 --> loss:0.15975903391838073
step 251/334, epoch 83/501 --> loss:0.17335317730903627
step 301/334, epoch 83/501 --> loss:0.15581942558288575
step 51/334, epoch 84/501 --> loss:0.16225553989410402
step 101/334, epoch 84/501 --> loss:0.22249111413955688
step 151/334, epoch 84/501 --> loss:0.15986187100410462
step 201/334, epoch 84/501 --> loss:0.17455915808677674
step 251/334, epoch 84/501 --> loss:0.20484598517417907
step 301/334, epoch 84/501 --> loss:0.1945418882369995
step 51/334, epoch 85/501 --> loss:0.16675445318222046
step 101/334, epoch 85/501 --> loss:0.12891337156295776
step 151/334, epoch 85/501 --> loss:0.10645226240158082
step 201/334, epoch 85/501 --> loss:0.1357560086250305
step 251/334, epoch 85/501 --> loss:0.17400684118270873
step 301/334, epoch 85/501 --> loss:0.16825955748558044
step 51/334, epoch 86/501 --> loss:0.1657759976387024
step 101/334, epoch 86/501 --> loss:0.15603153109550477
step 151/334, epoch 86/501 --> loss:0.16027816414833068
step 201/334, epoch 86/501 --> loss:0.17554058790206908
step 251/334, epoch 86/501 --> loss:0.1844780457019806
step 301/334, epoch 86/501 --> loss:0.16205185294151306
step 51/334, epoch 87/501 --> loss:0.119066082239151
step 101/334, epoch 87/501 --> loss:0.17437036633491515
step 151/334, epoch 87/501 --> loss:0.20697299838066102
step 201/334, epoch 87/501 --> loss:0.22595129609107972
step 251/334, epoch 87/501 --> loss:0.1193024218082428
step 301/334, epoch 87/501 --> loss:0.1648946499824524
step 51/334, epoch 88/501 --> loss:0.13925768852233886
step 101/334, epoch 88/501 --> loss:0.1455379283428192
step 151/334, epoch 88/501 --> loss:0.1622105920314789
step 201/334, epoch 88/501 --> loss:0.13521520018577576
step 251/334, epoch 88/501 --> loss:0.1736710226535797
step 301/334, epoch 88/501 --> loss:0.24742139220237733
step 51/334, epoch 89/501 --> loss:0.2380080533027649
step 101/334, epoch 89/501 --> loss:0.1816568672657013
step 151/334, epoch 89/501 --> loss:0.1884984052181244
step 201/334, epoch 89/501 --> loss:0.2048674249649048
step 251/334, epoch 89/501 --> loss:0.22100689888000488
step 301/334, epoch 89/501 --> loss:0.2129779875278473
step 51/334, epoch 90/501 --> loss:0.17864025235176087
step 101/334, epoch 90/501 --> loss:0.15613847017288207
step 151/334, epoch 90/501 --> loss:0.18259647488594055
step 201/334, epoch 90/501 --> loss:0.15259711742401122
step 251/334, epoch 90/501 --> loss:0.12779155850410462
step 301/334, epoch 90/501 --> loss:0.1696013355255127
step 51/334, epoch 91/501 --> loss:0.1723643171787262
step 101/334, epoch 91/501 --> loss:0.14688243746757507
step 151/334, epoch 91/501 --> loss:0.12117915749549865
step 201/334, epoch 91/501 --> loss:0.13041574835777284
step 251/334, epoch 91/501 --> loss:0.1306512713432312
step 301/334, epoch 91/501 --> loss:0.14430052995681764

##########train dataset##########
acc--> [99.18475471098665]
F1--> {'F1': [0.9013903021185681], 'precision': [0.9285274113331174], 'recall': [0.8758037990677976]}
##########eval dataset##########
acc--> [99.01141669805871]
F1--> {'F1': [0.877357672872849], 'precision': [0.9070212341202801], 'recall': [0.8495822892890627]}
step 51/334, epoch 92/501 --> loss:0.13517752647399903
step 101/334, epoch 92/501 --> loss:0.16895446181297302
step 151/334, epoch 92/501 --> loss:0.14920956254005432
step 201/334, epoch 92/501 --> loss:0.18889232277870177
step 251/334, epoch 92/501 --> loss:0.15487669110298158
step 301/334, epoch 92/501 --> loss:0.17156473636627198
step 51/334, epoch 93/501 --> loss:0.22118326783180237
step 101/334, epoch 93/501 --> loss:0.12230082392692566
step 151/334, epoch 93/501 --> loss:0.13458057284355163
step 201/334, epoch 93/501 --> loss:0.1849871349334717
step 251/334, epoch 93/501 --> loss:0.1781715679168701
step 301/334, epoch 93/501 --> loss:0.1822791588306427
step 51/334, epoch 94/501 --> loss:0.1461631464958191
step 101/334, epoch 94/501 --> loss:0.15863441348075866
step 151/334, epoch 94/501 --> loss:0.15099295496940612
step 201/334, epoch 94/501 --> loss:0.12067671775817872
step 251/334, epoch 94/501 --> loss:0.14859036207199097
step 301/334, epoch 94/501 --> loss:0.14145421266555785
step 51/334, epoch 95/501 --> loss:0.17721280932426453
step 101/334, epoch 95/501 --> loss:0.13975391745567323
step 151/334, epoch 95/501 --> loss:0.12658335208892824
step 201/334, epoch 95/501 --> loss:0.15932106375694274
step 251/334, epoch 95/501 --> loss:0.11901663780212403
step 301/334, epoch 95/501 --> loss:0.15461087346076965
step 51/334, epoch 96/501 --> loss:0.1481670343875885
step 101/334, epoch 96/501 --> loss:0.2035491919517517
step 151/334, epoch 96/501 --> loss:0.13801755905151367
step 201/334, epoch 96/501 --> loss:0.17265741467475892
step 251/334, epoch 96/501 --> loss:0.16707762598991394
step 301/334, epoch 96/501 --> loss:0.17064077258110047
step 51/334, epoch 97/501 --> loss:0.28055827975273134
step 101/334, epoch 97/501 --> loss:0.14804869771003723
step 151/334, epoch 97/501 --> loss:0.1820910930633545
step 201/334, epoch 97/501 --> loss:0.1826237905025482
step 251/334, epoch 97/501 --> loss:0.15667204260826112
step 301/334, epoch 97/501 --> loss:0.10273555994033813
step 51/334, epoch 98/501 --> loss:0.14992215991020202
step 101/334, epoch 98/501 --> loss:0.15786356925964357
step 151/334, epoch 98/501 --> loss:0.20421170592308044
step 201/334, epoch 98/501 --> loss:0.14171300888061522
step 251/334, epoch 98/501 --> loss:0.15552706122398377
step 301/334, epoch 98/501 --> loss:0.1189074158668518
step 51/334, epoch 99/501 --> loss:0.15061562061309813
step 101/334, epoch 99/501 --> loss:0.1495841109752655
step 151/334, epoch 99/501 --> loss:0.1386645519733429
step 201/334, epoch 99/501 --> loss:0.1364276587963104
step 251/334, epoch 99/501 --> loss:0.14005335807800293
step 301/334, epoch 99/501 --> loss:0.09238701939582825
step 51/334, epoch 100/501 --> loss:0.11492305397987365
step 101/334, epoch 100/501 --> loss:0.10075450778007507
step 151/334, epoch 100/501 --> loss:0.1471947407722473
step 201/334, epoch 100/501 --> loss:0.1634758973121643
step 251/334, epoch 100/501 --> loss:0.21253393411636354
step 301/334, epoch 100/501 --> loss:0.20434921979904175
step 51/334, epoch 101/501 --> loss:0.15723849773406984
step 101/334, epoch 101/501 --> loss:0.18778107762336732
step 151/334, epoch 101/501 --> loss:0.15643402218818664
step 201/334, epoch 101/501 --> loss:0.1436118447780609
step 251/334, epoch 101/501 --> loss:0.11983575344085694
step 301/334, epoch 101/501 --> loss:0.10383744001388549

##########train dataset##########
acc--> [99.21835379784396]
F1--> {'F1': [0.9064675050777078], 'precision': [0.9232653938918559], 'recall': [0.8902795781914686]}
##########eval dataset##########
acc--> [98.95858964894352]
F1--> {'F1': [0.8725895604511202], 'precision': [0.8889766854385638], 'recall': [0.8568052888360199]}
step 51/334, epoch 102/501 --> loss:0.12774326920509338
step 101/334, epoch 102/501 --> loss:0.12528805255889894
step 151/334, epoch 102/501 --> loss:0.08352608323097228
step 201/334, epoch 102/501 --> loss:0.1287536907196045
step 251/334, epoch 102/501 --> loss:0.16055755972862243
step 301/334, epoch 102/501 --> loss:0.11490179538726807
step 51/334, epoch 103/501 --> loss:0.25992443203926086
step 101/334, epoch 103/501 --> loss:0.14547431349754333
step 151/334, epoch 103/501 --> loss:0.16575151324272155
step 201/334, epoch 103/501 --> loss:0.18576483130455018
step 251/334, epoch 103/501 --> loss:0.15149354696273803
step 301/334, epoch 103/501 --> loss:0.15169672012329102
step 51/334, epoch 104/501 --> loss:0.12589036345481872
step 101/334, epoch 104/501 --> loss:0.11766699552536011
step 151/334, epoch 104/501 --> loss:0.13310965657234192
step 201/334, epoch 104/501 --> loss:0.17930583119392396
step 251/334, epoch 104/501 --> loss:0.15816197037696839
step 301/334, epoch 104/501 --> loss:0.14519248962402342
step 51/334, epoch 105/501 --> loss:0.15369195699691773
step 101/334, epoch 105/501 --> loss:0.14163598537445068
step 151/334, epoch 105/501 --> loss:0.10646964311599731
step 201/334, epoch 105/501 --> loss:0.17460723400115966
step 251/334, epoch 105/501 --> loss:0.1302299952507019
step 301/334, epoch 105/501 --> loss:0.09412460803985595
step 51/334, epoch 106/501 --> loss:0.16232640266418458
step 101/334, epoch 106/501 --> loss:0.17792023181915284
step 151/334, epoch 106/501 --> loss:0.1396838104724884
step 201/334, epoch 106/501 --> loss:0.16198967814445495
step 251/334, epoch 106/501 --> loss:0.13407073855400087
step 301/334, epoch 106/501 --> loss:0.1651322865486145
step 51/334, epoch 107/501 --> loss:0.175671706199646
step 101/334, epoch 107/501 --> loss:0.15915846943855286
step 151/334, epoch 107/501 --> loss:0.1966548478603363
step 201/334, epoch 107/501 --> loss:0.1589191198348999
step 251/334, epoch 107/501 --> loss:0.22156475901603698
step 301/334, epoch 107/501 --> loss:0.1790028977394104
step 51/334, epoch 108/501 --> loss:0.19412410736083985
step 101/334, epoch 108/501 --> loss:0.14516859292984008
step 151/334, epoch 108/501 --> loss:0.18318985342979432
step 201/334, epoch 108/501 --> loss:0.15854721546173095
step 251/334, epoch 108/501 --> loss:0.11871891260147095
step 301/334, epoch 108/501 --> loss:0.15548787832260133
step 51/334, epoch 109/501 --> loss:0.18834500551223754
step 101/334, epoch 109/501 --> loss:0.18814136147499083
step 151/334, epoch 109/501 --> loss:0.1659810423851013
step 201/334, epoch 109/501 --> loss:0.1371190905570984
step 251/334, epoch 109/501 --> loss:0.1382396149635315
step 301/334, epoch 109/501 --> loss:0.14162444472312927
step 51/334, epoch 110/501 --> loss:0.10718149662017823
step 101/334, epoch 110/501 --> loss:0.12127633810043335
step 151/334, epoch 110/501 --> loss:0.1404282796382904
step 201/334, epoch 110/501 --> loss:0.17863044142723083
step 251/334, epoch 110/501 --> loss:0.14424003839492797
step 301/334, epoch 110/501 --> loss:0.16139559388160707
step 51/334, epoch 111/501 --> loss:0.14067073345184325
step 101/334, epoch 111/501 --> loss:0.12733723640441894
step 151/334, epoch 111/501 --> loss:0.09670096039772033
step 201/334, epoch 111/501 --> loss:0.11625431418418884
step 251/334, epoch 111/501 --> loss:0.14697438955307007
step 301/334, epoch 111/501 --> loss:0.11178812980651856

##########train dataset##########
acc--> [99.34094446537404]
F1--> {'F1': [0.9206364072029503], 'precision': [0.9438986052357922], 'recall': [0.8985027351349962]}
##########eval dataset##########
acc--> [99.12151960771087]
F1--> {'F1': [0.8912331006842117], 'precision': [0.9194133079698034], 'recall': [0.8647383845618793]}
save model!
step 51/334, epoch 112/501 --> loss:0.11214937448501587
step 101/334, epoch 112/501 --> loss:0.15499207735061646
step 151/334, epoch 112/501 --> loss:0.1197148609161377
step 201/334, epoch 112/501 --> loss:0.13532892227172852
step 251/334, epoch 112/501 --> loss:0.14020982027053833
step 301/334, epoch 112/501 --> loss:0.10604513525962829
step 51/334, epoch 113/501 --> loss:0.09090335369110107
step 101/334, epoch 113/501 --> loss:0.1034398090839386
step 151/334, epoch 113/501 --> loss:0.13878363609313965
step 201/334, epoch 113/501 --> loss:0.14074122071266174
step 251/334, epoch 113/501 --> loss:0.12711477160453796
step 301/334, epoch 113/501 --> loss:0.11875651717185974
step 51/334, epoch 114/501 --> loss:0.15603694558143616
step 101/334, epoch 114/501 --> loss:0.16729022622108458
step 151/334, epoch 114/501 --> loss:0.19708427548408508
step 201/334, epoch 114/501 --> loss:0.17186575770378112
step 251/334, epoch 114/501 --> loss:0.1483728837966919
step 301/334, epoch 114/501 --> loss:0.17009958863258362
step 51/334, epoch 115/501 --> loss:0.12035995006561279
step 101/334, epoch 115/501 --> loss:0.16017601490020753
step 151/334, epoch 115/501 --> loss:0.12195834398269653
step 201/334, epoch 115/501 --> loss:0.0909142816066742
step 251/334, epoch 115/501 --> loss:0.1416885244846344
step 301/334, epoch 115/501 --> loss:0.12547543048858642
step 51/334, epoch 116/501 --> loss:0.1068183434009552
step 101/334, epoch 116/501 --> loss:0.09865296125411988
step 151/334, epoch 116/501 --> loss:0.12030548572540284
step 201/334, epoch 116/501 --> loss:0.11372672080993652
step 251/334, epoch 116/501 --> loss:0.15640186190605163
step 301/334, epoch 116/501 --> loss:0.10400006532669068
step 51/334, epoch 117/501 --> loss:0.14219367980957032
step 101/334, epoch 117/501 --> loss:0.1309283208847046
step 151/334, epoch 117/501 --> loss:0.10018608093261719
step 201/334, epoch 117/501 --> loss:0.1397413682937622
step 251/334, epoch 117/501 --> loss:0.16423893451690674
step 301/334, epoch 117/501 --> loss:0.1448514246940613
step 51/334, epoch 118/501 --> loss:0.1340239989757538
step 101/334, epoch 118/501 --> loss:0.14395752787590027
step 151/334, epoch 118/501 --> loss:0.14007538795471192
step 201/334, epoch 118/501 --> loss:0.13143522262573243
step 251/334, epoch 118/501 --> loss:0.22523311018943787
step 301/334, epoch 118/501 --> loss:0.18412303805351257
step 51/334, epoch 119/501 --> loss:0.14867992758750914
step 101/334, epoch 119/501 --> loss:0.13990370631217958
step 151/334, epoch 119/501 --> loss:0.13739238381385804
step 201/334, epoch 119/501 --> loss:0.11827152132987977
step 251/334, epoch 119/501 --> loss:0.11459993600845336
step 301/334, epoch 119/501 --> loss:0.10265207290649414
step 51/334, epoch 120/501 --> loss:0.17742509722709657
step 101/334, epoch 120/501 --> loss:0.16148458838462829
step 151/334, epoch 120/501 --> loss:0.14822569608688355
step 201/334, epoch 120/501 --> loss:0.1766338098049164
step 251/334, epoch 120/501 --> loss:0.12180732488632202
step 301/334, epoch 120/501 --> loss:0.13015183329582214
step 51/334, epoch 121/501 --> loss:0.14823612332344055
step 101/334, epoch 121/501 --> loss:0.11667117118835449
step 151/334, epoch 121/501 --> loss:0.10765933990478516
step 201/334, epoch 121/501 --> loss:0.09351487636566162
step 251/334, epoch 121/501 --> loss:0.13341232895851135
step 301/334, epoch 121/501 --> loss:0.09214922904968262

##########train dataset##########
acc--> [99.38992534990459]
F1--> {'F1': [0.927789712004659], 'precision': [0.9344565845130041], 'recall': [0.9212271534144004]}
##########eval dataset##########
acc--> [99.03647622202081]
F1--> {'F1': [0.8847480565255197], 'precision': [0.8809704343139046], 'recall': [0.8885683014526846]}
step 51/334, epoch 122/501 --> loss:0.09052126169204712
step 101/334, epoch 122/501 --> loss:0.10868974208831787
step 151/334, epoch 122/501 --> loss:0.1097601318359375
step 201/334, epoch 122/501 --> loss:0.1264832854270935
step 251/334, epoch 122/501 --> loss:0.15001875281333923
step 301/334, epoch 122/501 --> loss:0.12493920683860779
step 51/334, epoch 123/501 --> loss:0.11313716530799865
step 101/334, epoch 123/501 --> loss:0.13606849312782288
step 151/334, epoch 123/501 --> loss:0.12976797223091124
step 201/334, epoch 123/501 --> loss:0.1154567265510559
step 251/334, epoch 123/501 --> loss:0.13928889513015746
step 301/334, epoch 123/501 --> loss:0.13521077752113342
step 51/334, epoch 124/501 --> loss:0.11081193447113037
step 101/334, epoch 124/501 --> loss:0.15055607318878172
step 151/334, epoch 124/501 --> loss:0.11794583916664124
step 201/334, epoch 124/501 --> loss:0.09786713480949402
step 251/334, epoch 124/501 --> loss:0.14084387540817261
step 301/334, epoch 124/501 --> loss:0.12400347352027893
step 51/334, epoch 125/501 --> loss:0.09107632517814636
step 101/334, epoch 125/501 --> loss:0.08767025113105774
step 151/334, epoch 125/501 --> loss:0.10258229732513428
step 201/334, epoch 125/501 --> loss:0.09551258087158203
step 251/334, epoch 125/501 --> loss:0.11130277156829833
step 301/334, epoch 125/501 --> loss:0.16584232330322266
step 51/334, epoch 126/501 --> loss:0.10820036292076111
step 101/334, epoch 126/501 --> loss:0.18361611127853394
step 151/334, epoch 126/501 --> loss:0.09729665279388428
step 201/334, epoch 126/501 --> loss:0.17914507031440735
step 251/334, epoch 126/501 --> loss:0.0973426079750061
step 301/334, epoch 126/501 --> loss:0.08859808921813965
step 51/334, epoch 127/501 --> loss:0.1271114468574524
step 101/334, epoch 127/501 --> loss:0.13951159715652467
step 151/334, epoch 127/501 --> loss:0.11539029955863953
step 201/334, epoch 127/501 --> loss:0.11074857711791992
step 251/334, epoch 127/501 --> loss:0.13337486743927002
step 301/334, epoch 127/501 --> loss:0.09256748557090759
step 51/334, epoch 128/501 --> loss:0.15862540125846863
step 101/334, epoch 128/501 --> loss:0.11402883529663085
step 151/334, epoch 128/501 --> loss:0.12525247931480407
step 201/334, epoch 128/501 --> loss:0.1602258026599884
step 251/334, epoch 128/501 --> loss:0.15447935461997986
step 301/334, epoch 128/501 --> loss:0.10304102182388306
step 51/334, epoch 129/501 --> loss:0.1536218059062958
step 101/334, epoch 129/501 --> loss:0.12207981944084167
step 151/334, epoch 129/501 --> loss:0.13109467625617982
step 201/334, epoch 129/501 --> loss:0.07842829346656799
step 251/334, epoch 129/501 --> loss:0.08078374743461608
step 301/334, epoch 129/501 --> loss:0.1993497669696808
step 51/334, epoch 130/501 --> loss:0.10036178827285766
step 101/334, epoch 130/501 --> loss:0.15625009655952454
step 151/334, epoch 130/501 --> loss:0.12836507797241212
step 201/334, epoch 130/501 --> loss:0.09040195941925049
step 251/334, epoch 130/501 --> loss:0.08833937883377076
step 301/334, epoch 130/501 --> loss:0.11283162236213684
step 51/334, epoch 131/501 --> loss:0.13809824228286743
step 101/334, epoch 131/501 --> loss:0.16489089608192445
step 151/334, epoch 131/501 --> loss:0.25929021954536435
step 201/334, epoch 131/501 --> loss:0.1922648012638092
step 251/334, epoch 131/501 --> loss:0.14893987774848938
step 301/334, epoch 131/501 --> loss:0.09017241835594177

##########train dataset##########
acc--> [99.11530748216126]
F1--> {'F1': [0.8973414085972519], 'precision': [0.8861562792815351], 'recall': [0.9088227621367212]}
##########eval dataset##########
acc--> [98.78768653902343]
F1--> {'F1': [0.8570616569504151], 'precision': [0.8414851097117236], 'recall': [0.8732361254200589]}
step 51/334, epoch 132/501 --> loss:0.15834647417068481
step 101/334, epoch 132/501 --> loss:0.16121504187583924
step 151/334, epoch 132/501 --> loss:0.1884285545349121
step 201/334, epoch 132/501 --> loss:0.11418473005294799
step 251/334, epoch 132/501 --> loss:0.18050103068351744
step 301/334, epoch 132/501 --> loss:0.11648410320281982
step 51/334, epoch 133/501 --> loss:0.09852581858634948
step 101/334, epoch 133/501 --> loss:0.1075363564491272
step 151/334, epoch 133/501 --> loss:0.1198930311203003
step 201/334, epoch 133/501 --> loss:0.10705221414566041
step 251/334, epoch 133/501 --> loss:0.11647919178009034
step 301/334, epoch 133/501 --> loss:0.13258952856063844
step 51/334, epoch 134/501 --> loss:0.12130529165267945
step 101/334, epoch 134/501 --> loss:0.11339084386825561
step 151/334, epoch 134/501 --> loss:0.14940744161605835
step 201/334, epoch 134/501 --> loss:0.09159752607345581
step 251/334, epoch 134/501 --> loss:0.11558839797973633
step 301/334, epoch 134/501 --> loss:0.1275285029411316
step 51/334, epoch 135/501 --> loss:0.19493820190429687
step 101/334, epoch 135/501 --> loss:0.1918283247947693
step 151/334, epoch 135/501 --> loss:0.14853415966033937
step 201/334, epoch 135/501 --> loss:0.19491872668266297
step 251/334, epoch 135/501 --> loss:0.17109360456466674
step 301/334, epoch 135/501 --> loss:0.10019781351089478
step 51/334, epoch 136/501 --> loss:0.11181456208229065
step 101/334, epoch 136/501 --> loss:0.11825516819953918
step 151/334, epoch 136/501 --> loss:0.09617239713668824
step 201/334, epoch 136/501 --> loss:0.1400609266757965
step 251/334, epoch 136/501 --> loss:0.16987556099891662
step 301/334, epoch 136/501 --> loss:0.1624035942554474
step 51/334, epoch 137/501 --> loss:0.17667338252067566
step 101/334, epoch 137/501 --> loss:0.1895248281955719
step 151/334, epoch 137/501 --> loss:0.16921979427337647
step 201/334, epoch 137/501 --> loss:0.12140102624893188
step 251/334, epoch 137/501 --> loss:0.12690325498580932
step 301/334, epoch 137/501 --> loss:0.1915379285812378
step 51/334, epoch 138/501 --> loss:0.14686129331588746
step 101/334, epoch 138/501 --> loss:0.10476227402687073
step 151/334, epoch 138/501 --> loss:0.12601849555969238
step 201/334, epoch 138/501 --> loss:0.08403963208198548
step 251/334, epoch 138/501 --> loss:0.12977270126342774
step 301/334, epoch 138/501 --> loss:0.20654350399971008
step 51/334, epoch 139/501 --> loss:0.09237722635269165
step 101/334, epoch 139/501 --> loss:0.14026740789413453
step 151/334, epoch 139/501 --> loss:0.09193313837051392
step 201/334, epoch 139/501 --> loss:0.13924115300178527
step 251/334, epoch 139/501 --> loss:0.08865288853645324
step 301/334, epoch 139/501 --> loss:0.11099366426467895
step 51/334, epoch 140/501 --> loss:0.15085898876190185
step 101/334, epoch 140/501 --> loss:0.09760077595710755
step 151/334, epoch 140/501 --> loss:0.12160211682319641
step 201/334, epoch 140/501 --> loss:0.08430972933769226
step 251/334, epoch 140/501 --> loss:0.08077741980552673
step 301/334, epoch 140/501 --> loss:0.09818803548812866
step 51/334, epoch 141/501 --> loss:0.11153769850730896
step 101/334, epoch 141/501 --> loss:0.11312970161437988
step 151/334, epoch 141/501 --> loss:0.12989591002464296
step 201/334, epoch 141/501 --> loss:0.08803630113601685
step 251/334, epoch 141/501 --> loss:0.38955469369888307
step 301/334, epoch 141/501 --> loss:0.22594785809516907

##########train dataset##########
acc--> [99.12999270781786]
F1--> {'F1': [0.8947978582268459], 'precision': [0.9214480564826765], 'recall': [0.8696553245042938]}
##########eval dataset##########
acc--> [98.87902634453968]
F1--> {'F1': [0.8611729574820997], 'precision': [0.8886602845085755], 'recall': [0.8353444439531008]}
step 51/334, epoch 142/501 --> loss:0.15287451148033143
step 101/334, epoch 142/501 --> loss:0.19159214138984682
step 151/334, epoch 142/501 --> loss:0.15518586754798888
step 201/334, epoch 142/501 --> loss:0.2214268946647644
step 251/334, epoch 142/501 --> loss:0.15732063293457033
step 301/334, epoch 142/501 --> loss:0.11293476819992065
step 51/334, epoch 143/501 --> loss:0.1214644193649292
step 101/334, epoch 143/501 --> loss:0.1340812313556671
step 151/334, epoch 143/501 --> loss:0.12628705263137818
step 201/334, epoch 143/501 --> loss:0.15321263551712036
step 251/334, epoch 143/501 --> loss:0.3386286795139313
step 301/334, epoch 143/501 --> loss:0.18229525566101074
step 51/334, epoch 144/501 --> loss:0.1278311288356781
step 101/334, epoch 144/501 --> loss:0.1529615640640259
step 151/334, epoch 144/501 --> loss:0.13188279628753663
step 201/334, epoch 144/501 --> loss:0.16922648549079894
step 251/334, epoch 144/501 --> loss:0.14618002295494079
step 301/334, epoch 144/501 --> loss:0.15907832384109497
step 51/334, epoch 145/501 --> loss:0.08972949266433716
step 101/334, epoch 145/501 --> loss:0.15801507234573364
step 151/334, epoch 145/501 --> loss:0.15729942560195923
step 201/334, epoch 145/501 --> loss:0.14759354352951048
step 251/334, epoch 145/501 --> loss:0.13022588729858398
step 301/334, epoch 145/501 --> loss:0.12779200553894043
step 51/334, epoch 146/501 --> loss:0.10888950228691101
step 101/334, epoch 146/501 --> loss:0.08778812289237976
step 151/334, epoch 146/501 --> loss:0.13676513552665712
step 201/334, epoch 146/501 --> loss:0.11914785027503967
step 251/334, epoch 146/501 --> loss:0.13735947132110596
step 301/334, epoch 146/501 --> loss:0.13028422832489014
step 51/334, epoch 147/501 --> loss:0.07981673479080201
step 101/334, epoch 147/501 --> loss:0.1534701979160309
step 151/334, epoch 147/501 --> loss:0.10070250988006592
step 201/334, epoch 147/501 --> loss:0.13945039868354797
step 251/334, epoch 147/501 --> loss:0.09220007658004761
step 301/334, epoch 147/501 --> loss:0.1449364459514618
step 51/334, epoch 148/501 --> loss:0.12392564296722412
step 101/334, epoch 148/501 --> loss:0.14339864015579223
step 151/334, epoch 148/501 --> loss:0.10838708996772767
step 201/334, epoch 148/501 --> loss:0.08692457675933837
step 251/334, epoch 148/501 --> loss:0.16328508138656617
step 301/334, epoch 148/501 --> loss:0.10643567085266113
step 51/334, epoch 149/501 --> loss:0.15543660402297974
step 101/334, epoch 149/501 --> loss:0.196476811170578
step 151/334, epoch 149/501 --> loss:0.11768922328948975
step 201/334, epoch 149/501 --> loss:0.1247491180896759
step 251/334, epoch 149/501 --> loss:0.1022687816619873
step 301/334, epoch 149/501 --> loss:0.11227457523345948
step 51/334, epoch 150/501 --> loss:0.1414762568473816
step 101/334, epoch 150/501 --> loss:0.14161675930023193
step 151/334, epoch 150/501 --> loss:0.12286343932151794
step 201/334, epoch 150/501 --> loss:0.0817194938659668
step 251/334, epoch 150/501 --> loss:0.11244163393974305
step 301/334, epoch 150/501 --> loss:0.0963839328289032
step 51/334, epoch 151/501 --> loss:0.1416503918170929
step 101/334, epoch 151/501 --> loss:0.10928480505943299
step 151/334, epoch 151/501 --> loss:0.17173052549362183
step 201/334, epoch 151/501 --> loss:0.12238818168640136
step 251/334, epoch 151/501 --> loss:0.15673197388648988
step 301/334, epoch 151/501 --> loss:0.11388887643814087

##########train dataset##########
acc--> [99.47933201312198]
F1--> {'F1': [0.9373823875539483], 'precision': [0.9597429023339664], 'recall': [0.9160496250948583]}
##########eval dataset##########
acc--> [99.16811431590408]
F1--> {'F1': [0.8966595106434205], 'precision': [0.9282918757043243], 'recall': [0.8671212479589673]}
save model!
step 51/334, epoch 152/501 --> loss:0.11783804178237915
step 101/334, epoch 152/501 --> loss:0.07661793828010559
step 151/334, epoch 152/501 --> loss:0.08154327511787414
step 201/334, epoch 152/501 --> loss:0.13540339827537537
step 251/334, epoch 152/501 --> loss:0.09700952768325806
step 301/334, epoch 152/501 --> loss:0.08415523052215576
step 51/334, epoch 153/501 --> loss:0.07141093373298645
step 101/334, epoch 153/501 --> loss:0.1380885100364685
step 151/334, epoch 153/501 --> loss:0.1036445689201355
step 201/334, epoch 153/501 --> loss:0.11446577548980713
step 251/334, epoch 153/501 --> loss:0.13576781272888183
step 301/334, epoch 153/501 --> loss:0.07585722923278809
step 51/334, epoch 154/501 --> loss:0.07813342213630677
step 101/334, epoch 154/501 --> loss:0.09601054549217224
step 151/334, epoch 154/501 --> loss:0.12543214201927186
step 201/334, epoch 154/501 --> loss:0.09687956929206848
step 251/334, epoch 154/501 --> loss:0.10345011711120605
step 301/334, epoch 154/501 --> loss:0.13464331030845642
step 51/334, epoch 155/501 --> loss:0.1321284544467926
step 101/334, epoch 155/501 --> loss:0.07412487506866455
step 151/334, epoch 155/501 --> loss:0.08082519292831421
step 201/334, epoch 155/501 --> loss:0.07398402094841003
step 251/334, epoch 155/501 --> loss:0.1000491201877594
step 301/334, epoch 155/501 --> loss:0.1542072069644928
step 51/334, epoch 156/501 --> loss:0.13612584114074708
step 101/334, epoch 156/501 --> loss:0.09693268299102784
step 151/334, epoch 156/501 --> loss:0.08643569827079772
step 201/334, epoch 156/501 --> loss:0.09203547477722168
step 251/334, epoch 156/501 --> loss:0.1884075391292572
step 301/334, epoch 156/501 --> loss:0.14764950394630433
step 51/334, epoch 157/501 --> loss:0.15519521713256837
step 101/334, epoch 157/501 --> loss:0.15206209897994996
step 151/334, epoch 157/501 --> loss:0.11627892255783082
step 201/334, epoch 157/501 --> loss:0.12433728694915772
step 251/334, epoch 157/501 --> loss:0.11067339539527893
step 301/334, epoch 157/501 --> loss:0.1760405445098877
step 51/334, epoch 158/501 --> loss:0.1328090989589691
step 101/334, epoch 158/501 --> loss:0.11605046510696411
step 151/334, epoch 158/501 --> loss:0.17417865633964538
step 201/334, epoch 158/501 --> loss:0.12171053886413574
step 251/334, epoch 158/501 --> loss:0.13888070583343506
step 301/334, epoch 158/501 --> loss:0.15297797203063965
step 51/334, epoch 159/501 --> loss:0.1267430877685547
step 101/334, epoch 159/501 --> loss:0.3937117052078247
step 151/334, epoch 159/501 --> loss:0.2936928462982178
step 201/334, epoch 159/501 --> loss:0.13016813158988952
step 251/334, epoch 159/501 --> loss:0.14317988395690917
step 301/334, epoch 159/501 --> loss:0.1457330632209778
step 51/334, epoch 160/501 --> loss:0.13740654587745665
step 101/334, epoch 160/501 --> loss:0.14729030728340148
step 151/334, epoch 160/501 --> loss:0.13076938271522523
step 201/334, epoch 160/501 --> loss:0.1655103874206543
step 251/334, epoch 160/501 --> loss:0.12587490558624267
step 301/334, epoch 160/501 --> loss:0.1187095046043396
step 51/334, epoch 161/501 --> loss:0.08703201532363891
step 101/334, epoch 161/501 --> loss:0.09794536113739014
step 151/334, epoch 161/501 --> loss:0.2571487939357758
step 201/334, epoch 161/501 --> loss:0.17039663434028626
step 251/334, epoch 161/501 --> loss:0.12257021069526672
step 301/334, epoch 161/501 --> loss:0.1173603355884552

##########train dataset##########
acc--> [99.3808850752148]
F1--> {'F1': [0.9253581942643212], 'precision': [0.9499118222969718], 'recall': [0.9020514189350304]}
##########eval dataset##########
acc--> [99.08340000208662]
F1--> {'F1': [0.8864455533127927], 'precision': [0.9150580706118668], 'recall': [0.8595775184305727]}
step 51/334, epoch 162/501 --> loss:0.13086651921272277
step 101/334, epoch 162/501 --> loss:0.14641142129898072
step 151/334, epoch 162/501 --> loss:0.15039800286293029
step 201/334, epoch 162/501 --> loss:0.10735697865486145
step 251/334, epoch 162/501 --> loss:0.09604175567626953
step 301/334, epoch 162/501 --> loss:0.12770843625068665
step 51/334, epoch 163/501 --> loss:0.10988082766532897
step 101/334, epoch 163/501 --> loss:0.12030693650245666
step 151/334, epoch 163/501 --> loss:0.09208417415618897
step 201/334, epoch 163/501 --> loss:0.06963241696357728
step 251/334, epoch 163/501 --> loss:0.11209644198417663
step 301/334, epoch 163/501 --> loss:0.1030888831615448
step 51/334, epoch 164/501 --> loss:0.10862273693084717
step 101/334, epoch 164/501 --> loss:0.09745731472969055
step 151/334, epoch 164/501 --> loss:0.11001750469207763
step 201/334, epoch 164/501 --> loss:0.07820569038391113
step 251/334, epoch 164/501 --> loss:0.08220767855644226
step 301/334, epoch 164/501 --> loss:0.095183846950531
step 51/334, epoch 165/501 --> loss:0.0874496066570282
step 101/334, epoch 165/501 --> loss:0.11821865439414977
step 151/334, epoch 165/501 --> loss:0.09549720883369446
step 201/334, epoch 165/501 --> loss:0.16191018104553223
step 251/334, epoch 165/501 --> loss:0.09202622771263122
step 301/334, epoch 165/501 --> loss:0.1074725341796875
step 51/334, epoch 166/501 --> loss:0.1266616141796112
step 101/334, epoch 166/501 --> loss:0.10787092089653015
step 151/334, epoch 166/501 --> loss:0.106756591796875
step 201/334, epoch 166/501 --> loss:0.09197869896888733
step 251/334, epoch 166/501 --> loss:0.09333144307136536
step 301/334, epoch 166/501 --> loss:0.08656279683113098
step 51/334, epoch 167/501 --> loss:0.11674916625022888
step 101/334, epoch 167/501 --> loss:0.1355208647251129
step 151/334, epoch 167/501 --> loss:0.07786417007446289
step 201/334, epoch 167/501 --> loss:0.055025166273117064
step 251/334, epoch 167/501 --> loss:0.09062445878982545
step 301/334, epoch 167/501 --> loss:0.12017541408538818
step 51/334, epoch 168/501 --> loss:0.13479283332824707
step 101/334, epoch 168/501 --> loss:0.08961138129234314
step 151/334, epoch 168/501 --> loss:0.09890790104866028
step 201/334, epoch 168/501 --> loss:0.1314424443244934
step 251/334, epoch 168/501 --> loss:0.13923206448554992
step 301/334, epoch 168/501 --> loss:0.10830424904823303
step 51/334, epoch 169/501 --> loss:0.13925724029541015
step 101/334, epoch 169/501 --> loss:0.08339007019996643
step 151/334, epoch 169/501 --> loss:0.11315391778945923
step 201/334, epoch 169/501 --> loss:0.12024052739143372
step 251/334, epoch 169/501 --> loss:0.07587229490280151
step 301/334, epoch 169/501 --> loss:0.12041509509086609
step 51/334, epoch 170/501 --> loss:0.12363853454589843
step 101/334, epoch 170/501 --> loss:0.09028578758239746
step 151/334, epoch 170/501 --> loss:0.11892665028572083
step 201/334, epoch 170/501 --> loss:0.13573806524276733
step 251/334, epoch 170/501 --> loss:0.11954726338386536
step 301/334, epoch 170/501 --> loss:0.09796124458312988
step 51/334, epoch 171/501 --> loss:0.12666935443878174
step 101/334, epoch 171/501 --> loss:0.10328442811965942
step 151/334, epoch 171/501 --> loss:0.09711036801338196
step 201/334, epoch 171/501 --> loss:0.08353977203369141
step 251/334, epoch 171/501 --> loss:0.1416640055179596
step 301/334, epoch 171/501 --> loss:0.15724802017211914

##########train dataset##########
acc--> [99.3735537620147]
F1--> {'F1': [0.9234934762072271], 'precision': [0.961143849084311], 'recall': [0.8886908731988388]}
##########eval dataset##########
acc--> [99.09320006949763]
F1--> {'F1': [0.8850011494022321], 'precision': [0.9371771759480493], 'recall': [0.8383373357748823]}
step 51/334, epoch 172/501 --> loss:0.15844438195228577
step 101/334, epoch 172/501 --> loss:0.12170098781585693
step 151/334, epoch 172/501 --> loss:0.06000264763832092
step 201/334, epoch 172/501 --> loss:0.09550915837287903
step 251/334, epoch 172/501 --> loss:0.10723951697349549
step 301/334, epoch 172/501 --> loss:0.1301288950443268
step 51/334, epoch 173/501 --> loss:0.13406837344169617
step 101/334, epoch 173/501 --> loss:0.1566709017753601
step 151/334, epoch 173/501 --> loss:0.13078270196914674
step 201/334, epoch 173/501 --> loss:0.11568359255790711
step 251/334, epoch 173/501 --> loss:0.11475412964820862
step 301/334, epoch 173/501 --> loss:0.1646825385093689
step 51/334, epoch 174/501 --> loss:0.14156942486763
step 101/334, epoch 174/501 --> loss:0.1105145788192749
step 151/334, epoch 174/501 --> loss:0.12368655204772949
step 201/334, epoch 174/501 --> loss:0.07261351704597473
step 251/334, epoch 174/501 --> loss:0.08973584175109864
step 301/334, epoch 174/501 --> loss:0.10534802436828614
step 51/334, epoch 175/501 --> loss:0.1016128408908844
step 101/334, epoch 175/501 --> loss:0.09109955549240112
step 151/334, epoch 175/501 --> loss:0.12718268990516662
step 201/334, epoch 175/501 --> loss:0.08871956825256348
step 251/334, epoch 175/501 --> loss:0.12545971512794496
step 301/334, epoch 175/501 --> loss:0.12335460424423218
step 51/334, epoch 176/501 --> loss:0.14091744661331176
step 101/334, epoch 176/501 --> loss:0.09649400591850281
step 151/334, epoch 176/501 --> loss:0.07840124368667603
step 201/334, epoch 176/501 --> loss:0.1094294536113739
step 251/334, epoch 176/501 --> loss:0.1226460313796997
step 301/334, epoch 176/501 --> loss:0.13028677225112914
step 51/334, epoch 177/501 --> loss:0.11745910048484802
step 101/334, epoch 177/501 --> loss:0.11232642412185669
step 151/334, epoch 177/501 --> loss:0.1112670648097992
step 201/334, epoch 177/501 --> loss:0.12949580192565918
step 251/334, epoch 177/501 --> loss:0.12990216970443724
step 301/334, epoch 177/501 --> loss:0.11766739249229431
step 51/334, epoch 178/501 --> loss:0.12522385835647584
step 101/334, epoch 178/501 --> loss:0.14938972234725953
step 151/334, epoch 178/501 --> loss:0.1173413348197937
step 201/334, epoch 178/501 --> loss:0.06050051331520081
step 251/334, epoch 178/501 --> loss:0.07944753170013427
step 301/334, epoch 178/501 --> loss:0.12105965852737427
step 51/334, epoch 179/501 --> loss:0.08372196793556214
step 101/334, epoch 179/501 --> loss:0.08104469299316407
step 151/334, epoch 179/501 --> loss:0.09963705539703369
step 201/334, epoch 179/501 --> loss:0.06288964748382568
step 251/334, epoch 179/501 --> loss:0.10063249111175537
step 301/334, epoch 179/501 --> loss:0.10475824356079101
step 51/334, epoch 180/501 --> loss:0.07484854817390442
step 101/334, epoch 180/501 --> loss:0.16543415904045106
step 151/334, epoch 180/501 --> loss:0.10375652432441712
step 201/334, epoch 180/501 --> loss:0.10407795071601868
step 251/334, epoch 180/501 --> loss:0.14290328741073607
step 301/334, epoch 180/501 --> loss:0.17985822081565858
step 51/334, epoch 181/501 --> loss:0.10921965599060059
step 101/334, epoch 181/501 --> loss:0.12046188116073608
step 151/334, epoch 181/501 --> loss:0.1385070013999939
step 201/334, epoch 181/501 --> loss:0.08233697295188903
step 251/334, epoch 181/501 --> loss:0.10358859658241272
step 301/334, epoch 181/501 --> loss:0.09316699028015137

##########train dataset##########
acc--> [99.53971436553545]
F1--> {'F1': [0.9443184271363967], 'precision': [0.9728326546272765], 'recall': [0.9174375652177099]}
##########eval dataset##########
acc--> [99.12512671602643]
F1--> {'F1': [0.891442913280659], 'precision': [0.9217721091000366], 'recall': [0.8630553539842875]}
step 51/334, epoch 182/501 --> loss:0.1459546411037445
step 101/334, epoch 182/501 --> loss:0.14964552521705626
step 151/334, epoch 182/501 --> loss:0.1617532753944397
step 201/334, epoch 182/501 --> loss:0.12587178826332093
step 251/334, epoch 182/501 --> loss:0.15316434741020202
step 301/334, epoch 182/501 --> loss:0.10096197962760925
step 51/334, epoch 183/501 --> loss:0.11481048941612243
step 101/334, epoch 183/501 --> loss:0.11109411358833313
step 151/334, epoch 183/501 --> loss:0.09444355249404907
step 201/334, epoch 183/501 --> loss:0.14916158080101014
step 251/334, epoch 183/501 --> loss:0.12022424459457398
step 301/334, epoch 183/501 --> loss:0.13655653715133667
step 51/334, epoch 184/501 --> loss:0.1245660674571991
step 101/334, epoch 184/501 --> loss:0.1284905469417572
step 151/334, epoch 184/501 --> loss:0.07136643528938294
step 201/334, epoch 184/501 --> loss:0.10369729042053223
step 251/334, epoch 184/501 --> loss:0.10983804106712342
step 301/334, epoch 184/501 --> loss:0.09262740969657898
step 51/334, epoch 185/501 --> loss:0.1755360782146454
step 101/334, epoch 185/501 --> loss:0.15200879216194152
step 151/334, epoch 185/501 --> loss:0.1634909403324127
step 201/334, epoch 185/501 --> loss:0.10427722811698914
step 251/334, epoch 185/501 --> loss:0.17918454647064208
step 301/334, epoch 185/501 --> loss:0.13706485152244569
step 51/334, epoch 186/501 --> loss:0.14170331597328187
step 101/334, epoch 186/501 --> loss:0.12829338073730467
step 151/334, epoch 186/501 --> loss:0.1019332754611969
step 201/334, epoch 186/501 --> loss:0.08878329396247864
step 251/334, epoch 186/501 --> loss:0.11623958945274353
step 301/334, epoch 186/501 --> loss:0.0948654806613922
step 51/334, epoch 187/501 --> loss:0.09239841818809509
step 101/334, epoch 187/501 --> loss:0.06731420755386353
step 151/334, epoch 187/501 --> loss:0.13510900259017944
step 201/334, epoch 187/501 --> loss:0.10601380109786987
step 251/334, epoch 187/501 --> loss:0.09460466384887695
step 301/334, epoch 187/501 --> loss:0.08464439272880554
step 51/334, epoch 188/501 --> loss:0.10326598167419433
step 101/334, epoch 188/501 --> loss:0.07035291433334351
step 151/334, epoch 188/501 --> loss:0.09089661002159119
step 201/334, epoch 188/501 --> loss:0.05969059109687805
step 251/334, epoch 188/501 --> loss:0.0797305428981781
step 301/334, epoch 188/501 --> loss:0.11150287628173829
step 51/334, epoch 189/501 --> loss:0.09492828965187072
step 101/334, epoch 189/501 --> loss:0.11122358918190002
step 151/334, epoch 189/501 --> loss:0.10433645367622375
step 201/334, epoch 189/501 --> loss:0.08176121592521668
step 251/334, epoch 189/501 --> loss:0.05954046726226807
step 301/334, epoch 189/501 --> loss:0.07700935840606689
step 51/334, epoch 190/501 --> loss:0.07433079957962035
step 101/334, epoch 190/501 --> loss:0.08554578423500062
step 151/334, epoch 190/501 --> loss:0.08365103721618652
step 201/334, epoch 190/501 --> loss:0.11224872946739196
step 251/334, epoch 190/501 --> loss:0.0971115779876709
step 301/334, epoch 190/501 --> loss:0.08295352935791016
step 51/334, epoch 191/501 --> loss:0.08197129249572754
step 101/334, epoch 191/501 --> loss:0.09715177297592163
step 151/334, epoch 191/501 --> loss:0.13626754999160767
step 201/334, epoch 191/501 --> loss:0.07765951037406921
step 251/334, epoch 191/501 --> loss:0.12985530018806457
step 301/334, epoch 191/501 --> loss:0.07071434497833252

##########train dataset##########
acc--> [99.4781505583258]
F1--> {'F1': [0.9391700120416733], 'precision': [0.9315614028559865], 'recall': [0.946914097377721]}
##########eval dataset##########
acc--> [99.0739577085574]
F1--> {'F1': [0.8910625271351953], 'precision': [0.8729484237518039], 'recall': [0.9099547369799104]}
step 51/334, epoch 192/501 --> loss:0.0917024028301239
step 101/334, epoch 192/501 --> loss:0.09271429657936096
step 151/334, epoch 192/501 --> loss:0.0822381591796875
step 201/334, epoch 192/501 --> loss:0.14534873127937317
step 251/334, epoch 192/501 --> loss:0.06482742071151733
step 301/334, epoch 192/501 --> loss:0.10009890794754028
step 51/334, epoch 193/501 --> loss:0.09234321475028992
step 101/334, epoch 193/501 --> loss:0.12331084251403808
step 151/334, epoch 193/501 --> loss:0.10272283077239991
step 201/334, epoch 193/501 --> loss:0.11953338146209717
step 251/334, epoch 193/501 --> loss:0.10449989557266236
step 301/334, epoch 193/501 --> loss:0.10490582704544067
step 51/334, epoch 194/501 --> loss:0.09484061002731323
step 101/334, epoch 194/501 --> loss:0.08025198459625243
step 151/334, epoch 194/501 --> loss:0.06766337275505066
step 201/334, epoch 194/501 --> loss:0.05674811244010925
step 251/334, epoch 194/501 --> loss:0.17398304462432862
step 301/334, epoch 194/501 --> loss:0.08465168356895447
step 51/334, epoch 195/501 --> loss:0.11939033150672912
step 101/334, epoch 195/501 --> loss:0.1466386353969574
step 151/334, epoch 195/501 --> loss:0.14757682204246522
step 201/334, epoch 195/501 --> loss:0.11359232664108276
step 251/334, epoch 195/501 --> loss:0.11073347687721252
step 301/334, epoch 195/501 --> loss:0.13888715863227843
step 51/334, epoch 196/501 --> loss:0.09896610975265503
step 101/334, epoch 196/501 --> loss:0.10406547546386719
step 151/334, epoch 196/501 --> loss:0.14088799715042113
step 201/334, epoch 196/501 --> loss:0.10364896893501281
step 251/334, epoch 196/501 --> loss:0.11426149725914002
step 301/334, epoch 196/501 --> loss:0.08207277774810791
step 51/334, epoch 197/501 --> loss:0.08392788529396057
step 101/334, epoch 197/501 --> loss:0.11569172263145447
step 151/334, epoch 197/501 --> loss:0.0636117422580719
step 201/334, epoch 197/501 --> loss:0.11221852660179138
step 251/334, epoch 197/501 --> loss:0.11242929458618164
step 301/334, epoch 197/501 --> loss:0.08220975756645203
step 51/334, epoch 198/501 --> loss:0.08228413224220275
step 101/334, epoch 198/501 --> loss:0.11633656501770019
step 151/334, epoch 198/501 --> loss:0.08412911295890808
step 201/334, epoch 198/501 --> loss:0.15750847458839418
step 251/334, epoch 198/501 --> loss:0.11608824014663696
step 301/334, epoch 198/501 --> loss:0.11720269083976746
step 51/334, epoch 199/501 --> loss:0.10544259905815125
step 101/334, epoch 199/501 --> loss:0.1342403471469879
step 151/334, epoch 199/501 --> loss:0.0855009400844574
step 201/334, epoch 199/501 --> loss:0.09607412576675416
step 251/334, epoch 199/501 --> loss:0.09001672148704529
step 301/334, epoch 199/501 --> loss:0.1091930890083313
step 51/334, epoch 200/501 --> loss:0.08699394583702087
step 101/334, epoch 200/501 --> loss:0.0774605393409729
step 151/334, epoch 200/501 --> loss:0.13798664927482604
step 201/334, epoch 200/501 --> loss:0.12533534407615662
step 251/334, epoch 200/501 --> loss:0.08649456262588501
step 301/334, epoch 200/501 --> loss:0.13305967688560486
step 51/334, epoch 201/501 --> loss:0.06960378527641296
step 101/334, epoch 201/501 --> loss:0.12170689225196839
step 151/334, epoch 201/501 --> loss:0.06157960534095764
step 201/334, epoch 201/501 --> loss:0.08705120086669922
step 251/334, epoch 201/501 --> loss:0.09216188788414001
step 301/334, epoch 201/501 --> loss:0.07909562706947326

##########train dataset##########
acc--> [99.05238285643698]
F1--> {'F1': [0.894215033343892], 'precision': [0.8515424995485192], 'recall': [0.9414010644700362]}
##########eval dataset##########
acc--> [98.55658797349379]
F1--> {'F1': [0.837145111241383], 'precision': [0.789174089726467], 'recall': [0.891336842652451]}
step 51/334, epoch 202/501 --> loss:0.14195769667625427
step 101/334, epoch 202/501 --> loss:0.09033717393875122
step 151/334, epoch 202/501 --> loss:0.11401816964149475
step 201/334, epoch 202/501 --> loss:0.0762943994998932
step 251/334, epoch 202/501 --> loss:0.061442075967788695
step 301/334, epoch 202/501 --> loss:0.0740107798576355
step 51/334, epoch 203/501 --> loss:0.1255888330936432
step 101/334, epoch 203/501 --> loss:0.11578258395195007
step 151/334, epoch 203/501 --> loss:0.14273483157157899
step 201/334, epoch 203/501 --> loss:0.09214773058891296
step 251/334, epoch 203/501 --> loss:0.1409386384487152
step 301/334, epoch 203/501 --> loss:0.07925363540649415
step 51/334, epoch 204/501 --> loss:0.07044575333595277
step 101/334, epoch 204/501 --> loss:0.07690981388092041
step 151/334, epoch 204/501 --> loss:0.0841264283657074
step 201/334, epoch 204/501 --> loss:0.10294520020484925
step 251/334, epoch 204/501 --> loss:0.14389471411705018
step 301/334, epoch 204/501 --> loss:0.11811624526977539
step 51/334, epoch 205/501 --> loss:0.08361623167991639
step 101/334, epoch 205/501 --> loss:0.0980742645263672
step 151/334, epoch 205/501 --> loss:0.08146405816078187
step 201/334, epoch 205/501 --> loss:0.09648689985275269
step 251/334, epoch 205/501 --> loss:0.09053158164024352
step 301/334, epoch 205/501 --> loss:0.10334312796592712
step 51/334, epoch 206/501 --> loss:0.0881103777885437
step 101/334, epoch 206/501 --> loss:0.08788979887962341
step 151/334, epoch 206/501 --> loss:0.09927013158798217
step 201/334, epoch 206/501 --> loss:0.07378964304924011
step 251/334, epoch 206/501 --> loss:0.07754907965660095
step 301/334, epoch 206/501 --> loss:0.05329805850982666
step 51/334, epoch 207/501 --> loss:0.12258978366851807
step 101/334, epoch 207/501 --> loss:0.09493194580078125
step 151/334, epoch 207/501 --> loss:0.08719697713851929
step 201/334, epoch 207/501 --> loss:0.057034504413604734
step 251/334, epoch 207/501 --> loss:0.1024957263469696
step 301/334, epoch 207/501 --> loss:0.168617342710495
step 51/334, epoch 208/501 --> loss:0.08446210503578186
step 101/334, epoch 208/501 --> loss:0.09626341819763183
step 151/334, epoch 208/501 --> loss:0.16735219836235046
step 201/334, epoch 208/501 --> loss:0.10300910830497742
step 251/334, epoch 208/501 --> loss:0.09901426672935486
step 301/334, epoch 208/501 --> loss:0.09680267214775086
step 51/334, epoch 209/501 --> loss:0.16371994256973266
step 101/334, epoch 209/501 --> loss:0.13324135303497314
step 151/334, epoch 209/501 --> loss:0.10762036323547364
step 201/334, epoch 209/501 --> loss:0.0867366635799408
step 251/334, epoch 209/501 --> loss:0.09385260462760925
step 301/334, epoch 209/501 --> loss:0.12833677291870116
step 51/334, epoch 210/501 --> loss:0.1048976445198059
step 101/334, epoch 210/501 --> loss:0.07740617275238038
step 151/334, epoch 210/501 --> loss:0.08120755553245544
step 201/334, epoch 210/501 --> loss:0.09964643359184265
step 251/334, epoch 210/501 --> loss:0.09499786615371704
step 301/334, epoch 210/501 --> loss:0.0829296064376831
step 51/334, epoch 211/501 --> loss:0.07445972800254821
step 101/334, epoch 211/501 --> loss:0.09029996514320374
step 151/334, epoch 211/501 --> loss:0.07274484515190124
step 201/334, epoch 211/501 --> loss:0.08213077187538147
step 251/334, epoch 211/501 --> loss:0.06586957573890687
step 301/334, epoch 211/501 --> loss:0.06986370205879211

##########train dataset##########
acc--> [99.58553078203806]
F1--> {'F1': [0.9508465472464313], 'precision': [0.9595443890466822], 'recall': [0.9423147933984649]}
##########eval dataset##########
acc--> [99.07475936234847]
F1--> {'F1': [0.8894800762494022], 'precision': [0.8844700552223029], 'recall': [0.8945572923960622]}
step 51/334, epoch 212/501 --> loss:0.1802910315990448
step 101/334, epoch 212/501 --> loss:0.14040932774543763
step 151/334, epoch 212/501 --> loss:0.1279323947429657
step 201/334, epoch 212/501 --> loss:0.08889875411987305
step 251/334, epoch 212/501 --> loss:0.13434845924377442
step 301/334, epoch 212/501 --> loss:0.07358239412307739
step 51/334, epoch 213/501 --> loss:0.13198970317840575
step 101/334, epoch 213/501 --> loss:0.08944003582000733
step 151/334, epoch 213/501 --> loss:0.1069847047328949
step 201/334, epoch 213/501 --> loss:0.11981576204299926
step 251/334, epoch 213/501 --> loss:0.11485116243362427
step 301/334, epoch 213/501 --> loss:0.08454588770866395
step 51/334, epoch 214/501 --> loss:0.10949229121208191
step 101/334, epoch 214/501 --> loss:0.16141710042953492
step 151/334, epoch 214/501 --> loss:0.12028355836868286
step 201/334, epoch 214/501 --> loss:0.06571141958236694
step 251/334, epoch 214/501 --> loss:0.12674630403518677
step 301/334, epoch 214/501 --> loss:0.08806427121162415
step 51/334, epoch 215/501 --> loss:0.10226927518844604
step 101/334, epoch 215/501 --> loss:0.06767320275306701
step 151/334, epoch 215/501 --> loss:0.07258381485939026
step 201/334, epoch 215/501 --> loss:0.1120188045501709
step 251/334, epoch 215/501 --> loss:0.1145246684551239
step 301/334, epoch 215/501 --> loss:0.12451998829841614
step 51/334, epoch 216/501 --> loss:0.0654604971408844
step 101/334, epoch 216/501 --> loss:0.14245418667793275
step 151/334, epoch 216/501 --> loss:0.10919682621955872
step 201/334, epoch 216/501 --> loss:0.0931312894821167
step 251/334, epoch 216/501 --> loss:0.06423174262046814
step 301/334, epoch 216/501 --> loss:0.07028419017791748
step 51/334, epoch 217/501 --> loss:0.05919275879859924
step 101/334, epoch 217/501 --> loss:0.06936975598335265
step 151/334, epoch 217/501 --> loss:0.052342813014984134
step 201/334, epoch 217/501 --> loss:0.14981626033782958
step 251/334, epoch 217/501 --> loss:0.0830496871471405
step 301/334, epoch 217/501 --> loss:0.09079718947410584
step 51/334, epoch 218/501 --> loss:0.08896255254745483
step 101/334, epoch 218/501 --> loss:0.24846917033195495
step 151/334, epoch 218/501 --> loss:0.35177701234817504
step 201/334, epoch 218/501 --> loss:0.17892133593559265
step 251/334, epoch 218/501 --> loss:0.14311211228370666
step 301/334, epoch 218/501 --> loss:0.1453386914730072
step 51/334, epoch 219/501 --> loss:0.1090355908870697
step 101/334, epoch 219/501 --> loss:0.10825114488601685
step 151/334, epoch 219/501 --> loss:0.15178642630577088
step 201/334, epoch 219/501 --> loss:0.12332691669464112
step 251/334, epoch 219/501 --> loss:0.11334762692451478
step 301/334, epoch 219/501 --> loss:0.11083281874656677
step 51/334, epoch 220/501 --> loss:0.1148146390914917
step 101/334, epoch 220/501 --> loss:0.13164894461631774
step 151/334, epoch 220/501 --> loss:0.058224210739135744
step 201/334, epoch 220/501 --> loss:0.1465683698654175
step 251/334, epoch 220/501 --> loss:0.09319616913795471
step 301/334, epoch 220/501 --> loss:0.12497453093528747
step 51/334, epoch 221/501 --> loss:0.06837344408035279
step 101/334, epoch 221/501 --> loss:0.094638911485672
step 151/334, epoch 221/501 --> loss:0.11371317028999328
step 201/334, epoch 221/501 --> loss:0.10955059289932251
step 251/334, epoch 221/501 --> loss:0.08880428910255432
step 301/334, epoch 221/501 --> loss:0.11672303318977356

##########train dataset##########
acc--> [99.59568442768831]
F1--> {'F1': [0.9513744075540048], 'precision': [0.9740658761944674], 'recall': [0.9297256392734479]}
##########eval dataset##########
acc--> [99.20124644681813]
F1--> {'F1': [0.9008023122672701], 'precision': [0.9323073734303262], 'recall': [0.8713662696282798]}
save model!
step 51/334, epoch 222/501 --> loss:0.07515628099441528
step 101/334, epoch 222/501 --> loss:0.13962027072906494
step 151/334, epoch 222/501 --> loss:0.11352285385131836
step 201/334, epoch 222/501 --> loss:0.06476321339607238
step 251/334, epoch 222/501 --> loss:0.08729301929473877
step 301/334, epoch 222/501 --> loss:0.0788464069366455
step 51/334, epoch 223/501 --> loss:0.10877004384994507
step 101/334, epoch 223/501 --> loss:0.0809528148174286
step 151/334, epoch 223/501 --> loss:0.10030390501022339
step 201/334, epoch 223/501 --> loss:0.06398087382316589
step 251/334, epoch 223/501 --> loss:0.1254665470123291
step 301/334, epoch 223/501 --> loss:0.09318589568138122
step 51/334, epoch 224/501 --> loss:0.09251543760299683
step 101/334, epoch 224/501 --> loss:0.07121467113494873
step 151/334, epoch 224/501 --> loss:0.06692037105560303
step 201/334, epoch 224/501 --> loss:0.08215977787971497
step 251/334, epoch 224/501 --> loss:0.08130271792411804
step 301/334, epoch 224/501 --> loss:0.06219634532928467
step 51/334, epoch 225/501 --> loss:0.08587926149368286
step 101/334, epoch 225/501 --> loss:0.0818920087814331
step 151/334, epoch 225/501 --> loss:0.11978620767593384
step 201/334, epoch 225/501 --> loss:0.06528166174888611
step 251/334, epoch 225/501 --> loss:0.06613341212272644
step 301/334, epoch 225/501 --> loss:0.08193873524665833
step 51/334, epoch 226/501 --> loss:0.07328429460525512
step 101/334, epoch 226/501 --> loss:0.0790327775478363
step 151/334, epoch 226/501 --> loss:0.07939168453216552
step 201/334, epoch 226/501 --> loss:0.0549515688419342
step 251/334, epoch 226/501 --> loss:0.09186703205108643
step 301/334, epoch 226/501 --> loss:0.0887771737575531
step 51/334, epoch 227/501 --> loss:0.050195947885513306
step 101/334, epoch 227/501 --> loss:0.08132642388343811
step 151/334, epoch 227/501 --> loss:0.06262816786766053
step 201/334, epoch 227/501 --> loss:0.09349601984024047
step 251/334, epoch 227/501 --> loss:0.12413812041282654
step 301/334, epoch 227/501 --> loss:0.08899390816688538
step 51/334, epoch 228/501 --> loss:0.08243005633354188
step 101/334, epoch 228/501 --> loss:0.06738033771514892
step 151/334, epoch 228/501 --> loss:0.10701351881027221
step 201/334, epoch 228/501 --> loss:0.08879741668701172
step 251/334, epoch 228/501 --> loss:0.09503422260284423
step 301/334, epoch 228/501 --> loss:0.12129558801651001
step 51/334, epoch 229/501 --> loss:0.13945763945579528
step 101/334, epoch 229/501 --> loss:0.08285983800888061
step 151/334, epoch 229/501 --> loss:0.09319457530975342
step 201/334, epoch 229/501 --> loss:0.10888342142105102
step 251/334, epoch 229/501 --> loss:0.05653808116912842
step 301/334, epoch 229/501 --> loss:0.07492725849151612
step 51/334, epoch 230/501 --> loss:0.08096139669418335
step 101/334, epoch 230/501 --> loss:0.10234949231147766
step 151/334, epoch 230/501 --> loss:0.09832234144210815
step 201/334, epoch 230/501 --> loss:0.09828463792800904
step 251/334, epoch 230/501 --> loss:0.07642269849777222
step 301/334, epoch 230/501 --> loss:0.06786351799964904
step 51/334, epoch 231/501 --> loss:0.11799906611442566
step 101/334, epoch 231/501 --> loss:0.0803703272342682
step 151/334, epoch 231/501 --> loss:0.08385554552078248
step 201/334, epoch 231/501 --> loss:0.10349246859550476
step 251/334, epoch 231/501 --> loss:0.07616249442100526
step 301/334, epoch 231/501 --> loss:0.06316303253173829

##########train dataset##########
acc--> [99.62560584315104]
F1--> {'F1': [0.9554649674502564], 'precision': [0.9671720010496512], 'recall': [0.9440477182992243]}
##########eval dataset##########
acc--> [99.18552041574584]
F1--> {'F1': [0.9003220755869632], 'precision': [0.9175168686269525], 'recall': [0.8837695394704207]}
step 51/334, epoch 232/501 --> loss:0.08314153671264649
step 101/334, epoch 232/501 --> loss:0.09661891222000123
step 151/334, epoch 232/501 --> loss:0.08031144142150878
step 201/334, epoch 232/501 --> loss:0.0771183967590332
step 251/334, epoch 232/501 --> loss:0.09367820262908935
step 301/334, epoch 232/501 --> loss:0.08832818269729614
step 51/334, epoch 233/501 --> loss:0.1279555857181549
step 101/334, epoch 233/501 --> loss:0.08375436902046203
step 151/334, epoch 233/501 --> loss:0.05838683366775513
step 201/334, epoch 233/501 --> loss:0.10530209422111511
step 251/334, epoch 233/501 --> loss:0.14570841073989868
step 301/334, epoch 233/501 --> loss:0.08211421847343445
step 51/334, epoch 234/501 --> loss:0.1179639732837677
step 101/334, epoch 234/501 --> loss:0.1039019227027893
step 151/334, epoch 234/501 --> loss:0.05353450179100037
step 201/334, epoch 234/501 --> loss:0.06523951172828674
step 251/334, epoch 234/501 --> loss:0.07677721381187438
step 301/334, epoch 234/501 --> loss:0.05217593312263489
step 51/334, epoch 235/501 --> loss:0.07268843412399292
step 101/334, epoch 235/501 --> loss:0.08426557421684265
step 151/334, epoch 235/501 --> loss:0.09291354179382325
step 201/334, epoch 235/501 --> loss:0.10757617354393005
step 251/334, epoch 235/501 --> loss:0.10025656700134278
step 301/334, epoch 235/501 --> loss:0.0745350456237793
step 51/334, epoch 236/501 --> loss:0.059568225145339965
step 101/334, epoch 236/501 --> loss:0.09521071672439575
step 151/334, epoch 236/501 --> loss:0.08036500930786133
step 201/334, epoch 236/501 --> loss:0.08514007806777954
step 251/334, epoch 236/501 --> loss:0.07762249827384948
step 301/334, epoch 236/501 --> loss:0.06820014476776123
step 51/334, epoch 237/501 --> loss:0.10750116586685181
step 101/334, epoch 237/501 --> loss:0.10744178771972657
step 151/334, epoch 237/501 --> loss:0.09316502690315247
step 201/334, epoch 237/501 --> loss:0.13448445081710816
step 251/334, epoch 237/501 --> loss:0.13801440238952636
step 301/334, epoch 237/501 --> loss:0.05919684290885925
step 51/334, epoch 238/501 --> loss:0.07581137537956238
step 101/334, epoch 238/501 --> loss:0.05687639474868775
step 151/334, epoch 238/501 --> loss:0.061437265872955324
step 201/334, epoch 238/501 --> loss:0.13148337364196777
step 251/334, epoch 238/501 --> loss:0.07045757174491882
step 301/334, epoch 238/501 --> loss:0.11542128324508667
step 51/334, epoch 239/501 --> loss:0.1303824245929718
step 101/334, epoch 239/501 --> loss:0.18355732202529906
step 151/334, epoch 239/501 --> loss:0.09929092407226563
step 201/334, epoch 239/501 --> loss:0.1396409499645233
step 251/334, epoch 239/501 --> loss:0.1308436846733093
step 301/334, epoch 239/501 --> loss:0.1053100037574768
step 51/334, epoch 240/501 --> loss:0.17263072490692138
step 101/334, epoch 240/501 --> loss:0.1228337049484253
step 151/334, epoch 240/501 --> loss:0.09661733865737915
step 201/334, epoch 240/501 --> loss:0.1344542098045349
step 251/334, epoch 240/501 --> loss:0.087440345287323
step 301/334, epoch 240/501 --> loss:0.09230726838111877
step 51/334, epoch 241/501 --> loss:0.08077664494514465
step 101/334, epoch 241/501 --> loss:0.06738649964332581
step 151/334, epoch 241/501 --> loss:0.09191739678382874
step 201/334, epoch 241/501 --> loss:0.07026525735855102
step 251/334, epoch 241/501 --> loss:0.10208509564399719
step 301/334, epoch 241/501 --> loss:0.08085757255554199

##########train dataset##########
acc--> [99.60204740566202]
F1--> {'F1': [0.952821123314803], 'precision': [0.9612050059171474], 'recall': [0.944592056003754]}
##########eval dataset##########
acc--> [99.15541935049815]
F1--> {'F1': [0.897345285183341], 'precision': [0.9080359655447716], 'recall': [0.8869131743834778]}
step 51/334, epoch 242/501 --> loss:0.0678240990638733
step 101/334, epoch 242/501 --> loss:0.05906623721122742
step 151/334, epoch 242/501 --> loss:0.09071150183677673
step 201/334, epoch 242/501 --> loss:0.0919605851173401
step 251/334, epoch 242/501 --> loss:0.0572593092918396
step 301/334, epoch 242/501 --> loss:0.0615205717086792
step 51/334, epoch 243/501 --> loss:0.08834469556808472
step 101/334, epoch 243/501 --> loss:0.06705512285232544
step 151/334, epoch 243/501 --> loss:0.08719529747962952
step 201/334, epoch 243/501 --> loss:0.1104487669467926
step 251/334, epoch 243/501 --> loss:0.13495474815368652
step 301/334, epoch 243/501 --> loss:0.11770599007606507
step 51/334, epoch 244/501 --> loss:0.1001494026184082
step 101/334, epoch 244/501 --> loss:0.07347685813903809
step 151/334, epoch 244/501 --> loss:0.06232258915901184
step 201/334, epoch 244/501 --> loss:0.07745575189590453
step 251/334, epoch 244/501 --> loss:0.062428523302078244
step 301/334, epoch 244/501 --> loss:0.07946956038475037
step 51/334, epoch 245/501 --> loss:0.08701496839523315
step 101/334, epoch 245/501 --> loss:0.10634234547615051
step 151/334, epoch 245/501 --> loss:0.05874824166297912
step 201/334, epoch 245/501 --> loss:0.05636023640632629
step 251/334, epoch 245/501 --> loss:0.07299802780151367
step 301/334, epoch 245/501 --> loss:0.05535424828529358
step 51/334, epoch 246/501 --> loss:0.0671027934551239
step 101/334, epoch 246/501 --> loss:0.13639793038368225
step 151/334, epoch 246/501 --> loss:0.08941122174263
step 201/334, epoch 246/501 --> loss:0.09345773220062256
step 251/334, epoch 246/501 --> loss:0.09840564489364624
step 301/334, epoch 246/501 --> loss:0.07350208044052124
step 51/334, epoch 247/501 --> loss:0.11001563429832459
step 101/334, epoch 247/501 --> loss:0.07366839528083802
step 151/334, epoch 247/501 --> loss:0.07200334787368774
step 201/334, epoch 247/501 --> loss:0.09348581194877624
step 251/334, epoch 247/501 --> loss:0.07083757281303406
step 301/334, epoch 247/501 --> loss:0.201609765291214
step 51/334, epoch 248/501 --> loss:0.09459497094154358
step 101/334, epoch 248/501 --> loss:0.17453501582145692
step 151/334, epoch 248/501 --> loss:0.1243422257900238
step 201/334, epoch 248/501 --> loss:0.10447453141212464
step 251/334, epoch 248/501 --> loss:0.15854158759117126
step 301/334, epoch 248/501 --> loss:0.08845889925956726
step 51/334, epoch 249/501 --> loss:0.06791451215744018
step 101/334, epoch 249/501 --> loss:0.06881558656692505
step 151/334, epoch 249/501 --> loss:0.08271263957023621
step 201/334, epoch 249/501 --> loss:0.09719777703285218
step 251/334, epoch 249/501 --> loss:0.07881789207458496
step 301/334, epoch 249/501 --> loss:0.07289335250854492
step 51/334, epoch 250/501 --> loss:0.07457664370536804
step 101/334, epoch 250/501 --> loss:0.0656611168384552
step 151/334, epoch 250/501 --> loss:0.07049810528755188
step 201/334, epoch 250/501 --> loss:0.09521982550621033
step 251/334, epoch 250/501 --> loss:0.06354249596595764
step 301/334, epoch 250/501 --> loss:0.08136394500732422
step 51/334, epoch 251/501 --> loss:0.057034741640090945
step 101/334, epoch 251/501 --> loss:0.06524757385253906
step 151/334, epoch 251/501 --> loss:0.08596913576126099
step 201/334, epoch 251/501 --> loss:0.12359087705612183
step 251/334, epoch 251/501 --> loss:0.07249178647994996
step 301/334, epoch 251/501 --> loss:0.11524221301078796

##########train dataset##########
acc--> [99.61396179084507]
F1--> {'F1': [0.9542392980797554], 'precision': [0.962510950889558], 'recall': [0.9461184337913322]}
##########eval dataset##########
acc--> [99.1758685308008]
F1--> {'F1': [0.9005865435311007], 'precision': [0.9043288598511422], 'recall': [0.8968849902363429]}
step 51/334, epoch 252/501 --> loss:0.05649287223815918
step 101/334, epoch 252/501 --> loss:0.08395479679107666
step 151/334, epoch 252/501 --> loss:0.06510278463363647
step 201/334, epoch 252/501 --> loss:0.1026187252998352
step 251/334, epoch 252/501 --> loss:0.06366903185844422
step 301/334, epoch 252/501 --> loss:0.06543048024177552
step 51/334, epoch 253/501 --> loss:0.07539551615715027
step 101/334, epoch 253/501 --> loss:0.09352740287780761
step 151/334, epoch 253/501 --> loss:0.13725217580795288
step 201/334, epoch 253/501 --> loss:0.09699747562408448
step 251/334, epoch 253/501 --> loss:0.07746579051017762
step 301/334, epoch 253/501 --> loss:0.056158137321472165
step 51/334, epoch 254/501 --> loss:0.07864500522613525
step 101/334, epoch 254/501 --> loss:0.07119740843772888
step 151/334, epoch 254/501 --> loss:0.05619101285934448
step 201/334, epoch 254/501 --> loss:0.07916873693466187
step 251/334, epoch 254/501 --> loss:0.0575030255317688
step 301/334, epoch 254/501 --> loss:0.09291936993598938
step 51/334, epoch 255/501 --> loss:0.05095882892608643
step 101/334, epoch 255/501 --> loss:0.13531317591667175
step 151/334, epoch 255/501 --> loss:0.07568371057510376
step 201/334, epoch 255/501 --> loss:0.11000313758850097
step 251/334, epoch 255/501 --> loss:0.10153358817100525
step 301/334, epoch 255/501 --> loss:0.08405771851539612
step 51/334, epoch 256/501 --> loss:0.08204848408699035
step 101/334, epoch 256/501 --> loss:0.08797547221183777
step 151/334, epoch 256/501 --> loss:0.06299565553665161
step 201/334, epoch 256/501 --> loss:0.10353515982627869
step 251/334, epoch 256/501 --> loss:0.17063570141792297
step 301/334, epoch 256/501 --> loss:0.4048702597618103
step 51/334, epoch 257/501 --> loss:0.2760601031780243
step 101/334, epoch 257/501 --> loss:0.15345042824745178
step 151/334, epoch 257/501 --> loss:0.1802255916595459
step 201/334, epoch 257/501 --> loss:0.17355984210968017
step 251/334, epoch 257/501 --> loss:0.18623996257781983
step 301/334, epoch 257/501 --> loss:0.187436603307724
step 51/334, epoch 258/501 --> loss:0.16100312113761903
step 101/334, epoch 258/501 --> loss:0.14667919993400574
step 151/334, epoch 258/501 --> loss:0.11690680503845215
step 201/334, epoch 258/501 --> loss:0.16951332569122315
step 251/334, epoch 258/501 --> loss:0.11247448205947876
step 301/334, epoch 258/501 --> loss:0.13314370036125184
step 51/334, epoch 259/501 --> loss:0.14819446325302124
step 101/334, epoch 259/501 --> loss:0.12033309936523437
step 151/334, epoch 259/501 --> loss:0.10245115399360656
step 201/334, epoch 259/501 --> loss:0.0799877393245697
step 251/334, epoch 259/501 --> loss:0.10116369247436524
step 301/334, epoch 259/501 --> loss:0.1297418761253357
step 51/334, epoch 260/501 --> loss:0.07879779577255248
step 101/334, epoch 260/501 --> loss:0.11654905557632446
step 151/334, epoch 260/501 --> loss:0.12602593541145324
step 201/334, epoch 260/501 --> loss:0.09234761476516723
step 251/334, epoch 260/501 --> loss:0.06399802803993225
step 301/334, epoch 260/501 --> loss:0.09064454555511475
step 51/334, epoch 261/501 --> loss:0.12038968920707703
step 101/334, epoch 261/501 --> loss:0.06975426435470582
step 151/334, epoch 261/501 --> loss:0.05936988234519958
step 201/334, epoch 261/501 --> loss:0.06336459517478943
step 251/334, epoch 261/501 --> loss:0.07658414363861084
step 301/334, epoch 261/501 --> loss:0.06328503727912903

##########train dataset##########
acc--> [99.59429986565115]
F1--> {'F1': [0.951788161985559], 'precision': [0.9624959713959794], 'recall': [0.9413257608350223]}
##########eval dataset##########
acc--> [99.16331440548639]
F1--> {'F1': [0.8969967054066889], 'precision': [0.9197972957927691], 'recall': [0.8753086825601064]}
step 51/334, epoch 262/501 --> loss:0.11798181772232055
step 101/334, epoch 262/501 --> loss:0.06258482813835144
step 151/334, epoch 262/501 --> loss:0.12096283435821534
step 201/334, epoch 262/501 --> loss:0.2226737403869629
step 251/334, epoch 262/501 --> loss:0.39377565383911134
step 301/334, epoch 262/501 --> loss:0.18934507966041564
step 51/334, epoch 263/501 --> loss:0.16623612999916076
step 101/334, epoch 263/501 --> loss:0.12303380489349365
step 151/334, epoch 263/501 --> loss:0.10053140282630921
step 201/334, epoch 263/501 --> loss:0.1359574854373932
step 251/334, epoch 263/501 --> loss:0.11472453474998474
step 301/334, epoch 263/501 --> loss:0.10814762711524964
step 51/334, epoch 264/501 --> loss:0.1931062412261963
step 101/334, epoch 264/501 --> loss:0.08428138613700867
step 151/334, epoch 264/501 --> loss:0.13717202663421632
step 201/334, epoch 264/501 --> loss:0.11228258609771728
step 251/334, epoch 264/501 --> loss:0.07816722750663757
step 301/334, epoch 264/501 --> loss:0.1301927709579468
step 51/334, epoch 265/501 --> loss:0.08220513701438904
step 101/334, epoch 265/501 --> loss:0.11172656297683715
step 151/334, epoch 265/501 --> loss:0.07941186785697937
step 201/334, epoch 265/501 --> loss:0.1267076873779297
step 251/334, epoch 265/501 --> loss:0.07639116287231446
step 301/334, epoch 265/501 --> loss:0.09835393667221069
step 51/334, epoch 266/501 --> loss:0.06706528544425965
step 101/334, epoch 266/501 --> loss:0.09413542270660401
step 151/334, epoch 266/501 --> loss:0.05728076934814453
step 201/334, epoch 266/501 --> loss:0.10110502123832703
step 251/334, epoch 266/501 --> loss:0.08350873231887818
step 301/334, epoch 266/501 --> loss:0.059695063829421996
step 51/334, epoch 267/501 --> loss:0.11230127692222595
step 101/334, epoch 267/501 --> loss:0.13757964015007018
step 151/334, epoch 267/501 --> loss:0.0926011085510254
step 201/334, epoch 267/501 --> loss:0.12429467916488647
step 251/334, epoch 267/501 --> loss:0.09535796523094177
step 301/334, epoch 267/501 --> loss:0.12106193661689758
step 51/334, epoch 268/501 --> loss:0.11749991774559021
step 101/334, epoch 268/501 --> loss:0.12372113943099976
step 151/334, epoch 268/501 --> loss:0.09817428708076477
step 201/334, epoch 268/501 --> loss:0.09612648725509644
step 251/334, epoch 268/501 --> loss:0.08582353234291076
step 301/334, epoch 268/501 --> loss:0.0742848813533783
step 51/334, epoch 269/501 --> loss:0.1444385266304016
step 101/334, epoch 269/501 --> loss:0.07635265111923217
step 151/334, epoch 269/501 --> loss:0.0740399944782257
step 201/334, epoch 269/501 --> loss:0.1101197612285614
step 251/334, epoch 269/501 --> loss:0.11211814045906067
step 301/334, epoch 269/501 --> loss:0.1019850504398346
step 51/334, epoch 270/501 --> loss:0.09357908606529236
step 101/334, epoch 270/501 --> loss:0.07820800304412842
step 151/334, epoch 270/501 --> loss:0.13184651374816894
step 201/334, epoch 270/501 --> loss:0.08196276068687439
step 251/334, epoch 270/501 --> loss:0.05691144704818726
step 301/334, epoch 270/501 --> loss:0.05020346879959106
step 51/334, epoch 271/501 --> loss:0.12675401091575622
step 101/334, epoch 271/501 --> loss:0.09189158797264099
step 151/334, epoch 271/501 --> loss:0.11535007238388062
step 201/334, epoch 271/501 --> loss:0.07680453538894653
step 251/334, epoch 271/501 --> loss:0.08779881477355957
step 301/334, epoch 271/501 --> loss:0.057334411144256595

##########train dataset##########
acc--> [99.63773277577079]
F1--> {'F1': [0.9569558055381486], 'precision': [0.9675659136945471], 'recall': [0.946585652505361]}
##########eval dataset##########
acc--> [99.22130247634398]
F1--> {'F1': [0.9039463423703191], 'precision': [0.9288464841060824], 'recall': [0.8803558499129934]}
save model!
step 51/334, epoch 272/501 --> loss:0.14404882073402406
step 101/334, epoch 272/501 --> loss:0.11375422477722168
step 151/334, epoch 272/501 --> loss:0.07400919556617737
step 201/334, epoch 272/501 --> loss:0.11564126491546631
step 251/334, epoch 272/501 --> loss:0.09303974509239196
step 301/334, epoch 272/501 --> loss:0.12383077263832093
step 51/334, epoch 273/501 --> loss:0.07365869283676148
step 101/334, epoch 273/501 --> loss:0.12332741260528564
step 151/334, epoch 273/501 --> loss:0.10156153917312621
step 201/334, epoch 273/501 --> loss:0.07500192880630493
step 251/334, epoch 273/501 --> loss:0.06661071300506592
step 301/334, epoch 273/501 --> loss:0.09631722450256347
step 51/334, epoch 274/501 --> loss:0.05264175891876221
step 101/334, epoch 274/501 --> loss:0.11169135808944702
step 151/334, epoch 274/501 --> loss:0.06816471695899963
step 201/334, epoch 274/501 --> loss:0.0582626724243164
step 251/334, epoch 274/501 --> loss:0.08520035982131959
step 301/334, epoch 274/501 --> loss:0.08776012420654297
step 51/334, epoch 275/501 --> loss:0.069752539396286
step 101/334, epoch 275/501 --> loss:0.0741686475276947
step 151/334, epoch 275/501 --> loss:0.09563200354576111
step 201/334, epoch 275/501 --> loss:0.08418413400650024
step 251/334, epoch 275/501 --> loss:0.09505967259407043
step 301/334, epoch 275/501 --> loss:0.05807231903076172
step 51/334, epoch 276/501 --> loss:0.07623896956443786
step 101/334, epoch 276/501 --> loss:0.05799395442008972
step 151/334, epoch 276/501 --> loss:0.09716595292091369
step 201/334, epoch 276/501 --> loss:0.04154616117477417
step 251/334, epoch 276/501 --> loss:0.09121905565261841
step 301/334, epoch 276/501 --> loss:0.05919082760810852
step 51/334, epoch 277/501 --> loss:0.10497101187705994
step 101/334, epoch 277/501 --> loss:0.09966321587562561
step 151/334, epoch 277/501 --> loss:0.06404913663864135
step 201/334, epoch 277/501 --> loss:0.10574517130851746
step 251/334, epoch 277/501 --> loss:0.09624463558197022
step 301/334, epoch 277/501 --> loss:0.0525051474571228
step 51/334, epoch 278/501 --> loss:0.05910082340240479
step 101/334, epoch 278/501 --> loss:0.0875636863708496
step 151/334, epoch 278/501 --> loss:0.12002910852432251
step 201/334, epoch 278/501 --> loss:0.052169562578201295
step 251/334, epoch 278/501 --> loss:0.07470296502113342
step 301/334, epoch 278/501 --> loss:0.06245615363121033
step 51/334, epoch 279/501 --> loss:0.1121461546421051
step 101/334, epoch 279/501 --> loss:0.07872259974479676
step 151/334, epoch 279/501 --> loss:0.08877872586250306
step 201/334, epoch 279/501 --> loss:0.07920643091201782
step 251/334, epoch 279/501 --> loss:0.04790247082710266
step 301/334, epoch 279/501 --> loss:0.08781506299972534
step 51/334, epoch 280/501 --> loss:0.06673519611358643
step 101/334, epoch 280/501 --> loss:0.08032329440116882
step 151/334, epoch 280/501 --> loss:0.11818840384483337
step 201/334, epoch 280/501 --> loss:0.06941296696662903
step 251/334, epoch 280/501 --> loss:0.04663841009140015
step 301/334, epoch 280/501 --> loss:0.08201481461524963
step 51/334, epoch 281/501 --> loss:0.09317551016807556
step 101/334, epoch 281/501 --> loss:0.08232015252113342
step 151/334, epoch 281/501 --> loss:0.05390455961227417
step 201/334, epoch 281/501 --> loss:0.08068981766700745
step 251/334, epoch 281/501 --> loss:0.05701989531517029
step 301/334, epoch 281/501 --> loss:0.08574504733085632

##########train dataset##########
acc--> [99.65437040553971]
F1--> {'F1': [0.9589493814963445], 'precision': [0.969172880803127], 'recall': [0.9489491108785857]}
##########eval dataset##########
acc--> [99.20389170408211]
F1--> {'F1': [0.9024318158358827], 'precision': [0.9210259205089413], 'recall': [0.884583231433327]}
step 51/334, epoch 282/501 --> loss:0.09247124671936036
step 101/334, epoch 282/501 --> loss:0.05184078812599182
step 151/334, epoch 282/501 --> loss:0.11474961161613464
step 201/334, epoch 282/501 --> loss:0.12993714213371277
step 251/334, epoch 282/501 --> loss:0.09014846086502075
step 301/334, epoch 282/501 --> loss:0.09321767449378968
step 51/334, epoch 283/501 --> loss:0.07284902811050414
step 101/334, epoch 283/501 --> loss:0.052802096605300906
step 151/334, epoch 283/501 --> loss:0.06642663717269898
step 201/334, epoch 283/501 --> loss:0.0636441957950592
step 251/334, epoch 283/501 --> loss:0.0817121946811676
step 301/334, epoch 283/501 --> loss:0.07438778162002563
step 51/334, epoch 284/501 --> loss:0.043788299560546876
step 101/334, epoch 284/501 --> loss:0.0593619704246521
step 151/334, epoch 284/501 --> loss:0.10862622737884521
step 201/334, epoch 284/501 --> loss:0.11218285083770752
step 251/334, epoch 284/501 --> loss:0.11547998428344726
step 301/334, epoch 284/501 --> loss:0.057097047567367554
step 51/334, epoch 285/501 --> loss:0.1905746603012085
step 101/334, epoch 285/501 --> loss:0.18644892573356628
step 151/334, epoch 285/501 --> loss:0.15478915810585023
step 201/334, epoch 285/501 --> loss:0.08275059580802918
step 251/334, epoch 285/501 --> loss:0.10577652454376221
step 301/334, epoch 285/501 --> loss:0.09353495359420777
step 51/334, epoch 286/501 --> loss:0.07962765216827393
step 101/334, epoch 286/501 --> loss:0.11106912493705749
step 151/334, epoch 286/501 --> loss:0.09780656933784485
step 201/334, epoch 286/501 --> loss:0.09683865785598755
step 251/334, epoch 286/501 --> loss:0.07689527750015258
step 301/334, epoch 286/501 --> loss:0.06286632657051086
step 51/334, epoch 287/501 --> loss:0.06912802934646606
step 101/334, epoch 287/501 --> loss:0.06350842356681824
step 151/334, epoch 287/501 --> loss:0.10075181603431702
step 201/334, epoch 287/501 --> loss:0.07933783292770386
step 251/334, epoch 287/501 --> loss:0.09707865238189697
step 301/334, epoch 287/501 --> loss:0.06443891167640686
step 51/334, epoch 288/501 --> loss:0.05940068125724793
step 101/334, epoch 288/501 --> loss:0.07648885965347291
step 151/334, epoch 288/501 --> loss:0.09542699337005615
step 201/334, epoch 288/501 --> loss:0.062363431453704835
step 251/334, epoch 288/501 --> loss:0.06885618805885314
step 301/334, epoch 288/501 --> loss:0.06300037622451782
step 51/334, epoch 289/501 --> loss:0.08503910779953003
step 101/334, epoch 289/501 --> loss:0.07472477197647094
step 151/334, epoch 289/501 --> loss:0.0783038330078125
step 201/334, epoch 289/501 --> loss:0.11019347310066223
step 251/334, epoch 289/501 --> loss:0.09522449135780335
step 301/334, epoch 289/501 --> loss:0.09100592494010926
step 51/334, epoch 290/501 --> loss:0.06902253985404969
step 101/334, epoch 290/501 --> loss:0.11104141831398011
step 151/334, epoch 290/501 --> loss:0.0756320583820343
step 201/334, epoch 290/501 --> loss:0.08730627417564392
step 251/334, epoch 290/501 --> loss:0.07200305342674256
step 301/334, epoch 290/501 --> loss:0.09100494146347046
step 51/334, epoch 291/501 --> loss:0.10369401931762695
step 101/334, epoch 291/501 --> loss:0.14268873810768126
step 151/334, epoch 291/501 --> loss:0.08520876765251159
step 201/334, epoch 291/501 --> loss:0.1009559953212738
step 251/334, epoch 291/501 --> loss:0.06950281143188476
step 301/334, epoch 291/501 --> loss:0.07744951367378235

##########train dataset##########
acc--> [99.65432377669424]
F1--> {'F1': [0.9587935122071487], 'precision': [0.9726531869592635], 'recall': [0.945332989983981]}
##########eval dataset##########
acc--> [99.1878145740139]
F1--> {'F1': [0.9007403223691746], 'precision': [0.9166266420388626], 'recall': [0.8854049416457692]}
step 51/334, epoch 292/501 --> loss:0.06736016869544983
step 101/334, epoch 292/501 --> loss:0.050312800407409666
step 151/334, epoch 292/501 --> loss:0.06148473381996155
step 201/334, epoch 292/501 --> loss:0.05262247800827027
step 251/334, epoch 292/501 --> loss:0.0727453899383545
step 301/334, epoch 292/501 --> loss:0.09483428835868836
step 51/334, epoch 293/501 --> loss:0.08883502125740052
step 101/334, epoch 293/501 --> loss:0.11199518799781799
step 151/334, epoch 293/501 --> loss:0.08312296867370605
step 201/334, epoch 293/501 --> loss:0.09087062001228333
step 251/334, epoch 293/501 --> loss:0.06350844025611878
step 301/334, epoch 293/501 --> loss:0.09253504872322083
step 51/334, epoch 294/501 --> loss:0.1136655592918396
step 101/334, epoch 294/501 --> loss:0.0915920615196228
step 151/334, epoch 294/501 --> loss:0.07755528569221497
step 201/334, epoch 294/501 --> loss:0.11743089556694031
step 251/334, epoch 294/501 --> loss:0.11994692921638489
step 301/334, epoch 294/501 --> loss:0.07565739870071411
step 51/334, epoch 295/501 --> loss:0.09448962926864624
step 101/334, epoch 295/501 --> loss:0.0915449321269989
step 151/334, epoch 295/501 --> loss:0.0756253719329834
step 201/334, epoch 295/501 --> loss:0.10546866059303284
step 251/334, epoch 295/501 --> loss:0.11333101272583007
step 301/334, epoch 295/501 --> loss:0.1334259796142578
step 51/334, epoch 296/501 --> loss:0.05477280139923096
step 101/334, epoch 296/501 --> loss:0.1130187201499939
step 151/334, epoch 296/501 --> loss:0.11748428702354431
step 201/334, epoch 296/501 --> loss:0.08010587334632874
step 251/334, epoch 296/501 --> loss:0.0692288625240326
step 301/334, epoch 296/501 --> loss:0.11181384682655335
step 51/334, epoch 297/501 --> loss:0.06762229681015014
step 101/334, epoch 297/501 --> loss:0.10948073744773865
step 151/334, epoch 297/501 --> loss:0.12142090678215027
step 201/334, epoch 297/501 --> loss:0.046999512910842894
step 251/334, epoch 297/501 --> loss:0.06325593590736389
step 301/334, epoch 297/501 --> loss:0.12652623653411865
step 51/334, epoch 298/501 --> loss:0.09569602489471435
step 101/334, epoch 298/501 --> loss:0.06323118090629577
step 151/334, epoch 298/501 --> loss:0.1334747052192688
step 201/334, epoch 298/501 --> loss:0.06323683738708497
step 251/334, epoch 298/501 --> loss:0.04900325536727905
step 301/334, epoch 298/501 --> loss:0.06508682250976562
step 51/334, epoch 299/501 --> loss:0.06568195700645446
step 101/334, epoch 299/501 --> loss:0.12637695789337158
step 151/334, epoch 299/501 --> loss:0.04804997086524963
step 201/334, epoch 299/501 --> loss:0.0993673837184906
step 251/334, epoch 299/501 --> loss:0.08358482360839843
step 301/334, epoch 299/501 --> loss:0.10416308164596558
step 51/334, epoch 300/501 --> loss:0.06282495021820068
step 101/334, epoch 300/501 --> loss:0.04307162165641785
step 151/334, epoch 300/501 --> loss:0.12785178542137146
step 201/334, epoch 300/501 --> loss:0.08784106850624085
step 251/334, epoch 300/501 --> loss:0.10036461710929871
step 301/334, epoch 300/501 --> loss:0.07818808674812316
step 51/334, epoch 301/501 --> loss:0.06463485956192017
step 101/334, epoch 301/501 --> loss:0.07607194662094116
step 151/334, epoch 301/501 --> loss:0.11334818363189697
step 201/334, epoch 301/501 --> loss:0.060898228883743286
step 251/334, epoch 301/501 --> loss:0.09851519823074341
step 301/334, epoch 301/501 --> loss:0.10106958985328675

##########train dataset##########
acc--> [99.12455944608813]
F1--> {'F1': [0.895281748879376], 'precision': [0.9115370554192957], 'recall': [0.8796056913405663]}
##########eval dataset##########
acc--> [98.8029833749353]
F1--> {'F1': [0.8537247695113674], 'precision': [0.8686992541281505], 'recall': [0.8392674527210503]}
step 51/334, epoch 302/501 --> loss:0.12024756193161011
step 101/334, epoch 302/501 --> loss:0.06705767393112183
step 151/334, epoch 302/501 --> loss:0.07319173574447632
step 201/334, epoch 302/501 --> loss:0.1145589828491211
step 251/334, epoch 302/501 --> loss:0.05640924334526062
step 301/334, epoch 302/501 --> loss:0.05688048243522644
step 51/334, epoch 303/501 --> loss:0.05739656209945679
step 101/334, epoch 303/501 --> loss:0.061546612977981564
step 151/334, epoch 303/501 --> loss:0.05677288293838501
step 201/334, epoch 303/501 --> loss:0.07529592037200927
step 251/334, epoch 303/501 --> loss:0.08159027695655822
step 301/334, epoch 303/501 --> loss:0.06837886929512024
step 51/334, epoch 304/501 --> loss:0.052829821109771725
step 101/334, epoch 304/501 --> loss:0.06785906791687012
step 151/334, epoch 304/501 --> loss:0.08296861290931702
step 201/334, epoch 304/501 --> loss:0.048048750162124634
step 251/334, epoch 304/501 --> loss:0.06545193552970886
step 301/334, epoch 304/501 --> loss:0.11272257089614868
step 51/334, epoch 305/501 --> loss:0.06174156665802002
step 101/334, epoch 305/501 --> loss:0.12471866726875305
step 151/334, epoch 305/501 --> loss:0.15334765315055848
step 201/334, epoch 305/501 --> loss:0.09639722466468811
step 251/334, epoch 305/501 --> loss:0.06122431755065918
step 301/334, epoch 305/501 --> loss:0.10649403691291809
step 51/334, epoch 306/501 --> loss:0.11586066007614136
step 101/334, epoch 306/501 --> loss:0.06381444096565246
step 151/334, epoch 306/501 --> loss:0.055807102918624875
step 201/334, epoch 306/501 --> loss:0.07897390604019165
step 251/334, epoch 306/501 --> loss:0.07753345370292664
step 301/334, epoch 306/501 --> loss:0.08985760569572449
step 51/334, epoch 307/501 --> loss:0.11516464829444885
step 101/334, epoch 307/501 --> loss:0.16703348159790038
step 151/334, epoch 307/501 --> loss:0.11683133244514465
step 201/334, epoch 307/501 --> loss:0.16138299345970153
step 251/334, epoch 307/501 --> loss:0.05999287605285644
step 301/334, epoch 307/501 --> loss:0.12041512250900269
step 51/334, epoch 308/501 --> loss:0.0517275869846344
step 101/334, epoch 308/501 --> loss:0.074918714761734
step 151/334, epoch 308/501 --> loss:0.07381109833717346
step 201/334, epoch 308/501 --> loss:0.05961259365081787
step 251/334, epoch 308/501 --> loss:0.08213714241981507
step 301/334, epoch 308/501 --> loss:0.07082310557365418
step 51/334, epoch 309/501 --> loss:0.0557097852230072
step 101/334, epoch 309/501 --> loss:0.05258422017097473
step 151/334, epoch 309/501 --> loss:0.04715930223464966
step 201/334, epoch 309/501 --> loss:0.12510031580924988
step 251/334, epoch 309/501 --> loss:0.08995741724967957
step 301/334, epoch 309/501 --> loss:0.08802916646003724
step 51/334, epoch 310/501 --> loss:0.07046518206596375
step 101/334, epoch 310/501 --> loss:0.06756609916687012
step 151/334, epoch 310/501 --> loss:0.1075141179561615
step 201/334, epoch 310/501 --> loss:0.04764963626861572
step 251/334, epoch 310/501 --> loss:0.0819810700416565
step 301/334, epoch 310/501 --> loss:0.06531739950180054
step 51/334, epoch 311/501 --> loss:0.06275570988655091
step 101/334, epoch 311/501 --> loss:0.06306227445602416
step 151/334, epoch 311/501 --> loss:0.060665246248245236
step 201/334, epoch 311/501 --> loss:0.05963854193687439
step 251/334, epoch 311/501 --> loss:0.06207229495048523
step 301/334, epoch 311/501 --> loss:0.07248452663421631

##########train dataset##########
acc--> [99.67957229528152]
F1--> {'F1': [0.9619553295488099], 'precision': [0.9718699315077052], 'recall': [0.9522507718616187]}
##########eval dataset##########
acc--> [99.2415333878792]
F1--> {'F1': [0.9072711951521638], 'precision': [0.9236227688554371], 'recall': [0.8914981698331138]}
save model!
step 51/334, epoch 312/501 --> loss:0.044259339570999146
step 101/334, epoch 312/501 --> loss:0.05113249659538269
step 151/334, epoch 312/501 --> loss:0.07951252102851868
step 201/334, epoch 312/501 --> loss:0.0451916229724884
step 251/334, epoch 312/501 --> loss:0.05241015672683716
step 301/334, epoch 312/501 --> loss:0.057928019762039186
step 51/334, epoch 313/501 --> loss:0.045001542568206786
step 101/334, epoch 313/501 --> loss:0.07660878777503967
step 151/334, epoch 313/501 --> loss:0.07713094234466553
step 201/334, epoch 313/501 --> loss:0.0752039122581482
step 251/334, epoch 313/501 --> loss:0.06855196595191955
step 301/334, epoch 313/501 --> loss:0.09181543469429015
step 51/334, epoch 314/501 --> loss:0.05191176414489746
step 101/334, epoch 314/501 --> loss:0.09066253185272216
step 151/334, epoch 314/501 --> loss:0.07275662541389466
step 201/334, epoch 314/501 --> loss:0.053799219131469726
step 251/334, epoch 314/501 --> loss:0.06563565969467162
step 301/334, epoch 314/501 --> loss:0.07055776238441468
step 51/334, epoch 315/501 --> loss:0.06898090362548828
step 101/334, epoch 315/501 --> loss:0.11496042132377625
step 151/334, epoch 315/501 --> loss:0.09200266718864442
step 201/334, epoch 315/501 --> loss:0.06988907456398011
step 251/334, epoch 315/501 --> loss:0.07801721572875976
step 301/334, epoch 315/501 --> loss:0.06191219210624695
step 51/334, epoch 316/501 --> loss:0.07529201507568359
step 101/334, epoch 316/501 --> loss:0.06970145106315613
step 151/334, epoch 316/501 --> loss:0.06443081736564636
step 201/334, epoch 316/501 --> loss:0.05936320424079895
step 251/334, epoch 316/501 --> loss:0.14678048253059386
step 301/334, epoch 316/501 --> loss:0.1222939670085907
step 51/334, epoch 317/501 --> loss:0.060124701261520384
step 101/334, epoch 317/501 --> loss:0.09219679713249207
step 151/334, epoch 317/501 --> loss:0.11136850833892822
step 201/334, epoch 317/501 --> loss:0.05927882552146912
step 251/334, epoch 317/501 --> loss:0.06879072427749634
step 301/334, epoch 317/501 --> loss:0.07512469410896301
step 51/334, epoch 318/501 --> loss:0.054694167375564574
step 101/334, epoch 318/501 --> loss:0.07856469750404357
step 151/334, epoch 318/501 --> loss:0.09491063833236695
step 201/334, epoch 318/501 --> loss:0.0664626669883728
step 251/334, epoch 318/501 --> loss:0.08550824880599976
step 301/334, epoch 318/501 --> loss:0.05628013014793396
step 51/334, epoch 319/501 --> loss:0.05495712399482727
step 101/334, epoch 319/501 --> loss:0.0991242778301239
step 151/334, epoch 319/501 --> loss:0.05755010604858399
step 201/334, epoch 319/501 --> loss:0.04935208082199097
step 251/334, epoch 319/501 --> loss:0.0633795428276062
step 301/334, epoch 319/501 --> loss:0.09682237863540649
step 51/334, epoch 320/501 --> loss:0.06746330618858337
step 101/334, epoch 320/501 --> loss:0.058819180727005003
step 151/334, epoch 320/501 --> loss:0.09097767472267151
step 201/334, epoch 320/501 --> loss:0.06710936188697815
step 251/334, epoch 320/501 --> loss:0.07262189030647277
step 301/334, epoch 320/501 --> loss:0.07280135154724121
step 51/334, epoch 321/501 --> loss:0.06344039440155029
step 101/334, epoch 321/501 --> loss:0.07039406776428223
step 151/334, epoch 321/501 --> loss:0.06542217254638671
step 201/334, epoch 321/501 --> loss:0.13365275621414185
step 251/334, epoch 321/501 --> loss:0.1320987892150879
step 301/334, epoch 321/501 --> loss:0.1684948170185089

##########train dataset##########
acc--> [99.49763998539814]
F1--> {'F1': [0.9395275827264042], 'precision': [0.962881407094008], 'recall': [0.9172893111862761]}
##########eval dataset##########
acc--> [99.03323823489298]
F1--> {'F1': [0.8783694870301711], 'precision': [0.9219806732610097], 'recall': [0.838706816713498]}
step 51/334, epoch 322/501 --> loss:0.10551193356513977
step 101/334, epoch 322/501 --> loss:0.061094000339508056
step 151/334, epoch 322/501 --> loss:0.08933685183525085
step 201/334, epoch 322/501 --> loss:0.04585591673851013
step 251/334, epoch 322/501 --> loss:0.08726064920425415
step 301/334, epoch 322/501 --> loss:0.06888137459754944
step 51/334, epoch 323/501 --> loss:0.08455388069152832
step 101/334, epoch 323/501 --> loss:0.08329127907752991
step 151/334, epoch 323/501 --> loss:0.103800128698349
step 201/334, epoch 323/501 --> loss:0.08271029233932495
step 251/334, epoch 323/501 --> loss:0.08940001487731934
step 301/334, epoch 323/501 --> loss:0.060839238166809084
step 51/334, epoch 324/501 --> loss:0.0524735414981842
step 101/334, epoch 324/501 --> loss:0.10549869179725647
step 151/334, epoch 324/501 --> loss:0.11262601375579834
step 201/334, epoch 324/501 --> loss:0.07996989607810974
step 251/334, epoch 324/501 --> loss:0.0640038800239563
step 301/334, epoch 324/501 --> loss:0.08234617471694947
step 51/334, epoch 325/501 --> loss:0.15758509159088135
step 101/334, epoch 325/501 --> loss:0.08336398720741273
step 151/334, epoch 325/501 --> loss:0.0586743426322937
step 201/334, epoch 325/501 --> loss:0.05109192848205566
step 251/334, epoch 325/501 --> loss:0.12829665899276732
step 301/334, epoch 325/501 --> loss:0.07410493969917298
step 51/334, epoch 326/501 --> loss:0.10886348366737365
step 101/334, epoch 326/501 --> loss:0.1354825520515442
step 151/334, epoch 326/501 --> loss:0.11085697770118713
step 201/334, epoch 326/501 --> loss:0.05474896550178528
step 251/334, epoch 326/501 --> loss:0.11608035802841186
step 301/334, epoch 326/501 --> loss:0.08092243790626526
step 51/334, epoch 327/501 --> loss:0.09081347584724427
step 101/334, epoch 327/501 --> loss:0.053802731037139895
step 151/334, epoch 327/501 --> loss:0.06084693074226379
step 201/334, epoch 327/501 --> loss:0.07048227906227111
step 251/334, epoch 327/501 --> loss:0.07253252506256104
step 301/334, epoch 327/501 --> loss:0.06652294039726257
step 51/334, epoch 328/501 --> loss:0.18430912613868713
step 101/334, epoch 328/501 --> loss:0.16165597796440123
step 151/334, epoch 328/501 --> loss:0.19415385723114015
step 201/334, epoch 328/501 --> loss:0.15431286811828612
step 251/334, epoch 328/501 --> loss:0.12081210017204284
step 301/334, epoch 328/501 --> loss:0.10236958384513856
step 51/334, epoch 329/501 --> loss:0.11575661420822143
step 101/334, epoch 329/501 --> loss:0.10951874136924744
step 151/334, epoch 329/501 --> loss:0.10260905742645264
step 201/334, epoch 329/501 --> loss:0.10485517740249634
step 251/334, epoch 329/501 --> loss:0.0687594747543335
step 301/334, epoch 329/501 --> loss:0.11430557012557983
step 51/334, epoch 330/501 --> loss:0.06046950340270996
step 101/334, epoch 330/501 --> loss:0.0679866337776184
step 151/334, epoch 330/501 --> loss:0.08581483960151673
step 201/334, epoch 330/501 --> loss:0.09165716052055359
step 251/334, epoch 330/501 --> loss:0.08140819549560546
step 301/334, epoch 330/501 --> loss:0.080572429895401
step 51/334, epoch 331/501 --> loss:0.07874510049819947
step 101/334, epoch 331/501 --> loss:0.10955041050910949
step 151/334, epoch 331/501 --> loss:0.0794327986240387
step 201/334, epoch 331/501 --> loss:0.05699716687202454
step 251/334, epoch 331/501 --> loss:0.05575220823287964
step 301/334, epoch 331/501 --> loss:0.09120131134986878

##########train dataset##########
acc--> [99.66252902316369]
F1--> {'F1': [0.9600372878901647], 'precision': [0.9673383233072644], 'recall': [0.9528554869449363]}
##########eval dataset##########
acc--> [99.17329135737253]
F1--> {'F1': [0.8981098245268865], 'precision': [0.9220342343997578], 'recall': [0.8754050619195283]}
step 51/334, epoch 332/501 --> loss:0.060960958003997805
step 101/334, epoch 332/501 --> loss:0.040337023735046384
step 151/334, epoch 332/501 --> loss:0.05682040929794312
step 201/334, epoch 332/501 --> loss:0.0455231773853302
step 251/334, epoch 332/501 --> loss:0.07476091623306275
step 301/334, epoch 332/501 --> loss:0.05401259660720825
step 51/334, epoch 333/501 --> loss:0.11570497274398804
step 101/334, epoch 333/501 --> loss:0.07198299884796143
step 151/334, epoch 333/501 --> loss:0.1090563452243805
step 201/334, epoch 333/501 --> loss:0.059977014064788815
step 251/334, epoch 333/501 --> loss:0.12492924094200135
step 301/334, epoch 333/501 --> loss:0.0827905559539795
step 51/334, epoch 334/501 --> loss:0.07466890215873719
step 101/334, epoch 334/501 --> loss:0.06641669750213623
step 151/334, epoch 334/501 --> loss:0.06638477563858032
step 201/334, epoch 334/501 --> loss:0.07150820016860962
step 251/334, epoch 334/501 --> loss:0.06792083501815796
step 301/334, epoch 334/501 --> loss:0.0469942045211792
step 51/334, epoch 335/501 --> loss:0.04779427647590637
step 101/334, epoch 335/501 --> loss:0.04323160648345947
step 151/334, epoch 335/501 --> loss:0.04636917591094971
step 201/334, epoch 335/501 --> loss:0.0702733826637268
step 251/334, epoch 335/501 --> loss:0.04814492106437683
step 301/334, epoch 335/501 --> loss:0.11277663588523865
step 51/334, epoch 336/501 --> loss:0.0410239315032959
step 101/334, epoch 336/501 --> loss:0.08970045804977417
step 151/334, epoch 336/501 --> loss:0.040102181434631345
step 201/334, epoch 336/501 --> loss:0.07934810996055602
step 251/334, epoch 336/501 --> loss:0.06952139377593994
step 301/334, epoch 336/501 --> loss:0.08221307039260864
step 51/334, epoch 337/501 --> loss:0.035330091714859006
step 101/334, epoch 337/501 --> loss:0.10952882170677185
step 151/334, epoch 337/501 --> loss:0.062384728193283084
step 201/334, epoch 337/501 --> loss:0.09319026827812195
step 251/334, epoch 337/501 --> loss:0.08264322996139527
step 301/334, epoch 337/501 --> loss:0.10571797966957092
step 51/334, epoch 338/501 --> loss:0.052370182275772094
step 101/334, epoch 338/501 --> loss:0.06996707797050476
step 151/334, epoch 338/501 --> loss:0.044185086488723754
step 201/334, epoch 338/501 --> loss:0.0672240936756134
step 251/334, epoch 338/501 --> loss:0.06916521906852723
step 301/334, epoch 338/501 --> loss:0.0856553852558136
step 51/334, epoch 339/501 --> loss:0.0404307758808136
step 101/334, epoch 339/501 --> loss:0.0627951717376709
step 151/334, epoch 339/501 --> loss:0.0845454740524292
step 201/334, epoch 339/501 --> loss:0.0773603641986847
step 251/334, epoch 339/501 --> loss:0.07912634372711182
step 301/334, epoch 339/501 --> loss:0.06871158599853516
step 51/334, epoch 340/501 --> loss:0.07433658599853515
step 101/334, epoch 340/501 --> loss:0.07200996279716491
step 151/334, epoch 340/501 --> loss:0.043143762350082396
step 201/334, epoch 340/501 --> loss:0.04274051308631897
step 251/334, epoch 340/501 --> loss:0.06244061708450317
step 301/334, epoch 340/501 --> loss:0.11858446240425109
step 51/334, epoch 341/501 --> loss:0.04913003206253052
step 101/334, epoch 341/501 --> loss:0.05185105085372925
step 151/334, epoch 341/501 --> loss:0.1279425776004791
step 201/334, epoch 341/501 --> loss:0.06392699003219604
step 251/334, epoch 341/501 --> loss:0.056250582933425906
step 301/334, epoch 341/501 --> loss:0.07416040182113648

##########train dataset##########
acc--> [99.70063108337368]
F1--> {'F1': [0.9645446201834207], 'precision': [0.9719935279762802], 'recall': [0.957218861946278]}
##########eval dataset##########
acc--> [99.20165361485523]
F1--> {'F1': [0.9029508652465803], 'precision': [0.9138403850352], 'recall': [0.8923275775651105]}
step 51/334, epoch 342/501 --> loss:0.0441616952419281
step 101/334, epoch 342/501 --> loss:0.05928632974624634
step 151/334, epoch 342/501 --> loss:0.06622914791107178
step 201/334, epoch 342/501 --> loss:0.05432087182998657
step 251/334, epoch 342/501 --> loss:0.08426116943359375
step 301/334, epoch 342/501 --> loss:0.0807217800617218
step 51/334, epoch 343/501 --> loss:0.06837305784225464
step 101/334, epoch 343/501 --> loss:0.04823357343673706
step 151/334, epoch 343/501 --> loss:0.06426262378692627
step 201/334, epoch 343/501 --> loss:0.07589963674545289
step 251/334, epoch 343/501 --> loss:0.08592695355415345
step 301/334, epoch 343/501 --> loss:0.11406514763832093
step 51/334, epoch 344/501 --> loss:0.0792246973514557
step 101/334, epoch 344/501 --> loss:0.08602102637290955
step 151/334, epoch 344/501 --> loss:0.09197545647621155
step 201/334, epoch 344/501 --> loss:0.10119569659233094
step 251/334, epoch 344/501 --> loss:0.07453816771507263
step 301/334, epoch 344/501 --> loss:0.09823912024497986
step 51/334, epoch 345/501 --> loss:0.05883299469947815
step 101/334, epoch 345/501 --> loss:0.046822251081466676
step 151/334, epoch 345/501 --> loss:0.1500812304019928
step 201/334, epoch 345/501 --> loss:0.0652602994441986
step 251/334, epoch 345/501 --> loss:0.11405334115028382
step 301/334, epoch 345/501 --> loss:0.05401235818862915
step 51/334, epoch 346/501 --> loss:0.05978432059288025
step 101/334, epoch 346/501 --> loss:0.07952875137329102
step 151/334, epoch 346/501 --> loss:0.09049769520759582
step 201/334, epoch 346/501 --> loss:0.04325783133506775
step 251/334, epoch 346/501 --> loss:0.051578835248947144
step 301/334, epoch 346/501 --> loss:0.058883352279663084
step 51/334, epoch 347/501 --> loss:0.047719223499298094
step 101/334, epoch 347/501 --> loss:0.046925967931747435
step 151/334, epoch 347/501 --> loss:0.0724190378189087
step 201/334, epoch 347/501 --> loss:0.05336923003196716
step 251/334, epoch 347/501 --> loss:0.0472769832611084
step 301/334, epoch 347/501 --> loss:0.07549231290817261
step 51/334, epoch 348/501 --> loss:0.06645331501960755
step 101/334, epoch 348/501 --> loss:0.046329662799835206
step 151/334, epoch 348/501 --> loss:0.07746432185173034
step 201/334, epoch 348/501 --> loss:0.08364566683769226
step 251/334, epoch 348/501 --> loss:0.06603618621826172
step 301/334, epoch 348/501 --> loss:0.10383912682533264
step 51/334, epoch 349/501 --> loss:0.10663117289543152
step 101/334, epoch 349/501 --> loss:0.08794874429702759
step 151/334, epoch 349/501 --> loss:0.05491361737251282
step 201/334, epoch 349/501 --> loss:0.061322101354599
step 251/334, epoch 349/501 --> loss:0.06032602071762085
step 301/334, epoch 349/501 --> loss:0.04849559664726257
step 51/334, epoch 350/501 --> loss:0.07701603293418885
step 101/334, epoch 350/501 --> loss:0.07317370772361756
step 151/334, epoch 350/501 --> loss:0.07771412014961243
step 201/334, epoch 350/501 --> loss:0.08603346228599548
step 251/334, epoch 350/501 --> loss:0.0881909692287445
step 301/334, epoch 350/501 --> loss:0.08416609287261963
step 51/334, epoch 351/501 --> loss:0.08292574286460877
step 101/334, epoch 351/501 --> loss:0.06634273529052734
step 151/334, epoch 351/501 --> loss:0.03706398248672485
step 201/334, epoch 351/501 --> loss:0.0780903160572052
step 251/334, epoch 351/501 --> loss:0.09916248679161072
step 301/334, epoch 351/501 --> loss:0.06387100458145141

##########train dataset##########
acc--> [99.71440461497814]
F1--> {'F1': [0.9661437508871104], 'precision': [0.9745381306933307], 'recall': [0.957902578610972]}
##########eval dataset##########
acc--> [99.2320490425691]
F1--> {'F1': [0.9068724402183501], 'precision': [0.9155310911234055], 'recall': [0.8983858461844785]}
step 51/334, epoch 352/501 --> loss:0.05251060009002686
step 101/334, epoch 352/501 --> loss:0.08985971570014954
step 151/334, epoch 352/501 --> loss:0.10164949655532837
step 201/334, epoch 352/501 --> loss:0.09205332756042481
step 251/334, epoch 352/501 --> loss:0.05912653684616089
step 301/334, epoch 352/501 --> loss:0.07868285179138183
step 51/334, epoch 353/501 --> loss:0.04190253853797912
step 101/334, epoch 353/501 --> loss:0.13282384872436523
step 151/334, epoch 353/501 --> loss:0.07396014213562012
step 201/334, epoch 353/501 --> loss:0.06491266012191772
step 251/334, epoch 353/501 --> loss:0.08639129877090454
step 301/334, epoch 353/501 --> loss:0.09005865454673767
step 51/334, epoch 354/501 --> loss:0.10393954515457153
step 101/334, epoch 354/501 --> loss:0.11913750529289245
step 151/334, epoch 354/501 --> loss:0.07539477825164795
step 201/334, epoch 354/501 --> loss:0.07665365099906922
step 251/334, epoch 354/501 --> loss:0.11007109761238099
step 301/334, epoch 354/501 --> loss:0.06519530653953552
step 51/334, epoch 355/501 --> loss:0.0602756929397583
step 101/334, epoch 355/501 --> loss:0.05156212568283081
step 151/334, epoch 355/501 --> loss:0.05907577514648438
step 201/334, epoch 355/501 --> loss:0.06087157726287842
step 251/334, epoch 355/501 --> loss:0.08464706778526306
step 301/334, epoch 355/501 --> loss:0.08012061595916747
step 51/334, epoch 356/501 --> loss:0.045217193365097046
step 101/334, epoch 356/501 --> loss:0.0710794711112976
step 151/334, epoch 356/501 --> loss:0.07123542666435241
step 201/334, epoch 356/501 --> loss:0.07333974361419678
step 251/334, epoch 356/501 --> loss:0.06087532997131348
step 301/334, epoch 356/501 --> loss:0.08080723881721497
step 51/334, epoch 357/501 --> loss:0.05070221543312073
step 101/334, epoch 357/501 --> loss:0.061137715578079226
step 151/334, epoch 357/501 --> loss:0.06965752840042114
step 201/334, epoch 357/501 --> loss:0.04620451211929321
step 251/334, epoch 357/501 --> loss:0.06850909233093262
step 301/334, epoch 357/501 --> loss:0.060715395212173465
step 51/334, epoch 358/501 --> loss:0.21126907348632812
step 101/334, epoch 358/501 --> loss:0.1565249764919281
step 151/334, epoch 358/501 --> loss:0.12019995927810669
step 201/334, epoch 358/501 --> loss:0.1556548857688904
step 251/334, epoch 358/501 --> loss:0.1288492810726166
step 301/334, epoch 358/501 --> loss:0.07863700151443481
step 51/334, epoch 359/501 --> loss:0.10381259322166443
step 101/334, epoch 359/501 --> loss:0.06868152737617493
step 151/334, epoch 359/501 --> loss:0.08864980816841125
step 201/334, epoch 359/501 --> loss:0.1294182062149048
step 251/334, epoch 359/501 --> loss:0.06934721112251281
step 301/334, epoch 359/501 --> loss:0.07881381392478942
step 51/334, epoch 360/501 --> loss:0.058088781833648684
step 101/334, epoch 360/501 --> loss:0.07182334899902344
step 151/334, epoch 360/501 --> loss:0.05357207059860229
step 201/334, epoch 360/501 --> loss:0.0734263575077057
step 251/334, epoch 360/501 --> loss:0.09932196617126465
step 301/334, epoch 360/501 --> loss:0.0671281135082245
step 51/334, epoch 361/501 --> loss:0.08069692611694336
step 101/334, epoch 361/501 --> loss:0.09327805995941162
step 151/334, epoch 361/501 --> loss:0.0550923490524292
step 201/334, epoch 361/501 --> loss:0.08432711362838745
step 251/334, epoch 361/501 --> loss:0.06995337009429932
step 301/334, epoch 361/501 --> loss:0.0664229953289032

##########train dataset##########
acc--> [99.68444658299873]
F1--> {'F1': [0.9626820159587463], 'precision': [0.9686972483228086], 'recall': [0.9567509036429769]}
##########eval dataset##########
acc--> [99.219516944378]
F1--> {'F1': [0.9044709561987035], 'precision': [0.9218600539806502], 'recall': [0.8877353656908922]}
step 51/334, epoch 362/501 --> loss:0.09558473110198974
step 101/334, epoch 362/501 --> loss:0.05660498261451721
step 151/334, epoch 362/501 --> loss:0.07141677141189576
step 201/334, epoch 362/501 --> loss:0.044211527109146116
step 251/334, epoch 362/501 --> loss:0.10034437656402588
step 301/334, epoch 362/501 --> loss:0.06280849814414978
step 51/334, epoch 363/501 --> loss:0.11574245333671569
step 101/334, epoch 363/501 --> loss:0.05574066042900085
step 151/334, epoch 363/501 --> loss:0.04493546366691589
step 201/334, epoch 363/501 --> loss:0.0744538962841034
step 251/334, epoch 363/501 --> loss:0.04814842104911804
step 301/334, epoch 363/501 --> loss:0.0658058500289917
step 51/334, epoch 364/501 --> loss:0.04811245799064636
step 101/334, epoch 364/501 --> loss:0.057213412523269655
step 151/334, epoch 364/501 --> loss:0.12227651715278626
step 201/334, epoch 364/501 --> loss:0.08731547594070435
step 251/334, epoch 364/501 --> loss:0.052003953456878665
step 301/334, epoch 364/501 --> loss:0.11845307230949402
step 51/334, epoch 365/501 --> loss:0.046798980236053465
step 101/334, epoch 365/501 --> loss:0.044051610231399536
step 151/334, epoch 365/501 --> loss:0.06932084560394287
step 201/334, epoch 365/501 --> loss:0.044527909755706786
step 251/334, epoch 365/501 --> loss:0.06096592426300049
step 301/334, epoch 365/501 --> loss:0.09094604849815369
step 51/334, epoch 366/501 --> loss:0.07454188108444214
step 101/334, epoch 366/501 --> loss:0.07231631517410278
step 151/334, epoch 366/501 --> loss:0.05346737504005432
step 201/334, epoch 366/501 --> loss:0.0904935610294342
step 251/334, epoch 366/501 --> loss:0.044972199201583865
step 301/334, epoch 366/501 --> loss:0.0629099178314209
step 51/334, epoch 367/501 --> loss:0.06427133560180665
step 101/334, epoch 367/501 --> loss:0.04761929273605347
step 151/334, epoch 367/501 --> loss:0.05169072985649109
step 201/334, epoch 367/501 --> loss:0.04876538753509521
step 251/334, epoch 367/501 --> loss:0.21002834916114807
step 301/334, epoch 367/501 --> loss:0.13223956346511842
step 51/334, epoch 368/501 --> loss:0.11378127813339234
step 101/334, epoch 368/501 --> loss:0.09992769718170166
step 151/334, epoch 368/501 --> loss:0.08846382737159729
step 201/334, epoch 368/501 --> loss:0.07329506993293762
step 251/334, epoch 368/501 --> loss:0.1317071270942688
step 301/334, epoch 368/501 --> loss:0.1293009901046753
step 51/334, epoch 369/501 --> loss:0.09871924161911011
step 101/334, epoch 369/501 --> loss:0.1473906719684601
step 151/334, epoch 369/501 --> loss:0.15060352444648742
step 201/334, epoch 369/501 --> loss:0.14419353604316712
step 251/334, epoch 369/501 --> loss:0.1123972237110138
step 301/334, epoch 369/501 --> loss:0.07785950303077697
step 51/334, epoch 370/501 --> loss:0.0850715148448944
step 101/334, epoch 370/501 --> loss:0.11460462927818299
step 151/334, epoch 370/501 --> loss:0.08222913026809692
step 201/334, epoch 370/501 --> loss:0.04551317572593689
step 251/334, epoch 370/501 --> loss:0.06569020867347718
step 301/334, epoch 370/501 --> loss:0.06057697176933288
step 51/334, epoch 371/501 --> loss:0.07079434156417846
step 101/334, epoch 371/501 --> loss:0.039196884632110594
step 151/334, epoch 371/501 --> loss:0.07169796228408813
step 201/334, epoch 371/501 --> loss:0.0870555579662323
step 251/334, epoch 371/501 --> loss:0.10976788878440857
step 301/334, epoch 371/501 --> loss:0.07426317811012267

##########train dataset##########
acc--> [99.68472778640421]
F1--> {'F1': [0.9627417237729321], 'precision': [0.9680617379431806], 'recall': [0.9574897533261633]}
##########eval dataset##########
acc--> [99.18337777738668]
F1--> {'F1': [0.9008467392130003], 'precision': [0.9106077848799563], 'recall': [0.8913025245444371]}
step 51/334, epoch 372/501 --> loss:0.12829208970069886
step 101/334, epoch 372/501 --> loss:0.09666157364845276
step 151/334, epoch 372/501 --> loss:0.08599305391311646
step 201/334, epoch 372/501 --> loss:0.10435498595237731
step 251/334, epoch 372/501 --> loss:0.08501315355300904
step 301/334, epoch 372/501 --> loss:0.07244507789611816
step 51/334, epoch 373/501 --> loss:0.10665682315826416
step 101/334, epoch 373/501 --> loss:0.06329293489456177
step 151/334, epoch 373/501 --> loss:0.0706488013267517
step 201/334, epoch 373/501 --> loss:0.10420047998428345
step 251/334, epoch 373/501 --> loss:0.08983299255371094
step 301/334, epoch 373/501 --> loss:0.05705177545547485
step 51/334, epoch 374/501 --> loss:0.07515024065971375
step 101/334, epoch 374/501 --> loss:0.09889909982681275
step 151/334, epoch 374/501 --> loss:0.07946663498878478
step 201/334, epoch 374/501 --> loss:0.0659520697593689
step 251/334, epoch 374/501 --> loss:0.08396483421325683
step 301/334, epoch 374/501 --> loss:0.049743320941925045
step 51/334, epoch 375/501 --> loss:0.06425731062889099
step 101/334, epoch 375/501 --> loss:0.052285029888153076
step 151/334, epoch 375/501 --> loss:0.06950660347938538
step 201/334, epoch 375/501 --> loss:0.08266196966171264
step 251/334, epoch 375/501 --> loss:0.0990086305141449
step 301/334, epoch 375/501 --> loss:0.04971771836280823
step 51/334, epoch 376/501 --> loss:0.04417942523956299
step 101/334, epoch 376/501 --> loss:0.06622044682502747
step 151/334, epoch 376/501 --> loss:0.05365134119987488
step 201/334, epoch 376/501 --> loss:0.05938263654708862
step 251/334, epoch 376/501 --> loss:0.08354761600494384
step 301/334, epoch 376/501 --> loss:0.06310698390007019
step 51/334, epoch 377/501 --> loss:0.04366426587104797
step 101/334, epoch 377/501 --> loss:0.0683830165863037
step 151/334, epoch 377/501 --> loss:0.045238877534866336
step 201/334, epoch 377/501 --> loss:0.05356602072715759
step 251/334, epoch 377/501 --> loss:0.07215317130088807
step 301/334, epoch 377/501 --> loss:0.04820886254310608
step 51/334, epoch 378/501 --> loss:0.06549097895622254
step 101/334, epoch 378/501 --> loss:0.04867006659507751
step 151/334, epoch 378/501 --> loss:0.04525621652603149
step 201/334, epoch 378/501 --> loss:0.07716622233390807
step 251/334, epoch 378/501 --> loss:0.04960542321205139
step 301/334, epoch 378/501 --> loss:0.0403866958618164
step 51/334, epoch 379/501 --> loss:0.06733742952346802
step 101/334, epoch 379/501 --> loss:0.08030161499977112
step 151/334, epoch 379/501 --> loss:0.06021020889282227
step 201/334, epoch 379/501 --> loss:0.10480441808700562
step 251/334, epoch 379/501 --> loss:0.08714563846588134
step 301/334, epoch 379/501 --> loss:0.06159461736679077
step 51/334, epoch 380/501 --> loss:0.07352630257606506
step 101/334, epoch 380/501 --> loss:0.0745636761188507
step 151/334, epoch 380/501 --> loss:0.0743680214881897
step 201/334, epoch 380/501 --> loss:0.053485041856765746
step 251/334, epoch 380/501 --> loss:0.06419930577278138
step 301/334, epoch 380/501 --> loss:0.0856724488735199
step 51/334, epoch 381/501 --> loss:0.060091540813446045
step 101/334, epoch 381/501 --> loss:0.08473309159278869
step 151/334, epoch 381/501 --> loss:0.06367114543914795
step 201/334, epoch 381/501 --> loss:0.07098344802856445
step 251/334, epoch 381/501 --> loss:0.06123565673828125
step 301/334, epoch 381/501 --> loss:0.08897706151008605

##########train dataset##########
acc--> [99.69882342892888]
F1--> {'F1': [0.9643175237598067], 'precision': [0.972125586231988], 'recall': [0.9566437304338588]}
##########eval dataset##########
acc--> [99.23606532138751]
F1--> {'F1': [0.9057398840073495], 'precision': [0.9309748953123265], 'recall': [0.8818462821367323]}
step 51/334, epoch 382/501 --> loss:0.07032480359077453
step 101/334, epoch 382/501 --> loss:0.04232945203781128
step 151/334, epoch 382/501 --> loss:0.0569854736328125
step 201/334, epoch 382/501 --> loss:0.08344687104225158
step 251/334, epoch 382/501 --> loss:0.0947520935535431
step 301/334, epoch 382/501 --> loss:0.06442014455795288
step 51/334, epoch 383/501 --> loss:0.1302616500854492
step 101/334, epoch 383/501 --> loss:0.047755968570709226
step 151/334, epoch 383/501 --> loss:0.044417698383331296
step 201/334, epoch 383/501 --> loss:0.0453744900226593
step 251/334, epoch 383/501 --> loss:0.07243372321128845
step 301/334, epoch 383/501 --> loss:0.078105149269104
step 51/334, epoch 384/501 --> loss:0.050994977951049805
step 101/334, epoch 384/501 --> loss:0.052493921518325805
step 151/334, epoch 384/501 --> loss:0.06568753719329834
step 201/334, epoch 384/501 --> loss:0.08508668303489685
step 251/334, epoch 384/501 --> loss:0.03571770191192627
step 301/334, epoch 384/501 --> loss:0.06390891075134278
step 51/334, epoch 385/501 --> loss:0.08442796230316162
step 101/334, epoch 385/501 --> loss:0.04704302072525025
step 151/334, epoch 385/501 --> loss:0.04601841807365417
step 201/334, epoch 385/501 --> loss:0.06200141906738281
step 251/334, epoch 385/501 --> loss:0.07096293091773986
step 301/334, epoch 385/501 --> loss:0.05327007293701172
step 51/334, epoch 386/501 --> loss:0.0517945122718811
step 101/334, epoch 386/501 --> loss:0.03925489664077759
step 151/334, epoch 386/501 --> loss:0.04591413140296936
step 201/334, epoch 386/501 --> loss:0.057735117673873904
step 251/334, epoch 386/501 --> loss:0.08656892538070679
step 301/334, epoch 386/501 --> loss:0.12383386135101318
step 51/334, epoch 387/501 --> loss:0.07244845509529113
step 101/334, epoch 387/501 --> loss:0.06641420483589172
step 151/334, epoch 387/501 --> loss:0.09271126389503478
step 201/334, epoch 387/501 --> loss:0.1165307378768921
step 251/334, epoch 387/501 --> loss:0.11433878064155578
step 301/334, epoch 387/501 --> loss:0.07847467064857483
step 51/334, epoch 388/501 --> loss:0.07253834128379821
step 101/334, epoch 388/501 --> loss:0.06323760032653808
step 151/334, epoch 388/501 --> loss:0.08345147490501403
step 201/334, epoch 388/501 --> loss:0.06582712054252625
step 251/334, epoch 388/501 --> loss:0.04202325224876404
step 301/334, epoch 388/501 --> loss:0.05895607948303223
step 51/334, epoch 389/501 --> loss:0.0699827241897583
step 101/334, epoch 389/501 --> loss:0.043500958681106566
step 151/334, epoch 389/501 --> loss:0.06422470808029175
step 201/334, epoch 389/501 --> loss:0.08059854626655578
step 251/334, epoch 389/501 --> loss:0.06901569843292236
step 301/334, epoch 389/501 --> loss:0.06396546244621276
step 51/334, epoch 390/501 --> loss:0.06660314321517945
step 101/334, epoch 390/501 --> loss:0.050631961822509765
step 151/334, epoch 390/501 --> loss:0.08537932395935059
step 201/334, epoch 390/501 --> loss:0.055842273235321045
step 251/334, epoch 390/501 --> loss:0.03913006782531738
step 301/334, epoch 390/501 --> loss:0.047304341793060305
step 51/334, epoch 391/501 --> loss:0.05992752075195312
step 101/334, epoch 391/501 --> loss:0.06500952243804932
step 151/334, epoch 391/501 --> loss:0.05877341389656067
step 201/334, epoch 391/501 --> loss:0.11562983870506287
step 251/334, epoch 391/501 --> loss:0.039964317083358764
step 301/334, epoch 391/501 --> loss:0.05333080410957337

##########train dataset##########
acc--> [99.72663367335144]
F1--> {'F1': [0.9676322934951657], 'precision': [0.974844382235948], 'recall': [0.9605359870686443]}
##########eval dataset##########
acc--> [99.24426341619353]
F1--> {'F1': [0.9074776602979351], 'precision': [0.9251534647940244], 'recall': [0.8904742393673755]}
save model!
step 51/334, epoch 392/501 --> loss:0.05838396310806274
step 101/334, epoch 392/501 --> loss:0.04950284600257873
step 151/334, epoch 392/501 --> loss:0.03986892819404602
step 201/334, epoch 392/501 --> loss:0.055268968343734744
step 251/334, epoch 392/501 --> loss:0.07658275842666626
step 301/334, epoch 392/501 --> loss:0.048487166166305544
step 51/334, epoch 393/501 --> loss:0.06693123459815979
step 101/334, epoch 393/501 --> loss:0.07815172910690307
step 151/334, epoch 393/501 --> loss:0.055231674909591674
step 201/334, epoch 393/501 --> loss:0.12325499415397644
step 251/334, epoch 393/501 --> loss:0.09476287841796875
step 301/334, epoch 393/501 --> loss:0.05907513499259949
step 51/334, epoch 394/501 --> loss:0.051277616024017335
step 101/334, epoch 394/501 --> loss:0.09470617890357971
step 151/334, epoch 394/501 --> loss:0.037370185852050784
step 201/334, epoch 394/501 --> loss:0.06534930944442749
step 251/334, epoch 394/501 --> loss:0.039285695552825926
step 301/334, epoch 394/501 --> loss:0.0811154305934906
step 51/334, epoch 395/501 --> loss:0.16273234605789186
step 101/334, epoch 395/501 --> loss:0.13503981590270997
step 151/334, epoch 395/501 --> loss:0.11124722361564636
step 201/334, epoch 395/501 --> loss:0.08910219550132752
step 251/334, epoch 395/501 --> loss:0.06219687581062317
step 301/334, epoch 395/501 --> loss:0.06605756163597107
step 51/334, epoch 396/501 --> loss:0.08209753513336182
step 101/334, epoch 396/501 --> loss:0.056530749797821044
step 151/334, epoch 396/501 --> loss:0.055556445121765136
step 201/334, epoch 396/501 --> loss:0.11415963411331177
step 251/334, epoch 396/501 --> loss:0.05845166444778442
step 301/334, epoch 396/501 --> loss:0.07861768364906312
step 51/334, epoch 397/501 --> loss:0.07077166080474853
step 101/334, epoch 397/501 --> loss:0.040213642120361326
step 151/334, epoch 397/501 --> loss:0.06498480439186097
step 201/334, epoch 397/501 --> loss:0.06308127641677856
step 251/334, epoch 397/501 --> loss:0.12911414742469787
step 301/334, epoch 397/501 --> loss:0.1054135501384735
step 51/334, epoch 398/501 --> loss:0.0754668653011322
step 101/334, epoch 398/501 --> loss:0.10569579720497131
step 151/334, epoch 398/501 --> loss:0.0490861976146698
step 201/334, epoch 398/501 --> loss:0.08473402500152588
step 251/334, epoch 398/501 --> loss:0.07616464138031005
step 301/334, epoch 398/501 --> loss:0.05594212412834167
step 51/334, epoch 399/501 --> loss:0.0657751977443695
step 101/334, epoch 399/501 --> loss:0.1349383795261383
step 151/334, epoch 399/501 --> loss:0.14218180656433105
step 201/334, epoch 399/501 --> loss:0.07878511190414429
step 251/334, epoch 399/501 --> loss:0.1121389639377594
step 301/334, epoch 399/501 --> loss:0.06423531174659729
step 51/334, epoch 400/501 --> loss:0.09812357664108276
step 101/334, epoch 400/501 --> loss:0.08890237927436828
step 151/334, epoch 400/501 --> loss:0.048053845167160034
step 201/334, epoch 400/501 --> loss:0.04737081527709961
step 251/334, epoch 400/501 --> loss:0.04903468608856201
step 301/334, epoch 400/501 --> loss:0.07130597352981567
step 51/334, epoch 401/501 --> loss:0.06457043051719666
step 101/334, epoch 401/501 --> loss:0.040419676303863526
step 151/334, epoch 401/501 --> loss:0.055177435874938965
step 201/334, epoch 401/501 --> loss:0.04311867475509643
step 251/334, epoch 401/501 --> loss:0.05207846522331238
step 301/334, epoch 401/501 --> loss:0.044508475065231326

##########train dataset##########
acc--> [99.70830825101655]
F1--> {'F1': [0.9654547490865488], 'precision': [0.9728814665506186], 'recall': [0.9581504082526342]}
##########eval dataset##########
acc--> [99.21996215926447]
F1--> {'F1': [0.9042440386920071], 'precision': [0.9244584785062137], 'recall': [0.8849042821447457]}
step 51/334, epoch 402/501 --> loss:0.07918692827224731
step 101/334, epoch 402/501 --> loss:0.09792805314064026
step 151/334, epoch 402/501 --> loss:0.1154231858253479
step 201/334, epoch 402/501 --> loss:0.08410155773162842
step 251/334, epoch 402/501 --> loss:0.07290297269821167
step 301/334, epoch 402/501 --> loss:0.09428386211395264
step 51/334, epoch 403/501 --> loss:0.10293984174728393
step 101/334, epoch 403/501 --> loss:0.09709635138511657
step 151/334, epoch 403/501 --> loss:0.05751984477043152
step 201/334, epoch 403/501 --> loss:0.05034360885620117
step 251/334, epoch 403/501 --> loss:0.06102325320243836
step 301/334, epoch 403/501 --> loss:0.07477222800254822
step 51/334, epoch 404/501 --> loss:0.07515233516693115
step 101/334, epoch 404/501 --> loss:0.055365393161773684
step 151/334, epoch 404/501 --> loss:0.06012463331222534
step 201/334, epoch 404/501 --> loss:0.06804706811904908
step 251/334, epoch 404/501 --> loss:0.0586473536491394
step 301/334, epoch 404/501 --> loss:0.06354797005653381
step 51/334, epoch 405/501 --> loss:0.06294453859329224
step 101/334, epoch 405/501 --> loss:0.050390042066574096
step 151/334, epoch 405/501 --> loss:0.08019469380378723
step 201/334, epoch 405/501 --> loss:0.06073451280593872
step 251/334, epoch 405/501 --> loss:0.06887099623680115
step 301/334, epoch 405/501 --> loss:0.06799941897392273
step 51/334, epoch 406/501 --> loss:0.07794486284255982
step 101/334, epoch 406/501 --> loss:0.05390688419342041
step 151/334, epoch 406/501 --> loss:0.08920474052429199
step 201/334, epoch 406/501 --> loss:0.037089122533798216
step 251/334, epoch 406/501 --> loss:0.05477686882019043
step 301/334, epoch 406/501 --> loss:0.03984294176101685
step 51/334, epoch 407/501 --> loss:0.07913689136505127
step 101/334, epoch 407/501 --> loss:0.07557579636573791
step 151/334, epoch 407/501 --> loss:0.10960562586784363
step 201/334, epoch 407/501 --> loss:0.07668634414672852
step 251/334, epoch 407/501 --> loss:0.11635266900062562
step 301/334, epoch 407/501 --> loss:0.06470540404319763
step 51/334, epoch 408/501 --> loss:0.0775458002090454
step 101/334, epoch 408/501 --> loss:0.061546390056610105
step 151/334, epoch 408/501 --> loss:0.05084776639938354
step 201/334, epoch 408/501 --> loss:0.077388676404953
step 251/334, epoch 408/501 --> loss:0.088994722366333
step 301/334, epoch 408/501 --> loss:0.08252341032028199
step 51/334, epoch 409/501 --> loss:0.05290787577629089
step 101/334, epoch 409/501 --> loss:0.067453054189682
step 151/334, epoch 409/501 --> loss:0.08046244263648987
step 201/334, epoch 409/501 --> loss:0.07460476636886597
step 251/334, epoch 409/501 --> loss:0.06326493144035339
step 301/334, epoch 409/501 --> loss:0.05002437710762024
step 51/334, epoch 410/501 --> loss:0.05284242272377014
step 101/334, epoch 410/501 --> loss:0.0658635926246643
step 151/334, epoch 410/501 --> loss:0.06284282445907592
step 201/334, epoch 410/501 --> loss:0.06708561062812805
step 251/334, epoch 410/501 --> loss:0.042624086141586304
step 301/334, epoch 410/501 --> loss:0.051531941890716554
step 51/334, epoch 411/501 --> loss:0.05849653482437134
step 101/334, epoch 411/501 --> loss:0.04600202083587646
step 151/334, epoch 411/501 --> loss:0.052188242673873904
step 201/334, epoch 411/501 --> loss:0.07001524448394775
step 251/334, epoch 411/501 --> loss:0.03961839914321899
step 301/334, epoch 411/501 --> loss:0.06451929688453674

##########train dataset##########
acc--> [99.7272913403191]
F1--> {'F1': [0.967734566885432], 'precision': [0.9741997177241148], 'recall': [0.9613645287599634]}
##########eval dataset##########
acc--> [99.2396217005706]
F1--> {'F1': [0.9064795404969979], 'precision': [0.928591274604999], 'recall': [0.8854059038357135]}
step 51/334, epoch 412/501 --> loss:0.04785281419754028
step 101/334, epoch 412/501 --> loss:0.07600221395492554
step 151/334, epoch 412/501 --> loss:0.05899611830711365
step 201/334, epoch 412/501 --> loss:0.2912815511226654
step 251/334, epoch 412/501 --> loss:0.16112964868545532
step 301/334, epoch 412/501 --> loss:0.09715240120887757
step 51/334, epoch 413/501 --> loss:0.06828764081001282
step 101/334, epoch 413/501 --> loss:0.06642499446868896
step 151/334, epoch 413/501 --> loss:0.11869869351387025
step 201/334, epoch 413/501 --> loss:0.07026775956153869
step 251/334, epoch 413/501 --> loss:0.12595172047615052
step 301/334, epoch 413/501 --> loss:0.13868606686592103
step 51/334, epoch 414/501 --> loss:0.09528737664222717
step 101/334, epoch 414/501 --> loss:0.08602097272872924
step 151/334, epoch 414/501 --> loss:0.07945384860038757
step 201/334, epoch 414/501 --> loss:0.07588232278823853
step 251/334, epoch 414/501 --> loss:0.10332141041755677
step 301/334, epoch 414/501 --> loss:0.10796826004981995
step 51/334, epoch 415/501 --> loss:0.07196958422660828
step 101/334, epoch 415/501 --> loss:0.0767548930644989
step 151/334, epoch 415/501 --> loss:0.07031901717185975
step 201/334, epoch 415/501 --> loss:0.09839878082275391
step 251/334, epoch 415/501 --> loss:0.05809544920921326
step 301/334, epoch 415/501 --> loss:0.07568945407867432
step 51/334, epoch 416/501 --> loss:0.0458123505115509
step 101/334, epoch 416/501 --> loss:0.04826377272605896
step 151/334, epoch 416/501 --> loss:0.06547721982002258
step 201/334, epoch 416/501 --> loss:0.07758110642433166
step 251/334, epoch 416/501 --> loss:0.057381782531738285
step 301/334, epoch 416/501 --> loss:0.05778401494026184
step 51/334, epoch 417/501 --> loss:0.07324142456054687
step 101/334, epoch 417/501 --> loss:0.042118278741836546
step 151/334, epoch 417/501 --> loss:0.04462416768074036
step 201/334, epoch 417/501 --> loss:0.0450376307964325
step 251/334, epoch 417/501 --> loss:0.07274477362632752
step 301/334, epoch 417/501 --> loss:0.06769352436065673
step 51/334, epoch 418/501 --> loss:0.05274892449378967
step 101/334, epoch 418/501 --> loss:0.05533260583877563
step 151/334, epoch 418/501 --> loss:0.06873785734176635
step 201/334, epoch 418/501 --> loss:0.049045225381851194
step 251/334, epoch 418/501 --> loss:0.042146748304367064
step 301/334, epoch 418/501 --> loss:0.05008500814437866
step 51/334, epoch 419/501 --> loss:0.09196614742279052
step 101/334, epoch 419/501 --> loss:0.042799735069274904
step 151/334, epoch 419/501 --> loss:0.04891142249107361
step 201/334, epoch 419/501 --> loss:0.041154135465621945
step 251/334, epoch 419/501 --> loss:0.04944742798805237
step 301/334, epoch 419/501 --> loss:0.03761326193809509
step 51/334, epoch 420/501 --> loss:0.05511049151420593
step 101/334, epoch 420/501 --> loss:0.04816730141639709
step 151/334, epoch 420/501 --> loss:0.056612870693206786
step 201/334, epoch 420/501 --> loss:0.05938995718955994
step 251/334, epoch 420/501 --> loss:0.06669376611709595
step 301/334, epoch 420/501 --> loss:0.042390438318252566
step 51/334, epoch 421/501 --> loss:0.06927234411239624
step 101/334, epoch 421/501 --> loss:0.05001845598220825
step 151/334, epoch 421/501 --> loss:0.04715233564376831
step 201/334, epoch 421/501 --> loss:0.08078486680984497
step 251/334, epoch 421/501 --> loss:0.03708920955657959
step 301/334, epoch 421/501 --> loss:0.04174304723739624

##########train dataset##########
acc--> [99.7320600694843]
F1--> {'F1': [0.9683043481762875], 'precision': [0.9745970033177899], 'recall': [0.9621023026769352]}
##########eval dataset##########
acc--> [99.21524768738571]
F1--> {'F1': [0.9047382894024806], 'precision': [0.9143244134621858], 'recall': [0.8953608813645532]}
step 51/334, epoch 422/501 --> loss:0.06680366516113281
step 101/334, epoch 422/501 --> loss:0.08158500552177429
step 151/334, epoch 422/501 --> loss:0.03645092248916626
step 201/334, epoch 422/501 --> loss:0.058703098297119144
step 251/334, epoch 422/501 --> loss:0.043146021366119384
step 301/334, epoch 422/501 --> loss:0.04594150424003601
step 51/334, epoch 423/501 --> loss:0.061946909427642825
step 101/334, epoch 423/501 --> loss:0.05186100244522095
step 151/334, epoch 423/501 --> loss:0.11439964413642884
step 201/334, epoch 423/501 --> loss:0.08224457263946533
step 251/334, epoch 423/501 --> loss:0.05821761131286621
step 301/334, epoch 423/501 --> loss:0.10700185298919677
step 51/334, epoch 424/501 --> loss:0.05336892008781433
step 101/334, epoch 424/501 --> loss:0.06800312876701355
step 151/334, epoch 424/501 --> loss:0.053356232643127444
step 201/334, epoch 424/501 --> loss:0.06479336857795716
step 251/334, epoch 424/501 --> loss:0.05070197701454163
step 301/334, epoch 424/501 --> loss:0.053751975297927856
step 51/334, epoch 425/501 --> loss:0.07302605628967285
step 101/334, epoch 425/501 --> loss:0.10882858276367187
step 151/334, epoch 425/501 --> loss:0.07780282974243163
step 201/334, epoch 425/501 --> loss:0.05168733596801758
step 251/334, epoch 425/501 --> loss:0.03784263253211975
step 301/334, epoch 425/501 --> loss:0.04226633429527283
step 51/334, epoch 426/501 --> loss:0.06184447765350342
step 101/334, epoch 426/501 --> loss:0.07103070139884948
step 151/334, epoch 426/501 --> loss:0.10537867069244385
step 201/334, epoch 426/501 --> loss:0.04388614535331726
step 251/334, epoch 426/501 --> loss:0.05611019492149353
step 301/334, epoch 426/501 --> loss:0.18294573426246644
step 51/334, epoch 427/501 --> loss:0.13630297064781188
step 101/334, epoch 427/501 --> loss:0.1272496247291565
step 151/334, epoch 427/501 --> loss:0.10388800501823425
step 201/334, epoch 427/501 --> loss:0.057065004110336305
step 251/334, epoch 427/501 --> loss:0.1090677273273468
step 301/334, epoch 427/501 --> loss:0.10107892394065857
step 51/334, epoch 428/501 --> loss:0.07925033211708069
step 101/334, epoch 428/501 --> loss:0.08268756628036499
step 151/334, epoch 428/501 --> loss:0.04875939846038818
step 201/334, epoch 428/501 --> loss:0.046303584575653076
step 251/334, epoch 428/501 --> loss:0.10939961075782775
step 301/334, epoch 428/501 --> loss:0.0652263593673706
step 51/334, epoch 429/501 --> loss:0.053411463499069216
step 101/334, epoch 429/501 --> loss:0.04166409850120544
step 151/334, epoch 429/501 --> loss:0.03913802623748779
step 201/334, epoch 429/501 --> loss:0.15492066502571106
step 251/334, epoch 429/501 --> loss:0.17018656730651854
step 301/334, epoch 429/501 --> loss:0.1138401448726654
step 51/334, epoch 430/501 --> loss:0.14215172886848448
step 101/334, epoch 430/501 --> loss:0.07223726630210876
step 151/334, epoch 430/501 --> loss:0.12490840077400207
step 201/334, epoch 430/501 --> loss:0.08205579996109008
step 251/334, epoch 430/501 --> loss:0.10969493746757507
step 301/334, epoch 430/501 --> loss:0.1164103627204895
step 51/334, epoch 431/501 --> loss:0.07467581033706665
step 101/334, epoch 431/501 --> loss:0.10618303656578064
step 151/334, epoch 431/501 --> loss:0.06565693736076356
step 201/334, epoch 431/501 --> loss:0.053418039083480834
step 251/334, epoch 431/501 --> loss:0.0793883204460144
step 301/334, epoch 431/501 --> loss:0.11183188438415527

##########train dataset##########
acc--> [99.69805677061078]
F1--> {'F1': [0.964237564673832], 'precision': [0.9717451760349783], 'recall': [0.9568549167888399]}
##########eval dataset##########
acc--> [99.1812952130002]
F1--> {'F1': [0.9000325154750679], 'precision': [0.9150645694465601], 'recall': [0.8854960289604974]}
step 51/334, epoch 432/501 --> loss:0.06128835320472717
step 101/334, epoch 432/501 --> loss:0.09302904129028321
step 151/334, epoch 432/501 --> loss:0.0819247305393219
step 201/334, epoch 432/501 --> loss:0.05434056520462036
step 251/334, epoch 432/501 --> loss:0.10577728271484375
step 301/334, epoch 432/501 --> loss:0.07510874152183533
step 51/334, epoch 433/501 --> loss:0.12170021295547485
step 101/334, epoch 433/501 --> loss:0.04417011857032776
step 151/334, epoch 433/501 --> loss:0.07949068546295165
step 201/334, epoch 433/501 --> loss:0.05613088488578796
step 251/334, epoch 433/501 --> loss:0.11159552216529846
step 301/334, epoch 433/501 --> loss:0.03761551976203918
step 51/334, epoch 434/501 --> loss:0.09683226704597474
step 101/334, epoch 434/501 --> loss:0.08151495695114136
step 151/334, epoch 434/501 --> loss:0.0987239921092987
step 201/334, epoch 434/501 --> loss:0.06997544050216675
step 251/334, epoch 434/501 --> loss:0.06562786698341369
step 301/334, epoch 434/501 --> loss:0.06448733448982238
step 51/334, epoch 435/501 --> loss:0.04238094687461853
step 101/334, epoch 435/501 --> loss:0.04238294005393982
step 151/334, epoch 435/501 --> loss:0.06793378353118897
step 201/334, epoch 435/501 --> loss:0.05555768013000488
step 251/334, epoch 435/501 --> loss:0.08273147583007813
step 301/334, epoch 435/501 --> loss:0.0579819917678833
step 51/334, epoch 436/501 --> loss:0.05369110941886902
step 101/334, epoch 436/501 --> loss:0.047036471366882326
step 151/334, epoch 436/501 --> loss:0.06613990664482117
step 201/334, epoch 436/501 --> loss:0.044180275201797486
step 251/334, epoch 436/501 --> loss:0.060957175493240354
step 301/334, epoch 436/501 --> loss:0.041412631273269655
step 51/334, epoch 437/501 --> loss:0.20112180829048157
step 101/334, epoch 437/501 --> loss:0.08176005363464356
step 151/334, epoch 437/501 --> loss:0.09602870464324952
step 201/334, epoch 437/501 --> loss:0.07360708117485046
step 251/334, epoch 437/501 --> loss:0.07963037371635437
step 301/334, epoch 437/501 --> loss:0.11768075227737426
step 51/334, epoch 438/501 --> loss:0.08456571578979492
step 101/334, epoch 438/501 --> loss:0.05008078455924988
step 151/334, epoch 438/501 --> loss:0.057439810037612914
step 201/334, epoch 438/501 --> loss:0.09094515085220337
step 251/334, epoch 438/501 --> loss:0.0833738076686859
step 301/334, epoch 438/501 --> loss:0.046022121906280515
step 51/334, epoch 439/501 --> loss:0.04891334652900696
step 101/334, epoch 439/501 --> loss:0.036572810411453244
step 151/334, epoch 439/501 --> loss:0.047862392663955686
step 201/334, epoch 439/501 --> loss:0.04881710290908813
step 251/334, epoch 439/501 --> loss:0.10105003356933594
step 301/334, epoch 439/501 --> loss:0.17872517466545104
step 51/334, epoch 440/501 --> loss:0.12425734996795654
step 101/334, epoch 440/501 --> loss:0.115061194896698
step 151/334, epoch 440/501 --> loss:0.1080066478252411
step 201/334, epoch 440/501 --> loss:0.10052077293395996
step 251/334, epoch 440/501 --> loss:0.12266911029815673
step 301/334, epoch 440/501 --> loss:0.12039531826972961
step 51/334, epoch 441/501 --> loss:0.10126592874526978
step 101/334, epoch 441/501 --> loss:0.08991166949272156
step 151/334, epoch 441/501 --> loss:0.10778068900108337
step 201/334, epoch 441/501 --> loss:0.05708866238594055
step 251/334, epoch 441/501 --> loss:0.09563117146492005
step 301/334, epoch 441/501 --> loss:0.07241819858551025

##########train dataset##########
acc--> [99.61777877664298]
F1--> {'F1': [0.9543036380560583], 'precision': [0.9710380191235403], 'recall': [0.9381459303757798]}
##########eval dataset##########
acc--> [99.13137641165163]
F1--> {'F1': [0.8913120456009622], 'precision': [0.9299893270445923], 'recall': [0.8557326074131031]}
step 51/334, epoch 442/501 --> loss:0.052053687572479246
step 101/334, epoch 442/501 --> loss:0.07876202702522278
step 151/334, epoch 442/501 --> loss:0.0627203106880188
step 201/334, epoch 442/501 --> loss:0.10353427290916443
step 251/334, epoch 442/501 --> loss:0.0751000964641571
step 301/334, epoch 442/501 --> loss:0.06774998426437379
step 51/334, epoch 443/501 --> loss:0.047452763319015503
step 101/334, epoch 443/501 --> loss:0.0769547975063324
step 151/334, epoch 443/501 --> loss:0.07413549780845642
step 201/334, epoch 443/501 --> loss:0.10588371872901917
step 251/334, epoch 443/501 --> loss:0.07245437979698181
step 301/334, epoch 443/501 --> loss:0.07187295913696289
step 51/334, epoch 444/501 --> loss:0.08225273847579956
step 101/334, epoch 444/501 --> loss:0.072767254114151
step 151/334, epoch 444/501 --> loss:0.09115220785140991
step 201/334, epoch 444/501 --> loss:0.052589092254638675
step 251/334, epoch 444/501 --> loss:0.0905525279045105
step 301/334, epoch 444/501 --> loss:0.06317958116531372
step 51/334, epoch 445/501 --> loss:0.06282416224479676
step 101/334, epoch 445/501 --> loss:0.09490038156509399
step 151/334, epoch 445/501 --> loss:0.06932567954063415
step 201/334, epoch 445/501 --> loss:0.08235608339309693
step 251/334, epoch 445/501 --> loss:0.09791465878486633
step 301/334, epoch 445/501 --> loss:0.053059300184249876
step 51/334, epoch 446/501 --> loss:0.05025089025497437
step 101/334, epoch 446/501 --> loss:0.040352500677108764
step 151/334, epoch 446/501 --> loss:0.07160462617874146
step 201/334, epoch 446/501 --> loss:0.04643179416656494
step 251/334, epoch 446/501 --> loss:0.0793649661540985
step 301/334, epoch 446/501 --> loss:0.08189718842506409
step 51/334, epoch 447/501 --> loss:0.09979707837104797
step 101/334, epoch 447/501 --> loss:0.16612277507781983
step 151/334, epoch 447/501 --> loss:0.15438271760940553
step 201/334, epoch 447/501 --> loss:0.09768380999565124
step 251/334, epoch 447/501 --> loss:0.08141229391098022
step 301/334, epoch 447/501 --> loss:0.08946784138679505
step 51/334, epoch 448/501 --> loss:0.0927807068824768
step 101/334, epoch 448/501 --> loss:0.09448163509368897
step 151/334, epoch 448/501 --> loss:0.07139266133308411
step 201/334, epoch 448/501 --> loss:0.07634356021881103
step 251/334, epoch 448/501 --> loss:0.08764404773712159
step 301/334, epoch 448/501 --> loss:0.08354856610298157
step 51/334, epoch 449/501 --> loss:0.067619549036026
step 101/334, epoch 449/501 --> loss:0.09770098686218262
step 151/334, epoch 449/501 --> loss:0.10031120896339417
step 201/334, epoch 449/501 --> loss:0.09643537521362305
step 251/334, epoch 449/501 --> loss:0.07204525351524353
step 301/334, epoch 449/501 --> loss:0.06237807035446167
step 51/334, epoch 450/501 --> loss:0.1224308443069458
step 101/334, epoch 450/501 --> loss:0.06879311323165893
step 151/334, epoch 450/501 --> loss:0.08431441783905029
step 201/334, epoch 450/501 --> loss:0.046192667484283446
step 251/334, epoch 450/501 --> loss:0.053721776008605955
step 301/334, epoch 450/501 --> loss:0.08746568441390991
step 51/334, epoch 451/501 --> loss:0.06919928193092346
step 101/334, epoch 451/501 --> loss:0.05764267206192017
step 151/334, epoch 451/501 --> loss:0.05712115406990051
step 201/334, epoch 451/501 --> loss:0.08817067980766297
step 251/334, epoch 451/501 --> loss:0.06928105711936951
step 301/334, epoch 451/501 --> loss:0.03818191170692444

##########train dataset##########
acc--> [99.64816447809461]
F1--> {'F1': [0.9578432597894869], 'precision': [0.9768691140271113], 'recall': [0.9395539738797646]}
##########eval dataset##########
acc--> [99.15180623478531]
F1--> {'F1': [0.8925369923082851], 'precision': [0.9441384259234914], 'recall': [0.8462927222344516]}
step 51/334, epoch 452/501 --> loss:0.2008567225933075
step 101/334, epoch 452/501 --> loss:0.12191957473754883
step 151/334, epoch 452/501 --> loss:0.14816656947135926
step 201/334, epoch 452/501 --> loss:0.12061795711517334
step 251/334, epoch 452/501 --> loss:0.11072576642036439
step 301/334, epoch 452/501 --> loss:0.09873374819755554
step 51/334, epoch 453/501 --> loss:0.11432132601737977
step 101/334, epoch 453/501 --> loss:0.126072655916214
step 151/334, epoch 453/501 --> loss:0.14253334879875182
step 201/334, epoch 453/501 --> loss:0.13982171893119813
step 251/334, epoch 453/501 --> loss:0.09594891905784607
step 301/334, epoch 453/501 --> loss:0.1136819052696228
step 51/334, epoch 454/501 --> loss:0.08467044234275818
step 101/334, epoch 454/501 --> loss:0.12549269080162048
step 151/334, epoch 454/501 --> loss:0.13123388171195985
step 201/334, epoch 454/501 --> loss:0.06756868600845337
step 251/334, epoch 454/501 --> loss:0.11025245547294617
step 301/334, epoch 454/501 --> loss:0.12286765933036804
step 51/334, epoch 455/501 --> loss:0.07553879261016845
step 101/334, epoch 455/501 --> loss:0.10510037660598755
step 151/334, epoch 455/501 --> loss:0.12737949848175048
step 201/334, epoch 455/501 --> loss:0.07053549170494079
step 251/334, epoch 455/501 --> loss:0.10675242185592651
step 301/334, epoch 455/501 --> loss:0.11405659198760987
step 51/334, epoch 456/501 --> loss:0.06840884447097778
step 101/334, epoch 456/501 --> loss:0.05153751492500305
step 151/334, epoch 456/501 --> loss:0.09679171442985535
step 201/334, epoch 456/501 --> loss:0.05824907422065735
step 251/334, epoch 456/501 --> loss:0.061175488233566284
step 301/334, epoch 456/501 --> loss:0.04765434980392456
step 51/334, epoch 457/501 --> loss:0.056469327211380003
step 101/334, epoch 457/501 --> loss:0.07250757336616516
step 151/334, epoch 457/501 --> loss:0.09486782073974609
step 201/334, epoch 457/501 --> loss:0.07180482029914856
step 251/334, epoch 457/501 --> loss:0.09693681955337524
step 301/334, epoch 457/501 --> loss:0.05321136236190796
step 51/334, epoch 458/501 --> loss:0.10528075933456421
step 101/334, epoch 458/501 --> loss:0.07635347485542297
step 151/334, epoch 458/501 --> loss:0.1066083550453186
step 201/334, epoch 458/501 --> loss:0.0775022804737091
step 251/334, epoch 458/501 --> loss:0.07962414026260375
step 301/334, epoch 458/501 --> loss:0.08955544710159302
step 51/334, epoch 459/501 --> loss:0.07949114799499511
step 101/334, epoch 459/501 --> loss:0.09426832675933838
step 151/334, epoch 459/501 --> loss:0.06465881466865539
step 201/334, epoch 459/501 --> loss:0.041911039352416996
step 251/334, epoch 459/501 --> loss:0.08014737129211426
step 301/334, epoch 459/501 --> loss:0.06376257181167602
step 51/334, epoch 460/501 --> loss:0.04505292534828186
step 101/334, epoch 460/501 --> loss:0.04688893675804138
step 151/334, epoch 460/501 --> loss:0.05657110810279846
step 201/334, epoch 460/501 --> loss:0.07705586552619934
step 251/334, epoch 460/501 --> loss:0.08495088696479797
step 301/334, epoch 460/501 --> loss:0.1024971604347229
step 51/334, epoch 461/501 --> loss:0.05345266222953796
step 101/334, epoch 461/501 --> loss:0.08368527889251709
step 151/334, epoch 461/501 --> loss:0.04492332100868225
step 201/334, epoch 461/501 --> loss:0.04245511293411255
step 251/334, epoch 461/501 --> loss:0.07496567249298096
step 301/334, epoch 461/501 --> loss:0.07247225999832153

##########train dataset##########
acc--> [99.71523935713195]
F1--> {'F1': [0.9663971336598433], 'precision': [0.9702768650987951], 'recall': [0.9625582258457118]}
##########eval dataset##########
acc--> [99.17250171837598]
F1--> {'F1': [0.8987432712028414], 'precision': [0.9157729645396644], 'recall': [0.8823450172578672]}
step 51/334, epoch 462/501 --> loss:0.048682186603546146
step 101/334, epoch 462/501 --> loss:0.0661126184463501
step 151/334, epoch 462/501 --> loss:0.06575337648391724
step 201/334, epoch 462/501 --> loss:0.04345171809196472
step 251/334, epoch 462/501 --> loss:0.061371445655822754
step 301/334, epoch 462/501 --> loss:0.10575176000595093
step 51/334, epoch 463/501 --> loss:0.12312655210494995
step 101/334, epoch 463/501 --> loss:0.08593395113945007
step 151/334, epoch 463/501 --> loss:0.11156440734863281
step 201/334, epoch 463/501 --> loss:0.08634690403938293
step 251/334, epoch 463/501 --> loss:0.06053765296936035
step 301/334, epoch 463/501 --> loss:0.07714268207550048
step 51/334, epoch 464/501 --> loss:0.08271915674209594
step 101/334, epoch 464/501 --> loss:0.08884661316871643
step 151/334, epoch 464/501 --> loss:0.06293382167816162
step 201/334, epoch 464/501 --> loss:0.10007899522781372
step 251/334, epoch 464/501 --> loss:0.08869387030601501
step 301/334, epoch 464/501 --> loss:0.05572261691093445
step 51/334, epoch 465/501 --> loss:0.08984429717063903
step 101/334, epoch 465/501 --> loss:0.0914187216758728
step 151/334, epoch 465/501 --> loss:0.04678374886512757
step 201/334, epoch 465/501 --> loss:0.04209640979766846
step 251/334, epoch 465/501 --> loss:0.08981421947479248
step 301/334, epoch 465/501 --> loss:0.06108397483825684
step 51/334, epoch 466/501 --> loss:0.1114002001285553
step 101/334, epoch 466/501 --> loss:0.07342449545860291
step 151/334, epoch 466/501 --> loss:0.05781105637550354
step 201/334, epoch 466/501 --> loss:0.08853209137916565
step 251/334, epoch 466/501 --> loss:0.049583334922790524
step 301/334, epoch 466/501 --> loss:0.06940298676490783
step 51/334, epoch 467/501 --> loss:0.05596375584602356
step 101/334, epoch 467/501 --> loss:0.07700793147087097
step 151/334, epoch 467/501 --> loss:0.08306757092475892
step 201/334, epoch 467/501 --> loss:0.055951240062713625
step 251/334, epoch 467/501 --> loss:0.0830892276763916
step 301/334, epoch 467/501 --> loss:0.040001556873321534
step 51/334, epoch 468/501 --> loss:0.06516724705696106
step 101/334, epoch 468/501 --> loss:0.05234212636947632
step 151/334, epoch 468/501 --> loss:0.05648642659187317
step 201/334, epoch 468/501 --> loss:0.08210366487503051
step 251/334, epoch 468/501 --> loss:0.08375522255897522
step 301/334, epoch 468/501 --> loss:0.18890273571014404
step 51/334, epoch 469/501 --> loss:0.09508996844291687
step 101/334, epoch 469/501 --> loss:0.12263024210929871
step 151/334, epoch 469/501 --> loss:0.07159268140792846
step 201/334, epoch 469/501 --> loss:0.0709534502029419
step 251/334, epoch 469/501 --> loss:0.05596218585968018
step 301/334, epoch 469/501 --> loss:0.10036831498146057
step 51/334, epoch 470/501 --> loss:0.09054963111877441
step 101/334, epoch 470/501 --> loss:0.06214314579963684
step 151/334, epoch 470/501 --> loss:0.06692547678947448
step 201/334, epoch 470/501 --> loss:0.04487745881080627
step 251/334, epoch 470/501 --> loss:0.15581629991531373
step 301/334, epoch 470/501 --> loss:0.13593136310577392
step 51/334, epoch 471/501 --> loss:0.10308227896690368
step 101/334, epoch 471/501 --> loss:0.0643878674507141
step 151/334, epoch 471/501 --> loss:0.10866295456886292
step 201/334, epoch 471/501 --> loss:0.08855676412582397
step 251/334, epoch 471/501 --> loss:0.07714332103729248
step 301/334, epoch 471/501 --> loss:0.07098509430885315

##########train dataset##########
acc--> [99.68953656488453]
F1--> {'F1': [0.9631054053189423], 'precision': [0.9739173911046187], 'recall': [0.9525406236210341]}
##########eval dataset##########
acc--> [99.15093048976125]
F1--> {'F1': [0.8956485204800702], 'precision': [0.9167898041322184], 'recall': [0.8754698493757785]}
step 51/334, epoch 472/501 --> loss:0.06764387011528016
step 101/334, epoch 472/501 --> loss:0.06045671224594116
step 151/334, epoch 472/501 --> loss:0.10226013064384461
step 201/334, epoch 472/501 --> loss:0.05515644669532776
step 251/334, epoch 472/501 --> loss:0.05996513247489929
step 301/334, epoch 472/501 --> loss:0.0615589439868927
step 51/334, epoch 473/501 --> loss:0.0831137478351593
step 101/334, epoch 473/501 --> loss:0.041605452299118044
step 151/334, epoch 473/501 --> loss:0.06540892839431763
step 201/334, epoch 473/501 --> loss:0.045021302700042724
step 251/334, epoch 473/501 --> loss:0.05471194982528686
step 301/334, epoch 473/501 --> loss:0.05785255193710327
step 51/334, epoch 474/501 --> loss:0.04885516285896301
step 101/334, epoch 474/501 --> loss:0.07070035576820373
step 151/334, epoch 474/501 --> loss:0.10473267197608947
step 201/334, epoch 474/501 --> loss:0.037436496019363406
step 251/334, epoch 474/501 --> loss:0.06191075325012207
step 301/334, epoch 474/501 --> loss:0.06649497628211976
step 51/334, epoch 475/501 --> loss:0.07360591650009156
step 101/334, epoch 475/501 --> loss:0.051817370653152464
step 151/334, epoch 475/501 --> loss:0.0737743353843689
step 201/334, epoch 475/501 --> loss:0.05719019889831543
step 251/334, epoch 475/501 --> loss:0.04114444732666016
step 301/334, epoch 475/501 --> loss:0.04662333369255066
step 51/334, epoch 476/501 --> loss:0.06687957525253296
step 101/334, epoch 476/501 --> loss:0.08291587471961975
step 151/334, epoch 476/501 --> loss:0.07506142497062683
step 201/334, epoch 476/501 --> loss:0.0621572995185852
step 251/334, epoch 476/501 --> loss:0.04498520851135254
step 301/334, epoch 476/501 --> loss:0.06538290977478027
step 51/334, epoch 477/501 --> loss:0.07986894488334656
step 101/334, epoch 477/501 --> loss:0.049419292211532594
step 151/334, epoch 477/501 --> loss:0.10296218037605286
step 201/334, epoch 477/501 --> loss:0.03456040382385254
step 251/334, epoch 477/501 --> loss:0.03942214488983154
step 301/334, epoch 477/501 --> loss:0.05231485605239868
step 51/334, epoch 478/501 --> loss:0.05179970741271973
step 101/334, epoch 478/501 --> loss:0.03792464137077332
step 151/334, epoch 478/501 --> loss:0.0745239508152008
step 201/334, epoch 478/501 --> loss:0.10008166432380676
step 251/334, epoch 478/501 --> loss:0.06131255984306336
step 301/334, epoch 478/501 --> loss:0.04788862943649292
step 51/334, epoch 479/501 --> loss:0.0647823977470398
step 101/334, epoch 479/501 --> loss:0.05340816378593445
step 151/334, epoch 479/501 --> loss:0.07899010300636292
step 201/334, epoch 479/501 --> loss:0.05940346598625183
step 251/334, epoch 479/501 --> loss:0.10344166874885559
step 301/334, epoch 479/501 --> loss:0.06817982792854309
step 51/334, epoch 480/501 --> loss:0.07500975966453552
step 101/334, epoch 480/501 --> loss:0.0851193380355835
step 151/334, epoch 480/501 --> loss:0.04756322503089905
step 201/334, epoch 480/501 --> loss:0.08113191485404968
step 251/334, epoch 480/501 --> loss:0.21072407960891723
step 301/334, epoch 480/501 --> loss:0.13065112233161927
step 51/334, epoch 481/501 --> loss:0.06928010940551758
step 101/334, epoch 481/501 --> loss:0.07651233196258544
step 151/334, epoch 481/501 --> loss:0.08464009523391723
step 201/334, epoch 481/501 --> loss:0.11851663827896118
step 251/334, epoch 481/501 --> loss:0.12015760898590087
step 301/334, epoch 481/501 --> loss:0.08123815536499024

##########train dataset##########
acc--> [99.6190692227896]
F1--> {'F1': [0.9549249560350733], 'precision': [0.9614631131540264], 'recall': [0.9484849849924235]}
##########eval dataset##########
acc--> [99.13238632188136]
F1--> {'F1': [0.8937997316982955], 'precision': [0.9110481135326929], 'recall': [0.8772019516405304]}
step 51/334, epoch 482/501 --> loss:0.0942866861820221
step 101/334, epoch 482/501 --> loss:0.09506527066230774
step 151/334, epoch 482/501 --> loss:0.044352734088897706
step 201/334, epoch 482/501 --> loss:0.06833256244659423
step 251/334, epoch 482/501 --> loss:0.09967117190361023
step 301/334, epoch 482/501 --> loss:0.07636270046234131
step 51/334, epoch 483/501 --> loss:0.05681020140647888
step 101/334, epoch 483/501 --> loss:0.06859691977500916
step 151/334, epoch 483/501 --> loss:0.06445598006248474
step 201/334, epoch 483/501 --> loss:0.07211541056632996
step 251/334, epoch 483/501 --> loss:0.06334286570549011
step 301/334, epoch 483/501 --> loss:0.05700994729995727
step 51/334, epoch 484/501 --> loss:0.05511115312576294
step 101/334, epoch 484/501 --> loss:0.06605324268341065
step 151/334, epoch 484/501 --> loss:0.061864478588104246
step 201/334, epoch 484/501 --> loss:0.049912807941436765
step 251/334, epoch 484/501 --> loss:0.24177563428878784
step 301/334, epoch 484/501 --> loss:0.19176520109176637
step 51/334, epoch 485/501 --> loss:0.10411185145378113
step 101/334, epoch 485/501 --> loss:0.14140655040740968
step 151/334, epoch 485/501 --> loss:0.12784650564193725
step 201/334, epoch 485/501 --> loss:0.14227724075317383
step 251/334, epoch 485/501 --> loss:0.1154993462562561
step 301/334, epoch 485/501 --> loss:0.06559170007705689
step 51/334, epoch 486/501 --> loss:0.10756662368774414
step 101/334, epoch 486/501 --> loss:0.07058995485305786
step 151/334, epoch 486/501 --> loss:0.11061933398246765
step 201/334, epoch 486/501 --> loss:0.08114740490913391
step 251/334, epoch 486/501 --> loss:0.05209317564964294
step 301/334, epoch 486/501 --> loss:0.0636790418624878
step 51/334, epoch 487/501 --> loss:0.06791680455207824
step 101/334, epoch 487/501 --> loss:0.07356922864913941
step 151/334, epoch 487/501 --> loss:0.08253337502479553
step 201/334, epoch 487/501 --> loss:0.056607471704483034
step 251/334, epoch 487/501 --> loss:0.09387945890426636
step 301/334, epoch 487/501 --> loss:0.09637435078620911
step 51/334, epoch 488/501 --> loss:0.10296538949012757
step 101/334, epoch 488/501 --> loss:0.07096932649612427
step 151/334, epoch 488/501 --> loss:0.08682549953460693
step 201/334, epoch 488/501 --> loss:0.05238610029220581
step 251/334, epoch 488/501 --> loss:0.10320716619491577
step 301/334, epoch 488/501 --> loss:0.07189205646514893
step 51/334, epoch 489/501 --> loss:0.04574284553527832
step 101/334, epoch 489/501 --> loss:0.03772518396377564
step 151/334, epoch 489/501 --> loss:0.06826608777046203
step 201/334, epoch 489/501 --> loss:0.07033576250076294
step 251/334, epoch 489/501 --> loss:0.051234980821609495
step 301/334, epoch 489/501 --> loss:0.08655696630477905
step 51/334, epoch 490/501 --> loss:0.08699448943138123
step 101/334, epoch 490/501 --> loss:0.06653580188751221
step 151/334, epoch 490/501 --> loss:0.047216634750366214
step 201/334, epoch 490/501 --> loss:0.09738922357559204
step 251/334, epoch 490/501 --> loss:0.08014378547668458
step 301/334, epoch 490/501 --> loss:0.09796728849411011
step 51/334, epoch 491/501 --> loss:0.087471022605896
step 101/334, epoch 491/501 --> loss:0.07433617115020752
step 151/334, epoch 491/501 --> loss:0.09214152693748474
step 201/334, epoch 491/501 --> loss:0.10120917677879333
step 251/334, epoch 491/501 --> loss:0.07589237570762634
step 301/334, epoch 491/501 --> loss:0.04548889994621277

##########train dataset##########
acc--> [99.68384984820338]
F1--> {'F1': [0.9622339061003835], 'precision': [0.9782493808749404], 'recall': [0.9467440590804439]}
##########eval dataset##########
acc--> [99.2066697915746]
F1--> {'F1': [0.9003819868315541], 'precision': [0.9430752724364673], 'recall': [0.8613958970603312]}
step 51/334, epoch 492/501 --> loss:0.09978176355361938
step 101/334, epoch 492/501 --> loss:0.044855471849441525
step 151/334, epoch 492/501 --> loss:0.04318182349205017
step 201/334, epoch 492/501 --> loss:0.084080970287323
step 251/334, epoch 492/501 --> loss:0.04924250841140747
step 301/334, epoch 492/501 --> loss:0.0693432879447937
step 51/334, epoch 493/501 --> loss:0.05193978548049927
step 101/334, epoch 493/501 --> loss:0.04420137882232666
step 151/334, epoch 493/501 --> loss:0.06474195957183838
step 201/334, epoch 493/501 --> loss:0.07905534744262695
step 251/334, epoch 493/501 --> loss:0.08013718962669372
step 301/334, epoch 493/501 --> loss:0.04383474946022034
step 51/334, epoch 494/501 --> loss:0.058349512815475464
step 101/334, epoch 494/501 --> loss:0.06253786087036133
step 151/334, epoch 494/501 --> loss:0.10786830425262452
step 201/334, epoch 494/501 --> loss:0.1663423204421997
step 251/334, epoch 494/501 --> loss:0.14267558813095094
step 301/334, epoch 494/501 --> loss:0.11189371347427368
step 51/334, epoch 495/501 --> loss:0.09315274000167846
step 101/334, epoch 495/501 --> loss:0.1266966664791107
step 151/334, epoch 495/501 --> loss:0.11114587426185608
step 201/334, epoch 495/501 --> loss:0.09585243940353394
step 251/334, epoch 495/501 --> loss:0.06517646074295044
step 301/334, epoch 495/501 --> loss:0.07594231605529785
step 51/334, epoch 496/501 --> loss:0.21401856184005738
step 101/334, epoch 496/501 --> loss:0.139555207490921
step 151/334, epoch 496/501 --> loss:0.14910318374633788
step 201/334, epoch 496/501 --> loss:0.11513567328453064
step 251/334, epoch 496/501 --> loss:0.08847342252731323
step 301/334, epoch 496/501 --> loss:0.09601945400238038
step 51/334, epoch 497/501 --> loss:0.08893455386161804
step 101/334, epoch 497/501 --> loss:0.15843016505241395
step 151/334, epoch 497/501 --> loss:0.06048494577407837
step 201/334, epoch 497/501 --> loss:0.0912651264667511
step 251/334, epoch 497/501 --> loss:0.1252739715576172
step 301/334, epoch 497/501 --> loss:0.09650382399559021
step 51/334, epoch 498/501 --> loss:0.08657093524932861
step 101/334, epoch 498/501 --> loss:0.06887991428375244
step 151/334, epoch 498/501 --> loss:0.06977340459823608
step 201/334, epoch 498/501 --> loss:0.07498276948928834
step 251/334, epoch 498/501 --> loss:0.0498179292678833
step 301/334, epoch 498/501 --> loss:0.06092897057533264
step 51/334, epoch 499/501 --> loss:0.075063054561615
step 101/334, epoch 499/501 --> loss:0.06674330830574035
step 151/334, epoch 499/501 --> loss:0.08661346316337586
step 201/334, epoch 499/501 --> loss:0.0736015808582306
step 251/334, epoch 499/501 --> loss:0.059862284660339354
step 301/334, epoch 499/501 --> loss:0.09941635012626648
step 51/334, epoch 500/501 --> loss:0.04846571803092956
step 101/334, epoch 500/501 --> loss:0.08206094145774841
step 151/334, epoch 500/501 --> loss:0.06864092469215394
step 201/334, epoch 500/501 --> loss:0.08087510108947754
step 251/334, epoch 500/501 --> loss:0.11281834721565247
step 301/334, epoch 500/501 --> loss:0.05977251172065735
step 51/334, epoch 501/501 --> loss:0.08347583293914795
step 101/334, epoch 501/501 --> loss:0.07216814279556275
step 151/334, epoch 501/501 --> loss:0.044973485469818116
step 201/334, epoch 501/501 --> loss:0.043744678497314456
step 251/334, epoch 501/501 --> loss:0.05155882954597473
step 301/334, epoch 501/501 --> loss:0.06483558654785156

##########train dataset##########
acc--> [99.70327462423845]
F1--> {'F1': [0.9649534601477637], 'precision': [0.9697208290086374], 'recall': [0.9602426390690317]}
##########eval dataset##########
acc--> [99.22791528575965]
F1--> {'F1': [0.9051818519582274], 'precision': [0.92580960492935], 'recall': [0.8854628334074186]}
