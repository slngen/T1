##########Config##########
{'device': 'cuda:0', 'class_nums': 2, 'data_path': '/Code/T1/Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': '/Code/T1/Models/DeepLabv3', 'log_path': '/Code/T1/Logs/DeepLabv3', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (model): DeepLabV3(
    (backbone): IntermediateLayerGetter(
      (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (classifier): DeepLabHead(
      (0): ASPP(
        (convs): ModuleList(
          (0): Sequential(
            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (3): ASPPConv(
            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (4): ASPPPooling(
            (0): AdaptiveAvgPool2d(output_size=1)
            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
          )
        )
        (project): Sequential(
          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.5, inplace=False)
        )
      )
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.6979922068119049
step 101/334, epoch 1/501 --> loss:0.488701376914978
step 151/334, epoch 1/501 --> loss:0.42728929162025453
step 201/334, epoch 1/501 --> loss:0.41115875482559205
step 251/334, epoch 1/501 --> loss:0.4067551600933075
step 301/334, epoch 1/501 --> loss:0.3298662936687469

##########train dataset##########
acc--> [97.97010160716934]
F1--> {'F1': [0.709684016349592], 'precision': [0.8941075383645862], 'recall': [0.5883381100696982]}
##########eval dataset##########
acc--> [98.0262015435261]
F1--> {'F1': [0.7304877536812409], 'precision': [0.8911816820843882], 'recall': [0.6188988540607917]}
save model!
step 51/334, epoch 2/501 --> loss:0.3535034191608429
step 101/334, epoch 2/501 --> loss:0.3182259500026703
step 151/334, epoch 2/501 --> loss:0.28819605350494387
step 201/334, epoch 2/501 --> loss:0.3120278799533844
step 251/334, epoch 2/501 --> loss:0.2891476821899414
step 301/334, epoch 2/501 --> loss:0.29869584560394286
step 51/334, epoch 3/501 --> loss:0.2798590362071991
step 101/334, epoch 3/501 --> loss:0.31440826654434206
step 151/334, epoch 3/501 --> loss:0.30442824840545657
step 201/334, epoch 3/501 --> loss:0.3013198435306549
step 251/334, epoch 3/501 --> loss:0.2815290772914886
step 301/334, epoch 3/501 --> loss:0.29359184741973876
step 51/334, epoch 4/501 --> loss:0.26578028440475465
step 101/334, epoch 4/501 --> loss:0.24775445938110352
step 151/334, epoch 4/501 --> loss:0.25423442363739013
step 201/334, epoch 4/501 --> loss:0.271638925075531
step 251/334, epoch 4/501 --> loss:0.26567575335502625
step 301/334, epoch 4/501 --> loss:0.28682058334350585
step 51/334, epoch 5/501 --> loss:0.27609007358551024
step 101/334, epoch 5/501 --> loss:0.2737263906002045
step 151/334, epoch 5/501 --> loss:0.26102460741996764
step 201/334, epoch 5/501 --> loss:0.2388942801952362
step 251/334, epoch 5/501 --> loss:0.22502811670303344
step 301/334, epoch 5/501 --> loss:0.2326757490634918
step 51/334, epoch 6/501 --> loss:0.2213628327846527
step 101/334, epoch 6/501 --> loss:0.24053889036178588
step 151/334, epoch 6/501 --> loss:0.24109315037727355
step 201/334, epoch 6/501 --> loss:0.207826828956604
step 251/334, epoch 6/501 --> loss:0.3046989727020264
step 301/334, epoch 6/501 --> loss:0.24848365426063537
step 51/334, epoch 7/501 --> loss:0.25894328832626345
step 101/334, epoch 7/501 --> loss:0.26683871388435365
step 151/334, epoch 7/501 --> loss:0.25800405740737914
step 201/334, epoch 7/501 --> loss:0.25602972984313965
step 251/334, epoch 7/501 --> loss:0.21064809322357178
step 301/334, epoch 7/501 --> loss:0.2064330005645752
step 51/334, epoch 8/501 --> loss:0.26486549973487855
step 101/334, epoch 8/501 --> loss:0.21894689321517943
step 151/334, epoch 8/501 --> loss:0.273665144443512
step 201/334, epoch 8/501 --> loss:0.22986984372138977
step 251/334, epoch 8/501 --> loss:0.27689822793006896
step 301/334, epoch 8/501 --> loss:0.23194117546081544
step 51/334, epoch 9/501 --> loss:0.18755125999450684
step 101/334, epoch 9/501 --> loss:0.22844486713409423
step 151/334, epoch 9/501 --> loss:0.20497783660888672
step 201/334, epoch 9/501 --> loss:0.18582835793495178
step 251/334, epoch 9/501 --> loss:0.24804848194122314
step 301/334, epoch 9/501 --> loss:0.2264004862308502
step 51/334, epoch 10/501 --> loss:0.19676513195037842
step 101/334, epoch 10/501 --> loss:0.18132087111473083
step 151/334, epoch 10/501 --> loss:0.19795628905296325
step 201/334, epoch 10/501 --> loss:0.22502865314483642
step 251/334, epoch 10/501 --> loss:0.2196277606487274
step 301/334, epoch 10/501 --> loss:0.2135537111759186
step 51/334, epoch 11/501 --> loss:0.21860247254371643
step 101/334, epoch 11/501 --> loss:0.2231529998779297
step 151/334, epoch 11/501 --> loss:0.22233901143074036
step 201/334, epoch 11/501 --> loss:0.20962093472480775
step 251/334, epoch 11/501 --> loss:0.219460711479187
step 301/334, epoch 11/501 --> loss:0.23031002879142762

##########train dataset##########
acc--> [98.54717304700033]
F1--> {'F1': [0.8232019288368964], 'precision': [0.8455005730627573], 'recall': [0.8020587259480815]}
##########eval dataset##########
acc--> [98.50313482008548]
F1--> {'F1': [0.8214343284740058], 'precision': [0.84786963508883], 'recall': [0.7966070022649483]}
save model!
step 51/334, epoch 12/501 --> loss:0.20470136046409607
step 101/334, epoch 12/501 --> loss:0.22885586619377135
step 151/334, epoch 12/501 --> loss:0.2359262490272522
step 201/334, epoch 12/501 --> loss:0.2279597818851471
step 251/334, epoch 12/501 --> loss:0.22383150696754456
step 301/334, epoch 12/501 --> loss:0.20229201793670654
step 51/334, epoch 13/501 --> loss:0.20454467654228212
step 101/334, epoch 13/501 --> loss:0.20120410680770873
step 151/334, epoch 13/501 --> loss:0.1779392147064209
step 201/334, epoch 13/501 --> loss:0.1782487690448761
step 251/334, epoch 13/501 --> loss:0.24446132898330689
step 301/334, epoch 13/501 --> loss:0.21893827199935914
step 51/334, epoch 14/501 --> loss:0.20964863896369934
step 101/334, epoch 14/501 --> loss:0.21302428603172302
step 151/334, epoch 14/501 --> loss:0.21768877744674683
step 201/334, epoch 14/501 --> loss:0.22514140248298645
step 251/334, epoch 14/501 --> loss:0.1709692060947418
step 301/334, epoch 14/501 --> loss:0.1864437186717987
step 51/334, epoch 15/501 --> loss:0.21188650488853455
step 101/334, epoch 15/501 --> loss:0.21371073126792908
step 151/334, epoch 15/501 --> loss:0.23681746363639833
step 201/334, epoch 15/501 --> loss:0.18208285570144653
step 251/334, epoch 15/501 --> loss:0.2064342951774597
step 301/334, epoch 15/501 --> loss:0.16659414410591125
step 51/334, epoch 16/501 --> loss:0.1800869297981262
step 101/334, epoch 16/501 --> loss:0.19707876682281494
step 151/334, epoch 16/501 --> loss:0.17400354027748108
step 201/334, epoch 16/501 --> loss:0.18544149160385132
step 251/334, epoch 16/501 --> loss:0.19920599579811096
step 301/334, epoch 16/501 --> loss:0.1812833285331726
step 51/334, epoch 17/501 --> loss:0.2191094958782196
step 101/334, epoch 17/501 --> loss:0.2711599826812744
step 151/334, epoch 17/501 --> loss:0.19189964532852172
step 201/334, epoch 17/501 --> loss:0.21011607170104982
step 251/334, epoch 17/501 --> loss:0.1877372670173645
step 301/334, epoch 17/501 --> loss:0.16750091910362244
step 51/334, epoch 18/501 --> loss:0.18552884936332703
step 101/334, epoch 18/501 --> loss:0.206963392496109
step 151/334, epoch 18/501 --> loss:0.20154783248901367
step 201/334, epoch 18/501 --> loss:0.16079625010490417
step 251/334, epoch 18/501 --> loss:0.19035745024681092
step 301/334, epoch 18/501 --> loss:0.16107142329216004
step 51/334, epoch 19/501 --> loss:0.1482649576663971
step 101/334, epoch 19/501 --> loss:0.17498687386512757
step 151/334, epoch 19/501 --> loss:0.16740769267082214
step 201/334, epoch 19/501 --> loss:0.18623690843582152
step 251/334, epoch 19/501 --> loss:0.17989813566207885
step 301/334, epoch 19/501 --> loss:0.1515100109577179
step 51/334, epoch 20/501 --> loss:0.15594864010810852
step 101/334, epoch 20/501 --> loss:0.16629849910736083
step 151/334, epoch 20/501 --> loss:0.13871240973472596
step 201/334, epoch 20/501 --> loss:0.1566749370098114
step 251/334, epoch 20/501 --> loss:0.16579783082008362
step 301/334, epoch 20/501 --> loss:0.18091647148132325
step 51/334, epoch 21/501 --> loss:0.1740228021144867
step 101/334, epoch 21/501 --> loss:0.18716997027397156
step 151/334, epoch 21/501 --> loss:0.1697177767753601
step 201/334, epoch 21/501 --> loss:0.16588210821151733
step 251/334, epoch 21/501 --> loss:0.3030942106246948
step 301/334, epoch 21/501 --> loss:0.2182491171360016

##########train dataset##########
acc--> [87.48111760611582]
F1--> {'F1': [0.3206200665248432], 'precision': [0.20788885842306823], 'recall': [0.7004854112410492]}
##########eval dataset##########
acc--> [87.81419355203303]
F1--> {'F1': [0.3352697807625681], 'precision': [0.21935348254656947], 'recall': [0.7110203857218228]}
step 51/334, epoch 22/501 --> loss:0.20768486261367797
step 101/334, epoch 22/501 --> loss:0.18153472423553466
step 151/334, epoch 22/501 --> loss:0.16640644669532775
step 201/334, epoch 22/501 --> loss:0.16884418249130248
step 251/334, epoch 22/501 --> loss:0.1871372652053833
step 301/334, epoch 22/501 --> loss:0.18502750635147094
step 51/334, epoch 23/501 --> loss:0.15606396198272704
step 101/334, epoch 23/501 --> loss:0.18264616131782532
step 151/334, epoch 23/501 --> loss:0.1867653739452362
step 201/334, epoch 23/501 --> loss:0.17607710361480713
step 251/334, epoch 23/501 --> loss:0.16526073336601257
step 301/334, epoch 23/501 --> loss:0.18501198530197144
step 51/334, epoch 24/501 --> loss:0.18481996893882752
step 101/334, epoch 24/501 --> loss:0.23484549522399903
step 151/334, epoch 24/501 --> loss:0.2015658450126648
step 201/334, epoch 24/501 --> loss:0.17753683447837829
step 251/334, epoch 24/501 --> loss:0.1762639617919922
step 301/334, epoch 24/501 --> loss:0.18459343791007995
step 51/334, epoch 25/501 --> loss:0.21546494603157043
step 101/334, epoch 25/501 --> loss:0.1946023404598236
step 151/334, epoch 25/501 --> loss:0.18166227221488954
step 201/334, epoch 25/501 --> loss:0.17040932416915894
step 251/334, epoch 25/501 --> loss:0.1439652419090271
step 301/334, epoch 25/501 --> loss:0.16629366397857667
step 51/334, epoch 26/501 --> loss:0.1556309485435486
step 101/334, epoch 26/501 --> loss:0.19812775850296022
step 151/334, epoch 26/501 --> loss:0.1664619517326355
step 201/334, epoch 26/501 --> loss:0.14791044354438782
step 251/334, epoch 26/501 --> loss:0.18957321524620055
step 301/334, epoch 26/501 --> loss:0.14012624740600585
step 51/334, epoch 27/501 --> loss:0.14274810791015624
step 101/334, epoch 27/501 --> loss:0.1657822871208191
step 151/334, epoch 27/501 --> loss:0.19480248808860778
step 201/334, epoch 27/501 --> loss:0.18042697429656981
step 251/334, epoch 27/501 --> loss:0.18318192720413207
step 301/334, epoch 27/501 --> loss:0.16497833967208864
step 51/334, epoch 28/501 --> loss:0.1470233428478241
step 101/334, epoch 28/501 --> loss:0.1687026357650757
step 151/334, epoch 28/501 --> loss:0.15087960362434388
step 201/334, epoch 28/501 --> loss:0.1610830819606781
step 251/334, epoch 28/501 --> loss:0.1511649465560913
step 301/334, epoch 28/501 --> loss:0.14615830421447754
step 51/334, epoch 29/501 --> loss:0.14880672216415405
step 101/334, epoch 29/501 --> loss:0.14674684286117554
step 151/334, epoch 29/501 --> loss:0.13903249979019164
step 201/334, epoch 29/501 --> loss:0.1710727608203888
step 251/334, epoch 29/501 --> loss:0.18839764833450318
step 301/334, epoch 29/501 --> loss:0.18007917761802672
step 51/334, epoch 30/501 --> loss:0.1563914978504181
step 101/334, epoch 30/501 --> loss:0.12668874144554138
step 151/334, epoch 30/501 --> loss:0.15344531774520875
step 201/334, epoch 30/501 --> loss:0.12722469806671144
step 251/334, epoch 30/501 --> loss:0.15629477500915528
step 301/334, epoch 30/501 --> loss:0.18788254976272584
step 51/334, epoch 31/501 --> loss:0.19533852338790894
step 101/334, epoch 31/501 --> loss:0.17161370754241945
step 151/334, epoch 31/501 --> loss:0.2403928208351135
step 201/334, epoch 31/501 --> loss:0.1827823305130005
step 251/334, epoch 31/501 --> loss:0.20882916808128357
step 301/334, epoch 31/501 --> loss:0.17560457229614257

##########train dataset##########
acc--> [98.87060302001761]
F1--> {'F1': [0.8594569681719878], 'precision': [0.9042584261079957], 'recall': [0.8188943742989342]}
##########eval dataset##########
acc--> [98.7350624064382]
F1--> {'F1': [0.8450940667438211], 'precision': [0.8976611335078976], 'recall': [0.7983519628207139]}
save model!
step 51/334, epoch 32/501 --> loss:0.16215793848037718
step 101/334, epoch 32/501 --> loss:0.16003082394599916
step 151/334, epoch 32/501 --> loss:0.17279252529144287
step 201/334, epoch 32/501 --> loss:0.13983263492584228
step 251/334, epoch 32/501 --> loss:0.16233733177185058
step 301/334, epoch 32/501 --> loss:0.1488801097869873
step 51/334, epoch 33/501 --> loss:0.15016533613204955
step 101/334, epoch 33/501 --> loss:0.17950940608978272
step 151/334, epoch 33/501 --> loss:0.15179697871208192
step 201/334, epoch 33/501 --> loss:0.14715755581855774
step 251/334, epoch 33/501 --> loss:0.1545854938030243
step 301/334, epoch 33/501 --> loss:0.13865696310997008
step 51/334, epoch 34/501 --> loss:0.15695230960845946
step 101/334, epoch 34/501 --> loss:0.189446884393692
step 151/334, epoch 34/501 --> loss:0.18968995928764343
step 201/334, epoch 34/501 --> loss:0.1547512412071228
step 251/334, epoch 34/501 --> loss:0.137169052362442
step 301/334, epoch 34/501 --> loss:0.13110218644142152
step 51/334, epoch 35/501 --> loss:0.11967198729515076
step 101/334, epoch 35/501 --> loss:0.14976787090301513
step 151/334, epoch 35/501 --> loss:0.16983518362045288
step 201/334, epoch 35/501 --> loss:0.16326144218444824
step 251/334, epoch 35/501 --> loss:0.15477319478988646
step 301/334, epoch 35/501 --> loss:0.12512324929237365
step 51/334, epoch 36/501 --> loss:0.17425036191940307
step 101/334, epoch 36/501 --> loss:0.14785913705825807
step 151/334, epoch 36/501 --> loss:0.14359720349311828
step 201/334, epoch 36/501 --> loss:0.15811361670494078
step 251/334, epoch 36/501 --> loss:0.1369238328933716
step 301/334, epoch 36/501 --> loss:0.14812909960746765
step 51/334, epoch 37/501 --> loss:0.17348593950271607
step 101/334, epoch 37/501 --> loss:0.15034944772720338
step 151/334, epoch 37/501 --> loss:0.13601989388465882
step 201/334, epoch 37/501 --> loss:0.15352463722229004
step 251/334, epoch 37/501 --> loss:0.15506582260131835
step 301/334, epoch 37/501 --> loss:0.15080586433410645
step 51/334, epoch 38/501 --> loss:0.12665272355079651
step 101/334, epoch 38/501 --> loss:0.14216641426086427
step 151/334, epoch 38/501 --> loss:0.13878812074661254
step 201/334, epoch 38/501 --> loss:0.1683844482898712
step 251/334, epoch 38/501 --> loss:0.16315164804458618
step 301/334, epoch 38/501 --> loss:0.14314351558685304
step 51/334, epoch 39/501 --> loss:0.1180290150642395
step 101/334, epoch 39/501 --> loss:0.16545837998390198
step 151/334, epoch 39/501 --> loss:0.12269762516021729
step 201/334, epoch 39/501 --> loss:0.15715821743011474
step 251/334, epoch 39/501 --> loss:0.17659247159957886
step 301/334, epoch 39/501 --> loss:0.13735488772392274
step 51/334, epoch 40/501 --> loss:0.11348363280296325
step 101/334, epoch 40/501 --> loss:0.1715645170211792
step 151/334, epoch 40/501 --> loss:0.1572985279560089
step 201/334, epoch 40/501 --> loss:0.15641245007514953
step 251/334, epoch 40/501 --> loss:0.1845654821395874
step 301/334, epoch 40/501 --> loss:0.1959824538230896
step 51/334, epoch 41/501 --> loss:0.2071761155128479
step 101/334, epoch 41/501 --> loss:0.158725608587265
step 151/334, epoch 41/501 --> loss:0.14881505846977233
step 201/334, epoch 41/501 --> loss:0.17617844581604003
step 251/334, epoch 41/501 --> loss:0.1337135088443756
step 301/334, epoch 41/501 --> loss:0.12969481229782104

##########train dataset##########
acc--> [99.0909609313639]
F1--> {'F1': [0.887675982501195], 'precision': [0.9267247086149071], 'recall': [0.8517941303892428]}
##########eval dataset##########
acc--> [98.87100847165172]
F1--> {'F1': [0.8611830889466205], 'precision': [0.9189253660642709], 'recall': [0.8102772756055313]}
save model!
step 51/334, epoch 42/501 --> loss:0.16079695343971254
step 101/334, epoch 42/501 --> loss:0.15518109679222106
step 151/334, epoch 42/501 --> loss:0.1510133445262909
step 201/334, epoch 42/501 --> loss:0.12863682866096496
step 251/334, epoch 42/501 --> loss:0.15910419464111328
step 301/334, epoch 42/501 --> loss:0.15594658613204956
step 51/334, epoch 43/501 --> loss:0.1262058162689209
step 101/334, epoch 43/501 --> loss:0.15025424122810363
step 151/334, epoch 43/501 --> loss:0.14595097541809082
step 201/334, epoch 43/501 --> loss:0.12502753257751464
step 251/334, epoch 43/501 --> loss:0.12768150448799134
step 301/334, epoch 43/501 --> loss:0.13257224082946778
step 51/334, epoch 44/501 --> loss:0.13614325404167174
step 101/334, epoch 44/501 --> loss:0.12016087412834167
step 151/334, epoch 44/501 --> loss:0.13906843543052674
step 201/334, epoch 44/501 --> loss:0.13386589884757996
step 251/334, epoch 44/501 --> loss:0.12921641826629637
step 301/334, epoch 44/501 --> loss:0.1295414686203003
step 51/334, epoch 45/501 --> loss:0.14918930530548097
step 101/334, epoch 45/501 --> loss:0.14692318201065063
step 151/334, epoch 45/501 --> loss:0.1640162932872772
step 201/334, epoch 45/501 --> loss:0.14673584818840027
step 251/334, epoch 45/501 --> loss:0.15295676231384278
step 301/334, epoch 45/501 --> loss:0.1288555705547333
step 51/334, epoch 46/501 --> loss:0.1317969274520874
step 101/334, epoch 46/501 --> loss:0.15584467172622682
step 151/334, epoch 46/501 --> loss:0.16578715205192565
step 201/334, epoch 46/501 --> loss:0.1226089346408844
step 251/334, epoch 46/501 --> loss:0.14422785520553588
step 301/334, epoch 46/501 --> loss:0.1555572509765625
step 51/334, epoch 47/501 --> loss:0.1757776606082916
step 101/334, epoch 47/501 --> loss:0.17760802507400514
step 151/334, epoch 47/501 --> loss:0.16292273998260498
step 201/334, epoch 47/501 --> loss:0.1438081705570221
step 251/334, epoch 47/501 --> loss:0.15002943992614745
step 301/334, epoch 47/501 --> loss:0.13809998393058776
step 51/334, epoch 48/501 --> loss:0.13788507819175722
step 101/334, epoch 48/501 --> loss:0.0993078339099884
step 151/334, epoch 48/501 --> loss:0.11949701189994812
step 201/334, epoch 48/501 --> loss:0.11595478653907776
step 251/334, epoch 48/501 --> loss:0.1394710946083069
step 301/334, epoch 48/501 --> loss:0.17824169278144836
step 51/334, epoch 49/501 --> loss:0.10696598649024963
step 101/334, epoch 49/501 --> loss:0.11599837422370911
step 151/334, epoch 49/501 --> loss:0.14448091626167298
step 201/334, epoch 49/501 --> loss:0.12331817030906678
step 251/334, epoch 49/501 --> loss:0.13963677167892455
step 301/334, epoch 49/501 --> loss:0.14867888569831847
step 51/334, epoch 50/501 --> loss:0.12165175795555115
step 101/334, epoch 50/501 --> loss:0.10235217094421387
step 151/334, epoch 50/501 --> loss:0.18871556639671325
step 201/334, epoch 50/501 --> loss:0.13495673298835753
step 251/334, epoch 50/501 --> loss:0.12461096525192261
step 301/334, epoch 50/501 --> loss:0.1130061149597168
step 51/334, epoch 51/501 --> loss:0.14651612043380738
step 101/334, epoch 51/501 --> loss:0.14363819003105163
step 151/334, epoch 51/501 --> loss:0.14358012318611146
step 201/334, epoch 51/501 --> loss:0.1191510272026062
step 251/334, epoch 51/501 --> loss:0.11166840434074402
step 301/334, epoch 51/501 --> loss:0.13310316801071168

##########train dataset##########
acc--> [99.17020307840264]
F1--> {'F1': [0.8979395057549993], 'precision': [0.9327626328079419], 'recall': [0.8656322071190625]}
##########eval dataset##########
acc--> [98.93307957018628]
F1--> {'F1': [0.8706812212921112], 'precision': [0.9143077296862041], 'recall': [0.8310375032920421]}
save model!
step 51/334, epoch 52/501 --> loss:0.13102739572525024
step 101/334, epoch 52/501 --> loss:0.1296441900730133
step 151/334, epoch 52/501 --> loss:0.10774231553077698
step 201/334, epoch 52/501 --> loss:0.11112685441970825
step 251/334, epoch 52/501 --> loss:0.1236566436290741
step 301/334, epoch 52/501 --> loss:0.09687412023544312
step 51/334, epoch 53/501 --> loss:0.13876295685768128
step 101/334, epoch 53/501 --> loss:0.15943077087402344
step 151/334, epoch 53/501 --> loss:0.12947027444839476
step 201/334, epoch 53/501 --> loss:0.12748655319213867
step 251/334, epoch 53/501 --> loss:0.11865245342254639
step 301/334, epoch 53/501 --> loss:0.13652499556541442
step 51/334, epoch 54/501 --> loss:0.1339145064353943
step 101/334, epoch 54/501 --> loss:0.10152720451354981
step 151/334, epoch 54/501 --> loss:0.13659273862838744
step 201/334, epoch 54/501 --> loss:0.14280788898468016
step 251/334, epoch 54/501 --> loss:0.13565774202346803
step 301/334, epoch 54/501 --> loss:0.12106668829917908
step 51/334, epoch 55/501 --> loss:0.10861876130104064
step 101/334, epoch 55/501 --> loss:0.20098097801208495
step 151/334, epoch 55/501 --> loss:0.12780072331428527
step 201/334, epoch 55/501 --> loss:0.12815153002738952
step 251/334, epoch 55/501 --> loss:0.1278641951084137
step 301/334, epoch 55/501 --> loss:0.11562469840049744
step 51/334, epoch 56/501 --> loss:0.12030027985572815
step 101/334, epoch 56/501 --> loss:0.11390580177307129
step 151/334, epoch 56/501 --> loss:0.12553002953529357
step 201/334, epoch 56/501 --> loss:0.10928498387336731
step 251/334, epoch 56/501 --> loss:0.11794253826141357
step 301/334, epoch 56/501 --> loss:0.11320455431938171
step 51/334, epoch 57/501 --> loss:0.10202903985977173
step 101/334, epoch 57/501 --> loss:0.09903838992118835
step 151/334, epoch 57/501 --> loss:0.15252781987190248
step 201/334, epoch 57/501 --> loss:0.11457256078720093
step 251/334, epoch 57/501 --> loss:0.11669129848480225
step 301/334, epoch 57/501 --> loss:0.10648872613906861
step 51/334, epoch 58/501 --> loss:0.11923151016235352
step 101/334, epoch 58/501 --> loss:0.12066749215126038
step 151/334, epoch 58/501 --> loss:0.12829373240470887
step 201/334, epoch 58/501 --> loss:0.12275187015533447
step 251/334, epoch 58/501 --> loss:0.1180142343044281
step 301/334, epoch 58/501 --> loss:0.1063014817237854
step 51/334, epoch 59/501 --> loss:0.12044953823089599
step 101/334, epoch 59/501 --> loss:0.12600232124328614
step 151/334, epoch 59/501 --> loss:0.11958841323852538
step 201/334, epoch 59/501 --> loss:0.14170657277107238
step 251/334, epoch 59/501 --> loss:0.10268589973449707
step 301/334, epoch 59/501 --> loss:0.12571995496749877
step 51/334, epoch 60/501 --> loss:0.11468973159790039
step 101/334, epoch 60/501 --> loss:0.1378118896484375
step 151/334, epoch 60/501 --> loss:0.10731608748435974
step 201/334, epoch 60/501 --> loss:0.14824585914611815
step 251/334, epoch 60/501 --> loss:0.12478352904319763
step 301/334, epoch 60/501 --> loss:0.12690643668174745
step 51/334, epoch 61/501 --> loss:0.1780603575706482
step 101/334, epoch 61/501 --> loss:0.1478349530696869
step 151/334, epoch 61/501 --> loss:0.10607866287231445
step 201/334, epoch 61/501 --> loss:0.11838003635406494
step 251/334, epoch 61/501 --> loss:0.10984201908111572
step 301/334, epoch 61/501 --> loss:0.11945616364479066

##########train dataset##########
acc--> [99.03956965019592]
F1--> {'F1': [0.8822592107474847], 'precision': [0.9132575461120704], 'recall': [0.8533054654360094]}
##########eval dataset##########
acc--> [98.84180117361988]
F1--> {'F1': [0.8620581174008578], 'precision': [0.8882652149317928], 'recall': [0.8373625413061965]}
step 51/334, epoch 62/501 --> loss:0.11336127042770386
step 101/334, epoch 62/501 --> loss:0.09475281596183777
step 151/334, epoch 62/501 --> loss:0.09104993939399719
step 201/334, epoch 62/501 --> loss:0.1260688292980194
step 251/334, epoch 62/501 --> loss:0.10071422219276428
step 301/334, epoch 62/501 --> loss:0.12849850535392762
step 51/334, epoch 63/501 --> loss:0.1337706232070923
step 101/334, epoch 63/501 --> loss:0.10799628257751465
step 151/334, epoch 63/501 --> loss:0.10133796215057372
step 201/334, epoch 63/501 --> loss:0.13132665991783143
step 251/334, epoch 63/501 --> loss:0.10252435088157653
step 301/334, epoch 63/501 --> loss:0.10861382007598877
step 51/334, epoch 64/501 --> loss:0.09886914134025573
step 101/334, epoch 64/501 --> loss:0.12560200572013855
step 151/334, epoch 64/501 --> loss:0.10616842746734619
step 201/334, epoch 64/501 --> loss:0.1435653328895569
step 251/334, epoch 64/501 --> loss:0.1843522310256958
step 301/334, epoch 64/501 --> loss:0.12088431596755982
step 51/334, epoch 65/501 --> loss:0.08960222721099853
step 101/334, epoch 65/501 --> loss:0.08904829263687133
step 151/334, epoch 65/501 --> loss:0.09192596793174744
step 201/334, epoch 65/501 --> loss:0.13038081645965577
step 251/334, epoch 65/501 --> loss:0.10901676416397095
step 301/334, epoch 65/501 --> loss:0.11651411414146423
step 51/334, epoch 66/501 --> loss:0.10800737977027892
step 101/334, epoch 66/501 --> loss:0.10505613565444946
step 151/334, epoch 66/501 --> loss:0.09002434372901917
step 201/334, epoch 66/501 --> loss:0.12991875410079956
step 251/334, epoch 66/501 --> loss:0.10287914037704468
step 301/334, epoch 66/501 --> loss:0.09994442224502563
step 51/334, epoch 67/501 --> loss:0.1165750527381897
step 101/334, epoch 67/501 --> loss:0.10898672223091126
step 151/334, epoch 67/501 --> loss:0.09708256840705871
step 201/334, epoch 67/501 --> loss:0.097609144449234
step 251/334, epoch 67/501 --> loss:0.13177356362342835
step 301/334, epoch 67/501 --> loss:0.1700989544391632
step 51/334, epoch 68/501 --> loss:0.13383691072463988
step 101/334, epoch 68/501 --> loss:0.10373894453048706
step 151/334, epoch 68/501 --> loss:0.12172640442848205
step 201/334, epoch 68/501 --> loss:0.14292710900306702
step 251/334, epoch 68/501 --> loss:0.10476676940917969
step 301/334, epoch 68/501 --> loss:0.1337815999984741
step 51/334, epoch 69/501 --> loss:0.1044498312473297
step 101/334, epoch 69/501 --> loss:0.11135861992835999
step 151/334, epoch 69/501 --> loss:0.09082934737205506
step 201/334, epoch 69/501 --> loss:0.09645693778991699
step 251/334, epoch 69/501 --> loss:0.09231252193450928
step 301/334, epoch 69/501 --> loss:0.0901307213306427
step 51/334, epoch 70/501 --> loss:0.08141302347183227
step 101/334, epoch 70/501 --> loss:0.1247168493270874
step 151/334, epoch 70/501 --> loss:0.12518932700157165
step 201/334, epoch 70/501 --> loss:0.10855899572372436
step 251/334, epoch 70/501 --> loss:0.1136610448360443
step 301/334, epoch 70/501 --> loss:0.10575860142707824
step 51/334, epoch 71/501 --> loss:0.09787976861000061
step 101/334, epoch 71/501 --> loss:0.11591033935546875
step 151/334, epoch 71/501 --> loss:0.10113402128219605
step 201/334, epoch 71/501 --> loss:0.09743683099746704
step 251/334, epoch 71/501 --> loss:0.10042733550071717
step 301/334, epoch 71/501 --> loss:0.0924566388130188

##########train dataset##########
acc--> [94.37454875864246]
F1--> {'F1': [0.5199732801714857], 'precision': [0.4061395668932939], 'recall': [0.7224861217313072]}
##########eval dataset##########
acc--> [94.36689628807365]
F1--> {'F1': [0.5253025388794834], 'precision': [0.4131198961557676], 'recall': [0.7211409716233166]}
step 51/334, epoch 72/501 --> loss:0.13302140235900878
step 101/334, epoch 72/501 --> loss:0.13608174443244933
step 151/334, epoch 72/501 --> loss:0.11338447451591492
step 201/334, epoch 72/501 --> loss:0.08567675828933716
step 251/334, epoch 72/501 --> loss:0.1058434021472931
step 301/334, epoch 72/501 --> loss:0.10776935338973999
step 51/334, epoch 73/501 --> loss:0.09164689421653748
step 101/334, epoch 73/501 --> loss:0.11362687587738036
step 151/334, epoch 73/501 --> loss:0.10054493427276612
step 201/334, epoch 73/501 --> loss:0.11894701957702637
step 251/334, epoch 73/501 --> loss:0.0947724461555481
step 301/334, epoch 73/501 --> loss:0.08428072333335876
step 51/334, epoch 74/501 --> loss:0.13388547778129578
step 101/334, epoch 74/501 --> loss:0.1289525830745697
step 151/334, epoch 74/501 --> loss:0.10747109413146973
step 201/334, epoch 74/501 --> loss:0.11669125199317933
step 251/334, epoch 74/501 --> loss:0.10671154499053954
step 301/334, epoch 74/501 --> loss:0.08712725400924683
step 51/334, epoch 75/501 --> loss:0.1067777693271637
step 101/334, epoch 75/501 --> loss:0.09052936553955078
step 151/334, epoch 75/501 --> loss:0.10001707673072815
step 201/334, epoch 75/501 --> loss:0.10607003450393676
step 251/334, epoch 75/501 --> loss:0.11416998505592346
step 301/334, epoch 75/501 --> loss:0.11632346510887145
step 51/334, epoch 76/501 --> loss:0.0956446123123169
step 101/334, epoch 76/501 --> loss:0.1138886022567749
step 151/334, epoch 76/501 --> loss:0.11221755504608154
step 201/334, epoch 76/501 --> loss:0.1081444776058197
step 251/334, epoch 76/501 --> loss:0.12362645983695984
step 301/334, epoch 76/501 --> loss:0.11616952180862426
step 51/334, epoch 77/501 --> loss:0.14245814681053162
step 101/334, epoch 77/501 --> loss:0.12888816714286805
step 151/334, epoch 77/501 --> loss:0.12202744722366334
step 201/334, epoch 77/501 --> loss:0.14333810567855834
step 251/334, epoch 77/501 --> loss:0.24363162279129028
step 301/334, epoch 77/501 --> loss:0.13177656412124633
step 51/334, epoch 78/501 --> loss:0.11304311633110047
step 101/334, epoch 78/501 --> loss:0.10658448219299316
step 151/334, epoch 78/501 --> loss:0.08818129539489745
step 201/334, epoch 78/501 --> loss:0.10380574941635132
step 251/334, epoch 78/501 --> loss:0.09551129698753356
step 301/334, epoch 78/501 --> loss:0.09710127711296082
step 51/334, epoch 79/501 --> loss:0.08974643468856812
step 101/334, epoch 79/501 --> loss:0.08911958336830139
step 151/334, epoch 79/501 --> loss:0.12503172755241393
step 201/334, epoch 79/501 --> loss:0.10645903587341309
step 251/334, epoch 79/501 --> loss:0.1148297905921936
step 301/334, epoch 79/501 --> loss:0.1161263656616211
step 51/334, epoch 80/501 --> loss:0.2191466522216797
step 101/334, epoch 80/501 --> loss:0.15947089672088624
step 151/334, epoch 80/501 --> loss:0.13162321209907532
step 201/334, epoch 80/501 --> loss:0.132351096868515
step 251/334, epoch 80/501 --> loss:0.13695979714393616
step 301/334, epoch 80/501 --> loss:0.17468091249465942
step 51/334, epoch 81/501 --> loss:0.09099197268486023
step 101/334, epoch 81/501 --> loss:0.11586215019226075
step 151/334, epoch 81/501 --> loss:0.07491883754730225
step 201/334, epoch 81/501 --> loss:0.11947993516921997
step 251/334, epoch 81/501 --> loss:0.09555300712585449
step 301/334, epoch 81/501 --> loss:0.09080193877220154

##########train dataset##########
acc--> [99.31064515601648]
F1--> {'F1': [0.9159704741377], 'precision': [0.9424112401680721], 'recall': [0.890982342681322]}
##########eval dataset##########
acc--> [98.99134464880682]
F1--> {'F1': [0.8785879031462458], 'precision': [0.915649166874461], 'recall': [0.844419292136814]}
save model!
step 51/334, epoch 82/501 --> loss:0.10165565490722656
step 101/334, epoch 82/501 --> loss:0.1057129180431366
step 151/334, epoch 82/501 --> loss:0.07746948838233948
step 201/334, epoch 82/501 --> loss:0.07859814524650574
step 251/334, epoch 82/501 --> loss:0.07517801523208618
step 301/334, epoch 82/501 --> loss:0.11198938012123108
step 51/334, epoch 83/501 --> loss:0.10219489574432374
step 101/334, epoch 83/501 --> loss:0.14099319219589235
step 151/334, epoch 83/501 --> loss:0.09714357256889343
step 201/334, epoch 83/501 --> loss:0.09111378669738769
step 251/334, epoch 83/501 --> loss:0.10318576097488404
step 301/334, epoch 83/501 --> loss:0.11263098835945129
step 51/334, epoch 84/501 --> loss:0.09749501228332519
step 101/334, epoch 84/501 --> loss:0.09369635224342346
step 151/334, epoch 84/501 --> loss:0.10100292444229125
step 201/334, epoch 84/501 --> loss:0.08206141352653504
step 251/334, epoch 84/501 --> loss:0.10979177594184876
step 301/334, epoch 84/501 --> loss:0.09694860935211182
step 51/334, epoch 85/501 --> loss:0.09852657794952392
step 101/334, epoch 85/501 --> loss:0.09127889633178711
step 151/334, epoch 85/501 --> loss:0.1190971839427948
step 201/334, epoch 85/501 --> loss:0.10773272037506104
step 251/334, epoch 85/501 --> loss:0.08114289045333863
step 301/334, epoch 85/501 --> loss:0.09736447334289551
step 51/334, epoch 86/501 --> loss:0.08762111067771912
step 101/334, epoch 86/501 --> loss:0.08399389028549194
step 151/334, epoch 86/501 --> loss:0.09575309634208679
step 201/334, epoch 86/501 --> loss:0.07657354712486267
step 251/334, epoch 86/501 --> loss:0.076445232629776
step 301/334, epoch 86/501 --> loss:0.09894084930419922
step 51/334, epoch 87/501 --> loss:0.08443070888519287
step 101/334, epoch 87/501 --> loss:0.08635801672935486
step 151/334, epoch 87/501 --> loss:0.11218253135681153
step 201/334, epoch 87/501 --> loss:0.11163292407989502
step 251/334, epoch 87/501 --> loss:0.09097506403923035
step 301/334, epoch 87/501 --> loss:0.08574196696281433
step 51/334, epoch 88/501 --> loss:0.07504191040992737
step 101/334, epoch 88/501 --> loss:0.08452653288841247
step 151/334, epoch 88/501 --> loss:0.0817388665676117
step 201/334, epoch 88/501 --> loss:0.09014726758003234
step 251/334, epoch 88/501 --> loss:0.08918277263641357
step 301/334, epoch 88/501 --> loss:0.08680508852005005
step 51/334, epoch 89/501 --> loss:0.11081096053123474
step 101/334, epoch 89/501 --> loss:0.1025381600856781
step 151/334, epoch 89/501 --> loss:0.16113752603530884
step 201/334, epoch 89/501 --> loss:0.12633492708206176
step 251/334, epoch 89/501 --> loss:0.09924395442008972
step 301/334, epoch 89/501 --> loss:0.1125211501121521
step 51/334, epoch 90/501 --> loss:0.11563880443572998
step 101/334, epoch 90/501 --> loss:0.09650957226753235
step 151/334, epoch 90/501 --> loss:0.09977212429046631
step 201/334, epoch 90/501 --> loss:0.09491518020629883
step 251/334, epoch 90/501 --> loss:0.08467309951782226
step 301/334, epoch 90/501 --> loss:0.08594584345817566
step 51/334, epoch 91/501 --> loss:0.09657398462295533
step 101/334, epoch 91/501 --> loss:0.08075012564659119
step 151/334, epoch 91/501 --> loss:0.07009362936019897
step 201/334, epoch 91/501 --> loss:0.07884783029556275
step 251/334, epoch 91/501 --> loss:0.10136063814163208
step 301/334, epoch 91/501 --> loss:0.08011828541755676

##########train dataset##########
acc--> [99.37646162835928]
F1--> {'F1': [0.9257962595978736], 'precision': [0.929198301879247], 'recall': [0.9224289650475807]}
##########eval dataset##########
acc--> [99.04023151280232]
F1--> {'F1': [0.8877278631383961], 'precision': [0.8977401788362136], 'recall': [0.8779461942426063]}
save model!
step 51/334, epoch 92/501 --> loss:0.07518811821937561
step 101/334, epoch 92/501 --> loss:0.07929502725601197
step 151/334, epoch 92/501 --> loss:0.08592858791351318
step 201/334, epoch 92/501 --> loss:0.09041952490806579
step 251/334, epoch 92/501 --> loss:0.07655470252037049
step 301/334, epoch 92/501 --> loss:0.06974188208580018
step 51/334, epoch 93/501 --> loss:0.12438072323799133
step 101/334, epoch 93/501 --> loss:0.09883374094963074
step 151/334, epoch 93/501 --> loss:0.09861879706382752
step 201/334, epoch 93/501 --> loss:0.0882885754108429
step 251/334, epoch 93/501 --> loss:0.09420918107032776
step 301/334, epoch 93/501 --> loss:0.09454225182533264
step 51/334, epoch 94/501 --> loss:0.09436001181602478
step 101/334, epoch 94/501 --> loss:0.10208031415939331
step 151/334, epoch 94/501 --> loss:0.07510023593902587
step 201/334, epoch 94/501 --> loss:0.07579553365707398
step 251/334, epoch 94/501 --> loss:0.067268887758255
step 301/334, epoch 94/501 --> loss:0.09279853701591492
step 51/334, epoch 95/501 --> loss:0.08108644485473633
step 101/334, epoch 95/501 --> loss:0.07280715942382812
step 151/334, epoch 95/501 --> loss:0.08399455904960633
step 201/334, epoch 95/501 --> loss:0.09799584984779358
step 251/334, epoch 95/501 --> loss:0.07842699646949768
step 301/334, epoch 95/501 --> loss:0.08534683585166931
step 51/334, epoch 96/501 --> loss:0.09045832991600036
step 101/334, epoch 96/501 --> loss:0.07785189390182495
step 151/334, epoch 96/501 --> loss:0.07289485096931457
step 201/334, epoch 96/501 --> loss:0.09224034190177917
step 251/334, epoch 96/501 --> loss:0.10611987233161926
step 301/334, epoch 96/501 --> loss:0.12291510343551636
step 51/334, epoch 97/501 --> loss:0.12386653423309327
step 101/334, epoch 97/501 --> loss:0.10870374202728271
step 151/334, epoch 97/501 --> loss:0.09853867769241333
step 201/334, epoch 97/501 --> loss:0.09732768654823304
step 251/334, epoch 97/501 --> loss:0.10511677980422973
step 301/334, epoch 97/501 --> loss:0.10199256062507629
step 51/334, epoch 98/501 --> loss:0.08838329911231994
step 101/334, epoch 98/501 --> loss:0.09409230709075928
step 151/334, epoch 98/501 --> loss:0.13157548308372496
step 201/334, epoch 98/501 --> loss:0.15150206923484802
step 251/334, epoch 98/501 --> loss:0.1256619393825531
step 301/334, epoch 98/501 --> loss:0.08741390466690063
step 51/334, epoch 99/501 --> loss:0.08949984073638916
step 101/334, epoch 99/501 --> loss:0.09364900469779969
step 151/334, epoch 99/501 --> loss:0.09945518136024475
step 201/334, epoch 99/501 --> loss:0.08859321117401123
step 251/334, epoch 99/501 --> loss:0.07807146906852722
step 301/334, epoch 99/501 --> loss:0.10105152606964111
step 51/334, epoch 100/501 --> loss:0.09147963404655457
step 101/334, epoch 100/501 --> loss:0.0763949692249298
step 151/334, epoch 100/501 --> loss:0.09127120852470398
step 201/334, epoch 100/501 --> loss:0.08176447629928589
step 251/334, epoch 100/501 --> loss:0.07536819458007812
step 301/334, epoch 100/501 --> loss:0.07963686943054199
step 51/334, epoch 101/501 --> loss:0.09485574007034302
step 101/334, epoch 101/501 --> loss:0.09082818746566773
step 151/334, epoch 101/501 --> loss:0.08561051845550537
step 201/334, epoch 101/501 --> loss:0.10566729903221131
step 251/334, epoch 101/501 --> loss:0.08201634764671326
step 301/334, epoch 101/501 --> loss:0.08227570652961731

##########train dataset##########
acc--> [99.06035524474562]
F1--> {'F1': [0.8841231106644856], 'precision': [0.9210416107462597], 'recall': [0.8500594189372248]}
##########eval dataset##########
acc--> [98.6703901048863]
F1--> {'F1': [0.8379354388666851], 'precision': [0.8854043933375478], 'recall': [0.7953063510745194]}
step 51/334, epoch 102/501 --> loss:0.12080011963844299
step 101/334, epoch 102/501 --> loss:0.08679808139801025
step 151/334, epoch 102/501 --> loss:0.1550027024745941
step 201/334, epoch 102/501 --> loss:0.11886895179748536
step 251/334, epoch 102/501 --> loss:0.12897940516471862
step 301/334, epoch 102/501 --> loss:0.0941982090473175
step 51/334, epoch 103/501 --> loss:0.0979806935787201
step 101/334, epoch 103/501 --> loss:0.09966381072998047
step 151/334, epoch 103/501 --> loss:0.07669173240661621
step 201/334, epoch 103/501 --> loss:0.08702096343040466
step 251/334, epoch 103/501 --> loss:0.1031320881843567
step 301/334, epoch 103/501 --> loss:0.09325292944908142
step 51/334, epoch 104/501 --> loss:0.07316667675971984
step 101/334, epoch 104/501 --> loss:0.07504340291023254
step 151/334, epoch 104/501 --> loss:0.11122304558753968
step 201/334, epoch 104/501 --> loss:0.1128794515132904
step 251/334, epoch 104/501 --> loss:0.07423297882080078
step 301/334, epoch 104/501 --> loss:0.07993064045906068
step 51/334, epoch 105/501 --> loss:0.06826663613319398
step 101/334, epoch 105/501 --> loss:0.08068962097167968
step 151/334, epoch 105/501 --> loss:0.08284902811050415
step 201/334, epoch 105/501 --> loss:0.07293617606163025
step 251/334, epoch 105/501 --> loss:0.07558862805366516
step 301/334, epoch 105/501 --> loss:0.08042287111282348
step 51/334, epoch 106/501 --> loss:0.09886248230934143
step 101/334, epoch 106/501 --> loss:0.0891153109073639
step 151/334, epoch 106/501 --> loss:0.08332238197326661
step 201/334, epoch 106/501 --> loss:0.07629587769508361
step 251/334, epoch 106/501 --> loss:0.07561512470245361
step 301/334, epoch 106/501 --> loss:0.08203587770462036
step 51/334, epoch 107/501 --> loss:0.07368374109268189
step 101/334, epoch 107/501 --> loss:0.05929784655570984
step 151/334, epoch 107/501 --> loss:0.08029869794845582
step 201/334, epoch 107/501 --> loss:0.09620007395744323
step 251/334, epoch 107/501 --> loss:0.06803946018218994
step 301/334, epoch 107/501 --> loss:0.08695633292198181
step 51/334, epoch 108/501 --> loss:0.07237541913986206
step 101/334, epoch 108/501 --> loss:0.0547160267829895
step 151/334, epoch 108/501 --> loss:0.07985968232154846
step 201/334, epoch 108/501 --> loss:0.08942913055419922
step 251/334, epoch 108/501 --> loss:0.06494478583335876
step 301/334, epoch 108/501 --> loss:0.08014805912971497
step 51/334, epoch 109/501 --> loss:0.073973069190979
step 101/334, epoch 109/501 --> loss:0.09691773056983947
step 151/334, epoch 109/501 --> loss:0.0974669861793518
step 201/334, epoch 109/501 --> loss:0.07380699157714844
step 251/334, epoch 109/501 --> loss:0.07665137171745301
step 301/334, epoch 109/501 --> loss:0.10004908561706544
step 51/334, epoch 110/501 --> loss:0.07710220456123353
step 101/334, epoch 110/501 --> loss:0.08879112124443055
step 151/334, epoch 110/501 --> loss:0.07783110976219178
step 201/334, epoch 110/501 --> loss:0.08176324367523194
step 251/334, epoch 110/501 --> loss:0.10885346412658692
step 301/334, epoch 110/501 --> loss:0.08387158989906311
step 51/334, epoch 111/501 --> loss:0.059147495031356814
step 101/334, epoch 111/501 --> loss:0.07181853771209717
step 151/334, epoch 111/501 --> loss:0.06298732042312621
step 201/334, epoch 111/501 --> loss:0.10725647807121277
step 251/334, epoch 111/501 --> loss:0.08572083950042725
step 301/334, epoch 111/501 --> loss:0.08408501386642456

##########train dataset##########
acc--> [99.4933335397561]
F1--> {'F1': [0.9386462489301228], 'precision': [0.9590375671750723], 'recall': [0.919113592497728]}
##########eval dataset##########
acc--> [99.06185881045823]
F1--> {'F1': [0.8880603996331627], 'precision': [0.9168485025473192], 'recall': [0.8610344856329764]}
save model!
step 51/334, epoch 112/501 --> loss:0.08121034502983093
step 101/334, epoch 112/501 --> loss:0.07715640425682067
step 151/334, epoch 112/501 --> loss:0.07385944128036499
step 201/334, epoch 112/501 --> loss:0.11086470365524292
step 251/334, epoch 112/501 --> loss:0.07316200137138366
step 301/334, epoch 112/501 --> loss:0.08058613181114196
step 51/334, epoch 113/501 --> loss:0.06470370054244995
step 101/334, epoch 113/501 --> loss:0.0823256766796112
step 151/334, epoch 113/501 --> loss:0.07694817423820495
step 201/334, epoch 113/501 --> loss:0.06030897974967957
step 251/334, epoch 113/501 --> loss:0.061134142875671385
step 301/334, epoch 113/501 --> loss:0.08518681287765503
step 51/334, epoch 114/501 --> loss:0.06408690333366394
step 101/334, epoch 114/501 --> loss:0.07397205948829651
step 151/334, epoch 114/501 --> loss:0.06706675291061401
step 201/334, epoch 114/501 --> loss:0.0783833122253418
step 251/334, epoch 114/501 --> loss:0.07079809784889221
step 301/334, epoch 114/501 --> loss:0.07124353766441345
step 51/334, epoch 115/501 --> loss:0.06487553119659424
step 101/334, epoch 115/501 --> loss:0.08498555541038513
step 151/334, epoch 115/501 --> loss:0.06447610259056091
step 201/334, epoch 115/501 --> loss:0.06104773283004761
step 251/334, epoch 115/501 --> loss:0.08334040880203247
step 301/334, epoch 115/501 --> loss:0.07042620182037354
step 51/334, epoch 116/501 --> loss:0.06698387742042541
step 101/334, epoch 116/501 --> loss:0.05995252251625061
step 151/334, epoch 116/501 --> loss:0.07842202544212341
step 201/334, epoch 116/501 --> loss:0.06984908819198608
step 251/334, epoch 116/501 --> loss:0.08446179628372193
step 301/334, epoch 116/501 --> loss:0.0722656226158142
step 51/334, epoch 117/501 --> loss:0.10883644819259644
step 101/334, epoch 117/501 --> loss:0.09785109043121337
step 151/334, epoch 117/501 --> loss:0.12153193831443787
step 201/334, epoch 117/501 --> loss:0.08904013156890869
step 251/334, epoch 117/501 --> loss:0.10395750641822815
step 301/334, epoch 117/501 --> loss:0.14384962916374205
step 51/334, epoch 118/501 --> loss:0.06497398972511291
step 101/334, epoch 118/501 --> loss:0.10246189117431641
step 151/334, epoch 118/501 --> loss:0.10465514659881592
step 201/334, epoch 118/501 --> loss:0.11851626634597778
step 251/334, epoch 118/501 --> loss:0.09904186248779297
step 301/334, epoch 118/501 --> loss:0.13575652718544007
step 51/334, epoch 119/501 --> loss:0.1418459701538086
step 101/334, epoch 119/501 --> loss:0.12657350659370423
step 151/334, epoch 119/501 --> loss:0.10776374220848084
step 201/334, epoch 119/501 --> loss:0.093182852268219
step 251/334, epoch 119/501 --> loss:0.08752714991569518
step 301/334, epoch 119/501 --> loss:0.12183878302574158
step 51/334, epoch 120/501 --> loss:0.08826490759849548
step 101/334, epoch 120/501 --> loss:0.06924800157546997
step 151/334, epoch 120/501 --> loss:0.07525691628456116
step 201/334, epoch 120/501 --> loss:0.05925358414649964
step 251/334, epoch 120/501 --> loss:0.06394517302513122
step 301/334, epoch 120/501 --> loss:0.06330901265144348
step 51/334, epoch 121/501 --> loss:0.05624320864677429
step 101/334, epoch 121/501 --> loss:0.0825428307056427
step 151/334, epoch 121/501 --> loss:0.0781080198287964
step 201/334, epoch 121/501 --> loss:0.06408734917640686
step 251/334, epoch 121/501 --> loss:0.0731613266468048
step 301/334, epoch 121/501 --> loss:0.07806053876876831

##########train dataset##########
acc--> [99.50299629522954]
F1--> {'F1': [0.9406497200614267], 'precision': [0.9473931838163319], 'recall': [0.9340114352704397]}
##########eval dataset##########
acc--> [99.06960902042344]
F1--> {'F1': [0.8917874150782528], 'precision': [0.8965957708508738], 'recall': [0.8870402509986737]}
save model!
step 51/334, epoch 122/501 --> loss:0.06829950213432312
step 101/334, epoch 122/501 --> loss:0.08033910751342774
step 151/334, epoch 122/501 --> loss:0.06386194348335267
step 201/334, epoch 122/501 --> loss:0.07106769800186158
step 251/334, epoch 122/501 --> loss:0.06406783103942872
step 301/334, epoch 122/501 --> loss:0.06312852144241334
step 51/334, epoch 123/501 --> loss:0.08950846552848817
step 101/334, epoch 123/501 --> loss:0.07778860688209534
step 151/334, epoch 123/501 --> loss:0.06960639357566833
step 201/334, epoch 123/501 --> loss:0.08499017357826233
step 251/334, epoch 123/501 --> loss:0.11712940692901612
step 301/334, epoch 123/501 --> loss:0.08855124115943909
step 51/334, epoch 124/501 --> loss:0.09014375448226929
step 101/334, epoch 124/501 --> loss:0.07470393300056458
step 151/334, epoch 124/501 --> loss:0.06487601399421691
step 201/334, epoch 124/501 --> loss:0.06533264398574828
step 251/334, epoch 124/501 --> loss:0.0705296277999878
step 301/334, epoch 124/501 --> loss:0.06804941773414612
step 51/334, epoch 125/501 --> loss:0.06549450993537903
step 101/334, epoch 125/501 --> loss:0.08992886424064636
step 151/334, epoch 125/501 --> loss:0.08751564383506776
step 201/334, epoch 125/501 --> loss:0.07345064640045167
step 251/334, epoch 125/501 --> loss:0.07876697897911072
step 301/334, epoch 125/501 --> loss:0.11314630746841431
step 51/334, epoch 126/501 --> loss:0.07803367495536805
step 101/334, epoch 126/501 --> loss:0.06552336931228638
step 151/334, epoch 126/501 --> loss:0.07812236547470093
step 201/334, epoch 126/501 --> loss:0.06950030446052552
step 251/334, epoch 126/501 --> loss:0.06157625913619995
step 301/334, epoch 126/501 --> loss:0.07273878574371338
step 51/334, epoch 127/501 --> loss:0.07833382844924927
step 101/334, epoch 127/501 --> loss:0.09265279293060302
step 151/334, epoch 127/501 --> loss:0.07838469624519348
step 201/334, epoch 127/501 --> loss:0.07484616279602051
step 251/334, epoch 127/501 --> loss:0.08284182548522949
step 301/334, epoch 127/501 --> loss:0.09739961862564087
step 51/334, epoch 128/501 --> loss:0.1272772204875946
step 101/334, epoch 128/501 --> loss:0.07471555590629578
step 151/334, epoch 128/501 --> loss:0.0801462721824646
step 201/334, epoch 128/501 --> loss:0.08079272985458374
step 251/334, epoch 128/501 --> loss:0.07383918166160583
step 301/334, epoch 128/501 --> loss:0.07382744669914246
step 51/334, epoch 129/501 --> loss:0.08233157396316529
step 101/334, epoch 129/501 --> loss:0.05843239068984985
step 151/334, epoch 129/501 --> loss:0.08318871021270752
step 201/334, epoch 129/501 --> loss:0.0973569905757904
step 251/334, epoch 129/501 --> loss:0.07357460618019104
step 301/334, epoch 129/501 --> loss:0.07266904234886169
step 51/334, epoch 130/501 --> loss:0.07576620578765869
step 101/334, epoch 130/501 --> loss:0.09801482915878296
step 151/334, epoch 130/501 --> loss:0.10854944586753845
step 201/334, epoch 130/501 --> loss:0.13298336625099183
step 251/334, epoch 130/501 --> loss:0.10421183824539185
step 301/334, epoch 130/501 --> loss:0.07165710091590881
step 51/334, epoch 131/501 --> loss:0.09926586866378784
step 101/334, epoch 131/501 --> loss:0.07963485360145568
step 151/334, epoch 131/501 --> loss:0.08443660855293274
step 201/334, epoch 131/501 --> loss:0.06518299221992492
step 251/334, epoch 131/501 --> loss:0.07412100672721862
step 301/334, epoch 131/501 --> loss:0.07291082739830017

##########train dataset##########
acc--> [99.44318979475425]
F1--> {'F1': [0.933628432313474], 'precision': [0.9386036348762016], 'recall': [0.9287155897391959]}
##########eval dataset##########
acc--> [98.96896509151514]
F1--> {'F1': [0.8791455982350541], 'precision': [0.8909194440659883], 'recall': [0.8676886245360887]}
step 51/334, epoch 132/501 --> loss:0.08345069766044616
step 101/334, epoch 132/501 --> loss:0.07693680882453918
step 151/334, epoch 132/501 --> loss:0.07739328622817992
step 201/334, epoch 132/501 --> loss:0.07115642547607422
step 251/334, epoch 132/501 --> loss:0.0677282965183258
step 301/334, epoch 132/501 --> loss:0.056589269638061525
step 51/334, epoch 133/501 --> loss:0.05905643105506897
step 101/334, epoch 133/501 --> loss:0.05604673266410828
step 151/334, epoch 133/501 --> loss:0.0622855818271637
step 201/334, epoch 133/501 --> loss:0.06878806591033935
step 251/334, epoch 133/501 --> loss:0.06025700807571411
step 301/334, epoch 133/501 --> loss:0.06858769655227662
step 51/334, epoch 134/501 --> loss:0.0745451533794403
step 101/334, epoch 134/501 --> loss:0.05895626783370972
step 151/334, epoch 134/501 --> loss:0.06820844769477845
step 201/334, epoch 134/501 --> loss:0.05903358697891235
step 251/334, epoch 134/501 --> loss:0.059810963869094846
step 301/334, epoch 134/501 --> loss:0.06008256912231445
step 51/334, epoch 135/501 --> loss:0.0658824872970581
step 101/334, epoch 135/501 --> loss:0.05322043538093567
step 151/334, epoch 135/501 --> loss:0.08348117709159851
step 201/334, epoch 135/501 --> loss:0.06665916323661804
step 251/334, epoch 135/501 --> loss:0.07518330097198486
step 301/334, epoch 135/501 --> loss:0.07298421859741211
step 51/334, epoch 136/501 --> loss:0.06125172615051269
step 101/334, epoch 136/501 --> loss:0.06638585090637207
step 151/334, epoch 136/501 --> loss:0.05866166591644287
step 201/334, epoch 136/501 --> loss:0.056024962663650514
step 251/334, epoch 136/501 --> loss:0.06204041004180908
step 301/334, epoch 136/501 --> loss:0.06131346225738525
step 51/334, epoch 137/501 --> loss:0.050341945886611936
step 101/334, epoch 137/501 --> loss:0.06104442596435547
step 151/334, epoch 137/501 --> loss:0.052231166362762455
step 201/334, epoch 137/501 --> loss:0.07267746806144715
step 251/334, epoch 137/501 --> loss:0.07415478467941285
step 301/334, epoch 137/501 --> loss:0.07151567697525024
step 51/334, epoch 138/501 --> loss:0.08118182420730591
step 101/334, epoch 138/501 --> loss:0.05645431637763977
step 151/334, epoch 138/501 --> loss:0.062093575000762936
step 201/334, epoch 138/501 --> loss:0.06419798612594604
step 251/334, epoch 138/501 --> loss:0.06056547284126282
step 301/334, epoch 138/501 --> loss:0.06653608441352844
step 51/334, epoch 139/501 --> loss:0.06542542815208435
step 101/334, epoch 139/501 --> loss:0.06360787034034729
step 151/334, epoch 139/501 --> loss:0.06426332235336303
step 201/334, epoch 139/501 --> loss:0.08337446212768555
step 251/334, epoch 139/501 --> loss:0.07570873022079468
step 301/334, epoch 139/501 --> loss:0.06312925815582275
step 51/334, epoch 140/501 --> loss:0.06293861031532287
step 101/334, epoch 140/501 --> loss:0.049940489530563355
step 151/334, epoch 140/501 --> loss:0.07683677792549133
step 201/334, epoch 140/501 --> loss:0.06909164786338806
step 251/334, epoch 140/501 --> loss:0.09528224110603332
step 301/334, epoch 140/501 --> loss:0.07106590867042542
step 51/334, epoch 141/501 --> loss:0.07591405391693115
step 101/334, epoch 141/501 --> loss:0.08415868282318115
step 151/334, epoch 141/501 --> loss:0.07166832327842712
step 201/334, epoch 141/501 --> loss:0.053072328567504885
step 251/334, epoch 141/501 --> loss:0.061900943517684937
step 301/334, epoch 141/501 --> loss:0.053306055068969724

##########train dataset##########
acc--> [99.55943493436936]
F1--> {'F1': [0.9472996392197288], 'precision': [0.955734509791078], 'recall': [0.9390221757208237]}
##########eval dataset##########
acc--> [99.14419486244587]
F1--> {'F1': [0.8982978527529939], 'precision': [0.923435854708318], 'recall': [0.8745016770078251]}
save model!
step 51/334, epoch 142/501 --> loss:0.06086347699165344
step 101/334, epoch 142/501 --> loss:0.06210301399230957
step 151/334, epoch 142/501 --> loss:0.05245352506637573
step 201/334, epoch 142/501 --> loss:0.06445197939872742
step 251/334, epoch 142/501 --> loss:0.05736248612403869
step 301/334, epoch 142/501 --> loss:0.05561728000640869
step 51/334, epoch 143/501 --> loss:0.06094454407691956
step 101/334, epoch 143/501 --> loss:0.07633679032325745
step 151/334, epoch 143/501 --> loss:0.0908206605911255
step 201/334, epoch 143/501 --> loss:0.11026222109794617
step 251/334, epoch 143/501 --> loss:0.06615481853485107
step 301/334, epoch 143/501 --> loss:0.0938505232334137
step 51/334, epoch 144/501 --> loss:0.11849387884140014
step 101/334, epoch 144/501 --> loss:0.08766080141067505
step 151/334, epoch 144/501 --> loss:0.08252107381820678
step 201/334, epoch 144/501 --> loss:0.07071302652359009
step 251/334, epoch 144/501 --> loss:0.06130905270576477
step 301/334, epoch 144/501 --> loss:0.0956019914150238
step 51/334, epoch 145/501 --> loss:0.06704738020896911
step 101/334, epoch 145/501 --> loss:0.11592961430549621
step 151/334, epoch 145/501 --> loss:0.09019568085670471
step 201/334, epoch 145/501 --> loss:0.0789226770401001
step 251/334, epoch 145/501 --> loss:0.07740756750106811
step 301/334, epoch 145/501 --> loss:0.08841460943222046
step 51/334, epoch 146/501 --> loss:0.1002844762802124
step 101/334, epoch 146/501 --> loss:0.05878240346908569
step 151/334, epoch 146/501 --> loss:0.0638104784488678
step 201/334, epoch 146/501 --> loss:0.05570104598999023
step 251/334, epoch 146/501 --> loss:0.06827221393585205
step 301/334, epoch 146/501 --> loss:0.0775965404510498
step 51/334, epoch 147/501 --> loss:0.07824981570243836
step 101/334, epoch 147/501 --> loss:0.08993127703666687
step 151/334, epoch 147/501 --> loss:0.10879881978034973
step 201/334, epoch 147/501 --> loss:0.06645263910293579
step 251/334, epoch 147/501 --> loss:0.08825603246688843
step 301/334, epoch 147/501 --> loss:0.07299960970878601
step 51/334, epoch 148/501 --> loss:0.06381010055541993
step 101/334, epoch 148/501 --> loss:0.061351190805435184
step 151/334, epoch 148/501 --> loss:0.06953376293182373
step 201/334, epoch 148/501 --> loss:0.06446058750152588
step 251/334, epoch 148/501 --> loss:0.05790446281433106
step 301/334, epoch 148/501 --> loss:0.052713632583618164
step 51/334, epoch 149/501 --> loss:0.06142747163772583
step 101/334, epoch 149/501 --> loss:0.05176517963409424
step 151/334, epoch 149/501 --> loss:0.057794599533081054
step 201/334, epoch 149/501 --> loss:0.05566218376159668
step 251/334, epoch 149/501 --> loss:0.07229704618453979
step 301/334, epoch 149/501 --> loss:0.06892203569412231
step 51/334, epoch 150/501 --> loss:0.05615604043006897
step 101/334, epoch 150/501 --> loss:0.055537679195404054
step 151/334, epoch 150/501 --> loss:0.058957897424697876
step 201/334, epoch 150/501 --> loss:0.0549380099773407
step 251/334, epoch 150/501 --> loss:0.05409701943397522
step 301/334, epoch 150/501 --> loss:0.06002490997314453
step 51/334, epoch 151/501 --> loss:0.058798929452896116
step 101/334, epoch 151/501 --> loss:0.05539843559265137
step 151/334, epoch 151/501 --> loss:0.05398468613624573
step 201/334, epoch 151/501 --> loss:0.06402413249015808
step 251/334, epoch 151/501 --> loss:0.06256495237350464
step 301/334, epoch 151/501 --> loss:0.059328193664550784

##########train dataset##########
acc--> [99.60078470796944]
F1--> {'F1': [0.951880634850207], 'precision': [0.9678822124822942], 'recall': [0.9364092212324283]}
##########eval dataset##########
acc--> [99.16936118458163]
F1--> {'F1': [0.9007528645120864], 'precision': [0.9312952791286565], 'recall': [0.8721595164813415]}
save model!
step 51/334, epoch 152/501 --> loss:0.051232277154922484
step 101/334, epoch 152/501 --> loss:0.053897647857666015
step 151/334, epoch 152/501 --> loss:0.047273679971694946
step 201/334, epoch 152/501 --> loss:0.06300650715827942
step 251/334, epoch 152/501 --> loss:0.0472908616065979
step 301/334, epoch 152/501 --> loss:0.05127061367034912
step 51/334, epoch 153/501 --> loss:0.15788792729377746
step 101/334, epoch 153/501 --> loss:0.12167338252067567
step 151/334, epoch 153/501 --> loss:0.1254844570159912
step 201/334, epoch 153/501 --> loss:0.0696173095703125
step 251/334, epoch 153/501 --> loss:0.06225981712341309
step 301/334, epoch 153/501 --> loss:0.0616762101650238
step 51/334, epoch 154/501 --> loss:0.05641499638557434
step 101/334, epoch 154/501 --> loss:0.06330538153648377
step 151/334, epoch 154/501 --> loss:0.07025273799896241
step 201/334, epoch 154/501 --> loss:0.08530470371246338
step 251/334, epoch 154/501 --> loss:0.06199461817741394
step 301/334, epoch 154/501 --> loss:0.07148805260658264
step 51/334, epoch 155/501 --> loss:0.06309890151023864
step 101/334, epoch 155/501 --> loss:0.07294084310531616
step 151/334, epoch 155/501 --> loss:0.07151068210601806
step 201/334, epoch 155/501 --> loss:0.06108758568763733
step 251/334, epoch 155/501 --> loss:0.05746960997581482
step 301/334, epoch 155/501 --> loss:0.06096968412399292
step 51/334, epoch 156/501 --> loss:0.05163547396659851
step 101/334, epoch 156/501 --> loss:0.04844582319259644
step 151/334, epoch 156/501 --> loss:0.05971745014190674
step 201/334, epoch 156/501 --> loss:0.04732715487480164
step 251/334, epoch 156/501 --> loss:0.05920794010162354
step 301/334, epoch 156/501 --> loss:0.06190156102180481
step 51/334, epoch 157/501 --> loss:0.05605904698371887
step 101/334, epoch 157/501 --> loss:0.048283106088638304
step 151/334, epoch 157/501 --> loss:0.04819349765777588
step 201/334, epoch 157/501 --> loss:0.05984646201133728
step 251/334, epoch 157/501 --> loss:0.06856355547904969
step 301/334, epoch 157/501 --> loss:0.05275085687637329
step 51/334, epoch 158/501 --> loss:0.04440060257911682
step 101/334, epoch 158/501 --> loss:0.06554080009460449
step 151/334, epoch 158/501 --> loss:0.047175285816192625
step 201/334, epoch 158/501 --> loss:0.04992516398429871
step 251/334, epoch 158/501 --> loss:0.04957651376724243
step 301/334, epoch 158/501 --> loss:0.06543872594833373
step 51/334, epoch 159/501 --> loss:0.06359218955039977
step 101/334, epoch 159/501 --> loss:0.04365749835968018
step 151/334, epoch 159/501 --> loss:0.06017809152603149
step 201/334, epoch 159/501 --> loss:0.052661548852920535
step 251/334, epoch 159/501 --> loss:0.05491223931312561
step 301/334, epoch 159/501 --> loss:0.05163755655288696
step 51/334, epoch 160/501 --> loss:0.05062068223953247
step 101/334, epoch 160/501 --> loss:0.06517855525016784
step 151/334, epoch 160/501 --> loss:0.056740663051605224
step 201/334, epoch 160/501 --> loss:0.06273625254631042
step 251/334, epoch 160/501 --> loss:0.054212130308151245
step 301/334, epoch 160/501 --> loss:0.1033952009677887
step 51/334, epoch 161/501 --> loss:0.06902338027954101
step 101/334, epoch 161/501 --> loss:0.07184871792793274
step 151/334, epoch 161/501 --> loss:0.062187987565994265
step 201/334, epoch 161/501 --> loss:0.05972283840179443
step 251/334, epoch 161/501 --> loss:0.11866109251976013
step 301/334, epoch 161/501 --> loss:0.13156856179237367

##########train dataset##########
acc--> [99.2101788744274]
F1--> {'F1': [0.9029612630022492], 'precision': [0.9368858609639459], 'recall': [0.8714169301943839]}
##########eval dataset##########
acc--> [98.83874541087587]
F1--> {'F1': [0.8587996362312286], 'precision': [0.905000265980055], 'recall': [0.8170960421706002]}
step 51/334, epoch 162/501 --> loss:0.090994553565979
step 101/334, epoch 162/501 --> loss:0.13489139199256897
step 151/334, epoch 162/501 --> loss:0.08671566486358642
step 201/334, epoch 162/501 --> loss:0.07452254056930542
step 251/334, epoch 162/501 --> loss:0.14745930671691895
step 301/334, epoch 162/501 --> loss:0.09698850631713868
step 51/334, epoch 163/501 --> loss:0.07662135839462281
step 101/334, epoch 163/501 --> loss:0.08026216506958007
step 151/334, epoch 163/501 --> loss:0.07262226343154907
step 201/334, epoch 163/501 --> loss:0.06450947642326355
step 251/334, epoch 163/501 --> loss:0.0695675241947174
step 301/334, epoch 163/501 --> loss:0.06846439123153686
step 51/334, epoch 164/501 --> loss:0.06723387837409973
step 101/334, epoch 164/501 --> loss:0.05830283999443054
step 151/334, epoch 164/501 --> loss:0.0571571159362793
step 201/334, epoch 164/501 --> loss:0.0633487331867218
step 251/334, epoch 164/501 --> loss:0.06004640102386474
step 301/334, epoch 164/501 --> loss:0.054267154932022096
step 51/334, epoch 165/501 --> loss:0.06626697540283204
step 101/334, epoch 165/501 --> loss:0.053817610740661624
step 151/334, epoch 165/501 --> loss:0.059851633310317995
step 201/334, epoch 165/501 --> loss:0.05823899269104004
step 251/334, epoch 165/501 --> loss:0.058130930662155154
step 301/334, epoch 165/501 --> loss:0.06723228335380554
step 51/334, epoch 166/501 --> loss:0.04973502278327942
step 101/334, epoch 166/501 --> loss:0.06001368045806885
step 151/334, epoch 166/501 --> loss:0.06858725309371948
step 201/334, epoch 166/501 --> loss:0.04966758012771606
step 251/334, epoch 166/501 --> loss:0.058032948970794675
step 301/334, epoch 166/501 --> loss:0.07592430949211121
step 51/334, epoch 167/501 --> loss:0.055227710008621214
step 101/334, epoch 167/501 --> loss:0.059530091285705564
step 151/334, epoch 167/501 --> loss:0.06029359698295593
step 201/334, epoch 167/501 --> loss:0.05482694029808045
step 251/334, epoch 167/501 --> loss:0.0653642475605011
step 301/334, epoch 167/501 --> loss:0.057358384132385254
step 51/334, epoch 168/501 --> loss:0.05799261450767517
step 101/334, epoch 168/501 --> loss:0.06940696001052857
step 151/334, epoch 168/501 --> loss:0.05680938959121704
step 201/334, epoch 168/501 --> loss:0.05288367390632629
step 251/334, epoch 168/501 --> loss:0.05956288456916809
step 301/334, epoch 168/501 --> loss:0.05356221318244934
step 51/334, epoch 169/501 --> loss:0.04476622581481934
step 101/334, epoch 169/501 --> loss:0.05262570023536682
step 151/334, epoch 169/501 --> loss:0.058855504989624025
step 201/334, epoch 169/501 --> loss:0.04899818897247314
step 251/334, epoch 169/501 --> loss:0.07604838848114014
step 301/334, epoch 169/501 --> loss:0.0488282310962677
step 51/334, epoch 170/501 --> loss:0.058003122806549075
step 101/334, epoch 170/501 --> loss:0.07052103400230408
step 151/334, epoch 170/501 --> loss:0.0847190248966217
step 201/334, epoch 170/501 --> loss:0.05848954439163208
step 251/334, epoch 170/501 --> loss:0.05720731019973755
step 301/334, epoch 170/501 --> loss:0.05499311566352844
step 51/334, epoch 171/501 --> loss:0.05015805840492248
step 101/334, epoch 171/501 --> loss:0.07666481137275696
step 151/334, epoch 171/501 --> loss:0.04500430107116699
step 201/334, epoch 171/501 --> loss:0.05877600312232971
step 251/334, epoch 171/501 --> loss:0.06191965818405151
step 301/334, epoch 171/501 --> loss:0.05402851939201355

##########train dataset##########
acc--> [99.61295369237007]
F1--> {'F1': [0.9533111596363439], 'precision': [0.9701129814196415], 'recall': [0.9370910856799192]}
##########eval dataset##########
acc--> [99.13806531476604]
F1--> {'F1': [0.8965632755649542], 'precision': [0.9313135895130841], 'recall': [0.8643222513885026]}
step 51/334, epoch 172/501 --> loss:0.05260172367095947
step 101/334, epoch 172/501 --> loss:0.04112330436706543
step 151/334, epoch 172/501 --> loss:0.055581200122833255
step 201/334, epoch 172/501 --> loss:0.058634895086288455
step 251/334, epoch 172/501 --> loss:0.05658789157867432
step 301/334, epoch 172/501 --> loss:0.048647119998931884
step 51/334, epoch 173/501 --> loss:0.05030733227729797
step 101/334, epoch 173/501 --> loss:0.06300777554512024
step 151/334, epoch 173/501 --> loss:0.04543920993804931
step 201/334, epoch 173/501 --> loss:0.06337345004081726
step 251/334, epoch 173/501 --> loss:0.05381922483444214
step 301/334, epoch 173/501 --> loss:0.05349043607711792
step 51/334, epoch 174/501 --> loss:0.04558513760566711
step 101/334, epoch 174/501 --> loss:0.05715161442756653
step 151/334, epoch 174/501 --> loss:0.07022446513175964
step 201/334, epoch 174/501 --> loss:0.060304735898971555
step 251/334, epoch 174/501 --> loss:0.06058879017829895
step 301/334, epoch 174/501 --> loss:0.05645069003105164
step 51/334, epoch 175/501 --> loss:0.04666187882423401
step 101/334, epoch 175/501 --> loss:0.06281000256538391
step 151/334, epoch 175/501 --> loss:0.05558902978897095
step 201/334, epoch 175/501 --> loss:0.053484275341033935
step 251/334, epoch 175/501 --> loss:0.057382781505584714
step 301/334, epoch 175/501 --> loss:0.06207818627357483
step 51/334, epoch 176/501 --> loss:0.05422005534172058
step 101/334, epoch 176/501 --> loss:0.05878978133201599
step 151/334, epoch 176/501 --> loss:0.04470041990280151
step 201/334, epoch 176/501 --> loss:0.05908539533615112
step 251/334, epoch 176/501 --> loss:0.06542463898658753
step 301/334, epoch 176/501 --> loss:0.06985023736953735
step 51/334, epoch 177/501 --> loss:0.059100462198257445
step 101/334, epoch 177/501 --> loss:0.05129357933998108
step 151/334, epoch 177/501 --> loss:0.0528848397731781
step 201/334, epoch 177/501 --> loss:0.04333758592605591
step 251/334, epoch 177/501 --> loss:0.06249997973442078
step 301/334, epoch 177/501 --> loss:0.052099344730377195
step 51/334, epoch 178/501 --> loss:0.04529617428779602
step 101/334, epoch 178/501 --> loss:0.05034021019935608
step 151/334, epoch 178/501 --> loss:0.053226977586746216
step 201/334, epoch 178/501 --> loss:0.0546647322177887
step 251/334, epoch 178/501 --> loss:0.050212690830230715
step 301/334, epoch 178/501 --> loss:0.05387871861457825
step 51/334, epoch 179/501 --> loss:0.04801042079925537
step 101/334, epoch 179/501 --> loss:0.041724344491958616
step 151/334, epoch 179/501 --> loss:0.05501194596290589
step 201/334, epoch 179/501 --> loss:0.045607211589813235
step 251/334, epoch 179/501 --> loss:0.038905454874038695
step 301/334, epoch 179/501 --> loss:0.052921104431152347
step 51/334, epoch 180/501 --> loss:0.053533179759979246
step 101/334, epoch 180/501 --> loss:0.046417523622512814
step 151/334, epoch 180/501 --> loss:0.05925451993942261
step 201/334, epoch 180/501 --> loss:0.042126389741897585
step 251/334, epoch 180/501 --> loss:0.04366782307624817
step 301/334, epoch 180/501 --> loss:0.04922392964363098
step 51/334, epoch 181/501 --> loss:0.04164874196052551
step 101/334, epoch 181/501 --> loss:0.04232827186584473
step 151/334, epoch 181/501 --> loss:0.049419212341308597
step 201/334, epoch 181/501 --> loss:0.05243527173995972
step 251/334, epoch 181/501 --> loss:0.03979169487953186
step 301/334, epoch 181/501 --> loss:0.061951855421066286

##########train dataset##########
acc--> [99.40829139368769]
F1--> {'F1': [0.9280168551576922], 'precision': [0.9527873667292395], 'recall': [0.9045111655297141]}
##########eval dataset##########
acc--> [98.94576786070635]
F1--> {'F1': [0.8710991610417691], 'precision': [0.9236533576027184], 'recall': [0.8242124048938211]}
step 51/334, epoch 182/501 --> loss:0.20290448546409606
step 101/334, epoch 182/501 --> loss:0.11228179693222046
step 151/334, epoch 182/501 --> loss:0.07576489686965943
step 201/334, epoch 182/501 --> loss:0.07930041313171386
step 251/334, epoch 182/501 --> loss:0.10037091612815857
step 301/334, epoch 182/501 --> loss:0.06381924152374267
step 51/334, epoch 183/501 --> loss:0.07414160370826721
step 101/334, epoch 183/501 --> loss:0.07105083703994751
step 151/334, epoch 183/501 --> loss:0.06227403998374939
step 201/334, epoch 183/501 --> loss:0.06747905611991882
step 251/334, epoch 183/501 --> loss:0.05091474294662476
step 301/334, epoch 183/501 --> loss:0.049038436412811276
step 51/334, epoch 184/501 --> loss:0.04834676742553711
step 101/334, epoch 184/501 --> loss:0.05890191674232483
step 151/334, epoch 184/501 --> loss:0.0593193256855011
step 201/334, epoch 184/501 --> loss:0.049758608341217044
step 251/334, epoch 184/501 --> loss:0.048039817810058595
step 301/334, epoch 184/501 --> loss:0.04449164628982544
step 51/334, epoch 185/501 --> loss:0.06759129166603088
step 101/334, epoch 185/501 --> loss:0.041470162868499756
step 151/334, epoch 185/501 --> loss:0.05985066652297974
step 201/334, epoch 185/501 --> loss:0.05066709280014038
step 251/334, epoch 185/501 --> loss:0.05689992189407349
step 301/334, epoch 185/501 --> loss:0.06295859456062317
step 51/334, epoch 186/501 --> loss:0.04791117906570434
step 101/334, epoch 186/501 --> loss:0.06033652663230896
step 151/334, epoch 186/501 --> loss:0.04311455845832825
step 201/334, epoch 186/501 --> loss:0.050977720022201536
step 251/334, epoch 186/501 --> loss:0.04083442449569702
step 301/334, epoch 186/501 --> loss:0.05878642320632935
step 51/334, epoch 187/501 --> loss:0.054036595821380616
step 101/334, epoch 187/501 --> loss:0.04296237826347351
step 151/334, epoch 187/501 --> loss:0.049555050134658815
step 201/334, epoch 187/501 --> loss:0.06493590831756592
step 251/334, epoch 187/501 --> loss:0.05362738609313965
step 301/334, epoch 187/501 --> loss:0.042829341888427734
step 51/334, epoch 188/501 --> loss:0.04231627941131592
step 101/334, epoch 188/501 --> loss:0.06253641128540038
step 151/334, epoch 188/501 --> loss:0.05855313062667847
step 201/334, epoch 188/501 --> loss:0.05039527058601379
step 251/334, epoch 188/501 --> loss:0.04423308491706848
step 301/334, epoch 188/501 --> loss:0.041550089120864865
step 51/334, epoch 189/501 --> loss:0.05068887591361999
step 101/334, epoch 189/501 --> loss:0.04064719557762146
step 151/334, epoch 189/501 --> loss:0.04815245151519775
step 201/334, epoch 189/501 --> loss:0.04200765252113342
step 251/334, epoch 189/501 --> loss:0.058223506212234495
step 301/334, epoch 189/501 --> loss:0.05311186194419861
step 51/334, epoch 190/501 --> loss:0.05537057161331177
step 101/334, epoch 190/501 --> loss:0.05677331566810608
step 151/334, epoch 190/501 --> loss:0.059416618347167965
step 201/334, epoch 190/501 --> loss:0.04313956022262573
step 251/334, epoch 190/501 --> loss:0.04968176245689392
step 301/334, epoch 190/501 --> loss:0.0583396303653717
step 51/334, epoch 191/501 --> loss:0.06167608261108398
step 101/334, epoch 191/501 --> loss:0.0627489686012268
step 151/334, epoch 191/501 --> loss:0.07782001852989197
step 201/334, epoch 191/501 --> loss:0.06597384929656983
step 251/334, epoch 191/501 --> loss:0.09149818658828736
step 301/334, epoch 191/501 --> loss:0.07109535336494446

##########train dataset##########
acc--> [99.46309001337316]
F1--> {'F1': [0.935662572186601], 'precision': [0.9456933652732469], 'recall': [0.9258521253617651]}
##########eval dataset##########
acc--> [99.00073688069213]
F1--> {'F1': [0.8814842537883804], 'precision': [0.9042716041227089], 'recall': [0.859826649850461]}
step 51/334, epoch 192/501 --> loss:0.06620726823806762
step 101/334, epoch 192/501 --> loss:0.07987011909484863
step 151/334, epoch 192/501 --> loss:0.053226355314254764
step 201/334, epoch 192/501 --> loss:0.055324195623397826
step 251/334, epoch 192/501 --> loss:0.051412006616592405
step 301/334, epoch 192/501 --> loss:0.049816263914108275
step 51/334, epoch 193/501 --> loss:0.05499473571777344
step 101/334, epoch 193/501 --> loss:0.05415784478187561
step 151/334, epoch 193/501 --> loss:0.049597887992858886
step 201/334, epoch 193/501 --> loss:0.1165474545955658
step 251/334, epoch 193/501 --> loss:0.08053631544113159
step 301/334, epoch 193/501 --> loss:0.07168704628944397
step 51/334, epoch 194/501 --> loss:0.06481227040290832
step 101/334, epoch 194/501 --> loss:0.06161984205245972
step 151/334, epoch 194/501 --> loss:0.04605483293533325
step 201/334, epoch 194/501 --> loss:0.05793022751808166
step 251/334, epoch 194/501 --> loss:0.04251254916191101
step 301/334, epoch 194/501 --> loss:0.04571122765541077
step 51/334, epoch 195/501 --> loss:0.050029548406600954
step 101/334, epoch 195/501 --> loss:0.05303378939628601
step 151/334, epoch 195/501 --> loss:0.055005686283111574
step 201/334, epoch 195/501 --> loss:0.056255823373794554
step 251/334, epoch 195/501 --> loss:0.04106295585632324
step 301/334, epoch 195/501 --> loss:0.05129305839538574
step 51/334, epoch 196/501 --> loss:0.04419863343238831
step 101/334, epoch 196/501 --> loss:0.04813636779785156
step 151/334, epoch 196/501 --> loss:0.06447880268096924
step 201/334, epoch 196/501 --> loss:0.0610646378993988
step 251/334, epoch 196/501 --> loss:0.04945639967918396
step 301/334, epoch 196/501 --> loss:0.054301280975341794
step 51/334, epoch 197/501 --> loss:0.05113403558731079
step 101/334, epoch 197/501 --> loss:0.04494120240211487
step 151/334, epoch 197/501 --> loss:0.05450674295425415
step 201/334, epoch 197/501 --> loss:0.0508601987361908
step 251/334, epoch 197/501 --> loss:0.06035062432289123
step 301/334, epoch 197/501 --> loss:0.05459575653076172
step 51/334, epoch 198/501 --> loss:0.0594453489780426
step 101/334, epoch 198/501 --> loss:0.04007835626602173
step 151/334, epoch 198/501 --> loss:0.05693731904029846
step 201/334, epoch 198/501 --> loss:0.06007478833198547
step 251/334, epoch 198/501 --> loss:0.08222677707672119
step 301/334, epoch 198/501 --> loss:0.061582863330841064
step 51/334, epoch 199/501 --> loss:0.06443119287490845
step 101/334, epoch 199/501 --> loss:0.057380937337875366
step 151/334, epoch 199/501 --> loss:0.0639159083366394
step 201/334, epoch 199/501 --> loss:0.05623842000961304
step 251/334, epoch 199/501 --> loss:0.07676689743995667
step 301/334, epoch 199/501 --> loss:0.08794694304466248
step 51/334, epoch 200/501 --> loss:0.06257025480270385
step 101/334, epoch 200/501 --> loss:0.054945855140686034
step 151/334, epoch 200/501 --> loss:0.04597244501113892
step 201/334, epoch 200/501 --> loss:0.0471574330329895
step 251/334, epoch 200/501 --> loss:0.05937066197395325
step 301/334, epoch 200/501 --> loss:0.06533129334449768
step 51/334, epoch 201/501 --> loss:0.04936542987823486
step 101/334, epoch 201/501 --> loss:0.057905758619308474
step 151/334, epoch 201/501 --> loss:0.042889622449874876
step 201/334, epoch 201/501 --> loss:0.06089923739433289
step 251/334, epoch 201/501 --> loss:0.04024600386619568
step 301/334, epoch 201/501 --> loss:0.05174033284187317

##########train dataset##########
acc--> [99.67416821233853]
F1--> {'F1': [0.9608289743273862], 'precision': [0.9743142263932464], 'recall': [0.9477216454579276]}
##########eval dataset##########
acc--> [99.17299699490636]
F1--> {'F1': [0.901495351299858], 'precision': [0.9289699149932684], 'recall': [0.8756086667647706]}
save model!
step 51/334, epoch 202/501 --> loss:0.0521864926815033
step 101/334, epoch 202/501 --> loss:0.05063614368438721
step 151/334, epoch 202/501 --> loss:0.04327231884002686
step 201/334, epoch 202/501 --> loss:0.038073257207870484
step 251/334, epoch 202/501 --> loss:0.04521460175514221
step 301/334, epoch 202/501 --> loss:0.05112003803253174
step 51/334, epoch 203/501 --> loss:0.06288302898406982
step 101/334, epoch 203/501 --> loss:0.07376808404922486
step 151/334, epoch 203/501 --> loss:0.06388465285301209
step 201/334, epoch 203/501 --> loss:0.052442733049392704
step 251/334, epoch 203/501 --> loss:0.051942379474639894
step 301/334, epoch 203/501 --> loss:0.04847926616668701
step 51/334, epoch 204/501 --> loss:0.05103418350219727
step 101/334, epoch 204/501 --> loss:0.04865302443504334
step 151/334, epoch 204/501 --> loss:0.048168199062347414
step 201/334, epoch 204/501 --> loss:0.04124978184700012
step 251/334, epoch 204/501 --> loss:0.08862372517585754
step 301/334, epoch 204/501 --> loss:0.057713502645492555
step 51/334, epoch 205/501 --> loss:0.03990625977516174
step 101/334, epoch 205/501 --> loss:0.0481234335899353
step 151/334, epoch 205/501 --> loss:0.05976971864700317
step 201/334, epoch 205/501 --> loss:0.051990244388580326
step 251/334, epoch 205/501 --> loss:0.04648924827575684
step 301/334, epoch 205/501 --> loss:0.057901294231414796
step 51/334, epoch 206/501 --> loss:0.04045063376426697
step 101/334, epoch 206/501 --> loss:0.04738173246383667
step 151/334, epoch 206/501 --> loss:0.03520976424217224
step 201/334, epoch 206/501 --> loss:0.05067150831222534
step 251/334, epoch 206/501 --> loss:0.04155802130699158
step 301/334, epoch 206/501 --> loss:0.044850846529006956
step 51/334, epoch 207/501 --> loss:0.040138550996780396
step 101/334, epoch 207/501 --> loss:0.03753303527832031
step 151/334, epoch 207/501 --> loss:0.056943997144699096
step 201/334, epoch 207/501 --> loss:0.06012375116348267
step 251/334, epoch 207/501 --> loss:0.06736758708953858
step 301/334, epoch 207/501 --> loss:0.06602799654006958
step 51/334, epoch 208/501 --> loss:0.06392467737197877
step 101/334, epoch 208/501 --> loss:0.04271264433860779
step 151/334, epoch 208/501 --> loss:0.04031391739845276
step 201/334, epoch 208/501 --> loss:0.048178086280822756
step 251/334, epoch 208/501 --> loss:0.04493994474411011
step 301/334, epoch 208/501 --> loss:0.0401105272769928
step 51/334, epoch 209/501 --> loss:0.06219698309898376
step 101/334, epoch 209/501 --> loss:0.041283752918243405
step 151/334, epoch 209/501 --> loss:0.06389128088951111
step 201/334, epoch 209/501 --> loss:0.043709125518798825
step 251/334, epoch 209/501 --> loss:0.041806726455688475
step 301/334, epoch 209/501 --> loss:0.0484889817237854
step 51/334, epoch 210/501 --> loss:0.04516892910003662
step 101/334, epoch 210/501 --> loss:0.039240199327468875
step 151/334, epoch 210/501 --> loss:0.04575096368789673
step 201/334, epoch 210/501 --> loss:0.04107283473014831
step 251/334, epoch 210/501 --> loss:0.06823819160461425
step 301/334, epoch 210/501 --> loss:0.04226135969161987
step 51/334, epoch 211/501 --> loss:0.0402511739730835
step 101/334, epoch 211/501 --> loss:0.04335150718688965
step 151/334, epoch 211/501 --> loss:0.04628891348838806
step 201/334, epoch 211/501 --> loss:0.048902667760849
step 251/334, epoch 211/501 --> loss:0.05488614439964294
step 301/334, epoch 211/501 --> loss:0.04668151259422302

##########train dataset##########
acc--> [99.68623764558441]
F1--> {'F1': [0.9622779497125435], 'precision': [0.9758316555856607], 'recall': [0.9491053174634906]}
##########eval dataset##########
acc--> [99.14257286518333]
F1--> {'F1': [0.8969246254850537], 'precision': [0.9334441495231413], 'recall': [0.8631642980964299]}
step 51/334, epoch 212/501 --> loss:0.04701836228370666
step 101/334, epoch 212/501 --> loss:0.04069413185119629
step 151/334, epoch 212/501 --> loss:0.0495000696182251
step 201/334, epoch 212/501 --> loss:0.04647091269493103
step 251/334, epoch 212/501 --> loss:0.03943111300468445
step 301/334, epoch 212/501 --> loss:0.042299633026123044
step 51/334, epoch 213/501 --> loss:0.0472374677658081
step 101/334, epoch 213/501 --> loss:0.04618030786514282
step 151/334, epoch 213/501 --> loss:0.03948355078697205
step 201/334, epoch 213/501 --> loss:0.04980547428131103
step 251/334, epoch 213/501 --> loss:0.0389648973941803
step 301/334, epoch 213/501 --> loss:0.037573426961898804
step 51/334, epoch 214/501 --> loss:0.0366063129901886
step 101/334, epoch 214/501 --> loss:0.03667652368545532
step 151/334, epoch 214/501 --> loss:0.03903391122817993
step 201/334, epoch 214/501 --> loss:0.056680080890655515
step 251/334, epoch 214/501 --> loss:0.052315176725387574
step 301/334, epoch 214/501 --> loss:0.04815827250480652
step 51/334, epoch 215/501 --> loss:0.04411671757698059
step 101/334, epoch 215/501 --> loss:0.053665217161178586
step 151/334, epoch 215/501 --> loss:0.052166892290115355
step 201/334, epoch 215/501 --> loss:0.04075681924819946
step 251/334, epoch 215/501 --> loss:0.04242514610290527
step 301/334, epoch 215/501 --> loss:0.03534564256668091
step 51/334, epoch 216/501 --> loss:0.03853615880012512
step 101/334, epoch 216/501 --> loss:0.04204490184783936
step 151/334, epoch 216/501 --> loss:0.050444281101226805
step 201/334, epoch 216/501 --> loss:0.03984007239341736
step 251/334, epoch 216/501 --> loss:0.046612107753753663
step 301/334, epoch 216/501 --> loss:0.039252231121063234
step 51/334, epoch 217/501 --> loss:0.03741476416587829
step 101/334, epoch 217/501 --> loss:0.04242313027381897
step 151/334, epoch 217/501 --> loss:0.03546398520469665
step 201/334, epoch 217/501 --> loss:0.04671948909759521
step 251/334, epoch 217/501 --> loss:0.043795363903045656
step 301/334, epoch 217/501 --> loss:0.033948380947113034
step 51/334, epoch 218/501 --> loss:0.035975260734558104
step 101/334, epoch 218/501 --> loss:0.035200507640838624
step 151/334, epoch 218/501 --> loss:0.04211069107055664
step 201/334, epoch 218/501 --> loss:0.04962313294410706
step 251/334, epoch 218/501 --> loss:0.03696388363838196
step 301/334, epoch 218/501 --> loss:0.04297764539718628
step 51/334, epoch 219/501 --> loss:0.046446919441223145
step 101/334, epoch 219/501 --> loss:0.04612020254135132
step 151/334, epoch 219/501 --> loss:0.03689536571502686
step 201/334, epoch 219/501 --> loss:0.04526357889175415
step 251/334, epoch 219/501 --> loss:0.03660153269767761
step 301/334, epoch 219/501 --> loss:0.04411034941673279
step 51/334, epoch 220/501 --> loss:0.04618920087814331
step 101/334, epoch 220/501 --> loss:0.04566437125205994
step 151/334, epoch 220/501 --> loss:0.09275108098983764
step 201/334, epoch 220/501 --> loss:0.07263491630554199
step 251/334, epoch 220/501 --> loss:0.08133965015411376
step 301/334, epoch 220/501 --> loss:0.059906085729599
step 51/334, epoch 221/501 --> loss:0.061208890676498415
step 101/334, epoch 221/501 --> loss:0.07145017027854919
step 151/334, epoch 221/501 --> loss:0.05532560467720032
step 201/334, epoch 221/501 --> loss:0.0671847128868103
step 251/334, epoch 221/501 --> loss:0.045335493087768554
step 301/334, epoch 221/501 --> loss:0.04748786449432373

##########train dataset##########
acc--> [99.68633748280568]
F1--> {'F1': [0.962342279913542], 'precision': [0.9745063355292545], 'recall': [0.9504879041297023]}
##########eval dataset##########
acc--> [99.11685986989612]
F1--> {'F1': [0.8949235148580486], 'precision': [0.9211355115703351], 'recall': [0.8701714753065146]}
step 51/334, epoch 222/501 --> loss:0.03841405034065246
step 101/334, epoch 222/501 --> loss:0.05456092476844788
step 151/334, epoch 222/501 --> loss:0.05447281718254089
step 201/334, epoch 222/501 --> loss:0.0563569438457489
step 251/334, epoch 222/501 --> loss:0.04761183261871338
step 301/334, epoch 222/501 --> loss:0.04753351807594299
step 51/334, epoch 223/501 --> loss:0.04089268684387207
step 101/334, epoch 223/501 --> loss:0.05052682042121887
step 151/334, epoch 223/501 --> loss:0.045851203203201296
step 201/334, epoch 223/501 --> loss:0.05142975687980652
step 251/334, epoch 223/501 --> loss:0.040910381078720096
step 301/334, epoch 223/501 --> loss:0.042861875295639035
step 51/334, epoch 224/501 --> loss:0.0472741436958313
step 101/334, epoch 224/501 --> loss:0.05217841982841492
step 151/334, epoch 224/501 --> loss:0.03158264279365539
step 201/334, epoch 224/501 --> loss:0.04869789481163025
step 251/334, epoch 224/501 --> loss:0.03817303419113159
step 301/334, epoch 224/501 --> loss:0.0434438693523407
step 51/334, epoch 225/501 --> loss:0.031186403036117555
step 101/334, epoch 225/501 --> loss:0.03741046071052551
step 151/334, epoch 225/501 --> loss:0.04223068475723266
step 201/334, epoch 225/501 --> loss:0.044678057432174685
step 251/334, epoch 225/501 --> loss:0.03807693958282471
step 301/334, epoch 225/501 --> loss:0.04873785138130188
step 51/334, epoch 226/501 --> loss:0.08447904348373413
step 101/334, epoch 226/501 --> loss:0.06774629116058349
step 151/334, epoch 226/501 --> loss:0.08896458864212037
step 201/334, epoch 226/501 --> loss:0.07438782334327698
step 251/334, epoch 226/501 --> loss:0.06234542965888977
step 301/334, epoch 226/501 --> loss:0.06121658682823181
step 51/334, epoch 227/501 --> loss:0.04727170705795288
step 101/334, epoch 227/501 --> loss:0.03878394246101379
step 151/334, epoch 227/501 --> loss:0.05857352137565613
step 201/334, epoch 227/501 --> loss:0.046340545415878294
step 251/334, epoch 227/501 --> loss:0.04164074063301086
step 301/334, epoch 227/501 --> loss:0.04451756000518799
step 51/334, epoch 228/501 --> loss:0.04492888331413269
step 101/334, epoch 228/501 --> loss:0.04483086347579956
step 151/334, epoch 228/501 --> loss:0.03809886455535889
step 201/334, epoch 228/501 --> loss:0.047794328927993776
step 251/334, epoch 228/501 --> loss:0.04304052710533142
step 301/334, epoch 228/501 --> loss:0.037380741834640505
step 51/334, epoch 229/501 --> loss:0.031242663860321044
step 101/334, epoch 229/501 --> loss:0.04760445713996887
step 151/334, epoch 229/501 --> loss:0.03449234962463379
step 201/334, epoch 229/501 --> loss:0.04289678692817688
step 251/334, epoch 229/501 --> loss:0.03263336777687073
step 301/334, epoch 229/501 --> loss:0.041485604047775265
step 51/334, epoch 230/501 --> loss:0.03208310127258301
step 101/334, epoch 230/501 --> loss:0.03413299560546875
step 151/334, epoch 230/501 --> loss:0.042228798866271976
step 201/334, epoch 230/501 --> loss:0.05067289113998413
step 251/334, epoch 230/501 --> loss:0.037792820930480954
step 301/334, epoch 230/501 --> loss:0.040773754119873044
step 51/334, epoch 231/501 --> loss:0.04424713730812073
step 101/334, epoch 231/501 --> loss:0.04351707696914673
step 151/334, epoch 231/501 --> loss:0.04578284740447998
step 201/334, epoch 231/501 --> loss:0.033588978052139284
step 251/334, epoch 231/501 --> loss:0.03914087891578674
step 301/334, epoch 231/501 --> loss:0.03600358724594116

##########train dataset##########
acc--> [99.7234165690808]
F1--> {'F1': [0.9668171618716281], 'precision': [0.9783178687985787], 'recall': [0.9555934761059112]}
##########eval dataset##########
acc--> [99.19274397721716]
F1--> {'F1': [0.9030630333713496], 'precision': [0.9387089996303614], 'recall': [0.8700344915014908]}
save model!
step 51/334, epoch 232/501 --> loss:0.036222809553146364
step 101/334, epoch 232/501 --> loss:0.037440227270126344
step 151/334, epoch 232/501 --> loss:0.03289866328239441
step 201/334, epoch 232/501 --> loss:0.04185551285743713
step 251/334, epoch 232/501 --> loss:0.03330295085906983
step 301/334, epoch 232/501 --> loss:0.04867685317993164
step 51/334, epoch 233/501 --> loss:0.04739307641983032
step 101/334, epoch 233/501 --> loss:0.032918126583099366
step 151/334, epoch 233/501 --> loss:0.04258935689926147
step 201/334, epoch 233/501 --> loss:0.09264315366744995
step 251/334, epoch 233/501 --> loss:0.08337031841278077
step 301/334, epoch 233/501 --> loss:0.0917111098766327
step 51/334, epoch 234/501 --> loss:0.09080004811286926
step 101/334, epoch 234/501 --> loss:0.05972435474395752
step 151/334, epoch 234/501 --> loss:0.06841224312782287
step 201/334, epoch 234/501 --> loss:0.05774227261543274
step 251/334, epoch 234/501 --> loss:0.06400971889495849
step 301/334, epoch 234/501 --> loss:0.04792371034622192
step 51/334, epoch 235/501 --> loss:0.061898363828659056
step 101/334, epoch 235/501 --> loss:0.05413660168647766
step 151/334, epoch 235/501 --> loss:0.050139060020446775
step 201/334, epoch 235/501 --> loss:0.04392453193664551
step 251/334, epoch 235/501 --> loss:0.059088789224624634
step 301/334, epoch 235/501 --> loss:0.07465221047401428
step 51/334, epoch 236/501 --> loss:0.05095777153968811
step 101/334, epoch 236/501 --> loss:0.06478156685829163
step 151/334, epoch 236/501 --> loss:0.06455005288124084
step 201/334, epoch 236/501 --> loss:0.05364038586616516
step 251/334, epoch 236/501 --> loss:0.07076916694641114
step 301/334, epoch 236/501 --> loss:0.04280585050582886
step 51/334, epoch 237/501 --> loss:0.04190945625305176
step 101/334, epoch 237/501 --> loss:0.067605140209198
step 151/334, epoch 237/501 --> loss:0.0572773551940918
step 201/334, epoch 237/501 --> loss:0.04419999241828918
step 251/334, epoch 237/501 --> loss:0.04467360019683838
step 301/334, epoch 237/501 --> loss:0.039225916862487796
step 51/334, epoch 238/501 --> loss:0.04250365495681763
step 101/334, epoch 238/501 --> loss:0.08235860228538514
step 151/334, epoch 238/501 --> loss:0.08720170140266419
step 201/334, epoch 238/501 --> loss:0.06126347661018371
step 251/334, epoch 238/501 --> loss:0.057322288751602175
step 301/334, epoch 238/501 --> loss:0.05288573980331421
step 51/334, epoch 239/501 --> loss:0.04480223655700684
step 101/334, epoch 239/501 --> loss:0.0418543267250061
step 151/334, epoch 239/501 --> loss:0.04651261925697327
step 201/334, epoch 239/501 --> loss:0.051380083560943604
step 251/334, epoch 239/501 --> loss:0.046614373922348025
step 301/334, epoch 239/501 --> loss:0.06141925096511841
step 51/334, epoch 240/501 --> loss:0.041692955493927
step 101/334, epoch 240/501 --> loss:0.04452792763710022
step 151/334, epoch 240/501 --> loss:0.0525102174282074
step 201/334, epoch 240/501 --> loss:0.04072306752204895
step 251/334, epoch 240/501 --> loss:0.03697452664375305
step 301/334, epoch 240/501 --> loss:0.03251476407051086
step 51/334, epoch 241/501 --> loss:0.04716004252433777
step 101/334, epoch 241/501 --> loss:0.041850190162658694
step 151/334, epoch 241/501 --> loss:0.03879555821418762
step 201/334, epoch 241/501 --> loss:0.03917421698570252
step 251/334, epoch 241/501 --> loss:0.04314979910850525
step 301/334, epoch 241/501 --> loss:0.04252447843551636

##########train dataset##########
acc--> [99.71505083928432]
F1--> {'F1': [0.9658775724183036], 'precision': [0.9754992229540647], 'recall': [0.9564536753755721]}
##########eval dataset##########
acc--> [99.17921865601097]
F1--> {'F1': [0.9019964250400396], 'precision': [0.9319130884694906], 'recall': [0.8739501897837665]}
step 51/334, epoch 242/501 --> loss:0.04792209267616272
step 101/334, epoch 242/501 --> loss:0.04169954299926758
step 151/334, epoch 242/501 --> loss:0.03794337749481201
step 201/334, epoch 242/501 --> loss:0.048416571617126467
step 251/334, epoch 242/501 --> loss:0.03563133239746094
step 301/334, epoch 242/501 --> loss:0.07402646660804749
step 51/334, epoch 243/501 --> loss:0.04687966108322143
step 101/334, epoch 243/501 --> loss:0.03839688062667847
step 151/334, epoch 243/501 --> loss:0.04539432525634766
step 201/334, epoch 243/501 --> loss:0.04433218240737915
step 251/334, epoch 243/501 --> loss:0.039160200357437135
step 301/334, epoch 243/501 --> loss:0.042194753885269165
step 51/334, epoch 244/501 --> loss:0.03738012671470642
step 101/334, epoch 244/501 --> loss:0.04167755722999573
step 151/334, epoch 244/501 --> loss:0.042301994562149045
step 201/334, epoch 244/501 --> loss:0.047999564409255985
step 251/334, epoch 244/501 --> loss:0.036141198873519895
step 301/334, epoch 244/501 --> loss:0.03660377502441406
step 51/334, epoch 245/501 --> loss:0.03219659447669983
step 101/334, epoch 245/501 --> loss:0.05276609182357788
step 151/334, epoch 245/501 --> loss:0.04107300281524658
step 201/334, epoch 245/501 --> loss:0.03894177198410034
step 251/334, epoch 245/501 --> loss:0.0580487060546875
step 301/334, epoch 245/501 --> loss:0.06307540059089661
step 51/334, epoch 246/501 --> loss:0.04088638424873352
step 101/334, epoch 246/501 --> loss:0.03969409704208374
step 151/334, epoch 246/501 --> loss:0.04121241331100464
step 201/334, epoch 246/501 --> loss:0.05581508874893189
step 251/334, epoch 246/501 --> loss:0.052137094736099246
step 301/334, epoch 246/501 --> loss:0.03610137462615967
step 51/334, epoch 247/501 --> loss:0.04439660906791687
step 101/334, epoch 247/501 --> loss:0.03658450722694397
step 151/334, epoch 247/501 --> loss:0.03476898431777954
step 201/334, epoch 247/501 --> loss:0.03657259702682495
step 251/334, epoch 247/501 --> loss:0.03231747031211853
step 301/334, epoch 247/501 --> loss:0.04917804956436157
step 51/334, epoch 248/501 --> loss:0.03607648849487305
step 101/334, epoch 248/501 --> loss:0.04196837902069092
step 151/334, epoch 248/501 --> loss:0.030711456537246704
step 201/334, epoch 248/501 --> loss:0.03431573033332825
step 251/334, epoch 248/501 --> loss:0.042918142080307004
step 301/334, epoch 248/501 --> loss:0.051178817749023435
step 51/334, epoch 249/501 --> loss:0.054073115587234495
step 101/334, epoch 249/501 --> loss:0.04458131194114685
step 151/334, epoch 249/501 --> loss:0.036459555625915525
step 201/334, epoch 249/501 --> loss:0.04139598965644836
step 251/334, epoch 249/501 --> loss:0.03868002533912659
step 301/334, epoch 249/501 --> loss:0.04610467791557312
step 51/334, epoch 250/501 --> loss:0.051765402555465696
step 101/334, epoch 250/501 --> loss:0.03252131700515747
step 151/334, epoch 250/501 --> loss:0.04328578352928161
step 201/334, epoch 250/501 --> loss:0.033899896144866944
step 251/334, epoch 250/501 --> loss:0.03751225352287293
step 301/334, epoch 250/501 --> loss:0.05563793182373047
step 51/334, epoch 251/501 --> loss:0.04441811680793762
step 101/334, epoch 251/501 --> loss:0.044663602113723756
step 151/334, epoch 251/501 --> loss:0.04673054456710815
step 201/334, epoch 251/501 --> loss:0.05825170993804932
step 251/334, epoch 251/501 --> loss:0.058804032802581785
step 301/334, epoch 251/501 --> loss:0.045564403533935545

##########train dataset##########
acc--> [99.62757598338862]
F1--> {'F1': [0.9556076857365011], 'precision': [0.960652989440148], 'recall': [0.9506249960565207]}
##########eval dataset##########
acc--> [99.05127244149361]
F1--> {'F1': [0.8885771263158754], 'precision': [0.9022755779432731], 'recall': [0.8752980980704044]}
step 51/334, epoch 252/501 --> loss:0.0378165876865387
step 101/334, epoch 252/501 --> loss:0.04527127027511597
step 151/334, epoch 252/501 --> loss:0.0354635226726532
step 201/334, epoch 252/501 --> loss:0.040204887390136716
step 251/334, epoch 252/501 --> loss:0.049614458084106444
step 301/334, epoch 252/501 --> loss:0.03501139760017395
step 51/334, epoch 253/501 --> loss:0.03625779390335083
step 101/334, epoch 253/501 --> loss:0.03920441746711731
step 151/334, epoch 253/501 --> loss:0.04454842448234558
step 201/334, epoch 253/501 --> loss:0.04349866032600403
step 251/334, epoch 253/501 --> loss:0.03704640507698059
step 301/334, epoch 253/501 --> loss:0.04330366730690002
step 51/334, epoch 254/501 --> loss:0.04662749290466309
step 101/334, epoch 254/501 --> loss:0.037614058256149295
step 151/334, epoch 254/501 --> loss:0.04175114631652832
step 201/334, epoch 254/501 --> loss:0.035744799375534056
step 251/334, epoch 254/501 --> loss:0.03891394734382629
step 301/334, epoch 254/501 --> loss:0.03589701533317566
step 51/334, epoch 255/501 --> loss:0.03919568777084351
step 101/334, epoch 255/501 --> loss:0.034622828960418704
step 151/334, epoch 255/501 --> loss:0.04405314087867737
step 201/334, epoch 255/501 --> loss:0.0298426878452301
step 251/334, epoch 255/501 --> loss:0.03319774150848389
step 301/334, epoch 255/501 --> loss:0.036421316862106326
step 51/334, epoch 256/501 --> loss:0.03669221162796021
step 101/334, epoch 256/501 --> loss:0.05286872744560242
step 151/334, epoch 256/501 --> loss:0.03583583354949951
step 201/334, epoch 256/501 --> loss:0.035851924419403075
step 251/334, epoch 256/501 --> loss:0.0426013970375061
step 301/334, epoch 256/501 --> loss:0.03345126867294312
step 51/334, epoch 257/501 --> loss:0.03856671214103699
step 101/334, epoch 257/501 --> loss:0.03168604016304016
step 151/334, epoch 257/501 --> loss:0.0502339231967926
step 201/334, epoch 257/501 --> loss:0.045414646863937376
step 251/334, epoch 257/501 --> loss:0.05381680011749267
step 301/334, epoch 257/501 --> loss:0.04061220169067383
step 51/334, epoch 258/501 --> loss:0.04234913945198059
step 101/334, epoch 258/501 --> loss:0.03350662350654602
step 151/334, epoch 258/501 --> loss:0.0524121344089508
step 201/334, epoch 258/501 --> loss:0.03976011276245117
step 251/334, epoch 258/501 --> loss:0.03490955948829651
step 301/334, epoch 258/501 --> loss:0.05435088396072388
step 51/334, epoch 259/501 --> loss:0.04986484408378601
step 101/334, epoch 259/501 --> loss:0.04613153100013733
step 151/334, epoch 259/501 --> loss:0.05409712791442871
step 201/334, epoch 259/501 --> loss:0.04131012916564941
step 251/334, epoch 259/501 --> loss:0.04005244493484497
step 301/334, epoch 259/501 --> loss:0.03352383971214294
step 51/334, epoch 260/501 --> loss:0.06311913251876831
step 101/334, epoch 260/501 --> loss:0.06758840799331665
step 151/334, epoch 260/501 --> loss:0.04610531687736511
step 201/334, epoch 260/501 --> loss:0.08866015195846558
step 251/334, epoch 260/501 --> loss:0.07575907707214355
step 301/334, epoch 260/501 --> loss:0.04020068168640137
step 51/334, epoch 261/501 --> loss:0.17031217336654664
step 101/334, epoch 261/501 --> loss:0.1376032340526581
step 151/334, epoch 261/501 --> loss:0.09341874122619628
step 201/334, epoch 261/501 --> loss:0.0912892746925354
step 251/334, epoch 261/501 --> loss:0.07642908811569214
step 301/334, epoch 261/501 --> loss:0.08083460450172425

##########train dataset##########
acc--> [99.61847220192772]
F1--> {'F1': [0.9540773886129162], 'precision': [0.9687001874761817], 'recall': [0.9398991977504307]}
##########eval dataset##########
acc--> [99.06507610543991]
F1--> {'F1': [0.8886669940511066], 'precision': [0.9155283203664967], 'recall': [0.8633463769082935]}
step 51/334, epoch 262/501 --> loss:0.07217564582824706
step 101/334, epoch 262/501 --> loss:0.05341896414756775
step 151/334, epoch 262/501 --> loss:0.058082557916641235
step 201/334, epoch 262/501 --> loss:0.050931973457336424
step 251/334, epoch 262/501 --> loss:0.05311875820159912
step 301/334, epoch 262/501 --> loss:0.054608280658721926
step 51/334, epoch 263/501 --> loss:0.044903333187103274
step 101/334, epoch 263/501 --> loss:0.037161389589309694
step 151/334, epoch 263/501 --> loss:0.041235705614089964
step 201/334, epoch 263/501 --> loss:0.04105043053627014
step 251/334, epoch 263/501 --> loss:0.04010722279548645
step 301/334, epoch 263/501 --> loss:0.03525790214538574
step 51/334, epoch 264/501 --> loss:0.03460431933403015
step 101/334, epoch 264/501 --> loss:0.0372253406047821
step 151/334, epoch 264/501 --> loss:0.051405259370803834
step 201/334, epoch 264/501 --> loss:0.03756823420524597
step 251/334, epoch 264/501 --> loss:0.0355219292640686
step 301/334, epoch 264/501 --> loss:0.03572170257568359
step 51/334, epoch 265/501 --> loss:0.04737320899963379
step 101/334, epoch 265/501 --> loss:0.03973643898963928
step 151/334, epoch 265/501 --> loss:0.030082459449768065
step 201/334, epoch 265/501 --> loss:0.036327571868896485
step 251/334, epoch 265/501 --> loss:0.03794788241386413
step 301/334, epoch 265/501 --> loss:0.050361948013305666
step 51/334, epoch 266/501 --> loss:0.03355629801750183
step 101/334, epoch 266/501 --> loss:0.031391910314559936
step 151/334, epoch 266/501 --> loss:0.02989356517791748
step 201/334, epoch 266/501 --> loss:0.031092692613601685
step 251/334, epoch 266/501 --> loss:0.04714791893959045
step 301/334, epoch 266/501 --> loss:0.039116039276123046
step 51/334, epoch 267/501 --> loss:0.0355977463722229
step 101/334, epoch 267/501 --> loss:0.034165571928024295
step 151/334, epoch 267/501 --> loss:0.03821954369544983
step 201/334, epoch 267/501 --> loss:0.04023314476013184
step 251/334, epoch 267/501 --> loss:0.03406660795211792
step 301/334, epoch 267/501 --> loss:0.036348713636398314
step 51/334, epoch 268/501 --> loss:0.03583400130271912
step 101/334, epoch 268/501 --> loss:0.039873039722442626
step 151/334, epoch 268/501 --> loss:0.032923622131347655
step 201/334, epoch 268/501 --> loss:0.03632430911064148
step 251/334, epoch 268/501 --> loss:0.04723827123641968
step 301/334, epoch 268/501 --> loss:0.03254091739654541
step 51/334, epoch 269/501 --> loss:0.034436702728271484
step 101/334, epoch 269/501 --> loss:0.04138359546661377
step 151/334, epoch 269/501 --> loss:0.04124153256416321
step 201/334, epoch 269/501 --> loss:0.03697804570198059
step 251/334, epoch 269/501 --> loss:0.03209354162216187
step 301/334, epoch 269/501 --> loss:0.034814252853393554
step 51/334, epoch 270/501 --> loss:0.043928003311157225
step 101/334, epoch 270/501 --> loss:0.033450837135314944
step 151/334, epoch 270/501 --> loss:0.03026634931564331
step 201/334, epoch 270/501 --> loss:0.030477571487426757
step 251/334, epoch 270/501 --> loss:0.04595379590988159
step 301/334, epoch 270/501 --> loss:0.03642512440681458
step 51/334, epoch 271/501 --> loss:0.0388268494606018
step 101/334, epoch 271/501 --> loss:0.028514187335968017
step 151/334, epoch 271/501 --> loss:0.03778878569602966
step 201/334, epoch 271/501 --> loss:0.027631871700286866
step 251/334, epoch 271/501 --> loss:0.027593812942504882
step 301/334, epoch 271/501 --> loss:0.03930715203285217

##########train dataset##########
acc--> [99.75116673953067]
F1--> {'F1': [0.9701750998544866], 'precision': [0.9807355608881846], 'recall': [0.9598494308711348]}
##########eval dataset##########
acc--> [99.18736735666168]
F1--> {'F1': [0.9023222771533239], 'precision': [0.9389268878422805], 'recall': [0.8684739262819129]}
step 51/334, epoch 272/501 --> loss:0.0376980459690094
step 101/334, epoch 272/501 --> loss:0.03016355514526367
step 151/334, epoch 272/501 --> loss:0.029214407205581664
step 201/334, epoch 272/501 --> loss:0.038393192291259766
step 251/334, epoch 272/501 --> loss:0.045294299125671386
step 301/334, epoch 272/501 --> loss:0.0490889048576355
step 51/334, epoch 273/501 --> loss:0.036600061655044556
step 101/334, epoch 273/501 --> loss:0.03155989408493042
step 151/334, epoch 273/501 --> loss:0.04143451452255249
step 201/334, epoch 273/501 --> loss:0.04106270313262939
step 251/334, epoch 273/501 --> loss:0.05164833903312683
step 301/334, epoch 273/501 --> loss:0.03494504928588867
step 51/334, epoch 274/501 --> loss:0.042520302534103396
step 101/334, epoch 274/501 --> loss:0.04224098801612854
step 151/334, epoch 274/501 --> loss:0.03164569616317749
step 201/334, epoch 274/501 --> loss:0.033394752740859984
step 251/334, epoch 274/501 --> loss:0.03344775199890137
step 301/334, epoch 274/501 --> loss:0.035979163646697995
step 51/334, epoch 275/501 --> loss:0.034504939317703244
step 101/334, epoch 275/501 --> loss:0.029189475774765015
step 151/334, epoch 275/501 --> loss:0.03732293128967285
step 201/334, epoch 275/501 --> loss:0.035430465936660764
step 251/334, epoch 275/501 --> loss:0.0396906578540802
step 301/334, epoch 275/501 --> loss:0.037099411487579344
step 51/334, epoch 276/501 --> loss:0.02983532428741455
step 101/334, epoch 276/501 --> loss:0.06542944073677064
step 151/334, epoch 276/501 --> loss:0.031129935979843138
step 201/334, epoch 276/501 --> loss:0.03346039295196533
step 251/334, epoch 276/501 --> loss:0.03075045347213745
step 301/334, epoch 276/501 --> loss:0.029252184629440306
step 51/334, epoch 277/501 --> loss:0.06072089076042175
step 101/334, epoch 277/501 --> loss:0.05270255208015442
step 151/334, epoch 277/501 --> loss:0.07173237442970276
step 201/334, epoch 277/501 --> loss:0.04915043115615845
step 251/334, epoch 277/501 --> loss:0.05776064038276672
step 301/334, epoch 277/501 --> loss:0.05108175992965698
step 51/334, epoch 278/501 --> loss:0.04201030611991882
step 101/334, epoch 278/501 --> loss:0.0431023895740509
step 151/334, epoch 278/501 --> loss:0.03607341527938843
step 201/334, epoch 278/501 --> loss:0.06685776114463807
step 251/334, epoch 278/501 --> loss:0.039712698459625245
step 301/334, epoch 278/501 --> loss:0.0401706063747406
step 51/334, epoch 279/501 --> loss:0.04146759748458862
step 101/334, epoch 279/501 --> loss:0.043116756677627564
step 151/334, epoch 279/501 --> loss:0.04942175269126892
step 201/334, epoch 279/501 --> loss:0.05332627415657044
step 251/334, epoch 279/501 --> loss:0.035432188510894774
step 301/334, epoch 279/501 --> loss:0.03727199554443359
step 51/334, epoch 280/501 --> loss:0.036095653772354123
step 101/334, epoch 280/501 --> loss:0.045694432258605956
step 151/334, epoch 280/501 --> loss:0.03362751603126526
step 201/334, epoch 280/501 --> loss:0.04414151310920715
step 251/334, epoch 280/501 --> loss:0.03149341583251953
step 301/334, epoch 280/501 --> loss:0.043349353075027464
step 51/334, epoch 281/501 --> loss:0.03247859239578247
step 101/334, epoch 281/501 --> loss:0.0350977611541748
step 151/334, epoch 281/501 --> loss:0.030221669673919677
step 201/334, epoch 281/501 --> loss:0.03533002018928528
step 251/334, epoch 281/501 --> loss:0.039321678876876834
step 301/334, epoch 281/501 --> loss:0.04727657079696655

##########train dataset##########
acc--> [99.75129489733908]
F1--> {'F1': [0.970185050048455], 'precision': [0.9809274843363274], 'recall': [0.959685137626823]}
##########eval dataset##########
acc--> [99.19907710891555]
F1--> {'F1': [0.9040341199518243], 'precision': [0.9375030581914415], 'recall': [0.8728818087655538]}
save model!
step 51/334, epoch 282/501 --> loss:0.026613388061523437
step 101/334, epoch 282/501 --> loss:0.04183239102363587
step 151/334, epoch 282/501 --> loss:0.039712475538253786
step 201/334, epoch 282/501 --> loss:0.0400933825969696
step 251/334, epoch 282/501 --> loss:0.028088804483413696
step 301/334, epoch 282/501 --> loss:0.03511331677436828
step 51/334, epoch 283/501 --> loss:0.035420918464660646
step 101/334, epoch 283/501 --> loss:0.03701262354850769
step 151/334, epoch 283/501 --> loss:0.0334381365776062
step 201/334, epoch 283/501 --> loss:0.025089856386184693
step 251/334, epoch 283/501 --> loss:0.03615097284317017
step 301/334, epoch 283/501 --> loss:0.030111072063446046
step 51/334, epoch 284/501 --> loss:0.027225011587142946
step 101/334, epoch 284/501 --> loss:0.034878811836242675
step 151/334, epoch 284/501 --> loss:0.040819076299667356
step 201/334, epoch 284/501 --> loss:0.032956002950668334
step 251/334, epoch 284/501 --> loss:0.03795312523841858
step 301/334, epoch 284/501 --> loss:0.031233718395233156
step 51/334, epoch 285/501 --> loss:0.03272818088531494
step 101/334, epoch 285/501 --> loss:0.039504001140594484
step 151/334, epoch 285/501 --> loss:0.02985278844833374
step 201/334, epoch 285/501 --> loss:0.03452187776565552
step 251/334, epoch 285/501 --> loss:0.03948974609375
step 301/334, epoch 285/501 --> loss:0.031725488901138306
step 51/334, epoch 286/501 --> loss:0.030321378707885743
step 101/334, epoch 286/501 --> loss:0.042578346729278564
step 151/334, epoch 286/501 --> loss:0.030845431089401246
step 201/334, epoch 286/501 --> loss:0.04153142809867859
step 251/334, epoch 286/501 --> loss:0.0272322154045105
step 301/334, epoch 286/501 --> loss:0.0430300784111023
step 51/334, epoch 287/501 --> loss:0.032526617050170896
step 101/334, epoch 287/501 --> loss:0.03542465209960938
step 151/334, epoch 287/501 --> loss:0.03511865019798279
step 201/334, epoch 287/501 --> loss:0.03098282217979431
step 251/334, epoch 287/501 --> loss:0.037062058448791506
step 301/334, epoch 287/501 --> loss:0.06075078248977661
step 51/334, epoch 288/501 --> loss:0.06222694396972656
step 101/334, epoch 288/501 --> loss:0.04230986475944519
step 151/334, epoch 288/501 --> loss:0.0559157919883728
step 201/334, epoch 288/501 --> loss:0.05985419750213623
step 251/334, epoch 288/501 --> loss:0.04241676568984985
step 301/334, epoch 288/501 --> loss:0.03738343834877014
step 51/334, epoch 289/501 --> loss:0.034304810762405394
step 101/334, epoch 289/501 --> loss:0.03867837905883789
step 151/334, epoch 289/501 --> loss:0.040706242322921755
step 201/334, epoch 289/501 --> loss:0.03794114828109741
step 251/334, epoch 289/501 --> loss:0.0317295503616333
step 301/334, epoch 289/501 --> loss:0.03482457041740417
step 51/334, epoch 290/501 --> loss:0.029586254358291625
step 101/334, epoch 290/501 --> loss:0.030364772081375124
step 151/334, epoch 290/501 --> loss:0.03178551197052002
step 201/334, epoch 290/501 --> loss:0.03294496536254883
step 251/334, epoch 290/501 --> loss:0.042105822563171386
step 301/334, epoch 290/501 --> loss:0.03556422829627991
step 51/334, epoch 291/501 --> loss:0.03201413154602051
step 101/334, epoch 291/501 --> loss:0.0327273166179657
step 151/334, epoch 291/501 --> loss:0.04508607387542725
step 201/334, epoch 291/501 --> loss:0.03062479376792908
step 251/334, epoch 291/501 --> loss:0.027860220670700073
step 301/334, epoch 291/501 --> loss:0.03793867945671082

##########train dataset##########
acc--> [99.76795598456401]
F1--> {'F1': [0.9722114760358926], 'precision': [0.981914780782561], 'recall': [0.9627078755540656]}
##########eval dataset##########
acc--> [99.19447410763055]
F1--> {'F1': [0.9044300360674414], 'precision': [0.9281281149162752], 'recall': [0.8819215044174854]}
save model!
step 51/334, epoch 292/501 --> loss:0.025380747318267824
step 101/334, epoch 292/501 --> loss:0.03883904933929443
step 151/334, epoch 292/501 --> loss:0.025013706684112548
step 201/334, epoch 292/501 --> loss:0.03363136529922486
step 251/334, epoch 292/501 --> loss:0.03227980613708496
step 301/334, epoch 292/501 --> loss:0.029951101541519164
step 51/334, epoch 293/501 --> loss:0.03647310256958008
step 101/334, epoch 293/501 --> loss:0.028040828704833983
step 151/334, epoch 293/501 --> loss:0.025750343799591065
step 201/334, epoch 293/501 --> loss:0.03810362935066223
step 251/334, epoch 293/501 --> loss:0.03013093590736389
step 301/334, epoch 293/501 --> loss:0.034763138294219974
step 51/334, epoch 294/501 --> loss:0.026195852756500243
step 101/334, epoch 294/501 --> loss:0.03223885297775268
step 151/334, epoch 294/501 --> loss:0.03728818535804748
step 201/334, epoch 294/501 --> loss:0.03826141595840454
step 251/334, epoch 294/501 --> loss:0.029905182123184205
step 301/334, epoch 294/501 --> loss:0.029461753368377686
step 51/334, epoch 295/501 --> loss:0.02736145257949829
step 101/334, epoch 295/501 --> loss:0.03842305183410644
step 151/334, epoch 295/501 --> loss:0.027284533977508546
step 201/334, epoch 295/501 --> loss:0.02510542392730713
step 251/334, epoch 295/501 --> loss:0.02909411907196045
step 301/334, epoch 295/501 --> loss:0.03878841042518616
step 51/334, epoch 296/501 --> loss:0.03355221509933472
step 101/334, epoch 296/501 --> loss:0.03023690581321716
step 151/334, epoch 296/501 --> loss:0.025740636587142943
step 201/334, epoch 296/501 --> loss:0.029651975631713866
step 251/334, epoch 296/501 --> loss:0.028247102499008178
step 301/334, epoch 296/501 --> loss:0.0402329683303833
step 51/334, epoch 297/501 --> loss:0.028995043039321898
step 101/334, epoch 297/501 --> loss:0.03376572132110596
step 151/334, epoch 297/501 --> loss:0.031419600248336794
step 201/334, epoch 297/501 --> loss:0.030597004890441894
step 251/334, epoch 297/501 --> loss:0.027910152673721312
step 301/334, epoch 297/501 --> loss:0.02946797966957092
step 51/334, epoch 298/501 --> loss:0.03567347884178162
step 101/334, epoch 298/501 --> loss:0.03168546915054321
step 151/334, epoch 298/501 --> loss:0.029749675989151
step 201/334, epoch 298/501 --> loss:0.060105350017547604
step 251/334, epoch 298/501 --> loss:0.07568686246871949
step 301/334, epoch 298/501 --> loss:0.06172384262084961
step 51/334, epoch 299/501 --> loss:0.051753222942352295
step 101/334, epoch 299/501 --> loss:0.04740271091461182
step 151/334, epoch 299/501 --> loss:0.04363688349723816
step 201/334, epoch 299/501 --> loss:0.033383800983428955
step 251/334, epoch 299/501 --> loss:0.056548925638198855
step 301/334, epoch 299/501 --> loss:0.044683263301849366
step 51/334, epoch 300/501 --> loss:0.044703996181488036
step 101/334, epoch 300/501 --> loss:0.03602915287017822
step 151/334, epoch 300/501 --> loss:0.037375787496566774
step 201/334, epoch 300/501 --> loss:0.0534759271144867
step 251/334, epoch 300/501 --> loss:0.055362770557403566
step 301/334, epoch 300/501 --> loss:0.038612034320831295
step 51/334, epoch 301/501 --> loss:0.033141411542892456
step 101/334, epoch 301/501 --> loss:0.04862789392471314
step 151/334, epoch 301/501 --> loss:0.05252909064292908
step 201/334, epoch 301/501 --> loss:0.04365462303161621
step 251/334, epoch 301/501 --> loss:0.03916977643966675
step 301/334, epoch 301/501 --> loss:0.03917038083076477

##########train dataset##########
acc--> [99.75344554556128]
F1--> {'F1': [0.9704506627669124], 'precision': [0.9809302220821764], 'recall': [0.9602024374951641]}
##########eval dataset##########
acc--> [99.16278575452674]
F1--> {'F1': [0.8989254298177611], 'precision': [0.939858600048258], 'recall': [0.861418102061025]}
step 51/334, epoch 302/501 --> loss:0.026043810844421388
step 101/334, epoch 302/501 --> loss:0.03865419387817383
step 151/334, epoch 302/501 --> loss:0.030846346616744996
step 201/334, epoch 302/501 --> loss:0.04686281561851501
step 251/334, epoch 302/501 --> loss:0.04005733370780945
step 301/334, epoch 302/501 --> loss:0.03724960446357727
step 51/334, epoch 303/501 --> loss:0.03985751032829285
step 101/334, epoch 303/501 --> loss:0.04008308172225952
step 151/334, epoch 303/501 --> loss:0.03799556374549866
step 201/334, epoch 303/501 --> loss:0.032185218334198
step 251/334, epoch 303/501 --> loss:0.035655604600906374
step 301/334, epoch 303/501 --> loss:0.03792451977729797
step 51/334, epoch 304/501 --> loss:0.02819334626197815
step 101/334, epoch 304/501 --> loss:0.0364467191696167
step 151/334, epoch 304/501 --> loss:0.03858520388603211
step 201/334, epoch 304/501 --> loss:0.03875696539878845
step 251/334, epoch 304/501 --> loss:0.029350790977478027
step 301/334, epoch 304/501 --> loss:0.025100512504577635
step 51/334, epoch 305/501 --> loss:0.028079822063446044
step 101/334, epoch 305/501 --> loss:0.03742462754249573
step 151/334, epoch 305/501 --> loss:0.10698641419410705
step 201/334, epoch 305/501 --> loss:0.08004996418952942
step 251/334, epoch 305/501 --> loss:0.05767225980758667
step 301/334, epoch 305/501 --> loss:0.04554925084114075
step 51/334, epoch 306/501 --> loss:0.040550349950790404
step 101/334, epoch 306/501 --> loss:0.035569130182266234
step 151/334, epoch 306/501 --> loss:0.032107362747192385
step 201/334, epoch 306/501 --> loss:0.039586968421936035
step 251/334, epoch 306/501 --> loss:0.03939559817314148
step 301/334, epoch 306/501 --> loss:0.04348724842071533
step 51/334, epoch 307/501 --> loss:0.033485835790634154
step 101/334, epoch 307/501 --> loss:0.024239189624786377
step 151/334, epoch 307/501 --> loss:0.029902132749557494
step 201/334, epoch 307/501 --> loss:0.0431320583820343
step 251/334, epoch 307/501 --> loss:0.049003396034240726
step 301/334, epoch 307/501 --> loss:0.044729485511779785
step 51/334, epoch 308/501 --> loss:0.044175570011138914
step 101/334, epoch 308/501 --> loss:0.03500703454017639
step 151/334, epoch 308/501 --> loss:0.027862176895141602
step 201/334, epoch 308/501 --> loss:0.03166618227958679
step 251/334, epoch 308/501 --> loss:0.033032069206237795
step 301/334, epoch 308/501 --> loss:0.03335394382476806
step 51/334, epoch 309/501 --> loss:0.03169060230255127
step 101/334, epoch 309/501 --> loss:0.027204267978668213
step 151/334, epoch 309/501 --> loss:0.03595301270484924
step 201/334, epoch 309/501 --> loss:0.03912577867507935
step 251/334, epoch 309/501 --> loss:0.03899984002113342
step 301/334, epoch 309/501 --> loss:0.04029651761054993
step 51/334, epoch 310/501 --> loss:0.037819525003433226
step 101/334, epoch 310/501 --> loss:0.033375685214996335
step 151/334, epoch 310/501 --> loss:0.03692705988883972
step 201/334, epoch 310/501 --> loss:0.04116753578186035
step 251/334, epoch 310/501 --> loss:0.03897401213645935
step 301/334, epoch 310/501 --> loss:0.03908687114715576
step 51/334, epoch 311/501 --> loss:0.03694197654724121
step 101/334, epoch 311/501 --> loss:0.029542003870010377
step 151/334, epoch 311/501 --> loss:0.03624890565872192
step 201/334, epoch 311/501 --> loss:0.03897358775138855
step 251/334, epoch 311/501 --> loss:0.03472831606864929
step 301/334, epoch 311/501 --> loss:0.036056467294692994

##########train dataset##########
acc--> [99.77529845435905]
F1--> {'F1': [0.973076909166309], 'precision': [0.9832998838808479], 'recall': [0.9630741097514396]}
##########eval dataset##########
acc--> [99.16995458193405]
F1--> {'F1': [0.8998727160397072], 'precision': [0.9400111994329635], 'recall': [0.8630308662953695]}
step 51/334, epoch 312/501 --> loss:0.02833491563796997
step 101/334, epoch 312/501 --> loss:0.028695958852767944
step 151/334, epoch 312/501 --> loss:0.03365430474281311
step 201/334, epoch 312/501 --> loss:0.028526073694229125
step 251/334, epoch 312/501 --> loss:0.024394750595092773
step 301/334, epoch 312/501 --> loss:0.03537390828132629
step 51/334, epoch 313/501 --> loss:0.03528946280479431
step 101/334, epoch 313/501 --> loss:0.02748624324798584
step 151/334, epoch 313/501 --> loss:0.03636852264404297
step 201/334, epoch 313/501 --> loss:0.029280567169189455
step 251/334, epoch 313/501 --> loss:0.02397952914237976
step 301/334, epoch 313/501 --> loss:0.04470581531524658
step 51/334, epoch 314/501 --> loss:0.02720705986022949
step 101/334, epoch 314/501 --> loss:0.03146951794624329
step 151/334, epoch 314/501 --> loss:0.02898457884788513
step 201/334, epoch 314/501 --> loss:0.0324355149269104
step 251/334, epoch 314/501 --> loss:0.03790079712867737
step 301/334, epoch 314/501 --> loss:0.025496251583099365
step 51/334, epoch 315/501 --> loss:0.027759387493133544
step 101/334, epoch 315/501 --> loss:0.02548858642578125
step 151/334, epoch 315/501 --> loss:0.03897413849830628
step 201/334, epoch 315/501 --> loss:0.028206533193588255
step 251/334, epoch 315/501 --> loss:0.02673220753669739
step 301/334, epoch 315/501 --> loss:0.023729608058929444
step 51/334, epoch 316/501 --> loss:0.04156805276870727
step 101/334, epoch 316/501 --> loss:0.03052112936973572
step 151/334, epoch 316/501 --> loss:0.04237411379814148
step 201/334, epoch 316/501 --> loss:0.026609630584716798
step 251/334, epoch 316/501 --> loss:0.03133884906768799
step 301/334, epoch 316/501 --> loss:0.02459408521652222
step 51/334, epoch 317/501 --> loss:0.03586568117141724
step 101/334, epoch 317/501 --> loss:0.03658284306526184
step 151/334, epoch 317/501 --> loss:0.026661784648895265
step 201/334, epoch 317/501 --> loss:0.030905202627182007
step 251/334, epoch 317/501 --> loss:0.030047402381896973
step 301/334, epoch 317/501 --> loss:0.028130587339401245
step 51/334, epoch 318/501 --> loss:0.03382318377494812
step 101/334, epoch 318/501 --> loss:0.023491570949554442
step 151/334, epoch 318/501 --> loss:0.02925449252128601
step 201/334, epoch 318/501 --> loss:0.025726802349090576
step 251/334, epoch 318/501 --> loss:0.03175250768661499
step 301/334, epoch 318/501 --> loss:0.02645154356956482
step 51/334, epoch 319/501 --> loss:0.03243246078491211
step 101/334, epoch 319/501 --> loss:0.03158503293991089
step 151/334, epoch 319/501 --> loss:0.031206982135772707
step 201/334, epoch 319/501 --> loss:0.024121630191802978
step 251/334, epoch 319/501 --> loss:0.02741085648536682
step 301/334, epoch 319/501 --> loss:0.036480889320373536
step 51/334, epoch 320/501 --> loss:0.0272162926197052
step 101/334, epoch 320/501 --> loss:0.03501923084259033
step 151/334, epoch 320/501 --> loss:0.033866901397705075
step 201/334, epoch 320/501 --> loss:0.03386955976486206
step 251/334, epoch 320/501 --> loss:0.02939042091369629
step 301/334, epoch 320/501 --> loss:0.032148526906967165
step 51/334, epoch 321/501 --> loss:0.039829444885253903
step 101/334, epoch 321/501 --> loss:0.051056342124938967
step 151/334, epoch 321/501 --> loss:0.040263301134109496
step 201/334, epoch 321/501 --> loss:0.05115640163421631
step 251/334, epoch 321/501 --> loss:0.04726563811302185
step 301/334, epoch 321/501 --> loss:0.038826237916946414

##########train dataset##########
acc--> [99.60566443095085]
F1--> {'F1': [0.9525587180193068], 'precision': [0.9666802490415158], 'recall': [0.9388535411191063]}
##########eval dataset##########
acc--> [98.93104639995511]
F1--> {'F1': [0.8713153230182799], 'precision': [0.9081828493845499], 'recall': [0.8373335075346694]}
step 51/334, epoch 322/501 --> loss:0.037763473987579343
step 101/334, epoch 322/501 --> loss:0.041742331981658935
step 151/334, epoch 322/501 --> loss:0.03263677716255188
step 201/334, epoch 322/501 --> loss:0.030750144720077515
step 251/334, epoch 322/501 --> loss:0.03837138652801514
step 301/334, epoch 322/501 --> loss:0.03426247000694275
step 51/334, epoch 323/501 --> loss:0.027970054149627686
step 101/334, epoch 323/501 --> loss:0.027772320508956908
step 151/334, epoch 323/501 --> loss:0.02918553590774536
step 201/334, epoch 323/501 --> loss:0.02585438370704651
step 251/334, epoch 323/501 --> loss:0.0370221221446991
step 301/334, epoch 323/501 --> loss:0.03135323643684387
step 51/334, epoch 324/501 --> loss:0.04840447425842285
step 101/334, epoch 324/501 --> loss:0.02786659598350525
step 151/334, epoch 324/501 --> loss:0.025758851766586304
step 201/334, epoch 324/501 --> loss:0.028790452480316163
step 251/334, epoch 324/501 --> loss:0.02983595252037048
step 301/334, epoch 324/501 --> loss:0.039007332324981686
step 51/334, epoch 325/501 --> loss:0.029493647813796996
step 101/334, epoch 325/501 --> loss:0.052638643980026247
step 151/334, epoch 325/501 --> loss:0.04140442609786987
step 201/334, epoch 325/501 --> loss:0.03438025236129761
step 251/334, epoch 325/501 --> loss:0.03199376463890076
step 301/334, epoch 325/501 --> loss:0.0590967583656311
step 51/334, epoch 326/501 --> loss:0.03571944832801819
step 101/334, epoch 326/501 --> loss:0.028604865074157715
step 151/334, epoch 326/501 --> loss:0.0324965763092041
step 201/334, epoch 326/501 --> loss:0.027268003225326538
step 251/334, epoch 326/501 --> loss:0.05205033898353577
step 301/334, epoch 326/501 --> loss:0.037109228372573855
step 51/334, epoch 327/501 --> loss:0.04966817617416382
step 101/334, epoch 327/501 --> loss:0.03525418400764466
step 151/334, epoch 327/501 --> loss:0.026768873929977417
step 201/334, epoch 327/501 --> loss:0.025161352157592774
step 251/334, epoch 327/501 --> loss:0.03487141609191895
step 301/334, epoch 327/501 --> loss:0.02730884909629822
step 51/334, epoch 328/501 --> loss:0.04621775507926941
step 101/334, epoch 328/501 --> loss:0.03477274894714356
step 151/334, epoch 328/501 --> loss:0.03034060835838318
step 201/334, epoch 328/501 --> loss:0.05189527630805969
step 251/334, epoch 328/501 --> loss:0.031254903078079224
step 301/334, epoch 328/501 --> loss:0.02607749342918396
step 51/334, epoch 329/501 --> loss:0.03066712498664856
step 101/334, epoch 329/501 --> loss:0.028637832403182982
step 151/334, epoch 329/501 --> loss:0.048880664110183714
step 201/334, epoch 329/501 --> loss:0.03506382346153259
step 251/334, epoch 329/501 --> loss:0.043722372055053714
step 301/334, epoch 329/501 --> loss:0.04138532400131226
step 51/334, epoch 330/501 --> loss:0.037637590169906615
step 101/334, epoch 330/501 --> loss:0.02672071933746338
step 151/334, epoch 330/501 --> loss:0.03047792673110962
step 201/334, epoch 330/501 --> loss:0.03633488893508911
step 251/334, epoch 330/501 --> loss:0.035343116521835326
step 301/334, epoch 330/501 --> loss:0.04750221848487854
step 51/334, epoch 331/501 --> loss:0.03912821412086487
step 101/334, epoch 331/501 --> loss:0.053889613151550296
step 151/334, epoch 331/501 --> loss:0.03919534206390381
step 201/334, epoch 331/501 --> loss:0.04155855417251587
step 251/334, epoch 331/501 --> loss:0.04042135596275329
step 301/334, epoch 331/501 --> loss:0.030173659324645996

##########train dataset##########
acc--> [99.77138077314025]
F1--> {'F1': [0.9726988736629174], 'precision': [0.9795843326421805], 'recall': [0.9659193946947351]}
##########eval dataset##########
acc--> [99.16713177270636]
F1--> {'F1': [0.9003022902653977], 'precision': [0.9326730765458875], 'recall': [0.870112481153731]}
step 51/334, epoch 332/501 --> loss:0.02607270121574402
step 101/334, epoch 332/501 --> loss:0.0311528742313385
step 151/334, epoch 332/501 --> loss:0.041413341760635373
step 201/334, epoch 332/501 --> loss:0.03906376123428345
step 251/334, epoch 332/501 --> loss:0.05698945045471191
step 301/334, epoch 332/501 --> loss:0.07704210638999939
step 51/334, epoch 333/501 --> loss:0.07137765169143677
step 101/334, epoch 333/501 --> loss:0.03827271938323975
step 151/334, epoch 333/501 --> loss:0.03817742347717285
step 201/334, epoch 333/501 --> loss:0.0297995662689209
step 251/334, epoch 333/501 --> loss:0.03759658694267273
step 301/334, epoch 333/501 --> loss:0.03540693640708923
step 51/334, epoch 334/501 --> loss:0.04320123434066772
step 101/334, epoch 334/501 --> loss:0.02650841951370239
step 151/334, epoch 334/501 --> loss:0.03284161210060119
step 201/334, epoch 334/501 --> loss:0.028905251026153565
step 251/334, epoch 334/501 --> loss:0.027479859590530394
step 301/334, epoch 334/501 --> loss:0.03256136536598206
step 51/334, epoch 335/501 --> loss:0.03247140645980835
step 101/334, epoch 335/501 --> loss:0.028659321069717407
step 151/334, epoch 335/501 --> loss:0.023356993198394776
step 201/334, epoch 335/501 --> loss:0.028451651334762573
step 251/334, epoch 335/501 --> loss:0.04029531717300415
step 301/334, epoch 335/501 --> loss:0.029944067001342774
step 51/334, epoch 336/501 --> loss:0.029624675512313844
step 101/334, epoch 336/501 --> loss:0.02321677088737488
step 151/334, epoch 336/501 --> loss:0.027096047401428222
step 201/334, epoch 336/501 --> loss:0.03325152397155762
step 251/334, epoch 336/501 --> loss:0.03151303768157959
step 301/334, epoch 336/501 --> loss:0.023433884382247926
step 51/334, epoch 337/501 --> loss:0.01762995481491089
step 101/334, epoch 337/501 --> loss:0.024657268524169922
step 151/334, epoch 337/501 --> loss:0.02739312529563904
step 201/334, epoch 337/501 --> loss:0.03272154927253723
step 251/334, epoch 337/501 --> loss:0.05720891356468201
step 301/334, epoch 337/501 --> loss:0.028403253555297853
step 51/334, epoch 338/501 --> loss:0.031312702894210814
step 101/334, epoch 338/501 --> loss:0.03192542672157288
step 151/334, epoch 338/501 --> loss:0.033175923824310304
step 201/334, epoch 338/501 --> loss:0.03152373552322388
step 251/334, epoch 338/501 --> loss:0.028030635118484498
step 301/334, epoch 338/501 --> loss:0.029435206651687622
step 51/334, epoch 339/501 --> loss:0.025492476224899294
step 101/334, epoch 339/501 --> loss:0.028330717086791992
step 151/334, epoch 339/501 --> loss:0.03863309741020202
step 201/334, epoch 339/501 --> loss:0.025265008211135864
step 251/334, epoch 339/501 --> loss:0.02706607222557068
step 301/334, epoch 339/501 --> loss:0.032592124938964843
step 51/334, epoch 340/501 --> loss:0.02638323783874512
step 101/334, epoch 340/501 --> loss:0.03318027019500733
step 151/334, epoch 340/501 --> loss:0.027850966453552246
step 201/334, epoch 340/501 --> loss:0.027460715770721435
step 251/334, epoch 340/501 --> loss:0.02455156445503235
step 301/334, epoch 340/501 --> loss:0.03174988865852356
step 51/334, epoch 341/501 --> loss:0.029186526536941527
step 101/334, epoch 341/501 --> loss:0.023944824934005737
step 151/334, epoch 341/501 --> loss:0.02542093276977539
step 201/334, epoch 341/501 --> loss:0.024216198921203615
step 251/334, epoch 341/501 --> loss:0.03852366805076599
step 301/334, epoch 341/501 --> loss:0.02911352038383484

##########train dataset##########
acc--> [99.7964759600639]
F1--> {'F1': [0.9756691849999243], 'precision': [0.9836406766127689], 'recall': [0.9678356969869772]}
##########eval dataset##########
acc--> [99.19184754004695]
F1--> {'F1': [0.9038810303280339], 'precision': [0.9299881077993062], 'recall': [0.8792091633040321]}
step 51/334, epoch 342/501 --> loss:0.036342780590057376
step 101/334, epoch 342/501 --> loss:0.02457388162612915
step 151/334, epoch 342/501 --> loss:0.02397104024887085
step 201/334, epoch 342/501 --> loss:0.021388981342315674
step 251/334, epoch 342/501 --> loss:0.025726037025451662
step 301/334, epoch 342/501 --> loss:0.031199694871902467
step 51/334, epoch 343/501 --> loss:0.022597922086715697
step 101/334, epoch 343/501 --> loss:0.0246780526638031
step 151/334, epoch 343/501 --> loss:0.035345276594161985
step 201/334, epoch 343/501 --> loss:0.027549532651901246
step 251/334, epoch 343/501 --> loss:0.03529331922531128
step 301/334, epoch 343/501 --> loss:0.02246414661407471
step 51/334, epoch 344/501 --> loss:0.026475197076797484
step 101/334, epoch 344/501 --> loss:0.03538157343864441
step 151/334, epoch 344/501 --> loss:0.022363724708557128
step 201/334, epoch 344/501 --> loss:0.029350980520248413
step 251/334, epoch 344/501 --> loss:0.021943047046661376
step 301/334, epoch 344/501 --> loss:0.027328687906265258
step 51/334, epoch 345/501 --> loss:0.02315963864326477
step 101/334, epoch 345/501 --> loss:0.030758900642395018
step 151/334, epoch 345/501 --> loss:0.029092907905578613
step 201/334, epoch 345/501 --> loss:0.02448626756668091
step 251/334, epoch 345/501 --> loss:0.030546512603759766
step 301/334, epoch 345/501 --> loss:0.05869505882263184
step 51/334, epoch 346/501 --> loss:0.06173244714736938
step 101/334, epoch 346/501 --> loss:0.02757744550704956
step 151/334, epoch 346/501 --> loss:0.03549724102020264
step 201/334, epoch 346/501 --> loss:0.034576406478881834
step 251/334, epoch 346/501 --> loss:0.03410979032516479
step 301/334, epoch 346/501 --> loss:0.03287019371986389
step 51/334, epoch 347/501 --> loss:0.07186893939971924
step 101/334, epoch 347/501 --> loss:0.09449011921882629
step 151/334, epoch 347/501 --> loss:0.05174021244049072
step 201/334, epoch 347/501 --> loss:0.03966625332832337
step 251/334, epoch 347/501 --> loss:0.03449513912200928
step 301/334, epoch 347/501 --> loss:0.03755094051361084
step 51/334, epoch 348/501 --> loss:0.04296979308128357
step 101/334, epoch 348/501 --> loss:0.033644939661026003
step 151/334, epoch 348/501 --> loss:0.035156911611557005
step 201/334, epoch 348/501 --> loss:0.02788203477859497
step 251/334, epoch 348/501 --> loss:0.033148269653320316
step 301/334, epoch 348/501 --> loss:0.04549638867378235
step 51/334, epoch 349/501 --> loss:0.03325944542884827
step 101/334, epoch 349/501 --> loss:0.024406957626342773
step 151/334, epoch 349/501 --> loss:0.029779021739959718
step 201/334, epoch 349/501 --> loss:0.02517068147659302
step 251/334, epoch 349/501 --> loss:0.030770996809005736
step 301/334, epoch 349/501 --> loss:0.04483688473701477
step 51/334, epoch 350/501 --> loss:0.028081574440002442
step 101/334, epoch 350/501 --> loss:0.028000860214233397
step 151/334, epoch 350/501 --> loss:0.02742711901664734
step 201/334, epoch 350/501 --> loss:0.026082220077514647
step 251/334, epoch 350/501 --> loss:0.02627389907836914
step 301/334, epoch 350/501 --> loss:0.04232049822807312
step 51/334, epoch 351/501 --> loss:0.03867997884750366
step 101/334, epoch 351/501 --> loss:0.02296814799308777
step 151/334, epoch 351/501 --> loss:0.026598548889160155
step 201/334, epoch 351/501 --> loss:0.03679051041603088
step 251/334, epoch 351/501 --> loss:0.022168389558792113
step 301/334, epoch 351/501 --> loss:0.024483537673950194

##########train dataset##########
acc--> [99.79886232711226]
F1--> {'F1': [0.9759406611283314], 'precision': [0.9844866328401751], 'recall': [0.9675516094117592]}
##########eval dataset##########
acc--> [99.1604175050257]
F1--> {'F1': [0.8990750384240228], 'precision': [0.9356286897662885], 'recall': [0.865279439239165]}
step 51/334, epoch 352/501 --> loss:0.03612520337104797
step 101/334, epoch 352/501 --> loss:0.0330843734741211
step 151/334, epoch 352/501 --> loss:0.03629757642745972
step 201/334, epoch 352/501 --> loss:0.020748850107192993
step 251/334, epoch 352/501 --> loss:0.0235623562335968
step 301/334, epoch 352/501 --> loss:0.02654988884925842
step 51/334, epoch 353/501 --> loss:0.0346967089176178
step 101/334, epoch 353/501 --> loss:0.029222090244293213
step 151/334, epoch 353/501 --> loss:0.03622906088829041
step 201/334, epoch 353/501 --> loss:0.09458032369613648
step 251/334, epoch 353/501 --> loss:0.04785189151763916
step 301/334, epoch 353/501 --> loss:0.04106333613395691
step 51/334, epoch 354/501 --> loss:0.04706598162651062
step 101/334, epoch 354/501 --> loss:0.04251854181289673
step 151/334, epoch 354/501 --> loss:0.04357554793357849
step 201/334, epoch 354/501 --> loss:0.03982114315032959
step 251/334, epoch 354/501 --> loss:0.039261307716369626
step 301/334, epoch 354/501 --> loss:0.04245110988616943
step 51/334, epoch 355/501 --> loss:0.02994709849357605
step 101/334, epoch 355/501 --> loss:0.03486564040184021
step 151/334, epoch 355/501 --> loss:0.03350028872489929
step 201/334, epoch 355/501 --> loss:0.03699984669685364
step 251/334, epoch 355/501 --> loss:0.03127058506011963
step 301/334, epoch 355/501 --> loss:0.05675184369087219
step 51/334, epoch 356/501 --> loss:0.035327852964401246
step 101/334, epoch 356/501 --> loss:0.055526915788650516
step 151/334, epoch 356/501 --> loss:0.03897703051567078
step 201/334, epoch 356/501 --> loss:0.03602779746055603
step 251/334, epoch 356/501 --> loss:0.041307790279388426
step 301/334, epoch 356/501 --> loss:0.03459756970405579
step 51/334, epoch 357/501 --> loss:0.025099179744720458
step 101/334, epoch 357/501 --> loss:0.031896439790725706
step 151/334, epoch 357/501 --> loss:0.02943168520927429
step 201/334, epoch 357/501 --> loss:0.03126895546913147
step 251/334, epoch 357/501 --> loss:0.0357904589176178
step 301/334, epoch 357/501 --> loss:0.027642611265182495
step 51/334, epoch 358/501 --> loss:0.025801640748977662
step 101/334, epoch 358/501 --> loss:0.029851853847503662
step 151/334, epoch 358/501 --> loss:0.03244466781616211
step 201/334, epoch 358/501 --> loss:0.0255404794216156
step 251/334, epoch 358/501 --> loss:0.037445824146270755
step 301/334, epoch 358/501 --> loss:0.027588878870010377
step 51/334, epoch 359/501 --> loss:0.030430651903152466
step 101/334, epoch 359/501 --> loss:0.026841154098510744
step 151/334, epoch 359/501 --> loss:0.030754704475402832
step 201/334, epoch 359/501 --> loss:0.027324509620666505
step 251/334, epoch 359/501 --> loss:0.025527700185775756
step 301/334, epoch 359/501 --> loss:0.0226568603515625
step 51/334, epoch 360/501 --> loss:0.02527803063392639
step 101/334, epoch 360/501 --> loss:0.02345860719680786
step 151/334, epoch 360/501 --> loss:0.02296296238899231
step 201/334, epoch 360/501 --> loss:0.037333511114120484
step 251/334, epoch 360/501 --> loss:0.02816520094871521
step 301/334, epoch 360/501 --> loss:0.02809468865394592
step 51/334, epoch 361/501 --> loss:0.027968432903289795
step 101/334, epoch 361/501 --> loss:0.023540239334106445
step 151/334, epoch 361/501 --> loss:0.04283650398254395
step 201/334, epoch 361/501 --> loss:0.027789435386657714
step 251/334, epoch 361/501 --> loss:0.02536651849746704
step 301/334, epoch 361/501 --> loss:0.024350780248641967

##########train dataset##########
acc--> [99.80313129803801]
F1--> {'F1': [0.9764371225402741], 'precision': [0.985586783392318], 'recall': [0.9674655962681641]}
##########eval dataset##########
acc--> [99.17098251435559]
F1--> {'F1': [0.9007510134338601], 'precision': [0.9332609523901305], 'recall': [0.87043911108341]}
step 51/334, epoch 362/501 --> loss:0.027288095951080324
step 101/334, epoch 362/501 --> loss:0.03315316677093506
step 151/334, epoch 362/501 --> loss:0.032348816394805906
step 201/334, epoch 362/501 --> loss:0.043717937469482424
step 251/334, epoch 362/501 --> loss:0.04574431180953979
step 301/334, epoch 362/501 --> loss:0.03786909103393555
step 51/334, epoch 363/501 --> loss:0.03331346869468689
step 101/334, epoch 363/501 --> loss:0.05089142322540283
step 151/334, epoch 363/501 --> loss:0.05382538199424744
step 201/334, epoch 363/501 --> loss:0.050060428380966186
step 251/334, epoch 363/501 --> loss:0.049826588630676266
step 301/334, epoch 363/501 --> loss:0.036415692567825314
step 51/334, epoch 364/501 --> loss:0.042641520500183105
step 101/334, epoch 364/501 --> loss:0.034047950506210324
step 151/334, epoch 364/501 --> loss:0.029744318723678588
step 201/334, epoch 364/501 --> loss:0.025299344062805176
step 251/334, epoch 364/501 --> loss:0.029436285495758056
step 301/334, epoch 364/501 --> loss:0.02652933359146118
step 51/334, epoch 365/501 --> loss:0.021335021257400513
step 101/334, epoch 365/501 --> loss:0.023489882946014406
step 151/334, epoch 365/501 --> loss:0.03129646778106689
step 201/334, epoch 365/501 --> loss:0.03345088124275208
step 251/334, epoch 365/501 --> loss:0.03861403107643127
step 301/334, epoch 365/501 --> loss:0.03250001192092895
step 51/334, epoch 366/501 --> loss:0.020999786853790284
step 101/334, epoch 366/501 --> loss:0.03634706974029541
step 151/334, epoch 366/501 --> loss:0.02499080777168274
step 201/334, epoch 366/501 --> loss:0.027563064098358153
step 251/334, epoch 366/501 --> loss:0.035774422883987425
step 301/334, epoch 366/501 --> loss:0.026267694234848024
step 51/334, epoch 367/501 --> loss:0.030939834117889406
step 101/334, epoch 367/501 --> loss:0.02490352749824524
step 151/334, epoch 367/501 --> loss:0.025474506616592407
step 201/334, epoch 367/501 --> loss:0.023901526927948
step 251/334, epoch 367/501 --> loss:0.02488677740097046
step 301/334, epoch 367/501 --> loss:0.03968435883522034
step 51/334, epoch 368/501 --> loss:0.02400347709655762
step 101/334, epoch 368/501 --> loss:0.02619237780570984
step 151/334, epoch 368/501 --> loss:0.0266569459438324
step 201/334, epoch 368/501 --> loss:0.04012119293212891
step 251/334, epoch 368/501 --> loss:0.02920262098312378
step 301/334, epoch 368/501 --> loss:0.0220379364490509
step 51/334, epoch 369/501 --> loss:0.02457390308380127
step 101/334, epoch 369/501 --> loss:0.01977945923805237
step 151/334, epoch 369/501 --> loss:0.018684877157211302
step 201/334, epoch 369/501 --> loss:0.028994622230529784
step 251/334, epoch 369/501 --> loss:0.036640158891677856
step 301/334, epoch 369/501 --> loss:0.024598085880279542
step 51/334, epoch 370/501 --> loss:0.02928208827972412
step 101/334, epoch 370/501 --> loss:0.03060427188873291
step 151/334, epoch 370/501 --> loss:0.026837098598480224
step 201/334, epoch 370/501 --> loss:0.02484200358390808
step 251/334, epoch 370/501 --> loss:0.02493904948234558
step 301/334, epoch 370/501 --> loss:0.027109355926513673
step 51/334, epoch 371/501 --> loss:0.022740765810012817
step 101/334, epoch 371/501 --> loss:0.03325218915939331
step 151/334, epoch 371/501 --> loss:0.029612207412719728
step 201/334, epoch 371/501 --> loss:0.040717173814773557
step 251/334, epoch 371/501 --> loss:0.027146810293197633
step 301/334, epoch 371/501 --> loss:0.03996711373329163

##########train dataset##########
acc--> [99.8046989426586]
F1--> {'F1': [0.9766827237478993], 'precision': [0.9833479801150876], 'recall': [0.9701170803035327]}
##########eval dataset##########
acc--> [99.19485858105574]
F1--> {'F1': [0.9039338526348418], 'precision': [0.9331894244731991], 'recall': [0.8764662440695025]}
step 51/334, epoch 372/501 --> loss:0.024101210832595824
step 101/334, epoch 372/501 --> loss:0.021517775058746337
step 151/334, epoch 372/501 --> loss:0.030375092029571532
step 201/334, epoch 372/501 --> loss:0.02809577465057373
step 251/334, epoch 372/501 --> loss:0.026292810440063475
step 301/334, epoch 372/501 --> loss:0.02752857208251953
step 51/334, epoch 373/501 --> loss:0.022121602296829225
step 101/334, epoch 373/501 --> loss:0.029958832263946533
step 151/334, epoch 373/501 --> loss:0.02607352614402771
step 201/334, epoch 373/501 --> loss:0.0382927143573761
step 251/334, epoch 373/501 --> loss:0.024895795583724976
step 301/334, epoch 373/501 --> loss:0.020496933460235595
step 51/334, epoch 374/501 --> loss:0.02814607858657837
step 101/334, epoch 374/501 --> loss:0.03094686508178711
step 151/334, epoch 374/501 --> loss:0.02306457281112671
step 201/334, epoch 374/501 --> loss:0.019487948417663575
step 251/334, epoch 374/501 --> loss:0.03804728865623474
step 301/334, epoch 374/501 --> loss:0.02560969114303589
step 51/334, epoch 375/501 --> loss:0.02775421380996704
step 101/334, epoch 375/501 --> loss:0.02083372712135315
step 151/334, epoch 375/501 --> loss:0.028910049200057984
step 201/334, epoch 375/501 --> loss:0.03512510061264038
step 251/334, epoch 375/501 --> loss:0.02150418043136597
step 301/334, epoch 375/501 --> loss:0.024451512098312377
step 51/334, epoch 376/501 --> loss:0.025612685680389404
step 101/334, epoch 376/501 --> loss:0.03186379432678223
step 151/334, epoch 376/501 --> loss:0.02235997676849365
step 201/334, epoch 376/501 --> loss:0.022564051151275636
step 251/334, epoch 376/501 --> loss:0.03196515083312988
step 301/334, epoch 376/501 --> loss:0.026943494081497193
step 51/334, epoch 377/501 --> loss:0.034027209281921385
step 101/334, epoch 377/501 --> loss:0.019943677186965943
step 151/334, epoch 377/501 --> loss:0.02575976014137268
step 201/334, epoch 377/501 --> loss:0.02695204496383667
step 251/334, epoch 377/501 --> loss:0.020839234590530397
step 301/334, epoch 377/501 --> loss:0.025148515701293946
step 51/334, epoch 378/501 --> loss:0.023620954751968383
step 101/334, epoch 378/501 --> loss:0.025418150424957275
step 151/334, epoch 378/501 --> loss:0.028308846950531007
step 201/334, epoch 378/501 --> loss:0.027958617210388184
step 251/334, epoch 378/501 --> loss:0.030128843784332275
step 301/334, epoch 378/501 --> loss:0.04564228653907776
step 51/334, epoch 379/501 --> loss:0.030854302644729614
step 101/334, epoch 379/501 --> loss:0.03238839864730835
step 151/334, epoch 379/501 --> loss:0.033584048748016355
step 201/334, epoch 379/501 --> loss:0.04637627959251404
step 251/334, epoch 379/501 --> loss:0.02397704601287842
step 301/334, epoch 379/501 --> loss:0.027637851238250733
step 51/334, epoch 380/501 --> loss:0.038559638261795044
step 101/334, epoch 380/501 --> loss:0.03751497030258179
step 151/334, epoch 380/501 --> loss:0.02906186819076538
step 201/334, epoch 380/501 --> loss:0.03926620841026306
step 251/334, epoch 380/501 --> loss:0.02996207118034363
step 301/334, epoch 380/501 --> loss:0.03989892721176148
step 51/334, epoch 381/501 --> loss:0.027276631593704224
step 101/334, epoch 381/501 --> loss:0.031215070486068724
step 151/334, epoch 381/501 --> loss:0.03687208414077759
step 201/334, epoch 381/501 --> loss:0.03655421733856201
step 251/334, epoch 381/501 --> loss:0.048420888185501096
step 301/334, epoch 381/501 --> loss:0.024242405891418458

##########train dataset##########
acc--> [99.80149156465019]
F1--> {'F1': [0.9762556776788586], 'precision': [0.9847798280364857], 'recall': [0.9678876576084235]}
##########eval dataset##########
acc--> [99.15388546172886]
F1--> {'F1': [0.8986147783137115], 'precision': [0.931930504394805], 'recall': [0.8676081639245696]}
step 51/334, epoch 382/501 --> loss:0.04885412931442261
step 101/334, epoch 382/501 --> loss:0.029465601444244385
step 151/334, epoch 382/501 --> loss:0.0230873703956604
step 201/334, epoch 382/501 --> loss:0.027499849796295165
step 251/334, epoch 382/501 --> loss:0.02956886649131775
step 301/334, epoch 382/501 --> loss:0.027924004793167114
step 51/334, epoch 383/501 --> loss:0.031042721271514893
step 101/334, epoch 383/501 --> loss:0.024550788402557373
step 151/334, epoch 383/501 --> loss:0.025054022073745727
step 201/334, epoch 383/501 --> loss:0.03581105828285217
step 251/334, epoch 383/501 --> loss:0.04270920753479004
step 301/334, epoch 383/501 --> loss:0.0220661461353302
step 51/334, epoch 384/501 --> loss:0.026680282354354858
step 101/334, epoch 384/501 --> loss:0.02716368079185486
step 151/334, epoch 384/501 --> loss:0.02041911840438843
step 201/334, epoch 384/501 --> loss:0.031243388652801515
step 251/334, epoch 384/501 --> loss:0.034457080364227295
step 301/334, epoch 384/501 --> loss:0.026260300874710082
step 51/334, epoch 385/501 --> loss:0.03564877271652222
step 101/334, epoch 385/501 --> loss:0.028768081665039063
step 151/334, epoch 385/501 --> loss:0.025987948179244994
step 201/334, epoch 385/501 --> loss:0.02578762412071228
step 251/334, epoch 385/501 --> loss:0.025556981563568115
step 301/334, epoch 385/501 --> loss:0.02367703914642334
step 51/334, epoch 386/501 --> loss:0.02340535044670105
step 101/334, epoch 386/501 --> loss:0.02626542329788208
step 151/334, epoch 386/501 --> loss:0.02088926911354065
step 201/334, epoch 386/501 --> loss:0.03493901133537292
step 251/334, epoch 386/501 --> loss:0.035310406684875485
step 301/334, epoch 386/501 --> loss:0.0593198037147522
step 51/334, epoch 387/501 --> loss:0.042584854364395144
step 101/334, epoch 387/501 --> loss:0.04968861937522888
step 151/334, epoch 387/501 --> loss:0.03723699331283569
step 201/334, epoch 387/501 --> loss:0.03400281429290772
step 251/334, epoch 387/501 --> loss:0.041638784408569336
step 301/334, epoch 387/501 --> loss:0.04173376202583313
step 51/334, epoch 388/501 --> loss:0.04592882752418518
step 101/334, epoch 388/501 --> loss:0.027204490900039673
step 151/334, epoch 388/501 --> loss:0.028825515508651735
step 201/334, epoch 388/501 --> loss:0.030308570861816406
step 251/334, epoch 388/501 --> loss:0.03296431541442871
step 301/334, epoch 388/501 --> loss:0.02641576647758484
step 51/334, epoch 389/501 --> loss:0.032072480916976925
step 101/334, epoch 389/501 --> loss:0.024005749225616456
step 151/334, epoch 389/501 --> loss:0.03104943037033081
step 201/334, epoch 389/501 --> loss:0.03673558831214905
step 251/334, epoch 389/501 --> loss:0.02971002697944641
step 301/334, epoch 389/501 --> loss:0.023791704177856445
step 51/334, epoch 390/501 --> loss:0.02092017889022827
step 101/334, epoch 390/501 --> loss:0.05771318316459656
step 151/334, epoch 390/501 --> loss:0.02769097328186035
step 201/334, epoch 390/501 --> loss:0.021842657327651976
step 251/334, epoch 390/501 --> loss:0.021600759029388426
step 301/334, epoch 390/501 --> loss:0.02622745156288147
step 51/334, epoch 391/501 --> loss:0.03387300610542297
step 101/334, epoch 391/501 --> loss:0.022753329277038575
step 151/334, epoch 391/501 --> loss:0.025580081939697265
step 201/334, epoch 391/501 --> loss:0.021784074306488037
step 251/334, epoch 391/501 --> loss:0.02050728440284729
step 301/334, epoch 391/501 --> loss:0.04043379664421082

##########train dataset##########
acc--> [99.81641536979832]
F1--> {'F1': [0.9780512862717364], 'precision': [0.9860932879148763], 'recall': [0.9701492334818167]}
##########eval dataset##########
acc--> [99.21227803066922]
F1--> {'F1': [0.9057422057883254], 'precision': [0.9379247467011257], 'recall': [0.8757042620018729]}
save model!
step 51/334, epoch 392/501 --> loss:0.024329425096511842
step 101/334, epoch 392/501 --> loss:0.023712117671966553
step 151/334, epoch 392/501 --> loss:0.026952625513076783
step 201/334, epoch 392/501 --> loss:0.02578213334083557
step 251/334, epoch 392/501 --> loss:0.021384429931640626
step 301/334, epoch 392/501 --> loss:0.027060667276382445
step 51/334, epoch 393/501 --> loss:0.022885235548019408
step 101/334, epoch 393/501 --> loss:0.0263651180267334
step 151/334, epoch 393/501 --> loss:0.03580229282379151
step 201/334, epoch 393/501 --> loss:0.022673747539520263
step 251/334, epoch 393/501 --> loss:0.023322867155075075
step 301/334, epoch 393/501 --> loss:0.025274640321731566
step 51/334, epoch 394/501 --> loss:0.02227150321006775
step 101/334, epoch 394/501 --> loss:0.0342911434173584
step 151/334, epoch 394/501 --> loss:0.022600854635238646
step 201/334, epoch 394/501 --> loss:0.024548389911651612
step 251/334, epoch 394/501 --> loss:0.02818810224533081
step 301/334, epoch 394/501 --> loss:0.01783906102180481
step 51/334, epoch 395/501 --> loss:0.026768455505371092
step 101/334, epoch 395/501 --> loss:0.027243555784225465
step 151/334, epoch 395/501 --> loss:0.035963785648345944
step 201/334, epoch 395/501 --> loss:0.020428760051727293
step 251/334, epoch 395/501 --> loss:0.03211769700050354
step 301/334, epoch 395/501 --> loss:0.026542683839797975
step 51/334, epoch 396/501 --> loss:0.028838751316070558
step 101/334, epoch 396/501 --> loss:0.03304538130760193
step 151/334, epoch 396/501 --> loss:0.025638866424560546
step 201/334, epoch 396/501 --> loss:0.0260178017616272
step 251/334, epoch 396/501 --> loss:0.020816572904586793
step 301/334, epoch 396/501 --> loss:0.019935035705566408
step 51/334, epoch 397/501 --> loss:0.02030955195426941
step 101/334, epoch 397/501 --> loss:0.023527184724807738
step 151/334, epoch 397/501 --> loss:0.025251810550689698
step 201/334, epoch 397/501 --> loss:0.030877907276153565
step 251/334, epoch 397/501 --> loss:0.027696496248245238
step 301/334, epoch 397/501 --> loss:0.025009024143218993
step 51/334, epoch 398/501 --> loss:0.028604639768600462
step 101/334, epoch 398/501 --> loss:0.03913794040679932
step 151/334, epoch 398/501 --> loss:0.027831082344055177
step 201/334, epoch 398/501 --> loss:0.03286370873451233
step 251/334, epoch 398/501 --> loss:0.028040964603424073
step 301/334, epoch 398/501 --> loss:0.027804298400878905
step 51/334, epoch 399/501 --> loss:0.03283746242523193
step 101/334, epoch 399/501 --> loss:0.03440817952156067
step 151/334, epoch 399/501 --> loss:0.10691917777061462
step 201/334, epoch 399/501 --> loss:0.07499043107032775
step 251/334, epoch 399/501 --> loss:0.048664703369140624
step 301/334, epoch 399/501 --> loss:0.05139931797981262
step 51/334, epoch 400/501 --> loss:0.039164745807647706
step 101/334, epoch 400/501 --> loss:0.02973435997962952
step 151/334, epoch 400/501 --> loss:0.02955470085144043
step 201/334, epoch 400/501 --> loss:0.026407458782196046
step 251/334, epoch 400/501 --> loss:0.03218142032623291
step 301/334, epoch 400/501 --> loss:0.03378738284111023
step 51/334, epoch 401/501 --> loss:0.03161173105239868
step 101/334, epoch 401/501 --> loss:0.027836875915527345
step 151/334, epoch 401/501 --> loss:0.033072400093078616
step 201/334, epoch 401/501 --> loss:0.022265634536743163
step 251/334, epoch 401/501 --> loss:0.05534343004226685
step 301/334, epoch 401/501 --> loss:0.02338026523590088

##########train dataset##########
acc--> [99.8024481711486]
F1--> {'F1': [0.976365850711848], 'precision': [0.9850702503950541], 'recall': [0.9678237582541124]}
##########eval dataset##########
acc--> [99.13542139247923]
F1--> {'F1': [0.8969640217955752], 'precision': [0.9248332676378146], 'recall': [0.870734699587148]}
step 51/334, epoch 402/501 --> loss:0.028757072687149048
step 101/334, epoch 402/501 --> loss:0.03641351222991943
step 151/334, epoch 402/501 --> loss:0.03494003891944885
step 201/334, epoch 402/501 --> loss:0.021650612354278564
step 251/334, epoch 402/501 --> loss:0.019484134912490843
step 301/334, epoch 402/501 --> loss:0.030278865098953247
step 51/334, epoch 403/501 --> loss:0.07441024541854858
step 101/334, epoch 403/501 --> loss:0.08283079266548157
step 151/334, epoch 403/501 --> loss:0.057318596839904784
step 201/334, epoch 403/501 --> loss:0.04062479019165039
step 251/334, epoch 403/501 --> loss:0.03368959784507752
step 301/334, epoch 403/501 --> loss:0.030022794008255006
step 51/334, epoch 404/501 --> loss:0.029389234781265258
step 101/334, epoch 404/501 --> loss:0.03155879139900208
step 151/334, epoch 404/501 --> loss:0.031409810781478885
step 201/334, epoch 404/501 --> loss:0.02232462167739868
step 251/334, epoch 404/501 --> loss:0.035248749256134033
step 301/334, epoch 404/501 --> loss:0.0361904501914978
step 51/334, epoch 405/501 --> loss:0.03205921769142151
step 101/334, epoch 405/501 --> loss:0.02412851929664612
step 151/334, epoch 405/501 --> loss:0.02460895299911499
step 201/334, epoch 405/501 --> loss:0.026362559795379638
step 251/334, epoch 405/501 --> loss:0.023480557203292847
step 301/334, epoch 405/501 --> loss:0.028232097625732422
step 51/334, epoch 406/501 --> loss:0.034744532108306886
step 101/334, epoch 406/501 --> loss:0.026360127925872803
step 151/334, epoch 406/501 --> loss:0.03684429168701172
step 201/334, epoch 406/501 --> loss:0.028314340114593505
step 251/334, epoch 406/501 --> loss:0.030844919681549073
step 301/334, epoch 406/501 --> loss:0.021980053186416625
step 51/334, epoch 407/501 --> loss:0.02696428894996643
step 101/334, epoch 407/501 --> loss:0.03205057024955749
step 151/334, epoch 407/501 --> loss:0.02585539221763611
step 201/334, epoch 407/501 --> loss:0.027021418809890747
step 251/334, epoch 407/501 --> loss:0.019528337717056275
step 301/334, epoch 407/501 --> loss:0.025758802890777588
step 51/334, epoch 408/501 --> loss:0.0330767560005188
step 101/334, epoch 408/501 --> loss:0.021404303312301635
step 151/334, epoch 408/501 --> loss:0.021755555868148802
step 201/334, epoch 408/501 --> loss:0.020667202472686767
step 251/334, epoch 408/501 --> loss:0.03590047717094422
step 301/334, epoch 408/501 --> loss:0.018295795917510987
step 51/334, epoch 409/501 --> loss:0.020051101446151732
step 101/334, epoch 409/501 --> loss:0.022142109870910646
step 151/334, epoch 409/501 --> loss:0.033394216299057006
step 201/334, epoch 409/501 --> loss:0.02152798295021057
step 251/334, epoch 409/501 --> loss:0.02895984411239624
step 301/334, epoch 409/501 --> loss:0.033608458042144775
step 51/334, epoch 410/501 --> loss:0.027147321701049804
step 101/334, epoch 410/501 --> loss:0.028289355039596557
step 151/334, epoch 410/501 --> loss:0.0235688316822052
step 201/334, epoch 410/501 --> loss:0.02499945044517517
step 251/334, epoch 410/501 --> loss:0.031419308185577394
step 301/334, epoch 410/501 --> loss:0.054821512699127196
step 51/334, epoch 411/501 --> loss:0.02682062029838562
step 101/334, epoch 411/501 --> loss:0.05454693198204041
step 151/334, epoch 411/501 --> loss:0.030185551643371583
step 201/334, epoch 411/501 --> loss:0.027596992254257203
step 251/334, epoch 411/501 --> loss:0.027914252281188965
step 301/334, epoch 411/501 --> loss:0.042383209466934205

##########train dataset##########
acc--> [99.80969137585544]
F1--> {'F1': [0.9772657009001856], 'precision': [0.9845037794368718], 'recall': [0.9701431284479652]}
##########eval dataset##########
acc--> [99.13797253385266]
F1--> {'F1': [0.8972600957022958], 'precision': [0.9252145238871045], 'recall': [0.8709547693979246]}
step 51/334, epoch 412/501 --> loss:0.027528804540634156
step 101/334, epoch 412/501 --> loss:0.02723899483680725
step 151/334, epoch 412/501 --> loss:0.026919753551483155
step 201/334, epoch 412/501 --> loss:0.0266745924949646
step 251/334, epoch 412/501 --> loss:0.022667449712753297
step 301/334, epoch 412/501 --> loss:0.02618525505065918
step 51/334, epoch 413/501 --> loss:0.021869680881500243
step 101/334, epoch 413/501 --> loss:0.02160441517829895
step 151/334, epoch 413/501 --> loss:0.027155501842498778
step 201/334, epoch 413/501 --> loss:0.0328868293762207
step 251/334, epoch 413/501 --> loss:0.02482054114341736
step 301/334, epoch 413/501 --> loss:0.030716683864593506
step 51/334, epoch 414/501 --> loss:0.017596102952957153
step 101/334, epoch 414/501 --> loss:0.04334112644195556
step 151/334, epoch 414/501 --> loss:0.03354662895202637
step 201/334, epoch 414/501 --> loss:0.038837482929229734
step 251/334, epoch 414/501 --> loss:0.03423349738121033
step 301/334, epoch 414/501 --> loss:0.02742916703224182
step 51/334, epoch 415/501 --> loss:0.023748205900192262
step 101/334, epoch 415/501 --> loss:0.021585628986358643
step 151/334, epoch 415/501 --> loss:0.03575883150100708
step 201/334, epoch 415/501 --> loss:0.020803288221359253
step 251/334, epoch 415/501 --> loss:0.028714659214019774
step 301/334, epoch 415/501 --> loss:0.03442081928253174
step 51/334, epoch 416/501 --> loss:0.028389167785644532
step 101/334, epoch 416/501 --> loss:0.027341376543045044
step 151/334, epoch 416/501 --> loss:0.02326907753944397
step 201/334, epoch 416/501 --> loss:0.02493465185165405
step 251/334, epoch 416/501 --> loss:0.02864888072013855
step 301/334, epoch 416/501 --> loss:0.025328844785690308
step 51/334, epoch 417/501 --> loss:0.02810243010520935
step 101/334, epoch 417/501 --> loss:0.021636770963668825
step 151/334, epoch 417/501 --> loss:0.019613890647888182
step 201/334, epoch 417/501 --> loss:0.03300059795379639
step 251/334, epoch 417/501 --> loss:0.022352490425109863
step 301/334, epoch 417/501 --> loss:0.03348009586334229
step 51/334, epoch 418/501 --> loss:0.019780365228652955
step 101/334, epoch 418/501 --> loss:0.024251514673233034
step 151/334, epoch 418/501 --> loss:0.025891040563583375
step 201/334, epoch 418/501 --> loss:0.03224129796028137
step 251/334, epoch 418/501 --> loss:0.02188602924346924
step 301/334, epoch 418/501 --> loss:0.02901269793510437
step 51/334, epoch 419/501 --> loss:0.028691561222076417
step 101/334, epoch 419/501 --> loss:0.02941325068473816
step 151/334, epoch 419/501 --> loss:0.018770625591278078
step 201/334, epoch 419/501 --> loss:0.03160752534866333
step 251/334, epoch 419/501 --> loss:0.022945348024368286
step 301/334, epoch 419/501 --> loss:0.028992916345596313
step 51/334, epoch 420/501 --> loss:0.02939966320991516
step 101/334, epoch 420/501 --> loss:0.022075231075286864
step 151/334, epoch 420/501 --> loss:0.028184930086135863
step 201/334, epoch 420/501 --> loss:0.02257201910018921
step 251/334, epoch 420/501 --> loss:0.02272621512413025
step 301/334, epoch 420/501 --> loss:0.025433378219604494
step 51/334, epoch 421/501 --> loss:0.022162331342697142
step 101/334, epoch 421/501 --> loss:0.02039041042327881
step 151/334, epoch 421/501 --> loss:0.020123940706253052
step 201/334, epoch 421/501 --> loss:0.043082038164138796
step 251/334, epoch 421/501 --> loss:0.021620831489562987
step 301/334, epoch 421/501 --> loss:0.023116747140884398

##########train dataset##########
acc--> [99.8251343917675]
F1--> {'F1': [0.9790985887334076], 'precision': [0.9869032224045099], 'recall': [0.9714262708961088]}
##########eval dataset##########
acc--> [99.21539920729461]
F1--> {'F1': [0.9062613725477411], 'precision': [0.9369048557261022], 'recall': [0.8775682919078902]}
save model!
step 51/334, epoch 422/501 --> loss:0.022758171558380128
step 101/334, epoch 422/501 --> loss:0.023799448013305663
step 151/334, epoch 422/501 --> loss:0.020887784957885742
step 201/334, epoch 422/501 --> loss:0.021685123443603516
step 251/334, epoch 422/501 --> loss:0.030270066261291504
step 301/334, epoch 422/501 --> loss:0.03053191304206848
step 51/334, epoch 423/501 --> loss:0.021239914894104005
step 101/334, epoch 423/501 --> loss:0.03557873487472534
step 151/334, epoch 423/501 --> loss:0.022240537405014037
step 201/334, epoch 423/501 --> loss:0.02349116086959839
step 251/334, epoch 423/501 --> loss:0.02564146637916565
step 301/334, epoch 423/501 --> loss:0.021411422491073608
step 51/334, epoch 424/501 --> loss:0.019985398054122926
step 101/334, epoch 424/501 --> loss:0.023798208236694336
step 151/334, epoch 424/501 --> loss:0.028282710313796998
step 201/334, epoch 424/501 --> loss:0.023565309047698976
step 251/334, epoch 424/501 --> loss:0.02417151093482971
step 301/334, epoch 424/501 --> loss:0.02024218559265137
step 51/334, epoch 425/501 --> loss:0.026608678102493285
step 101/334, epoch 425/501 --> loss:0.024779603481292725
step 151/334, epoch 425/501 --> loss:0.049048913717269896
step 201/334, epoch 425/501 --> loss:0.06294729351997376
step 251/334, epoch 425/501 --> loss:0.04692362308502197
step 301/334, epoch 425/501 --> loss:0.03889856815338135
step 51/334, epoch 426/501 --> loss:0.028676689863204957
step 101/334, epoch 426/501 --> loss:0.02674087166786194
step 151/334, epoch 426/501 --> loss:0.03048764705657959
step 201/334, epoch 426/501 --> loss:0.028699920177459717
step 251/334, epoch 426/501 --> loss:0.024397594928741453
step 301/334, epoch 426/501 --> loss:0.02272369384765625
step 51/334, epoch 427/501 --> loss:0.025427517890930177
step 101/334, epoch 427/501 --> loss:0.032639479637145995
step 151/334, epoch 427/501 --> loss:0.01807407021522522
step 201/334, epoch 427/501 --> loss:0.027542814016342163
step 251/334, epoch 427/501 --> loss:0.024868773221969606
step 301/334, epoch 427/501 --> loss:0.02518439054489136
step 51/334, epoch 428/501 --> loss:0.027342147827148437
step 101/334, epoch 428/501 --> loss:0.024999270439147948
step 151/334, epoch 428/501 --> loss:0.022232102155685426
step 201/334, epoch 428/501 --> loss:0.024722130298614503
step 251/334, epoch 428/501 --> loss:0.02066137433052063
step 301/334, epoch 428/501 --> loss:0.026186832189559937
step 51/334, epoch 429/501 --> loss:0.01833520531654358
step 101/334, epoch 429/501 --> loss:0.026159759759902954
step 151/334, epoch 429/501 --> loss:0.03130193948745728
step 201/334, epoch 429/501 --> loss:0.026228030920028688
step 251/334, epoch 429/501 --> loss:0.02162749171257019
step 301/334, epoch 429/501 --> loss:0.02458485245704651
step 51/334, epoch 430/501 --> loss:0.019861156940460204
step 101/334, epoch 430/501 --> loss:0.027832534313201904
step 151/334, epoch 430/501 --> loss:0.02611017942428589
step 201/334, epoch 430/501 --> loss:0.022675535678863525
step 251/334, epoch 430/501 --> loss:0.02223952651023865
step 301/334, epoch 430/501 --> loss:0.02766802191734314
step 51/334, epoch 431/501 --> loss:0.024918483495712282
step 101/334, epoch 431/501 --> loss:0.027454510927200318
step 151/334, epoch 431/501 --> loss:0.021970217227935792
step 201/334, epoch 431/501 --> loss:0.02264837622642517
step 251/334, epoch 431/501 --> loss:0.020893564224243166
step 301/334, epoch 431/501 --> loss:0.021156468391418458

##########train dataset##########
acc--> [99.82867789795642]
F1--> {'F1': [0.9795218082858853], 'precision': [0.9873410262881556], 'recall': [0.971835308164152]}
##########eval dataset##########
acc--> [99.18067645108151]
F1--> {'F1': [0.9016695812804337], 'precision': [0.936689738224889], 'recall': [0.8691829371600012]}
step 51/334, epoch 432/501 --> loss:0.02158733606338501
step 101/334, epoch 432/501 --> loss:0.024557716846466064
step 151/334, epoch 432/501 --> loss:0.02077966570854187
step 201/334, epoch 432/501 --> loss:0.024664758443832396
step 251/334, epoch 432/501 --> loss:0.01948397159576416
step 301/334, epoch 432/501 --> loss:0.027543171644210815
step 51/334, epoch 433/501 --> loss:0.022312874794006347
step 101/334, epoch 433/501 --> loss:0.019895285367965698
step 151/334, epoch 433/501 --> loss:0.027841057777404785
step 201/334, epoch 433/501 --> loss:0.023333969116210936
step 251/334, epoch 433/501 --> loss:0.019513750076293947
step 301/334, epoch 433/501 --> loss:0.023978637456893923
step 51/334, epoch 434/501 --> loss:0.020330235958099366
step 101/334, epoch 434/501 --> loss:0.027999155521392823
step 151/334, epoch 434/501 --> loss:0.02888563871383667
step 201/334, epoch 434/501 --> loss:0.023916994333267213
step 251/334, epoch 434/501 --> loss:0.03308840394020081
step 301/334, epoch 434/501 --> loss:0.018732972145080566
step 51/334, epoch 435/501 --> loss:0.022269028425216674
step 101/334, epoch 435/501 --> loss:0.021487680673599244
step 151/334, epoch 435/501 --> loss:0.037118700742721555
step 201/334, epoch 435/501 --> loss:0.024856585264205932
step 251/334, epoch 435/501 --> loss:0.024304764270782472
step 301/334, epoch 435/501 --> loss:0.0248568332195282
step 51/334, epoch 436/501 --> loss:0.031703306436538695
step 101/334, epoch 436/501 --> loss:0.02341084003448486
step 151/334, epoch 436/501 --> loss:0.021768133640289306
step 201/334, epoch 436/501 --> loss:0.01911462903022766
step 251/334, epoch 436/501 --> loss:0.02148860812187195
step 301/334, epoch 436/501 --> loss:0.02165951132774353
step 51/334, epoch 437/501 --> loss:0.01786914110183716
step 101/334, epoch 437/501 --> loss:0.018033753633499145
step 151/334, epoch 437/501 --> loss:0.021837036609649658
step 201/334, epoch 437/501 --> loss:0.03434197664260864
step 251/334, epoch 437/501 --> loss:0.023674346208572387
step 301/334, epoch 437/501 --> loss:0.03086336135864258
step 51/334, epoch 438/501 --> loss:0.027189011573791503
step 101/334, epoch 438/501 --> loss:0.021551493406295776
step 151/334, epoch 438/501 --> loss:0.01852832317352295
step 201/334, epoch 438/501 --> loss:0.02897425413131714
step 251/334, epoch 438/501 --> loss:0.024887347221374513
step 301/334, epoch 438/501 --> loss:0.019451181888580322
step 51/334, epoch 439/501 --> loss:0.01984183669090271
step 101/334, epoch 439/501 --> loss:0.018328937292098998
step 151/334, epoch 439/501 --> loss:0.027635526657104493
step 201/334, epoch 439/501 --> loss:0.029973384141921997
step 251/334, epoch 439/501 --> loss:0.02504869341850281
step 301/334, epoch 439/501 --> loss:0.02730636715888977
step 51/334, epoch 440/501 --> loss:0.047308748960494994
step 101/334, epoch 440/501 --> loss:0.06894008994102478
step 151/334, epoch 440/501 --> loss:0.07364619851112365
step 201/334, epoch 440/501 --> loss:0.06596177577972412
step 251/334, epoch 440/501 --> loss:0.053404189348220825
step 301/334, epoch 440/501 --> loss:0.05420473337173462
step 51/334, epoch 441/501 --> loss:0.04957677960395813
step 101/334, epoch 441/501 --> loss:0.04769810676574707
step 151/334, epoch 441/501 --> loss:0.03754239559173584
step 201/334, epoch 441/501 --> loss:0.0326995050907135
step 251/334, epoch 441/501 --> loss:0.03465829372406006
step 301/334, epoch 441/501 --> loss:0.034665799140930174

##########train dataset##########
acc--> [99.78417967605448]
F1--> {'F1': [0.9742006403160962], 'precision': [0.9821158443600033], 'recall': [0.9664218389807046]}
##########eval dataset##########
acc--> [99.13740116362355]
F1--> {'F1': [0.8983229782952125], 'precision': [0.9156099561370326], 'recall': [0.8816862999811255]}
step 51/334, epoch 442/501 --> loss:0.02336332082748413
step 101/334, epoch 442/501 --> loss:0.02578326940536499
step 151/334, epoch 442/501 --> loss:0.037074123620986936
step 201/334, epoch 442/501 --> loss:0.02310578465461731
step 251/334, epoch 442/501 --> loss:0.025282249450683594
step 301/334, epoch 442/501 --> loss:0.029089322090148927
step 51/334, epoch 443/501 --> loss:0.028908132314682006
step 101/334, epoch 443/501 --> loss:0.022491337060928346
step 151/334, epoch 443/501 --> loss:0.027618985176086425
step 201/334, epoch 443/501 --> loss:0.05177726626396179
step 251/334, epoch 443/501 --> loss:0.029069538116455077
step 301/334, epoch 443/501 --> loss:0.02987342596054077
step 51/334, epoch 444/501 --> loss:0.03397369503974915
step 101/334, epoch 444/501 --> loss:0.034513524770736694
step 151/334, epoch 444/501 --> loss:0.024059591293334962
step 201/334, epoch 444/501 --> loss:0.021265602111816405
step 251/334, epoch 444/501 --> loss:0.023067368268966673
step 301/334, epoch 444/501 --> loss:0.027352159023284913
step 51/334, epoch 445/501 --> loss:0.026766324043273927
step 101/334, epoch 445/501 --> loss:0.029522303342819214
step 151/334, epoch 445/501 --> loss:0.0194952917098999
step 201/334, epoch 445/501 --> loss:0.018648679256439208
step 251/334, epoch 445/501 --> loss:0.023849982023239135
step 301/334, epoch 445/501 --> loss:0.025236347913742064
step 51/334, epoch 446/501 --> loss:0.022663674354553222
step 101/334, epoch 446/501 --> loss:0.023071457147598267
step 151/334, epoch 446/501 --> loss:0.03392022609710693
step 201/334, epoch 446/501 --> loss:0.02085753798484802
step 251/334, epoch 446/501 --> loss:0.014744175672531128
step 301/334, epoch 446/501 --> loss:0.0289699387550354
step 51/334, epoch 447/501 --> loss:0.026174277067184448
step 101/334, epoch 447/501 --> loss:0.027107627391815187
step 151/334, epoch 447/501 --> loss:0.028708149194717408
step 201/334, epoch 447/501 --> loss:0.01892457365989685
step 251/334, epoch 447/501 --> loss:0.02015272617340088
step 301/334, epoch 447/501 --> loss:0.027994848489761352
step 51/334, epoch 448/501 --> loss:0.02136389374732971
step 101/334, epoch 448/501 --> loss:0.02043614983558655
step 151/334, epoch 448/501 --> loss:0.025777173042297364
step 201/334, epoch 448/501 --> loss:0.020059175491333008
step 251/334, epoch 448/501 --> loss:0.023033928871154786
step 301/334, epoch 448/501 --> loss:0.02844917893409729
step 51/334, epoch 449/501 --> loss:0.03868636012077332
step 101/334, epoch 449/501 --> loss:0.021361544132232665
step 151/334, epoch 449/501 --> loss:0.02985454797744751
step 201/334, epoch 449/501 --> loss:0.019035820960998536
step 251/334, epoch 449/501 --> loss:0.019555455446243285
step 301/334, epoch 449/501 --> loss:0.028678691387176512
step 51/334, epoch 450/501 --> loss:0.02473976731300354
step 101/334, epoch 450/501 --> loss:0.021844035387039183
step 151/334, epoch 450/501 --> loss:0.022133127450942994
step 201/334, epoch 450/501 --> loss:0.031110743284225462
step 251/334, epoch 450/501 --> loss:0.027611799240112304
step 301/334, epoch 450/501 --> loss:0.022581748962402344
step 51/334, epoch 451/501 --> loss:0.02037708282470703
step 101/334, epoch 451/501 --> loss:0.025889681577682497
step 151/334, epoch 451/501 --> loss:0.02321692943572998
step 201/334, epoch 451/501 --> loss:0.026584596633911134
step 251/334, epoch 451/501 --> loss:0.02439191222190857
step 301/334, epoch 451/501 --> loss:0.01885922074317932

##########train dataset##########
acc--> [99.83076046234291]
F1--> {'F1': [0.9797967014431903], 'precision': [0.986328668991881], 'recall': [0.9733605491213578]}
##########eval dataset##########
acc--> [99.20512589047654]
F1--> {'F1': [0.9048596367364774], 'precision': [0.9372936884049475], 'recall': [0.8746045306878092]}
step 51/334, epoch 452/501 --> loss:0.02574830412864685
step 101/334, epoch 452/501 --> loss:0.029216128587722778
step 151/334, epoch 452/501 --> loss:0.018414289951324464
step 201/334, epoch 452/501 --> loss:0.02707642674446106
step 251/334, epoch 452/501 --> loss:0.021948065757751465
step 301/334, epoch 452/501 --> loss:0.02009839653968811
step 51/334, epoch 453/501 --> loss:0.02042239308357239
step 101/334, epoch 453/501 --> loss:0.04464755177497864
step 151/334, epoch 453/501 --> loss:0.024166266918182373
step 201/334, epoch 453/501 --> loss:0.017973695993423463
step 251/334, epoch 453/501 --> loss:0.022876594066619873
step 301/334, epoch 453/501 --> loss:0.023915867805480957
step 51/334, epoch 454/501 --> loss:0.02179152250289917
step 101/334, epoch 454/501 --> loss:0.01879212975502014
step 151/334, epoch 454/501 --> loss:0.02389499545097351
step 201/334, epoch 454/501 --> loss:0.02560558319091797
step 251/334, epoch 454/501 --> loss:0.03155520796775818
step 301/334, epoch 454/501 --> loss:0.023787888288497924
step 51/334, epoch 455/501 --> loss:0.03030888915061951
step 101/334, epoch 455/501 --> loss:0.021054116487503053
step 151/334, epoch 455/501 --> loss:0.022207709550857543
step 201/334, epoch 455/501 --> loss:0.028778319358825685
step 251/334, epoch 455/501 --> loss:0.03859359979629517
step 301/334, epoch 455/501 --> loss:0.021508828401565552
step 51/334, epoch 456/501 --> loss:0.028193726539611816
step 101/334, epoch 456/501 --> loss:0.04352405309677124
step 151/334, epoch 456/501 --> loss:0.02083794116973877
step 201/334, epoch 456/501 --> loss:0.0209275758266449
step 251/334, epoch 456/501 --> loss:0.019625400304794312
step 301/334, epoch 456/501 --> loss:0.03696360826492309
step 51/334, epoch 457/501 --> loss:0.01841871976852417
step 101/334, epoch 457/501 --> loss:0.028904709815979004
step 151/334, epoch 457/501 --> loss:0.021355844736099243
step 201/334, epoch 457/501 --> loss:0.0266242253780365
step 251/334, epoch 457/501 --> loss:0.024341647624969483
step 301/334, epoch 457/501 --> loss:0.019340999126434326
step 51/334, epoch 458/501 --> loss:0.02194024682044983
step 101/334, epoch 458/501 --> loss:0.025482611656188967
step 151/334, epoch 458/501 --> loss:0.03080426573753357
step 201/334, epoch 458/501 --> loss:0.026480296850204466
step 251/334, epoch 458/501 --> loss:0.02066173791885376
step 301/334, epoch 458/501 --> loss:0.022305978536605833
step 51/334, epoch 459/501 --> loss:0.025602353811264036
step 101/334, epoch 459/501 --> loss:0.021067655086517333
step 151/334, epoch 459/501 --> loss:0.02683603286743164
step 201/334, epoch 459/501 --> loss:0.024981319904327393
step 251/334, epoch 459/501 --> loss:0.020104453563690186
step 301/334, epoch 459/501 --> loss:0.01867425799369812
step 51/334, epoch 460/501 --> loss:0.018052287101745605
step 101/334, epoch 460/501 --> loss:0.02259688377380371
step 151/334, epoch 460/501 --> loss:0.021799297332763673
step 201/334, epoch 460/501 --> loss:0.035339823961257934
step 251/334, epoch 460/501 --> loss:0.020124517679214478
step 301/334, epoch 460/501 --> loss:0.022959994077682497
step 51/334, epoch 461/501 --> loss:0.02442539930343628
step 101/334, epoch 461/501 --> loss:0.021818251609802247
step 151/334, epoch 461/501 --> loss:0.02655222177505493
step 201/334, epoch 461/501 --> loss:0.03306885123252869
step 251/334, epoch 461/501 --> loss:0.036447805166244504
step 301/334, epoch 461/501 --> loss:0.02679753541946411

##########train dataset##########
acc--> [99.8062909029348]
F1--> {'F1': [0.976868983492394], 'precision': [0.9836979114930547], 'recall': [0.9701440781198977]}
##########eval dataset##########
acc--> [99.16365148722203]
F1--> {'F1': [0.9005231903234148], 'precision': [0.9265773405849893], 'recall': [0.8759036375286888]}
step 51/334, epoch 462/501 --> loss:0.026610181331634522
step 101/334, epoch 462/501 --> loss:0.028286471366882324
step 151/334, epoch 462/501 --> loss:0.024248900413513182
step 201/334, epoch 462/501 --> loss:0.019395675659179688
step 251/334, epoch 462/501 --> loss:0.034404672384262085
step 301/334, epoch 462/501 --> loss:0.042343298196792604
step 51/334, epoch 463/501 --> loss:0.06918267130851746
step 101/334, epoch 463/501 --> loss:0.03762403964996338
step 151/334, epoch 463/501 --> loss:0.024914788007736208
step 201/334, epoch 463/501 --> loss:0.02852509617805481
step 251/334, epoch 463/501 --> loss:0.035487345457077026
step 301/334, epoch 463/501 --> loss:0.02392893671989441
step 51/334, epoch 464/501 --> loss:0.027816013097763062
step 101/334, epoch 464/501 --> loss:0.022393678426742555
step 151/334, epoch 464/501 --> loss:0.028144716024398803
step 201/334, epoch 464/501 --> loss:0.022198151350021362
step 251/334, epoch 464/501 --> loss:0.020719228982925414
step 301/334, epoch 464/501 --> loss:0.029744154214859007
step 51/334, epoch 465/501 --> loss:0.02754658579826355
step 101/334, epoch 465/501 --> loss:0.02221586585044861
step 151/334, epoch 465/501 --> loss:0.036215609312057494
step 201/334, epoch 465/501 --> loss:0.02560307741165161
step 251/334, epoch 465/501 --> loss:0.02499663472175598
step 301/334, epoch 465/501 --> loss:0.018685901165008546
step 51/334, epoch 466/501 --> loss:0.027814185619354247
step 101/334, epoch 466/501 --> loss:0.02337842106819153
step 151/334, epoch 466/501 --> loss:0.0251658296585083
step 201/334, epoch 466/501 --> loss:0.020120378732681275
step 251/334, epoch 466/501 --> loss:0.03275349497795105
step 301/334, epoch 466/501 --> loss:0.02296264171600342
step 51/334, epoch 467/501 --> loss:0.018876562118530272
step 101/334, epoch 467/501 --> loss:0.024016823768615723
step 151/334, epoch 467/501 --> loss:0.025162310600280763
step 201/334, epoch 467/501 --> loss:0.01905661940574646
step 251/334, epoch 467/501 --> loss:0.028415653705596924
step 301/334, epoch 467/501 --> loss:0.02758050799369812
step 51/334, epoch 468/501 --> loss:0.023899043798446654
step 101/334, epoch 468/501 --> loss:0.022288702726364136
step 151/334, epoch 468/501 --> loss:0.024013898372650146
step 201/334, epoch 468/501 --> loss:0.028028855323791502
step 251/334, epoch 468/501 --> loss:0.018564319610595702
step 301/334, epoch 468/501 --> loss:0.022118433713912963
step 51/334, epoch 469/501 --> loss:0.0227868127822876
step 101/334, epoch 469/501 --> loss:0.03382629036903381
step 151/334, epoch 469/501 --> loss:0.03110256791114807
step 201/334, epoch 469/501 --> loss:0.02132614016532898
step 251/334, epoch 469/501 --> loss:0.027801665067672728
step 301/334, epoch 469/501 --> loss:0.020290234088897706
step 51/334, epoch 470/501 --> loss:0.03330880880355835
step 101/334, epoch 470/501 --> loss:0.05938013553619385
step 151/334, epoch 470/501 --> loss:0.0373398756980896
step 201/334, epoch 470/501 --> loss:0.03543993473052978
step 251/334, epoch 470/501 --> loss:0.029635998010635375
step 301/334, epoch 470/501 --> loss:0.02674233913421631
step 51/334, epoch 471/501 --> loss:0.026061656475067137
step 101/334, epoch 471/501 --> loss:0.03525985598564148
step 151/334, epoch 471/501 --> loss:0.028219507932662966
step 201/334, epoch 471/501 --> loss:0.02408643960952759
step 251/334, epoch 471/501 --> loss:0.02093859076499939
step 301/334, epoch 471/501 --> loss:0.03492405772209167

##########train dataset##########
acc--> [99.81481253899373]
F1--> {'F1': [0.9778538952711573], 'precision': [0.9861571921383675], 'recall': [0.9696990889858408]}
##########eval dataset##########
acc--> [99.1868173460673]
F1--> {'F1': [0.9025204735049539], 'precision': [0.9363955103830551], 'recall': [0.8710200953838605]}
step 51/334, epoch 472/501 --> loss:0.028450652360916137
step 101/334, epoch 472/501 --> loss:0.03180042982101441
step 151/334, epoch 472/501 --> loss:0.024503498077392577
step 201/334, epoch 472/501 --> loss:0.021111972332000732
step 251/334, epoch 472/501 --> loss:0.027542855739593506
step 301/334, epoch 472/501 --> loss:0.025489358901977538
step 51/334, epoch 473/501 --> loss:0.018791210651397706
step 101/334, epoch 473/501 --> loss:0.02411752223968506
step 151/334, epoch 473/501 --> loss:0.026777815818786622
step 201/334, epoch 473/501 --> loss:0.026886017322540284
step 251/334, epoch 473/501 --> loss:0.03001899480819702
step 301/334, epoch 473/501 --> loss:0.022412179708480834
step 51/334, epoch 474/501 --> loss:0.020794310569763184
step 101/334, epoch 474/501 --> loss:0.021267762184143068
step 151/334, epoch 474/501 --> loss:0.03267668008804321
step 201/334, epoch 474/501 --> loss:0.019346630573272704
step 251/334, epoch 474/501 --> loss:0.02792808532714844
step 301/334, epoch 474/501 --> loss:0.02530893087387085
step 51/334, epoch 475/501 --> loss:0.031167030334472656
step 101/334, epoch 475/501 --> loss:0.03170116782188415
step 151/334, epoch 475/501 --> loss:0.021207984685897827
step 201/334, epoch 475/501 --> loss:0.02553527593612671
step 251/334, epoch 475/501 --> loss:0.020550150871276856
step 301/334, epoch 475/501 --> loss:0.019555609226226806
step 51/334, epoch 476/501 --> loss:0.022863122224807738
step 101/334, epoch 476/501 --> loss:0.0177885639667511
step 151/334, epoch 476/501 --> loss:0.02026032567024231
step 201/334, epoch 476/501 --> loss:0.02441920280456543
step 251/334, epoch 476/501 --> loss:0.03691222190856933
step 301/334, epoch 476/501 --> loss:0.018800065517425538
step 51/334, epoch 477/501 --> loss:0.020461019277572632
step 101/334, epoch 477/501 --> loss:0.020340631008148192
step 151/334, epoch 477/501 --> loss:0.025023040771484376
step 201/334, epoch 477/501 --> loss:0.022698692083358764
step 251/334, epoch 477/501 --> loss:0.02340890288352966
step 301/334, epoch 477/501 --> loss:0.017820721864700316
step 51/334, epoch 478/501 --> loss:0.027612850666046143
step 101/334, epoch 478/501 --> loss:0.018292033672332765
step 151/334, epoch 478/501 --> loss:0.021609643697738646
step 201/334, epoch 478/501 --> loss:0.02659399390220642
step 251/334, epoch 478/501 --> loss:0.040788494348526
step 301/334, epoch 478/501 --> loss:0.02272113561630249
step 51/334, epoch 479/501 --> loss:0.024096134901046753
step 101/334, epoch 479/501 --> loss:0.020888180732727052
step 151/334, epoch 479/501 --> loss:0.02230964779853821
step 201/334, epoch 479/501 --> loss:0.037549030780792234
step 251/334, epoch 479/501 --> loss:0.016549156904220583
step 301/334, epoch 479/501 --> loss:0.01894356608390808
step 51/334, epoch 480/501 --> loss:0.020291794538497925
step 101/334, epoch 480/501 --> loss:0.026026854515075682
step 151/334, epoch 480/501 --> loss:0.015550724267959594
step 201/334, epoch 480/501 --> loss:0.02361381649971008
step 251/334, epoch 480/501 --> loss:0.017022298574447634
step 301/334, epoch 480/501 --> loss:0.022175239324569704
step 51/334, epoch 481/501 --> loss:0.03101407527923584
step 101/334, epoch 481/501 --> loss:0.023307092189788818
step 151/334, epoch 481/501 --> loss:0.020040929317474365
step 201/334, epoch 481/501 --> loss:0.021396101713180543
step 251/334, epoch 481/501 --> loss:0.02830429553985596
step 301/334, epoch 481/501 --> loss:0.020576436519622803

##########train dataset##########
acc--> [99.83673438982683]
F1--> {'F1': [0.9804734805573353], 'precision': [0.9888712437991679], 'recall': [0.9722269800025735]}
##########eval dataset##########
acc--> [99.2198653734196]
F1--> {'F1': [0.9067372894832795], 'precision': [0.9380162198956842], 'recall': [0.8774864413817767]}
save model!
step 51/334, epoch 482/501 --> loss:0.020705047845840454
step 101/334, epoch 482/501 --> loss:0.02981731414794922
step 151/334, epoch 482/501 --> loss:0.01707757592201233
step 201/334, epoch 482/501 --> loss:0.018568724393844604
step 251/334, epoch 482/501 --> loss:0.031682326793670654
step 301/334, epoch 482/501 --> loss:0.02050201416015625
step 51/334, epoch 483/501 --> loss:0.02234890580177307
step 101/334, epoch 483/501 --> loss:0.02396656632423401
step 151/334, epoch 483/501 --> loss:0.020462816953659056
step 201/334, epoch 483/501 --> loss:0.024451934099197388
step 251/334, epoch 483/501 --> loss:0.01830918073654175
step 301/334, epoch 483/501 --> loss:0.025004695653915405
step 51/334, epoch 484/501 --> loss:0.02222031831741333
step 101/334, epoch 484/501 --> loss:0.01836900591850281
step 151/334, epoch 484/501 --> loss:0.018270845413208007
step 201/334, epoch 484/501 --> loss:0.015631263256073
step 251/334, epoch 484/501 --> loss:0.022917243242263793
step 301/334, epoch 484/501 --> loss:0.03235056519508362
step 51/334, epoch 485/501 --> loss:0.025376886129379272
step 101/334, epoch 485/501 --> loss:0.022789596319198607
step 151/334, epoch 485/501 --> loss:0.01795441746711731
step 201/334, epoch 485/501 --> loss:0.017554265260696412
step 251/334, epoch 485/501 --> loss:0.03462884426116943
step 301/334, epoch 485/501 --> loss:0.020159051418304444
step 51/334, epoch 486/501 --> loss:0.02471123814582825
step 101/334, epoch 486/501 --> loss:0.019632564783096315
step 151/334, epoch 486/501 --> loss:0.02266842722892761
step 201/334, epoch 486/501 --> loss:0.020222587585449217
step 251/334, epoch 486/501 --> loss:0.018925161361694337
step 301/334, epoch 486/501 --> loss:0.030956239700317384
step 51/334, epoch 487/501 --> loss:0.023388082981109618
step 101/334, epoch 487/501 --> loss:0.01975889563560486
step 151/334, epoch 487/501 --> loss:0.025784015655517578
step 201/334, epoch 487/501 --> loss:0.018509323596954345
step 251/334, epoch 487/501 --> loss:0.022034090757369996
step 301/334, epoch 487/501 --> loss:0.017168974876403807
step 51/334, epoch 488/501 --> loss:0.027802289724349977
step 101/334, epoch 488/501 --> loss:0.01821834921836853
step 151/334, epoch 488/501 --> loss:0.0191918408870697
step 201/334, epoch 488/501 --> loss:0.02883587121963501
step 251/334, epoch 488/501 --> loss:0.024197841882705688
step 301/334, epoch 488/501 --> loss:0.01724771499633789
step 51/334, epoch 489/501 --> loss:0.03291691660881042
step 101/334, epoch 489/501 --> loss:0.027140337228775024
step 151/334, epoch 489/501 --> loss:0.01954634189605713
step 201/334, epoch 489/501 --> loss:0.02002226948738098
step 251/334, epoch 489/501 --> loss:0.023854798078536986
step 301/334, epoch 489/501 --> loss:0.017862225770950316
step 51/334, epoch 490/501 --> loss:0.02654048681259155
step 101/334, epoch 490/501 --> loss:0.02184421181678772
step 151/334, epoch 490/501 --> loss:0.024142860174179076
step 201/334, epoch 490/501 --> loss:0.023345491886138915
step 251/334, epoch 490/501 --> loss:0.025726387500762938
step 301/334, epoch 490/501 --> loss:0.022209653854370116
step 51/334, epoch 491/501 --> loss:0.021266834735870363
step 101/334, epoch 491/501 --> loss:0.024095990657806397
step 151/334, epoch 491/501 --> loss:0.017502384185791017
step 201/334, epoch 491/501 --> loss:0.023809490203857423
step 251/334, epoch 491/501 --> loss:0.020559624433517457
step 301/334, epoch 491/501 --> loss:0.02827561378479004

##########train dataset##########
acc--> [99.83374871338428]
F1--> {'F1': [0.9801447073795885], 'precision': [0.9871142581316227], 'recall': [0.9732827438566073]}
##########eval dataset##########
acc--> [99.22238981524961]
F1--> {'F1': [0.9072779898838707], 'precision': [0.9360037676896897], 'recall': [0.8802722935337772]}
save model!
step 51/334, epoch 492/501 --> loss:0.017796946763992308
step 101/334, epoch 492/501 --> loss:0.019967942237854003
step 151/334, epoch 492/501 --> loss:0.025527849197387695
step 201/334, epoch 492/501 --> loss:0.02818553686141968
step 251/334, epoch 492/501 --> loss:0.020264471769332885
step 301/334, epoch 492/501 --> loss:0.032393128871917726
step 51/334, epoch 493/501 --> loss:0.022074823379516603
step 101/334, epoch 493/501 --> loss:0.02870513081550598
step 151/334, epoch 493/501 --> loss:0.024662227630615235
step 201/334, epoch 493/501 --> loss:0.01742482781410217
step 251/334, epoch 493/501 --> loss:0.022550947666168213
step 301/334, epoch 493/501 --> loss:0.02558436632156372
step 51/334, epoch 494/501 --> loss:0.02471633791923523
step 101/334, epoch 494/501 --> loss:0.02069209933280945
step 151/334, epoch 494/501 --> loss:0.023745518922805787
step 201/334, epoch 494/501 --> loss:0.02584684133529663
step 251/334, epoch 494/501 --> loss:0.023352354764938354
step 301/334, epoch 494/501 --> loss:0.01770140528678894
step 51/334, epoch 495/501 --> loss:0.016768324375152587
step 101/334, epoch 495/501 --> loss:0.020217708349227904
step 151/334, epoch 495/501 --> loss:0.031098928451538086
step 201/334, epoch 495/501 --> loss:0.02429134964942932
step 251/334, epoch 495/501 --> loss:0.02351252198219299
step 301/334, epoch 495/501 --> loss:0.02276828408241272
step 51/334, epoch 496/501 --> loss:0.024676848649978638
step 101/334, epoch 496/501 --> loss:0.024967751502990722
step 151/334, epoch 496/501 --> loss:0.016074891090393065
step 201/334, epoch 496/501 --> loss:0.018068803548812865
step 251/334, epoch 496/501 --> loss:0.018606462478637696
step 301/334, epoch 496/501 --> loss:0.029271671772003172
step 51/334, epoch 497/501 --> loss:0.01771791934967041
step 101/334, epoch 497/501 --> loss:0.02447147488594055
step 151/334, epoch 497/501 --> loss:0.02307081937789917
step 201/334, epoch 497/501 --> loss:0.025440831184387207
step 251/334, epoch 497/501 --> loss:0.024803849458694457
step 301/334, epoch 497/501 --> loss:0.020100239515304565
step 51/334, epoch 498/501 --> loss:0.02280093789100647
step 101/334, epoch 498/501 --> loss:0.018331474065780638
step 151/334, epoch 498/501 --> loss:0.029443376064300537
step 201/334, epoch 498/501 --> loss:0.02325555443763733
step 251/334, epoch 498/501 --> loss:0.0228338623046875
step 301/334, epoch 498/501 --> loss:0.019479483366012573
step 51/334, epoch 499/501 --> loss:0.030666329860687257
step 101/334, epoch 499/501 --> loss:0.0231815242767334
step 151/334, epoch 499/501 --> loss:0.01974900722503662
step 201/334, epoch 499/501 --> loss:0.025035698413848877
step 251/334, epoch 499/501 --> loss:0.043282921314239504
step 301/334, epoch 499/501 --> loss:0.022896218299865722
step 51/334, epoch 500/501 --> loss:0.0659012746810913
step 101/334, epoch 500/501 --> loss:0.04064013123512268
step 151/334, epoch 500/501 --> loss:0.046067073345184326
step 201/334, epoch 500/501 --> loss:0.03453574180603027
step 251/334, epoch 500/501 --> loss:0.05136644959449768
step 301/334, epoch 500/501 --> loss:0.03366393804550171
step 51/334, epoch 501/501 --> loss:0.04976795792579651
step 101/334, epoch 501/501 --> loss:0.029510107040405274
step 151/334, epoch 501/501 --> loss:0.03289312362670899
step 201/334, epoch 501/501 --> loss:0.03029080033302307
step 251/334, epoch 501/501 --> loss:0.02476459860801697
step 301/334, epoch 501/501 --> loss:0.02705252170562744

##########train dataset##########
acc--> [99.81459999155703]
F1--> {'F1': [0.9778490028108822], 'precision': [0.9852247408482127], 'recall': [0.9705927302742655]}
##########eval dataset##########
acc--> [99.2059789408887]
F1--> {'F1': [0.9056893954304743], 'precision': [0.9305175884399816], 'recall': [0.8821611874675384]}
