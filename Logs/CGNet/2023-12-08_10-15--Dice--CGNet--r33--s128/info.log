##########Config##########
{'device': 'cuda:0', 'class_nums': 2, 'data_path': 'Datasets/WHU-BCD', 'image_size': 128, 'num_parallel_workers': 4, 'batch_size': 64, 'input_dim': 6, 'seed': 33, 'pretrained': False, 'resume': '', 'eval_epochs': 10, 'start_eval_epochs': 0, 'eval_traindata': True, 'epoch_size': 501, 'loss_monitor_step': 50, 'metrics_List': ['acc', 'F1'], 'save_metrics_List': ['F1'], 'save_model_path': 'Models/CGNet', 'log_path': 'Logs/CGNet', 'lr_init': 0.0005, 'lr_max': 0.0005, 'lr_end': 5e-05, 'warmup_epochs': 0}

##########Network##########
Backbone(
  (initial_layer): CGBlock(
    (conv1): Conv2d(6, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (context_module): CGBlock(
    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (multi_scale_module): CGBlock(
    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (final_layer): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))
  (softmax): Softmax(dim=1)
)

##########Training##########
step 51/334, epoch 1/501 --> loss:0.8829679417610169
step 101/334, epoch 1/501 --> loss:0.8661040103435517
step 151/334, epoch 1/501 --> loss:0.8580734658241272
step 201/334, epoch 1/501 --> loss:0.8530854904651641
step 251/334, epoch 1/501 --> loss:0.8432946920394897
step 301/334, epoch 1/501 --> loss:0.837052960395813

##########train dataset##########
acc--> [94.79501450334183]
F1--> {'F1': [0.5777529516437594], 'precision': [0.43909787468907197], 'recall': [0.8444065645930934]}
##########eval dataset##########
acc--> [94.93257684448355]
F1--> {'F1': [0.5864100250973573], 'precision': [0.45300996525023135], 'recall': [0.8311865330235505]}
save model!
step 51/334, epoch 2/501 --> loss:0.8471835684776307
step 101/334, epoch 2/501 --> loss:0.841259150505066
step 151/334, epoch 2/501 --> loss:0.8533678543567658
step 201/334, epoch 2/501 --> loss:0.8557852578163146
step 251/334, epoch 2/501 --> loss:0.8259447252750397
step 301/334, epoch 2/501 --> loss:0.8492344439029693
step 51/334, epoch 3/501 --> loss:0.8341918611526489
step 101/334, epoch 3/501 --> loss:0.8416774845123292
step 151/334, epoch 3/501 --> loss:0.8509912419319153
step 201/334, epoch 3/501 --> loss:0.8458661949634552
step 251/334, epoch 3/501 --> loss:0.8539333188533783
step 301/334, epoch 3/501 --> loss:0.8293576669692994
step 51/334, epoch 4/501 --> loss:0.8441868841648101
step 101/334, epoch 4/501 --> loss:0.8546362817287445
step 151/334, epoch 4/501 --> loss:0.8414977288246155
step 201/334, epoch 4/501 --> loss:0.8421447670459747
step 251/334, epoch 4/501 --> loss:0.8461878228187562
step 301/334, epoch 4/501 --> loss:0.8328771412372589
step 51/334, epoch 5/501 --> loss:0.8517422688007354
step 101/334, epoch 5/501 --> loss:0.840633111000061
step 151/334, epoch 5/501 --> loss:0.8443578028678894
step 201/334, epoch 5/501 --> loss:0.8565247285366059
step 251/334, epoch 5/501 --> loss:0.8259788048267365
step 301/334, epoch 5/501 --> loss:0.8429610204696655
step 51/334, epoch 6/501 --> loss:0.8461435317993165
step 101/334, epoch 6/501 --> loss:0.8355501472949982
step 151/334, epoch 6/501 --> loss:0.8381919705867767
step 201/334, epoch 6/501 --> loss:0.8561422669887543
step 251/334, epoch 6/501 --> loss:0.8553007650375366
step 301/334, epoch 6/501 --> loss:0.8362951934337616
step 51/334, epoch 7/501 --> loss:0.8477775657176971
step 101/334, epoch 7/501 --> loss:0.8390894365310669
step 151/334, epoch 7/501 --> loss:0.8460965311527252
step 201/334, epoch 7/501 --> loss:0.8410719501972198
step 251/334, epoch 7/501 --> loss:0.8371485590934753
step 301/334, epoch 7/501 --> loss:0.8386030602455139
step 51/334, epoch 8/501 --> loss:0.8491218090057373
step 101/334, epoch 8/501 --> loss:0.8386826348304749
step 151/334, epoch 8/501 --> loss:0.8492713904380799
step 201/334, epoch 8/501 --> loss:0.8474355268478394
step 251/334, epoch 8/501 --> loss:0.8347410356998444
step 301/334, epoch 8/501 --> loss:0.8397938454151154
step 51/334, epoch 9/501 --> loss:0.8342067003250122
step 101/334, epoch 9/501 --> loss:0.8414996075630188
step 151/334, epoch 9/501 --> loss:0.8393290877342224
step 201/334, epoch 9/501 --> loss:0.8424884557724
step 251/334, epoch 9/501 --> loss:0.8372732889652252
step 301/334, epoch 9/501 --> loss:0.8397415447235107
step 51/334, epoch 10/501 --> loss:0.8275790309906006
step 101/334, epoch 10/501 --> loss:0.8413468647003174
step 151/334, epoch 10/501 --> loss:0.8355203545093537
step 201/334, epoch 10/501 --> loss:0.847651617527008
step 251/334, epoch 10/501 --> loss:0.853832710981369
step 301/334, epoch 10/501 --> loss:0.8303867518901825
step 51/334, epoch 11/501 --> loss:0.834915795326233
step 101/334, epoch 11/501 --> loss:0.8368336856365204
step 151/334, epoch 11/501 --> loss:0.8367779290676117
step 201/334, epoch 11/501 --> loss:0.8448480188846588
step 251/334, epoch 11/501 --> loss:0.8432902538776398
step 301/334, epoch 11/501 --> loss:0.8436419415473938

##########train dataset##########
acc--> [97.10420709358684]
F1--> {'F1': [0.6919606512930015], 'precision': [0.6274553389854078], 'recall': [0.7712608651790974]}
##########eval dataset##########
acc--> [97.16035270523396]
F1--> {'F1': [0.7027391015113523], 'precision': [0.6417092093572035], 'recall': [0.7766096832557968]}
save model!
step 51/334, epoch 12/501 --> loss:0.8493573999404908
step 101/334, epoch 12/501 --> loss:0.8304987812042236
step 151/334, epoch 12/501 --> loss:0.8471336567401886
step 201/334, epoch 12/501 --> loss:0.8269586503505707
step 251/334, epoch 12/501 --> loss:0.843282766342163
step 301/334, epoch 12/501 --> loss:0.8334221422672272
step 51/334, epoch 13/501 --> loss:0.8362229681015014
step 101/334, epoch 13/501 --> loss:0.8383446311950684
step 151/334, epoch 13/501 --> loss:0.8330072748661042
step 201/334, epoch 13/501 --> loss:0.8410969030857086
step 251/334, epoch 13/501 --> loss:0.844986652135849
step 301/334, epoch 13/501 --> loss:0.8343263578414917
step 51/334, epoch 14/501 --> loss:0.8369496655464173
step 101/334, epoch 14/501 --> loss:0.8364361488819122
step 151/334, epoch 14/501 --> loss:0.8477050995826722
step 201/334, epoch 14/501 --> loss:0.8377239775657653
step 251/334, epoch 14/501 --> loss:0.8317129027843475
step 301/334, epoch 14/501 --> loss:0.8462349665164948
step 51/334, epoch 15/501 --> loss:0.8348262453079224
step 101/334, epoch 15/501 --> loss:0.843704080581665
step 151/334, epoch 15/501 --> loss:0.8212708520889282
step 201/334, epoch 15/501 --> loss:0.8508644449710846
step 251/334, epoch 15/501 --> loss:0.8312111032009125
step 301/334, epoch 15/501 --> loss:0.8429409039020538
step 51/334, epoch 16/501 --> loss:0.841498761177063
step 101/334, epoch 16/501 --> loss:0.8371450591087342
step 151/334, epoch 16/501 --> loss:0.8407718443870544
step 201/334, epoch 16/501 --> loss:0.8377287542819977
step 251/334, epoch 16/501 --> loss:0.8430597054958343
step 301/334, epoch 16/501 --> loss:0.8399260151386261
step 51/334, epoch 17/501 --> loss:0.8420144748687745
step 101/334, epoch 17/501 --> loss:0.8390172290802002
step 151/334, epoch 17/501 --> loss:0.8468337666988373
step 201/334, epoch 17/501 --> loss:0.8463713157176972
step 251/334, epoch 17/501 --> loss:0.8379523634910584
step 301/334, epoch 17/501 --> loss:0.8314794087409973
step 51/334, epoch 18/501 --> loss:0.8416620051860809
step 101/334, epoch 18/501 --> loss:0.8302920651435852
step 151/334, epoch 18/501 --> loss:0.8383236634731293
step 201/334, epoch 18/501 --> loss:0.8427923309803009
step 251/334, epoch 18/501 --> loss:0.8389319908618927
step 301/334, epoch 18/501 --> loss:0.8339519906044006
step 51/334, epoch 19/501 --> loss:0.8331697404384613
step 101/334, epoch 19/501 --> loss:0.839060846567154
step 151/334, epoch 19/501 --> loss:0.8324176144599914
step 201/334, epoch 19/501 --> loss:0.8414514970779419
step 251/334, epoch 19/501 --> loss:0.8340668463706971
step 301/334, epoch 19/501 --> loss:0.8424312198162078
step 51/334, epoch 20/501 --> loss:0.8367643272876739
step 101/334, epoch 20/501 --> loss:0.8360061371326446
step 151/334, epoch 20/501 --> loss:0.8338994085788727
step 201/334, epoch 20/501 --> loss:0.8455871975421906
step 251/334, epoch 20/501 --> loss:0.8454873883724212
step 301/334, epoch 20/501 --> loss:0.8321689426898956
step 51/334, epoch 21/501 --> loss:0.8362642872333527
step 101/334, epoch 21/501 --> loss:0.8340016579627991
step 151/334, epoch 21/501 --> loss:0.8420514225959778
step 201/334, epoch 21/501 --> loss:0.8398824262619019
step 251/334, epoch 21/501 --> loss:0.8253571152687073
step 301/334, epoch 21/501 --> loss:0.8488813829421997

##########train dataset##########
acc--> [96.14886586747181]
F1--> {'F1': [0.6606673168764141], 'precision': [0.5256613360078441], 'recall': [0.88900322637412]}
##########eval dataset##########
acc--> [96.21486113139633]
F1--> {'F1': [0.6663682149957766], 'precision': [0.5382296350815621], 'recall': [0.874600360944026]}
step 51/334, epoch 22/501 --> loss:0.8545782387256622
step 101/334, epoch 22/501 --> loss:0.8460626482963562
step 151/334, epoch 22/501 --> loss:0.8213348650932312
step 201/334, epoch 22/501 --> loss:0.8264678800106049
step 251/334, epoch 22/501 --> loss:0.8309091174602509
step 301/334, epoch 22/501 --> loss:0.8545528137683869
step 51/334, epoch 23/501 --> loss:0.8417466330528259
step 101/334, epoch 23/501 --> loss:0.848303507566452
step 151/334, epoch 23/501 --> loss:0.8374097454547882
step 201/334, epoch 23/501 --> loss:0.8207577800750733
step 251/334, epoch 23/501 --> loss:0.8285027348995209
step 301/334, epoch 23/501 --> loss:0.8333624184131623
step 51/334, epoch 24/501 --> loss:0.8359838235378265
step 101/334, epoch 24/501 --> loss:0.8470096457004547
step 151/334, epoch 24/501 --> loss:0.8446784269809723
step 201/334, epoch 24/501 --> loss:0.8476271665096283
step 251/334, epoch 24/501 --> loss:0.8393164849281312
step 301/334, epoch 24/501 --> loss:0.832934045791626
step 51/334, epoch 25/501 --> loss:0.8299905753135681
step 101/334, epoch 25/501 --> loss:0.8350380039215088
step 151/334, epoch 25/501 --> loss:0.8496239984035492
step 201/334, epoch 25/501 --> loss:0.8380454432964325
step 251/334, epoch 25/501 --> loss:0.8326838004589081
step 301/334, epoch 25/501 --> loss:0.8356308221817017
step 51/334, epoch 26/501 --> loss:0.8273444211483002
step 101/334, epoch 26/501 --> loss:0.8271988332271576
step 151/334, epoch 26/501 --> loss:0.8402844178676605
step 201/334, epoch 26/501 --> loss:0.8274445223808289
step 251/334, epoch 26/501 --> loss:0.8413566470146179
step 301/334, epoch 26/501 --> loss:0.8528085172176361
step 51/334, epoch 27/501 --> loss:0.8337240290641784
step 101/334, epoch 27/501 --> loss:0.8340411341190338
step 151/334, epoch 27/501 --> loss:0.8342508113384247
step 201/334, epoch 27/501 --> loss:0.839823180437088
step 251/334, epoch 27/501 --> loss:0.8307038629055024
step 301/334, epoch 27/501 --> loss:0.8509650218486786
step 51/334, epoch 28/501 --> loss:0.8421192991733552
step 101/334, epoch 28/501 --> loss:0.8455213344097138
step 151/334, epoch 28/501 --> loss:0.833349906206131
step 201/334, epoch 28/501 --> loss:0.8347557842731476
step 251/334, epoch 28/501 --> loss:0.831772346496582
step 301/334, epoch 28/501 --> loss:0.8420877659320831
step 51/334, epoch 29/501 --> loss:0.8272809243202209
step 101/334, epoch 29/501 --> loss:0.8265133285522461
step 151/334, epoch 29/501 --> loss:0.8360459864139557
step 201/334, epoch 29/501 --> loss:0.8452469503879547
step 251/334, epoch 29/501 --> loss:0.8522591304779052
step 301/334, epoch 29/501 --> loss:0.8367430257797241
step 51/334, epoch 30/501 --> loss:0.8442611265182495
step 101/334, epoch 30/501 --> loss:0.8297903466224671
step 151/334, epoch 30/501 --> loss:0.8414521193504334
step 201/334, epoch 30/501 --> loss:0.8313904428482055
step 251/334, epoch 30/501 --> loss:0.8352696585655213
step 301/334, epoch 30/501 --> loss:0.8362038064002991
step 51/334, epoch 31/501 --> loss:0.8282177937030792
step 101/334, epoch 31/501 --> loss:0.8420675003528595
step 151/334, epoch 31/501 --> loss:0.8365365183353424
step 201/334, epoch 31/501 --> loss:0.8357851588726044
step 251/334, epoch 31/501 --> loss:0.8543264865875244
step 301/334, epoch 31/501 --> loss:0.8366556072235107

##########train dataset##########
acc--> [96.43060250097857]
F1--> {'F1': [0.6748470736503707], 'precision': [0.5479083164607901], 'recall': [0.8783558438361656]}
##########eval dataset##########
acc--> [96.47520504180717]
F1--> {'F1': [0.6798464439521086], 'precision': [0.5596120825078622], 'recall': [0.8658998044531229]}
step 51/334, epoch 32/501 --> loss:0.8429836368560791
step 101/334, epoch 32/501 --> loss:0.8316164064407349
step 151/334, epoch 32/501 --> loss:0.8226765835285187
step 201/334, epoch 32/501 --> loss:0.8443937253952026
step 251/334, epoch 32/501 --> loss:0.8297109138965607
step 301/334, epoch 32/501 --> loss:0.8386444699764252
step 51/334, epoch 33/501 --> loss:0.8473054885864257
step 101/334, epoch 33/501 --> loss:0.8407061791419983
step 151/334, epoch 33/501 --> loss:0.8371395587921142
step 201/334, epoch 33/501 --> loss:0.8180488443374634
step 251/334, epoch 33/501 --> loss:0.8316595065593719
step 301/334, epoch 33/501 --> loss:0.8352032327651977
step 51/334, epoch 34/501 --> loss:0.8299709129333496
step 101/334, epoch 34/501 --> loss:0.8199444401264191
step 151/334, epoch 34/501 --> loss:0.829471492767334
step 201/334, epoch 34/501 --> loss:0.8458030652999878
step 251/334, epoch 34/501 --> loss:0.8395929276943207
step 301/334, epoch 34/501 --> loss:0.8465272641181946
step 51/334, epoch 35/501 --> loss:0.8460508239269257
step 101/334, epoch 35/501 --> loss:0.8130237555503845
step 151/334, epoch 35/501 --> loss:0.8413318586349487
step 201/334, epoch 35/501 --> loss:0.8329794299602509
step 251/334, epoch 35/501 --> loss:0.8414114248752594
step 301/334, epoch 35/501 --> loss:0.848310513496399
step 51/334, epoch 36/501 --> loss:0.8330608010292053
step 101/334, epoch 36/501 --> loss:0.8391214501857758
step 151/334, epoch 36/501 --> loss:0.8335969066619873
step 201/334, epoch 36/501 --> loss:0.8381838500499725
step 251/334, epoch 36/501 --> loss:0.8261158108711243
step 301/334, epoch 36/501 --> loss:0.8460506236553192
step 51/334, epoch 37/501 --> loss:0.8546720802783966
step 101/334, epoch 37/501 --> loss:0.8337947475910187
step 151/334, epoch 37/501 --> loss:0.8250635528564453
step 201/334, epoch 37/501 --> loss:0.8412605202198029
step 251/334, epoch 37/501 --> loss:0.8299553513526916
step 301/334, epoch 37/501 --> loss:0.8326042532920838
step 51/334, epoch 38/501 --> loss:0.8345071446895599
step 101/334, epoch 38/501 --> loss:0.8358602404594422
step 151/334, epoch 38/501 --> loss:0.8354635107517242
step 201/334, epoch 38/501 --> loss:0.830845535993576
step 251/334, epoch 38/501 --> loss:0.8438350141048432
step 301/334, epoch 38/501 --> loss:0.8409086298942566
step 51/334, epoch 39/501 --> loss:0.8253963959217071
step 101/334, epoch 39/501 --> loss:0.8287218034267425
step 151/334, epoch 39/501 --> loss:0.8371131002902985
step 201/334, epoch 39/501 --> loss:0.8360925817489624
step 251/334, epoch 39/501 --> loss:0.8463899230957032
step 301/334, epoch 39/501 --> loss:0.8447743093967438
step 51/334, epoch 40/501 --> loss:0.8333371567726136
step 101/334, epoch 40/501 --> loss:0.8383892107009888
step 151/334, epoch 40/501 --> loss:0.842072502374649
step 201/334, epoch 40/501 --> loss:0.826404242515564
step 251/334, epoch 40/501 --> loss:0.835577803850174
step 301/334, epoch 40/501 --> loss:0.8204249966144562
step 51/334, epoch 41/501 --> loss:0.8407966077327729
step 101/334, epoch 41/501 --> loss:0.8343806838989258
step 151/334, epoch 41/501 --> loss:0.8231233179569244
step 201/334, epoch 41/501 --> loss:0.8395046412944793
step 251/334, epoch 41/501 --> loss:0.8486164116859436
step 301/334, epoch 41/501 --> loss:0.8352027237415314

##########train dataset##########
acc--> [96.45572085929167]
F1--> {'F1': [0.6860405644009643], 'precision': [0.5475747877252163], 'recall': [0.918252511389844]}
##########eval dataset##########
acc--> [96.46049960078199]
F1--> {'F1': [0.6860762071465865], 'precision': [0.5562797519414571], 'recall': [0.8948935773268254]}
step 51/334, epoch 42/501 --> loss:0.8345528137683869
step 101/334, epoch 42/501 --> loss:0.8402700972557068
step 151/334, epoch 42/501 --> loss:0.8296260142326355
step 201/334, epoch 42/501 --> loss:0.8331042921543121
step 251/334, epoch 42/501 --> loss:0.8456952476501465
step 301/334, epoch 42/501 --> loss:0.8337585937976837
step 51/334, epoch 43/501 --> loss:0.8462917459011078
step 101/334, epoch 43/501 --> loss:0.8393064320087433
step 151/334, epoch 43/501 --> loss:0.8379512929916382
step 201/334, epoch 43/501 --> loss:0.8293572413921356
step 251/334, epoch 43/501 --> loss:0.8241457068920135
step 301/334, epoch 43/501 --> loss:0.8337158572673797
step 51/334, epoch 44/501 --> loss:0.8262489414215088
step 101/334, epoch 44/501 --> loss:0.8330555891990662
step 151/334, epoch 44/501 --> loss:0.8294960916042328
step 201/334, epoch 44/501 --> loss:0.8487898802757263
step 251/334, epoch 44/501 --> loss:0.8353168177604675
step 301/334, epoch 44/501 --> loss:0.8351318621635437
step 51/334, epoch 45/501 --> loss:0.8321016943454742
step 101/334, epoch 45/501 --> loss:0.8374975037574768
step 151/334, epoch 45/501 --> loss:0.8263311100006103
step 201/334, epoch 45/501 --> loss:0.8331902229785919
step 251/334, epoch 45/501 --> loss:0.8384483873844146
step 301/334, epoch 45/501 --> loss:0.8410171031951904
step 51/334, epoch 46/501 --> loss:0.847775764465332
step 101/334, epoch 46/501 --> loss:0.8310079634189605
step 151/334, epoch 46/501 --> loss:0.8417000114917755
step 201/334, epoch 46/501 --> loss:0.8361810457706451
step 251/334, epoch 46/501 --> loss:0.8327071440219879
step 301/334, epoch 46/501 --> loss:0.8315659403800965
step 51/334, epoch 47/501 --> loss:0.8389288020133973
step 101/334, epoch 47/501 --> loss:0.8421742498874665
step 151/334, epoch 47/501 --> loss:0.8164700424671173
step 201/334, epoch 47/501 --> loss:0.8361016488075257
step 251/334, epoch 47/501 --> loss:0.8299523890018463
step 301/334, epoch 47/501 --> loss:0.8395658874511719
step 51/334, epoch 48/501 --> loss:0.8496479654312133
step 101/334, epoch 48/501 --> loss:0.8311966753005982
step 151/334, epoch 48/501 --> loss:0.822090939283371
step 201/334, epoch 48/501 --> loss:0.837834050655365
step 251/334, epoch 48/501 --> loss:0.8323011076450348
step 301/334, epoch 48/501 --> loss:0.8406440484523773
step 51/334, epoch 49/501 --> loss:0.8242357218265534
step 101/334, epoch 49/501 --> loss:0.8332938754558563
step 151/334, epoch 49/501 --> loss:0.8398483753204345
step 201/334, epoch 49/501 --> loss:0.8245317316055298
step 251/334, epoch 49/501 --> loss:0.8395495307445526
step 301/334, epoch 49/501 --> loss:0.8418106460571289
step 51/334, epoch 50/501 --> loss:0.8387025725841523
step 101/334, epoch 50/501 --> loss:0.8306412279605866
step 151/334, epoch 50/501 --> loss:0.8369541120529175
step 201/334, epoch 50/501 --> loss:0.8218035888671875
step 251/334, epoch 50/501 --> loss:0.832617107629776
step 301/334, epoch 50/501 --> loss:0.8365152835845947
step 51/334, epoch 51/501 --> loss:0.8375485944747925
step 101/334, epoch 51/501 --> loss:0.8365041899681092
step 151/334, epoch 51/501 --> loss:0.8342842137813569
step 201/334, epoch 51/501 --> loss:0.8332601833343506
step 251/334, epoch 51/501 --> loss:0.837067437171936
step 301/334, epoch 51/501 --> loss:0.8329980647563935

##########train dataset##########
acc--> [94.92276323659344]
F1--> {'F1': [0.6087531695509029], 'precision': [0.4509098670494027], 'recall': [0.9366435188649028]}
##########eval dataset##########
acc--> [94.9714927639902]
F1--> {'F1': [0.6111424970717747], 'precision': [0.45897713313018673], 'recall': [0.9142626549393176]}
step 51/334, epoch 52/501 --> loss:0.8320613873004913
step 101/334, epoch 52/501 --> loss:0.8391700327396393
step 151/334, epoch 52/501 --> loss:0.8325296008586883
step 201/334, epoch 52/501 --> loss:0.8346101796627045
step 251/334, epoch 52/501 --> loss:0.8311403846740723
step 301/334, epoch 52/501 --> loss:0.8328287947177887
step 51/334, epoch 53/501 --> loss:0.812502269744873
step 101/334, epoch 53/501 --> loss:0.8370678508281708
step 151/334, epoch 53/501 --> loss:0.8325412118434906
step 201/334, epoch 53/501 --> loss:0.8523182022571564
step 251/334, epoch 53/501 --> loss:0.834155570268631
step 301/334, epoch 53/501 --> loss:0.8425003612041473
step 51/334, epoch 54/501 --> loss:0.835756367444992
step 101/334, epoch 54/501 --> loss:0.8446357822418213
step 151/334, epoch 54/501 --> loss:0.8305387651920318
step 201/334, epoch 54/501 --> loss:0.8301475071907043
step 251/334, epoch 54/501 --> loss:0.8323115563392639
step 301/334, epoch 54/501 --> loss:0.8208660912513733
step 51/334, epoch 55/501 --> loss:0.8346478319168091
step 101/334, epoch 55/501 --> loss:0.8355961954593658
step 151/334, epoch 55/501 --> loss:0.831045949459076
step 201/334, epoch 55/501 --> loss:0.8346291971206665
step 251/334, epoch 55/501 --> loss:0.8291415309906006
step 301/334, epoch 55/501 --> loss:0.8429361033439636
step 51/334, epoch 56/501 --> loss:0.8283650326728821
step 101/334, epoch 56/501 --> loss:0.8395116245746612
step 151/334, epoch 56/501 --> loss:0.8473073434829712
step 201/334, epoch 56/501 --> loss:0.8296150267124176
step 251/334, epoch 56/501 --> loss:0.826619336605072
step 301/334, epoch 56/501 --> loss:0.8353801393508911
step 51/334, epoch 57/501 --> loss:0.8196016335487366
step 101/334, epoch 57/501 --> loss:0.8361359488964081
step 151/334, epoch 57/501 --> loss:0.8288982045650483
step 201/334, epoch 57/501 --> loss:0.8464948415756226
step 251/334, epoch 57/501 --> loss:0.8281682980060577
step 301/334, epoch 57/501 --> loss:0.8377883279323578
step 51/334, epoch 58/501 --> loss:0.8371386361122132
step 101/334, epoch 58/501 --> loss:0.8337919914722443
step 151/334, epoch 58/501 --> loss:0.827538697719574
step 201/334, epoch 58/501 --> loss:0.8292261815071106
step 251/334, epoch 58/501 --> loss:0.8345376694202423
step 301/334, epoch 58/501 --> loss:0.8402612257003784
step 51/334, epoch 59/501 --> loss:0.8400797426700592
step 101/334, epoch 59/501 --> loss:0.8299473333358764
step 151/334, epoch 59/501 --> loss:0.8177880263328552
step 201/334, epoch 59/501 --> loss:0.8338454675674438
step 251/334, epoch 59/501 --> loss:0.8497752821445466
step 301/334, epoch 59/501 --> loss:0.8358499872684478
step 51/334, epoch 60/501 --> loss:0.8450869834423065
step 101/334, epoch 60/501 --> loss:0.8168200242519379
step 151/334, epoch 60/501 --> loss:0.8430831480026245
step 201/334, epoch 60/501 --> loss:0.8389131319522858
step 251/334, epoch 60/501 --> loss:0.8397709739208221
step 301/334, epoch 60/501 --> loss:0.8307607734203338
step 51/334, epoch 61/501 --> loss:0.8364959633350373
step 101/334, epoch 61/501 --> loss:0.8216449582576751
step 151/334, epoch 61/501 --> loss:0.8264541983604431
step 201/334, epoch 61/501 --> loss:0.8419069170951843
step 251/334, epoch 61/501 --> loss:0.8393071782588959
step 301/334, epoch 61/501 --> loss:0.8339583468437195

##########train dataset##########
acc--> [95.93107082622699]
F1--> {'F1': [0.6514757094899948], 'precision': [0.5099390027619587], 'recall': [0.9017842504094261]}
##########eval dataset##########
acc--> [95.77367987844873]
F1--> {'F1': [0.6434754861847347], 'precision': [0.5063602579403916], 'recall': [0.8824407147359635]}
step 51/334, epoch 62/501 --> loss:0.8231776893138886
step 101/334, epoch 62/501 --> loss:0.823194682598114
step 151/334, epoch 62/501 --> loss:0.8408716034889221
step 201/334, epoch 62/501 --> loss:0.8337555718421936
step 251/334, epoch 62/501 --> loss:0.8342440414428711
step 301/334, epoch 62/501 --> loss:0.8423423337936401
step 51/334, epoch 63/501 --> loss:0.8459877240657806
step 101/334, epoch 63/501 --> loss:0.8327486193180085
step 151/334, epoch 63/501 --> loss:0.8343215453624725
step 201/334, epoch 63/501 --> loss:0.8260970282554626
step 251/334, epoch 63/501 --> loss:0.8204163110256195
step 301/334, epoch 63/501 --> loss:0.8411795806884765
step 51/334, epoch 64/501 --> loss:0.824141274690628
step 101/334, epoch 64/501 --> loss:0.8442085492610931
step 151/334, epoch 64/501 --> loss:0.8400427162647247
step 201/334, epoch 64/501 --> loss:0.8253622436523438
step 251/334, epoch 64/501 --> loss:0.8437188720703125
step 301/334, epoch 64/501 --> loss:0.8382648301124572
step 51/334, epoch 65/501 --> loss:0.838164370059967
step 101/334, epoch 65/501 --> loss:0.8236014258861541
step 151/334, epoch 65/501 --> loss:0.8396864628791809
step 201/334, epoch 65/501 --> loss:0.8349643385410309
step 251/334, epoch 65/501 --> loss:0.8448882699012756
step 301/334, epoch 65/501 --> loss:0.8222840714454651
step 51/334, epoch 66/501 --> loss:0.8263525867462158
step 101/334, epoch 66/501 --> loss:0.8388229382038116
step 151/334, epoch 66/501 --> loss:0.8263293743133545
step 201/334, epoch 66/501 --> loss:0.8391289830207824
step 251/334, epoch 66/501 --> loss:0.828615951538086
step 301/334, epoch 66/501 --> loss:0.8444633758068085
step 51/334, epoch 67/501 --> loss:0.8288587343692779
step 101/334, epoch 67/501 --> loss:0.8538545274734497
step 151/334, epoch 67/501 --> loss:0.831791285276413
step 201/334, epoch 67/501 --> loss:0.8269757270812989
step 251/334, epoch 67/501 --> loss:0.8271624863147735
step 301/334, epoch 67/501 --> loss:0.8272629451751708
step 51/334, epoch 68/501 --> loss:0.8455287718772888
step 101/334, epoch 68/501 --> loss:0.8294666254520416
step 151/334, epoch 68/501 --> loss:0.8375555777549744
step 201/334, epoch 68/501 --> loss:0.8246617269515991
step 251/334, epoch 68/501 --> loss:0.8230712246894837
step 301/334, epoch 68/501 --> loss:0.847471479177475
step 51/334, epoch 69/501 --> loss:0.8236066830158234
step 101/334, epoch 69/501 --> loss:0.8341055166721344
step 151/334, epoch 69/501 --> loss:0.8304991137981415
step 201/334, epoch 69/501 --> loss:0.8454135775566101
step 251/334, epoch 69/501 --> loss:0.8327881634235382
step 301/334, epoch 69/501 --> loss:0.8404426419734955
step 51/334, epoch 70/501 --> loss:0.8408997702598572
step 101/334, epoch 70/501 --> loss:0.8418037414550781
step 151/334, epoch 70/501 --> loss:0.843401472568512
step 201/334, epoch 70/501 --> loss:0.8290303719043731
step 251/334, epoch 70/501 --> loss:0.8304825115203858
step 301/334, epoch 70/501 --> loss:0.8175374758243561
step 51/334, epoch 71/501 --> loss:0.8292711746692657
step 101/334, epoch 71/501 --> loss:0.840832302570343
step 151/334, epoch 71/501 --> loss:0.8360322678089142
step 201/334, epoch 71/501 --> loss:0.838505140542984
step 251/334, epoch 71/501 --> loss:0.8207461023330689
step 301/334, epoch 71/501 --> loss:0.8281467974185943

##########train dataset##########
acc--> [94.68963388472044]
F1--> {'F1': [0.5934792006185539], 'precision': [0.4382092984974311], 'recall': [0.9191863780679784]}
##########eval dataset##########
acc--> [94.7988742084056]
F1--> {'F1': [0.597213547211788], 'precision': [0.4488415234971422], 'recall': [0.8921378399910365]}
step 51/334, epoch 72/501 --> loss:0.8412926864624023
step 101/334, epoch 72/501 --> loss:0.8497522342205047
step 151/334, epoch 72/501 --> loss:0.8396593618392945
step 201/334, epoch 72/501 --> loss:0.8266679430007935
step 251/334, epoch 72/501 --> loss:0.8288095092773438
step 301/334, epoch 72/501 --> loss:0.8232789611816407
step 51/334, epoch 73/501 --> loss:0.8368462347984313
step 101/334, epoch 73/501 --> loss:0.8225284683704376
step 151/334, epoch 73/501 --> loss:0.8355880677700043
step 201/334, epoch 73/501 --> loss:0.8348019373416901
step 251/334, epoch 73/501 --> loss:0.8319691944122315
step 301/334, epoch 73/501 --> loss:0.841660486459732
step 51/334, epoch 74/501 --> loss:0.831077345609665
step 101/334, epoch 74/501 --> loss:0.817419844865799
step 151/334, epoch 74/501 --> loss:0.8353827452659607
step 201/334, epoch 74/501 --> loss:0.8499441957473755
step 251/334, epoch 74/501 --> loss:0.842731831073761
step 301/334, epoch 74/501 --> loss:0.8288582956790924
step 51/334, epoch 75/501 --> loss:0.8267275500297546
step 101/334, epoch 75/501 --> loss:0.8405391490459442
step 151/334, epoch 75/501 --> loss:0.8140424358844757
step 201/334, epoch 75/501 --> loss:0.8341405475139618
step 251/334, epoch 75/501 --> loss:0.8412099575996399
step 301/334, epoch 75/501 --> loss:0.8423383724689484
step 51/334, epoch 76/501 --> loss:0.8361424422264099
step 101/334, epoch 76/501 --> loss:0.8275108516216279
step 151/334, epoch 76/501 --> loss:0.8322540450096131
step 201/334, epoch 76/501 --> loss:0.8437766885757446
step 251/334, epoch 76/501 --> loss:0.8293150663375854
step 301/334, epoch 76/501 --> loss:0.8382703471183777
step 51/334, epoch 77/501 --> loss:0.8362037825584412
step 101/334, epoch 77/501 --> loss:0.830287606716156
step 151/334, epoch 77/501 --> loss:0.8385181403160096
step 201/334, epoch 77/501 --> loss:0.8462600362300873
step 251/334, epoch 77/501 --> loss:0.8311688494682312
step 301/334, epoch 77/501 --> loss:0.8273933708667756
step 51/334, epoch 78/501 --> loss:0.8287397575378418
step 101/334, epoch 78/501 --> loss:0.842781058549881
step 151/334, epoch 78/501 --> loss:0.8171087872982025
step 201/334, epoch 78/501 --> loss:0.8457850110530853
step 251/334, epoch 78/501 --> loss:0.8262854182720184
step 301/334, epoch 78/501 --> loss:0.8395944499969482
step 51/334, epoch 79/501 --> loss:0.8386435759067535
step 101/334, epoch 79/501 --> loss:0.8331708133220672
step 151/334, epoch 79/501 --> loss:0.8265304362773895
step 201/334, epoch 79/501 --> loss:0.8416527259349823
step 251/334, epoch 79/501 --> loss:0.8326669621467591
step 301/334, epoch 79/501 --> loss:0.8264803612232208
step 51/334, epoch 80/501 --> loss:0.8445436131954193
step 101/334, epoch 80/501 --> loss:0.8360987508296966
step 151/334, epoch 80/501 --> loss:0.8428953385353088
step 201/334, epoch 80/501 --> loss:0.8222676932811737
step 251/334, epoch 80/501 --> loss:0.8403658974170685
step 301/334, epoch 80/501 --> loss:0.8254895842075348
step 51/334, epoch 81/501 --> loss:0.8252499043941498
step 101/334, epoch 81/501 --> loss:0.81582559466362
step 151/334, epoch 81/501 --> loss:0.847511066198349
step 201/334, epoch 81/501 --> loss:0.8423323440551758
step 251/334, epoch 81/501 --> loss:0.8426645278930665
step 301/334, epoch 81/501 --> loss:0.8340810179710388

##########train dataset##########
acc--> [97.13081757495489]
F1--> {'F1': [0.725656073038535], 'precision': [0.6079887413181752], 'recall': [0.8998138168170349]}
##########eval dataset##########
acc--> [97.07772696320026]
F1--> {'F1': [0.7205418202816096], 'precision': [0.6140886328828328], 'recall': [0.8716565218331311]}
save model!
step 51/334, epoch 82/501 --> loss:0.8319018840789795
step 101/334, epoch 82/501 --> loss:0.8324394834041595
step 151/334, epoch 82/501 --> loss:0.8279475712776184
step 201/334, epoch 82/501 --> loss:0.8297532188892365
step 251/334, epoch 82/501 --> loss:0.835634993314743
step 301/334, epoch 82/501 --> loss:0.8389506566524506
step 51/334, epoch 83/501 --> loss:0.8437350296974182
step 101/334, epoch 83/501 --> loss:0.8283254814147949
step 151/334, epoch 83/501 --> loss:0.833885977268219
step 201/334, epoch 83/501 --> loss:0.8368838691711425
step 251/334, epoch 83/501 --> loss:0.8143242013454437
step 301/334, epoch 83/501 --> loss:0.8386430525779724
step 51/334, epoch 84/501 --> loss:0.8390854287147522
step 101/334, epoch 84/501 --> loss:0.8360025930404663
step 151/334, epoch 84/501 --> loss:0.830657742023468
step 201/334, epoch 84/501 --> loss:0.8238194549083709
step 251/334, epoch 84/501 --> loss:0.8376560604572296
step 301/334, epoch 84/501 --> loss:0.8335111904144287
step 51/334, epoch 85/501 --> loss:0.8517443311214447
step 101/334, epoch 85/501 --> loss:0.829818000793457
step 151/334, epoch 85/501 --> loss:0.8248744308948517
step 201/334, epoch 85/501 --> loss:0.8328388345241546
step 251/334, epoch 85/501 --> loss:0.8237613427639008
step 301/334, epoch 85/501 --> loss:0.83461723446846
step 51/334, epoch 86/501 --> loss:0.8202813708782196
step 101/334, epoch 86/501 --> loss:0.8496483731269836
step 151/334, epoch 86/501 --> loss:0.837671080827713
step 201/334, epoch 86/501 --> loss:0.83751296043396
step 251/334, epoch 86/501 --> loss:0.828897260427475
step 301/334, epoch 86/501 --> loss:0.8344842720031739
step 51/334, epoch 87/501 --> loss:0.8247743165493011
step 101/334, epoch 87/501 --> loss:0.8338217878341675
step 151/334, epoch 87/501 --> loss:0.8431450283527374
step 201/334, epoch 87/501 --> loss:0.8403652846813202
step 251/334, epoch 87/501 --> loss:0.8267564678192139
step 301/334, epoch 87/501 --> loss:0.8262770640850067
step 51/334, epoch 88/501 --> loss:0.8361108553409576
step 101/334, epoch 88/501 --> loss:0.839616242647171
step 151/334, epoch 88/501 --> loss:0.8262457489967346
step 201/334, epoch 88/501 --> loss:0.8217613542079926
step 251/334, epoch 88/501 --> loss:0.8374979341030121
step 301/334, epoch 88/501 --> loss:0.8342654585838318
step 51/334, epoch 89/501 --> loss:0.8314141774177551
step 101/334, epoch 89/501 --> loss:0.8437865376472473
step 151/334, epoch 89/501 --> loss:0.8286702799797058
step 201/334, epoch 89/501 --> loss:0.8363800179958344
step 251/334, epoch 89/501 --> loss:0.8393365001678467
step 301/334, epoch 89/501 --> loss:0.8324228346347808
step 51/334, epoch 90/501 --> loss:0.8469975793361664
step 101/334, epoch 90/501 --> loss:0.829181866645813
step 151/334, epoch 90/501 --> loss:0.8223754775524139
step 201/334, epoch 90/501 --> loss:0.8262964749336242
step 251/334, epoch 90/501 --> loss:0.8323801040649415
step 301/334, epoch 90/501 --> loss:0.8251663029193879
step 51/334, epoch 91/501 --> loss:0.8277723383903504
step 101/334, epoch 91/501 --> loss:0.8382431256771088
step 151/334, epoch 91/501 --> loss:0.8401997447013855
step 201/334, epoch 91/501 --> loss:0.823537346124649
step 251/334, epoch 91/501 --> loss:0.8406469023227692
step 301/334, epoch 91/501 --> loss:0.8324045276641846

##########train dataset##########
acc--> [97.46488264360353]
F1--> {'F1': [0.7515560623451081], 'precision': [0.640475562721584], 'recall': [0.9092657658931774]}
##########eval dataset##########
acc--> [97.35270956577774]
F1--> {'F1': [0.7412064562535076], 'precision': [0.6417578302689884], 'recall': [0.8771425147371453]}
save model!
step 51/334, epoch 92/501 --> loss:0.8394374215602874
step 101/334, epoch 92/501 --> loss:0.8353098821640015
step 151/334, epoch 92/501 --> loss:0.8278049802780152
step 201/334, epoch 92/501 --> loss:0.8286665201187133
step 251/334, epoch 92/501 --> loss:0.8282629656791687
step 301/334, epoch 92/501 --> loss:0.8411402797698975
step 51/334, epoch 93/501 --> loss:0.8158530020713806
step 101/334, epoch 93/501 --> loss:0.8402558207511902
step 151/334, epoch 93/501 --> loss:0.8279079699516296
step 201/334, epoch 93/501 --> loss:0.8337371873855591
step 251/334, epoch 93/501 --> loss:0.8412751746177674
step 301/334, epoch 93/501 --> loss:0.8239192938804627
step 51/334, epoch 94/501 --> loss:0.8276886081695557
step 101/334, epoch 94/501 --> loss:0.8464747130870819
step 151/334, epoch 94/501 --> loss:0.839301917552948
step 201/334, epoch 94/501 --> loss:0.8232106578350067
step 251/334, epoch 94/501 --> loss:0.8239260244369507
step 301/334, epoch 94/501 --> loss:0.8308940637111664
step 51/334, epoch 95/501 --> loss:0.8405171263217927
step 101/334, epoch 95/501 --> loss:0.8207347476482392
step 151/334, epoch 95/501 --> loss:0.8275237488746643
step 201/334, epoch 95/501 --> loss:0.8369429576396942
step 251/334, epoch 95/501 --> loss:0.8278012645244598
step 301/334, epoch 95/501 --> loss:0.8426615524291993
step 51/334, epoch 96/501 --> loss:0.8284358382225037
step 101/334, epoch 96/501 --> loss:0.8432333159446717
step 151/334, epoch 96/501 --> loss:0.8339952754974366
step 201/334, epoch 96/501 --> loss:0.8366554284095764
step 251/334, epoch 96/501 --> loss:0.8265683281421662
step 301/334, epoch 96/501 --> loss:0.8205085229873658
step 51/334, epoch 97/501 --> loss:0.8231711626052857
step 101/334, epoch 97/501 --> loss:0.8309651398658753
step 151/334, epoch 97/501 --> loss:0.8440477418899536
step 201/334, epoch 97/501 --> loss:0.825422409772873
step 251/334, epoch 97/501 --> loss:0.8355065310001373
step 301/334, epoch 97/501 --> loss:0.847675404548645
step 51/334, epoch 98/501 --> loss:0.8346416580677033
step 101/334, epoch 98/501 --> loss:0.8325750195980072
step 151/334, epoch 98/501 --> loss:0.8358404684066773
step 201/334, epoch 98/501 --> loss:0.8248016202449798
step 251/334, epoch 98/501 --> loss:0.8404614174365997
step 301/334, epoch 98/501 --> loss:0.823122763633728
step 51/334, epoch 99/501 --> loss:0.8362746751308441
step 101/334, epoch 99/501 --> loss:0.8258650362491607
step 151/334, epoch 99/501 --> loss:0.8440024673938751
step 201/334, epoch 99/501 --> loss:0.8386995780467987
step 251/334, epoch 99/501 --> loss:0.8153711974620819
step 301/334, epoch 99/501 --> loss:0.8439255571365356
step 51/334, epoch 100/501 --> loss:0.8179820656776429
step 101/334, epoch 100/501 --> loss:0.8405536615848541
step 151/334, epoch 100/501 --> loss:0.8378448188304901
step 201/334, epoch 100/501 --> loss:0.8333073544502259
step 251/334, epoch 100/501 --> loss:0.8441038167476654
step 301/334, epoch 100/501 --> loss:0.8308016240596772
step 51/334, epoch 101/501 --> loss:0.8401220917701722
step 101/334, epoch 101/501 --> loss:0.8275525403022767
step 151/334, epoch 101/501 --> loss:0.8150682771205902
step 201/334, epoch 101/501 --> loss:0.8165555167198181
step 251/334, epoch 101/501 --> loss:0.8512855184078216
step 301/334, epoch 101/501 --> loss:0.8363062524795533

##########train dataset##########
acc--> [95.69878937556848]
F1--> {'F1': [0.63877249560519], 'precision': [0.4945360924563922], 'recall': [0.9018062285312911]}
##########eval dataset##########
acc--> [95.5899543128029]
F1--> {'F1': [0.6310482294449551], 'precision': [0.49423977896942733], 'recall': [0.8726024359320835]}
step 51/334, epoch 102/501 --> loss:0.8284167194366455
step 101/334, epoch 102/501 --> loss:0.8274320483207702
step 151/334, epoch 102/501 --> loss:0.8209809589385987
step 201/334, epoch 102/501 --> loss:0.8472525823116303
step 251/334, epoch 102/501 --> loss:0.8403093814849854
step 301/334, epoch 102/501 --> loss:0.8296557176113128
step 51/334, epoch 103/501 --> loss:0.843520005941391
step 101/334, epoch 103/501 --> loss:0.833976753950119
step 151/334, epoch 103/501 --> loss:0.8272058308124542
step 201/334, epoch 103/501 --> loss:0.8336751568317413
step 251/334, epoch 103/501 --> loss:0.8347221243381501
step 301/334, epoch 103/501 --> loss:0.8296673285961151
step 51/334, epoch 104/501 --> loss:0.8321365809440613
step 101/334, epoch 104/501 --> loss:0.8367045927047729
step 151/334, epoch 104/501 --> loss:0.8393209433555603
step 201/334, epoch 104/501 --> loss:0.8264756536483765
step 251/334, epoch 104/501 --> loss:0.852889119386673
step 301/334, epoch 104/501 --> loss:0.8206927251815795
step 51/334, epoch 105/501 --> loss:0.8175129449367523
step 101/334, epoch 105/501 --> loss:0.8311664271354675
step 151/334, epoch 105/501 --> loss:0.839544585943222
step 201/334, epoch 105/501 --> loss:0.8445152401924133
step 251/334, epoch 105/501 --> loss:0.8264217531681061
step 301/334, epoch 105/501 --> loss:0.8336434423923492
step 51/334, epoch 106/501 --> loss:0.8338564968109131
step 101/334, epoch 106/501 --> loss:0.8352625370025635
step 151/334, epoch 106/501 --> loss:0.8360195171833038
step 201/334, epoch 106/501 --> loss:0.8209217500686645
step 251/334, epoch 106/501 --> loss:0.8305693399906159
step 301/334, epoch 106/501 --> loss:0.8393559098243714
step 51/334, epoch 107/501 --> loss:0.8169251370429993
step 101/334, epoch 107/501 --> loss:0.8292958390712738
step 151/334, epoch 107/501 --> loss:0.8360245454311371
step 201/334, epoch 107/501 --> loss:0.828106495141983
step 251/334, epoch 107/501 --> loss:0.8283491611480713
step 301/334, epoch 107/501 --> loss:0.8562079358100891
step 51/334, epoch 108/501 --> loss:0.8383661663532257
step 101/334, epoch 108/501 --> loss:0.8319721901416779
step 151/334, epoch 108/501 --> loss:0.8450578141212464
step 201/334, epoch 108/501 --> loss:0.812145185470581
step 251/334, epoch 108/501 --> loss:0.8283010733127594
step 301/334, epoch 108/501 --> loss:0.8357926559448242
step 51/334, epoch 109/501 --> loss:0.8404517531394958
step 101/334, epoch 109/501 --> loss:0.8254406893253327
step 151/334, epoch 109/501 --> loss:0.8404723083972931
step 201/334, epoch 109/501 --> loss:0.8165702521800995
step 251/334, epoch 109/501 --> loss:0.8342242097854614
step 301/334, epoch 109/501 --> loss:0.8235531234741211
step 51/334, epoch 110/501 --> loss:0.8389349746704101
step 101/334, epoch 110/501 --> loss:0.827357349395752
step 151/334, epoch 110/501 --> loss:0.8315277171134948
step 201/334, epoch 110/501 --> loss:0.8262227272987366
step 251/334, epoch 110/501 --> loss:0.8236474955081939
step 301/334, epoch 110/501 --> loss:0.8421902227401733
step 51/334, epoch 111/501 --> loss:0.836021603345871
step 101/334, epoch 111/501 --> loss:0.8328476250171661
step 151/334, epoch 111/501 --> loss:0.8242655313014984
step 201/334, epoch 111/501 --> loss:0.8340385913848877
step 251/334, epoch 111/501 --> loss:0.828758682012558
step 301/334, epoch 111/501 --> loss:0.83570028424263

##########train dataset##########
acc--> [96.8823630665829]
F1--> {'F1': [0.7159477757421465], 'precision': [0.5813437037502361], 'recall': [0.9316811438493637]}
##########eval dataset##########
acc--> [96.74843682182514]
F1--> {'F1': [0.7032150361808055], 'precision': [0.5806882056028395], 'recall': [0.8912923086127892]}
step 51/334, epoch 112/501 --> loss:0.8330167496204376
step 101/334, epoch 112/501 --> loss:0.8420858693122864
step 151/334, epoch 112/501 --> loss:0.8381764364242553
step 201/334, epoch 112/501 --> loss:0.8314783835411071
step 251/334, epoch 112/501 --> loss:0.829610857963562
step 301/334, epoch 112/501 --> loss:0.8232440888881684
step 51/334, epoch 113/501 --> loss:0.8408919978141784
step 101/334, epoch 113/501 --> loss:0.8373198819160461
step 151/334, epoch 113/501 --> loss:0.8356159329414368
step 201/334, epoch 113/501 --> loss:0.824812902212143
step 251/334, epoch 113/501 --> loss:0.8144662976264954
step 301/334, epoch 113/501 --> loss:0.8446523785591126
step 51/334, epoch 114/501 --> loss:0.8258195579051971
step 101/334, epoch 114/501 --> loss:0.8321154999732971
step 151/334, epoch 114/501 --> loss:0.8221056258678436
step 201/334, epoch 114/501 --> loss:0.8457702600955963
step 251/334, epoch 114/501 --> loss:0.8271763122081757
step 301/334, epoch 114/501 --> loss:0.8368281471729279
step 51/334, epoch 115/501 --> loss:0.8414942526817322
step 101/334, epoch 115/501 --> loss:0.8413720917701721
step 151/334, epoch 115/501 --> loss:0.8350080120563507
step 201/334, epoch 115/501 --> loss:0.8281703734397888
step 251/334, epoch 115/501 --> loss:0.8291575598716736
step 301/334, epoch 115/501 --> loss:0.8227690315246582
step 51/334, epoch 116/501 --> loss:0.8405998861789703
step 101/334, epoch 116/501 --> loss:0.8315172505378723
step 151/334, epoch 116/501 --> loss:0.8276733541488648
step 201/334, epoch 116/501 --> loss:0.8381327164173126
step 251/334, epoch 116/501 --> loss:0.8251104176044464
step 301/334, epoch 116/501 --> loss:0.838121098279953
step 51/334, epoch 117/501 --> loss:0.823973093032837
step 101/334, epoch 117/501 --> loss:0.837729309797287
step 151/334, epoch 117/501 --> loss:0.8249615550041198
step 201/334, epoch 117/501 --> loss:0.8329140532016754
step 251/334, epoch 117/501 --> loss:0.8271348416805268
step 301/334, epoch 117/501 --> loss:0.8360678160190582
step 51/334, epoch 118/501 --> loss:0.8323109543323517
step 101/334, epoch 118/501 --> loss:0.8230846154689789
step 151/334, epoch 118/501 --> loss:0.8338145697116852
step 201/334, epoch 118/501 --> loss:0.8434892916679382
step 251/334, epoch 118/501 --> loss:0.8357284796237946
step 301/334, epoch 118/501 --> loss:0.8298607099056244
step 51/334, epoch 119/501 --> loss:0.8404020857810974
step 101/334, epoch 119/501 --> loss:0.827978640794754
step 151/334, epoch 119/501 --> loss:0.8343741130828858
step 201/334, epoch 119/501 --> loss:0.8266133189201355
step 251/334, epoch 119/501 --> loss:0.8344740438461303
step 301/334, epoch 119/501 --> loss:0.8305008459091187
step 51/334, epoch 120/501 --> loss:0.8338280928134918
step 101/334, epoch 120/501 --> loss:0.8325854110717773
step 151/334, epoch 120/501 --> loss:0.8322283792495727
step 201/334, epoch 120/501 --> loss:0.8453312516212463
step 251/334, epoch 120/501 --> loss:0.8268740558624268
step 301/334, epoch 120/501 --> loss:0.8188785016536713
step 51/334, epoch 121/501 --> loss:0.8338945031166076
step 101/334, epoch 121/501 --> loss:0.8121082937717438
step 151/334, epoch 121/501 --> loss:0.8298565483093262
step 201/334, epoch 121/501 --> loss:0.846508058309555
step 251/334, epoch 121/501 --> loss:0.8381526410579682
step 301/334, epoch 121/501 --> loss:0.8394824743270874

##########train dataset##########
acc--> [97.1496971081712]
F1--> {'F1': [0.7355261087565272], 'precision': [0.6041778343100956], 'recall': [0.9398649417271535]}
##########eval dataset##########
acc--> [96.94815541397742]
F1--> {'F1': [0.7184500167441812], 'precision': [0.5974523258427752], 'recall': [0.9009185482235488]}
step 51/334, epoch 122/501 --> loss:0.8344444859027863
step 101/334, epoch 122/501 --> loss:0.8310295748710632
step 151/334, epoch 122/501 --> loss:0.821847311258316
step 201/334, epoch 122/501 --> loss:0.8374735760688782
step 251/334, epoch 122/501 --> loss:0.831328628063202
step 301/334, epoch 122/501 --> loss:0.8304607272148132
step 51/334, epoch 123/501 --> loss:0.844835283756256
step 101/334, epoch 123/501 --> loss:0.8342188203334808
step 151/334, epoch 123/501 --> loss:0.8288331723213196
step 201/334, epoch 123/501 --> loss:0.8220879435539246
step 251/334, epoch 123/501 --> loss:0.8315782904624939
step 301/334, epoch 123/501 --> loss:0.8254834938049317
step 51/334, epoch 124/501 --> loss:0.8205412161350251
step 101/334, epoch 124/501 --> loss:0.8473562240600586
step 151/334, epoch 124/501 --> loss:0.8253738605976104
step 201/334, epoch 124/501 --> loss:0.8233616948127747
step 251/334, epoch 124/501 --> loss:0.8266752660274506
step 301/334, epoch 124/501 --> loss:0.8349891924858093
step 51/334, epoch 125/501 --> loss:0.8275888550281525
step 101/334, epoch 125/501 --> loss:0.8301366782188415
step 151/334, epoch 125/501 --> loss:0.8338813436031342
step 201/334, epoch 125/501 --> loss:0.8304940760135651
step 251/334, epoch 125/501 --> loss:0.8359845328330994
step 301/334, epoch 125/501 --> loss:0.8240585172176361
step 51/334, epoch 126/501 --> loss:0.8445061767101287
step 101/334, epoch 126/501 --> loss:0.828529064655304
step 151/334, epoch 126/501 --> loss:0.8377504420280456
step 201/334, epoch 126/501 --> loss:0.8275907897949218
step 251/334, epoch 126/501 --> loss:0.8339980030059815
step 301/334, epoch 126/501 --> loss:0.8194206523895263
step 51/334, epoch 127/501 --> loss:0.8348136210441589
step 101/334, epoch 127/501 --> loss:0.8289356398582458
step 151/334, epoch 127/501 --> loss:0.8349474406242371
step 201/334, epoch 127/501 --> loss:0.8337323892116547
step 251/334, epoch 127/501 --> loss:0.8344393420219421
step 301/334, epoch 127/501 --> loss:0.8267239940166473
step 51/334, epoch 128/501 --> loss:0.8261015272140503
step 101/334, epoch 128/501 --> loss:0.8238974761962891
step 151/334, epoch 128/501 --> loss:0.8275837016105652
step 201/334, epoch 128/501 --> loss:0.838174295425415
step 251/334, epoch 128/501 --> loss:0.8353208684921265
step 301/334, epoch 128/501 --> loss:0.8428148436546326
step 51/334, epoch 129/501 --> loss:0.824712678194046
step 101/334, epoch 129/501 --> loss:0.8301603031158448
step 151/334, epoch 129/501 --> loss:0.8352886331081391
step 201/334, epoch 129/501 --> loss:0.8417547714710235
step 251/334, epoch 129/501 --> loss:0.8333446753025054
step 301/334, epoch 129/501 --> loss:0.8345000290870667
step 51/334, epoch 130/501 --> loss:0.8299212610721588
step 101/334, epoch 130/501 --> loss:0.828532919883728
step 151/334, epoch 130/501 --> loss:0.8276772725582123
step 201/334, epoch 130/501 --> loss:0.8325162518024445
step 251/334, epoch 130/501 --> loss:0.8388191044330597
step 301/334, epoch 130/501 --> loss:0.8301886439323425
step 51/334, epoch 131/501 --> loss:0.8299146044254303
step 101/334, epoch 131/501 --> loss:0.8367539978027344
step 151/334, epoch 131/501 --> loss:0.815496062040329
step 201/334, epoch 131/501 --> loss:0.8374633777141571
step 251/334, epoch 131/501 --> loss:0.8300814533233642
step 301/334, epoch 131/501 --> loss:0.8323980832099914

##########train dataset##########
acc--> [95.62731307679894]
F1--> {'F1': [0.6475491481403337], 'precision': [0.49050532916166817], 'recall': [0.9525320729642764]}
##########eval dataset##########
acc--> [95.53622081418871]
F1--> {'F1': [0.6396712188668624], 'precision': [0.4912197237986164], 'recall': [0.9167288267346108]}
step 51/334, epoch 132/501 --> loss:0.8291740345954896
step 101/334, epoch 132/501 --> loss:0.8352168130874634
step 151/334, epoch 132/501 --> loss:0.8409424364566803
step 201/334, epoch 132/501 --> loss:0.8268579542636871
step 251/334, epoch 132/501 --> loss:0.8208474612236023
step 301/334, epoch 132/501 --> loss:0.8270555758476257
step 51/334, epoch 133/501 --> loss:0.8244282639026642
step 101/334, epoch 133/501 --> loss:0.8379513669013977
step 151/334, epoch 133/501 --> loss:0.8295339119434356
step 201/334, epoch 133/501 --> loss:0.8305538117885589
step 251/334, epoch 133/501 --> loss:0.8404910349845887
step 301/334, epoch 133/501 --> loss:0.8291147768497467
step 51/334, epoch 134/501 --> loss:0.8256456851959229
step 101/334, epoch 134/501 --> loss:0.8301925826072692
step 151/334, epoch 134/501 --> loss:0.8358719217777252
step 201/334, epoch 134/501 --> loss:0.8351289391517639
step 251/334, epoch 134/501 --> loss:0.8274074554443359
step 301/334, epoch 134/501 --> loss:0.8231840193271637
step 51/334, epoch 135/501 --> loss:0.8236556220054626
step 101/334, epoch 135/501 --> loss:0.8296416306495666
step 151/334, epoch 135/501 --> loss:0.8301205778121948
step 201/334, epoch 135/501 --> loss:0.8376698350906372
step 251/334, epoch 135/501 --> loss:0.8404395973682404
step 301/334, epoch 135/501 --> loss:0.8353817093372345
step 51/334, epoch 136/501 --> loss:0.8287638401985169
step 101/334, epoch 136/501 --> loss:0.8271555614471435
step 151/334, epoch 136/501 --> loss:0.8367683565616608
step 201/334, epoch 136/501 --> loss:0.8224021971225739
step 251/334, epoch 136/501 --> loss:0.8374784207344055
step 301/334, epoch 136/501 --> loss:0.827539074420929
step 51/334, epoch 137/501 --> loss:0.8279873144626617
step 101/334, epoch 137/501 --> loss:0.827001119852066
step 151/334, epoch 137/501 --> loss:0.836278818845749
step 201/334, epoch 137/501 --> loss:0.8280780029296875
step 251/334, epoch 137/501 --> loss:0.8361812996864318
step 301/334, epoch 137/501 --> loss:0.8292968201637269
step 51/334, epoch 138/501 --> loss:0.8383822357654571
step 101/334, epoch 138/501 --> loss:0.8368596923351288
step 151/334, epoch 138/501 --> loss:0.8247961258888244
step 201/334, epoch 138/501 --> loss:0.8199271392822266
step 251/334, epoch 138/501 --> loss:0.8322911584377288
step 301/334, epoch 138/501 --> loss:0.8290757203102112
step 51/334, epoch 139/501 --> loss:0.8344193291664124
step 101/334, epoch 139/501 --> loss:0.8223396193981171
step 151/334, epoch 139/501 --> loss:0.843051198720932
step 201/334, epoch 139/501 --> loss:0.8404470670223236
step 251/334, epoch 139/501 --> loss:0.8343892180919648
step 301/334, epoch 139/501 --> loss:0.8260669255256653
step 51/334, epoch 140/501 --> loss:0.8227041125297546
step 101/334, epoch 140/501 --> loss:0.8280677413940429
step 151/334, epoch 140/501 --> loss:0.834211300611496
step 201/334, epoch 140/501 --> loss:0.8392841017246246
step 251/334, epoch 140/501 --> loss:0.8360613310337066
step 301/334, epoch 140/501 --> loss:0.8320773720741272
step 51/334, epoch 141/501 --> loss:0.826673595905304
step 101/334, epoch 141/501 --> loss:0.8388345122337342
step 151/334, epoch 141/501 --> loss:0.8250713920593262
step 201/334, epoch 141/501 --> loss:0.8361317038536071
step 251/334, epoch 141/501 --> loss:0.8261037266254425
step 301/334, epoch 141/501 --> loss:0.8393755090236664

##########train dataset##########
acc--> [96.25000068655693]
F1--> {'F1': [0.6816509260193092], 'precision': [0.5308874965646441], 'recall': [0.9520227774736515]}
##########eval dataset##########
acc--> [96.09021831788817]
F1--> {'F1': [0.6685486591320924], 'precision': [0.527585881382476], 'recall': [0.9123187821616009]}
step 51/334, epoch 142/501 --> loss:0.8387331569194794
step 101/334, epoch 142/501 --> loss:0.8285967600345612
step 151/334, epoch 142/501 --> loss:0.8351339316368103
step 201/334, epoch 142/501 --> loss:0.8309323859214782
step 251/334, epoch 142/501 --> loss:0.8339734053611756
step 301/334, epoch 142/501 --> loss:0.8289699518680572
step 51/334, epoch 143/501 --> loss:0.8222735452651978
step 101/334, epoch 143/501 --> loss:0.8228487074375153
step 151/334, epoch 143/501 --> loss:0.8455705082416535
step 201/334, epoch 143/501 --> loss:0.8305862569808959
step 251/334, epoch 143/501 --> loss:0.8410427939891815
step 301/334, epoch 143/501 --> loss:0.8337321305274963
step 51/334, epoch 144/501 --> loss:0.8058758187294006
step 101/334, epoch 144/501 --> loss:0.8386472308635712
step 151/334, epoch 144/501 --> loss:0.839113154411316
step 201/334, epoch 144/501 --> loss:0.8428831422328948
step 251/334, epoch 144/501 --> loss:0.8415587437152863
step 301/334, epoch 144/501 --> loss:0.8261359131336212
step 51/334, epoch 145/501 --> loss:0.8316849207878113
step 101/334, epoch 145/501 --> loss:0.8348579978942872
step 151/334, epoch 145/501 --> loss:0.8302834975719452
step 201/334, epoch 145/501 --> loss:0.8181506156921386
step 251/334, epoch 145/501 --> loss:0.8355014145374298
step 301/334, epoch 145/501 --> loss:0.8308802926540375
step 51/334, epoch 146/501 --> loss:0.8273137319087982
step 101/334, epoch 146/501 --> loss:0.8243529510498047
step 151/334, epoch 146/501 --> loss:0.8285150647163391
step 201/334, epoch 146/501 --> loss:0.8281000792980194
step 251/334, epoch 146/501 --> loss:0.8365512013435363
step 301/334, epoch 146/501 --> loss:0.8310620486736298
step 51/334, epoch 147/501 --> loss:0.8280009043216705
step 101/334, epoch 147/501 --> loss:0.84005788564682
step 151/334, epoch 147/501 --> loss:0.8317242610454559
step 201/334, epoch 147/501 --> loss:0.839360181093216
step 251/334, epoch 147/501 --> loss:0.8268623280525208
step 301/334, epoch 147/501 --> loss:0.8337181007862091
step 51/334, epoch 148/501 --> loss:0.8284100842475891
step 101/334, epoch 148/501 --> loss:0.8270005786418915
step 151/334, epoch 148/501 --> loss:0.8221645247936249
step 201/334, epoch 148/501 --> loss:0.8345279121398925
step 251/334, epoch 148/501 --> loss:0.8390926659107208
step 301/334, epoch 148/501 --> loss:0.835321832895279
step 51/334, epoch 149/501 --> loss:0.8302694964408874
step 101/334, epoch 149/501 --> loss:0.8258235681056977
step 151/334, epoch 149/501 --> loss:0.8208591747283935
step 201/334, epoch 149/501 --> loss:0.8278829145431519
step 251/334, epoch 149/501 --> loss:0.8385815751552582
step 301/334, epoch 149/501 --> loss:0.84770867228508
step 51/334, epoch 150/501 --> loss:0.8251938831806183
step 101/334, epoch 150/501 --> loss:0.8426553094387055
step 151/334, epoch 150/501 --> loss:0.826139235496521
step 201/334, epoch 150/501 --> loss:0.8466167235374451
step 251/334, epoch 150/501 --> loss:0.8186442232131959
step 301/334, epoch 150/501 --> loss:0.8349997675418854
step 51/334, epoch 151/501 --> loss:0.8307002794742584
step 101/334, epoch 151/501 --> loss:0.8433397960662842
step 151/334, epoch 151/501 --> loss:0.8369650065898895
step 201/334, epoch 151/501 --> loss:0.8133195972442627
step 251/334, epoch 151/501 --> loss:0.8334424066543579
step 301/334, epoch 151/501 --> loss:0.8316858839988709

##########train dataset##########
acc--> [93.57474734374365]
F1--> {'F1': [0.5560248200653535], 'precision': [0.39234197503082074], 'recall': [0.9540727121735296]}
##########eval dataset##########
acc--> [93.5082409742665]
F1--> {'F1': [0.5499516761264892], 'precision': [0.39262028273905303], 'recall': [0.9177130407023861]}
step 51/334, epoch 152/501 --> loss:0.8201612043380737
step 101/334, epoch 152/501 --> loss:0.835750344991684
step 151/334, epoch 152/501 --> loss:0.8272823464870452
step 201/334, epoch 152/501 --> loss:0.8350127601623535
step 251/334, epoch 152/501 --> loss:0.8328329157829285
step 301/334, epoch 152/501 --> loss:0.8413582229614258
step 51/334, epoch 153/501 --> loss:0.8226877975463868
step 101/334, epoch 153/501 --> loss:0.820745325088501
step 151/334, epoch 153/501 --> loss:0.8325034415721894
step 201/334, epoch 153/501 --> loss:0.8402758848667145
step 251/334, epoch 153/501 --> loss:0.8360967993736267
step 301/334, epoch 153/501 --> loss:0.8307711744308471
step 51/334, epoch 154/501 --> loss:0.8358168756961822
step 101/334, epoch 154/501 --> loss:0.80890549659729
step 151/334, epoch 154/501 --> loss:0.8388659000396729
step 201/334, epoch 154/501 --> loss:0.8286842978000641
step 251/334, epoch 154/501 --> loss:0.8342356538772583
step 301/334, epoch 154/501 --> loss:0.8395410680770874
step 51/334, epoch 155/501 --> loss:0.8343075001239777
step 101/334, epoch 155/501 --> loss:0.8196849048137664
step 151/334, epoch 155/501 --> loss:0.8354385101795196
step 201/334, epoch 155/501 --> loss:0.8289396858215332
step 251/334, epoch 155/501 --> loss:0.834765065908432
step 301/334, epoch 155/501 --> loss:0.8270590484142304
step 51/334, epoch 156/501 --> loss:0.8359311163425446
step 101/334, epoch 156/501 --> loss:0.8332274723052978
step 151/334, epoch 156/501 --> loss:0.8424645090103149
step 201/334, epoch 156/501 --> loss:0.8327378249168396
step 251/334, epoch 156/501 --> loss:0.8137025737762451
step 301/334, epoch 156/501 --> loss:0.829404913187027
step 51/334, epoch 157/501 --> loss:0.8407129919528962
step 101/334, epoch 157/501 --> loss:0.8250846099853516
step 151/334, epoch 157/501 --> loss:0.8301013708114624
step 201/334, epoch 157/501 --> loss:0.827756507396698
step 251/334, epoch 157/501 --> loss:0.8246807992458344
step 301/334, epoch 157/501 --> loss:0.8260228538513184
step 51/334, epoch 158/501 --> loss:0.8371652948856354
step 101/334, epoch 158/501 --> loss:0.8339036452770233
step 151/334, epoch 158/501 --> loss:0.8282103073596955
step 201/334, epoch 158/501 --> loss:0.817617130279541
step 251/334, epoch 158/501 --> loss:0.8303157746791839
step 301/334, epoch 158/501 --> loss:0.8377414977550507
step 51/334, epoch 159/501 --> loss:0.8308024096488953
step 101/334, epoch 159/501 --> loss:0.8429779851436615
step 151/334, epoch 159/501 --> loss:0.8209189987182617
step 201/334, epoch 159/501 --> loss:0.8331457769870758
step 251/334, epoch 159/501 --> loss:0.828528208732605
step 301/334, epoch 159/501 --> loss:0.8314437365531921
step 51/334, epoch 160/501 --> loss:0.8345132851600647
step 101/334, epoch 160/501 --> loss:0.830931715965271
step 151/334, epoch 160/501 --> loss:0.8326304328441619
step 201/334, epoch 160/501 --> loss:0.8297898209095002
step 251/334, epoch 160/501 --> loss:0.8291402113437653
step 301/334, epoch 160/501 --> loss:0.8381785881519318
step 51/334, epoch 161/501 --> loss:0.8110514581203461
step 101/334, epoch 161/501 --> loss:0.8347695624828338
step 151/334, epoch 161/501 --> loss:0.8406904983520508
step 201/334, epoch 161/501 --> loss:0.8285101127624511
step 251/334, epoch 161/501 --> loss:0.8394652545452118
step 301/334, epoch 161/501 --> loss:0.8179554295539856

##########train dataset##########
acc--> [96.9601560005472]
F1--> {'F1': [0.7258949456125258], 'precision': [0.5856478225000455], 'recall': [0.9544799857651269]}
##########eval dataset##########
acc--> [96.6658077423485]
F1--> {'F1': [0.7017095875271686], 'precision': [0.5720524276811187], 'recall': [0.9073824232621862]}
step 51/334, epoch 162/501 --> loss:0.8162274301052094
step 101/334, epoch 162/501 --> loss:0.8436768937110901
step 151/334, epoch 162/501 --> loss:0.8378645253181457
step 201/334, epoch 162/501 --> loss:0.8190188598632813
step 251/334, epoch 162/501 --> loss:0.8340795052051544
step 301/334, epoch 162/501 --> loss:0.8279482853412629
step 51/334, epoch 163/501 --> loss:0.8296136295795441
step 101/334, epoch 163/501 --> loss:0.8322123003005981
step 151/334, epoch 163/501 --> loss:0.8303754913806916
step 201/334, epoch 163/501 --> loss:0.8298224878311157
step 251/334, epoch 163/501 --> loss:0.8358480751514434
step 301/334, epoch 163/501 --> loss:0.8217645895481109
step 51/334, epoch 164/501 --> loss:0.8377768135070801
step 101/334, epoch 164/501 --> loss:0.8364351058006286
step 151/334, epoch 164/501 --> loss:0.8235966753959656
step 201/334, epoch 164/501 --> loss:0.8328192746639251
step 251/334, epoch 164/501 --> loss:0.8265272831916809
step 301/334, epoch 164/501 --> loss:0.8245950436592102
step 51/334, epoch 165/501 --> loss:0.8232720351219177
step 101/334, epoch 165/501 --> loss:0.8338804244995117
step 151/334, epoch 165/501 --> loss:0.8371767330169678
step 201/334, epoch 165/501 --> loss:0.834970965385437
step 251/334, epoch 165/501 --> loss:0.8225591671466828
step 301/334, epoch 165/501 --> loss:0.8318098080158234
step 51/334, epoch 166/501 --> loss:0.8305847549438476
step 101/334, epoch 166/501 --> loss:0.8363442075252533
step 151/334, epoch 166/501 --> loss:0.8253475975990295
step 201/334, epoch 166/501 --> loss:0.8285308957099915
step 251/334, epoch 166/501 --> loss:0.8344611155986786
step 301/334, epoch 166/501 --> loss:0.8333037149906158
step 51/334, epoch 167/501 --> loss:0.8236781024932861
step 101/334, epoch 167/501 --> loss:0.8318669605255127
step 151/334, epoch 167/501 --> loss:0.8178621029853821
step 201/334, epoch 167/501 --> loss:0.8474053955078125
step 251/334, epoch 167/501 --> loss:0.8315179288387299
step 301/334, epoch 167/501 --> loss:0.8324110531806945
step 51/334, epoch 168/501 --> loss:0.8418095743656159
step 101/334, epoch 168/501 --> loss:0.8270970261096955
step 151/334, epoch 168/501 --> loss:0.8234176063537597
step 201/334, epoch 168/501 --> loss:0.8236678147315979
step 251/334, epoch 168/501 --> loss:0.8333862841129303
step 301/334, epoch 168/501 --> loss:0.830585527420044
step 51/334, epoch 169/501 --> loss:0.8256653988361359
step 101/334, epoch 169/501 --> loss:0.8313259422779083
step 151/334, epoch 169/501 --> loss:0.836071664094925
step 201/334, epoch 169/501 --> loss:0.8275325763225555
step 251/334, epoch 169/501 --> loss:0.8420323860645295
step 301/334, epoch 169/501 --> loss:0.826875375509262
step 51/334, epoch 170/501 --> loss:0.8326173532009125
step 101/334, epoch 170/501 --> loss:0.8260125541687011
step 151/334, epoch 170/501 --> loss:0.8402754282951355
step 201/334, epoch 170/501 --> loss:0.8260375666618347
step 251/334, epoch 170/501 --> loss:0.8265608060359955
step 301/334, epoch 170/501 --> loss:0.827933167219162
step 51/334, epoch 171/501 --> loss:0.8315572500228882
step 101/334, epoch 171/501 --> loss:0.8265474653244018
step 151/334, epoch 171/501 --> loss:0.8172394180297852
step 201/334, epoch 171/501 --> loss:0.8312878131866455
step 251/334, epoch 171/501 --> loss:0.8408702528476715
step 301/334, epoch 171/501 --> loss:0.8297717094421386

##########train dataset##########
acc--> [98.0509420079481]
F1--> {'F1': [0.795128668001887], 'precision': [0.7141094591375001], 'recall': [0.8968971708113875]}
##########eval dataset##########
acc--> [97.77774493733284]
F1--> {'F1': [0.767650371356902], 'precision': [0.7002806274127397], 'recall': [0.8493744921007302]}
save model!
step 51/334, epoch 172/501 --> loss:0.8187896823883056
step 101/334, epoch 172/501 --> loss:0.8466678285598754
step 151/334, epoch 172/501 --> loss:0.8264345371723175
step 201/334, epoch 172/501 --> loss:0.8298486518859863
step 251/334, epoch 172/501 --> loss:0.8272602427005767
step 301/334, epoch 172/501 --> loss:0.8362586843967438
step 51/334, epoch 173/501 --> loss:0.8235840547084808
step 101/334, epoch 173/501 --> loss:0.8361948370933533
step 151/334, epoch 173/501 --> loss:0.8314227497577668
step 201/334, epoch 173/501 --> loss:0.820367978811264
step 251/334, epoch 173/501 --> loss:0.8348705208301545
step 301/334, epoch 173/501 --> loss:0.8280855882167816
step 51/334, epoch 174/501 --> loss:0.8310747826099396
step 101/334, epoch 174/501 --> loss:0.8131294763088226
step 151/334, epoch 174/501 --> loss:0.8432795345783234
step 201/334, epoch 174/501 --> loss:0.8338560831546783
step 251/334, epoch 174/501 --> loss:0.8382007098197937
step 301/334, epoch 174/501 --> loss:0.8238590717315674
step 51/334, epoch 175/501 --> loss:0.8292908251285553
step 101/334, epoch 175/501 --> loss:0.8290290606021881
step 151/334, epoch 175/501 --> loss:0.820361932516098
step 201/334, epoch 175/501 --> loss:0.8387965989112854
step 251/334, epoch 175/501 --> loss:0.8217710924148559
step 301/334, epoch 175/501 --> loss:0.8474418270587921
step 51/334, epoch 176/501 --> loss:0.8422756338119507
step 101/334, epoch 176/501 --> loss:0.8390062248706818
step 151/334, epoch 176/501 --> loss:0.8224247634410858
step 201/334, epoch 176/501 --> loss:0.8296341621875762
step 251/334, epoch 176/501 --> loss:0.8270594608783722
step 301/334, epoch 176/501 --> loss:0.823875128030777
step 51/334, epoch 177/501 --> loss:0.8218360579013825
step 101/334, epoch 177/501 --> loss:0.8353458547592163
step 151/334, epoch 177/501 --> loss:0.8421539914608002
step 201/334, epoch 177/501 --> loss:0.8212578558921814
step 251/334, epoch 177/501 --> loss:0.8354765820503235
step 301/334, epoch 177/501 --> loss:0.8399368596076965
step 51/334, epoch 178/501 --> loss:0.8331551551818848
step 101/334, epoch 178/501 --> loss:0.8177980566024781
step 151/334, epoch 178/501 --> loss:0.8359808158874512
step 201/334, epoch 178/501 --> loss:0.8424347293376923
step 251/334, epoch 178/501 --> loss:0.8178855240345001
step 301/334, epoch 178/501 --> loss:0.8203746199607849
step 51/334, epoch 179/501 --> loss:0.8363474977016448
step 101/334, epoch 179/501 --> loss:0.8231409990787506
step 151/334, epoch 179/501 --> loss:0.8259056258201599
step 201/334, epoch 179/501 --> loss:0.8275176572799683
step 251/334, epoch 179/501 --> loss:0.82990598320961
step 301/334, epoch 179/501 --> loss:0.827385948896408
step 51/334, epoch 180/501 --> loss:0.8288300180435181
step 101/334, epoch 180/501 --> loss:0.8268893373012542
step 151/334, epoch 180/501 --> loss:0.8191141426563263
step 201/334, epoch 180/501 --> loss:0.8295410013198853
step 251/334, epoch 180/501 --> loss:0.8339392817020417
step 301/334, epoch 180/501 --> loss:0.8389391469955444
step 51/334, epoch 181/501 --> loss:0.841961795091629
step 101/334, epoch 181/501 --> loss:0.8243035006523133
step 151/334, epoch 181/501 --> loss:0.835665898323059
step 201/334, epoch 181/501 --> loss:0.8345894575119018
step 251/334, epoch 181/501 --> loss:0.8222984850406647
step 301/334, epoch 181/501 --> loss:0.8330829548835754

##########train dataset##########
acc--> [98.17164063140508]
F1--> {'F1': [0.8056892541491375], 'precision': [0.7300242707678252], 'recall': [0.8988651623902382]}
##########eval dataset##########
acc--> [97.88550829949313]
F1--> {'F1': [0.7756556066815348], 'precision': [0.7162964790222575], 'recall': [0.8457536101474179]}
save model!
step 51/334, epoch 182/501 --> loss:0.8269833934307098
step 101/334, epoch 182/501 --> loss:0.8338045787811279
step 151/334, epoch 182/501 --> loss:0.8188656675815582
step 201/334, epoch 182/501 --> loss:0.8317385971546173
step 251/334, epoch 182/501 --> loss:0.8313176774978638
step 301/334, epoch 182/501 --> loss:0.8337942397594452
step 51/334, epoch 183/501 --> loss:0.8238946294784546
step 101/334, epoch 183/501 --> loss:0.8368559718132019
step 151/334, epoch 183/501 --> loss:0.8333489227294922
step 201/334, epoch 183/501 --> loss:0.8303670024871826
step 251/334, epoch 183/501 --> loss:0.8222455203533172
step 301/334, epoch 183/501 --> loss:0.8195736372470855
step 51/334, epoch 184/501 --> loss:0.8292896234989167
step 101/334, epoch 184/501 --> loss:0.8187806332111358
step 151/334, epoch 184/501 --> loss:0.8306526935100556
step 201/334, epoch 184/501 --> loss:0.8264544880390168
step 251/334, epoch 184/501 --> loss:0.8370989871025085
step 301/334, epoch 184/501 --> loss:0.8368373000621796
step 51/334, epoch 185/501 --> loss:0.834221955537796
step 101/334, epoch 185/501 --> loss:0.8303537535667419
step 151/334, epoch 185/501 --> loss:0.8361651635169983
step 201/334, epoch 185/501 --> loss:0.8232588529586792
step 251/334, epoch 185/501 --> loss:0.8383063900470734
step 301/334, epoch 185/501 --> loss:0.8154714846611023
step 51/334, epoch 186/501 --> loss:0.8395742285251617
step 101/334, epoch 186/501 --> loss:0.8245488822460174
step 151/334, epoch 186/501 --> loss:0.8517665219306946
step 201/334, epoch 186/501 --> loss:0.8347674918174743
step 251/334, epoch 186/501 --> loss:0.8158623778820038
step 301/334, epoch 186/501 --> loss:0.8282025778293609
step 51/334, epoch 187/501 --> loss:0.8282127249240875
step 101/334, epoch 187/501 --> loss:0.827827388048172
step 151/334, epoch 187/501 --> loss:0.8363404965400696
step 201/334, epoch 187/501 --> loss:0.8287327015399932
step 251/334, epoch 187/501 --> loss:0.8338373386859894
step 301/334, epoch 187/501 --> loss:0.8390438282489776
step 51/334, epoch 188/501 --> loss:0.8399441504478454
step 101/334, epoch 188/501 --> loss:0.8308181941509247
step 151/334, epoch 188/501 --> loss:0.8165360617637635
step 201/334, epoch 188/501 --> loss:0.8335241353511811
step 251/334, epoch 188/501 --> loss:0.8194925677776337
step 301/334, epoch 188/501 --> loss:0.8316776907444
step 51/334, epoch 189/501 --> loss:0.825404440164566
step 101/334, epoch 189/501 --> loss:0.8300207889080048
step 151/334, epoch 189/501 --> loss:0.8390649032592773
step 201/334, epoch 189/501 --> loss:0.8255599129199982
step 251/334, epoch 189/501 --> loss:0.8199727928638458
step 301/334, epoch 189/501 --> loss:0.8354915344715118
step 51/334, epoch 190/501 --> loss:0.826236082315445
step 101/334, epoch 190/501 --> loss:0.8403722286224365
step 151/334, epoch 190/501 --> loss:0.8301725494861603
step 201/334, epoch 190/501 --> loss:0.8403603100776672
step 251/334, epoch 190/501 --> loss:0.8312855792045594
step 301/334, epoch 190/501 --> loss:0.8080725598335267
step 51/334, epoch 191/501 --> loss:0.8389539766311646
step 101/334, epoch 191/501 --> loss:0.8259208118915558
step 151/334, epoch 191/501 --> loss:0.8267121040821075
step 201/334, epoch 191/501 --> loss:0.8290407621860504
step 251/334, epoch 191/501 --> loss:0.8226734232902527
step 301/334, epoch 191/501 --> loss:0.8373831868171692

##########train dataset##########
acc--> [97.51570608269634]
F1--> {'F1': [0.7591892763792543], 'precision': [0.6420530873670198], 'recall': [0.9286182483661208]}
##########eval dataset##########
acc--> [97.17388603630317]
F1--> {'F1': [0.728475678417113], 'precision': [0.6228990252262033], 'recall': [0.8771591937122779]}
step 51/334, epoch 192/501 --> loss:0.8254312217235565
step 101/334, epoch 192/501 --> loss:0.8160452270507812
step 151/334, epoch 192/501 --> loss:0.8356373476982116
step 201/334, epoch 192/501 --> loss:0.8380197334289551
step 251/334, epoch 192/501 --> loss:0.8360721063613892
step 301/334, epoch 192/501 --> loss:0.8183737897872925
step 51/334, epoch 193/501 --> loss:0.8377494025230408
step 101/334, epoch 193/501 --> loss:0.8246031892299652
step 151/334, epoch 193/501 --> loss:0.8354516065120697
step 201/334, epoch 193/501 --> loss:0.8314329934120178
step 251/334, epoch 193/501 --> loss:0.8213388466835022
step 301/334, epoch 193/501 --> loss:0.8282084119319916
step 51/334, epoch 194/501 --> loss:0.8165128219127655
step 101/334, epoch 194/501 --> loss:0.8265131151676178
step 151/334, epoch 194/501 --> loss:0.8286211550235748
step 201/334, epoch 194/501 --> loss:0.8404227209091186
step 251/334, epoch 194/501 --> loss:0.8420358312129974
step 301/334, epoch 194/501 --> loss:0.8287052857875824
step 51/334, epoch 195/501 --> loss:0.8199243938922882
step 101/334, epoch 195/501 --> loss:0.8395638704299927
step 151/334, epoch 195/501 --> loss:0.8365805292129517
step 201/334, epoch 195/501 --> loss:0.8175284159183502
step 251/334, epoch 195/501 --> loss:0.8360322773456573
step 301/334, epoch 195/501 --> loss:0.8315556728839875
step 51/334, epoch 196/501 --> loss:0.8224555432796479
step 101/334, epoch 196/501 --> loss:0.8218934667110444
step 151/334, epoch 196/501 --> loss:0.8234567415714263
step 201/334, epoch 196/501 --> loss:0.8421116244792938
step 251/334, epoch 196/501 --> loss:0.840455197095871
step 301/334, epoch 196/501 --> loss:0.8223228645324707
step 51/334, epoch 197/501 --> loss:0.825442920923233
step 101/334, epoch 197/501 --> loss:0.8425835049152375
step 151/334, epoch 197/501 --> loss:0.826992883682251
step 201/334, epoch 197/501 --> loss:0.8310459125041961
step 251/334, epoch 197/501 --> loss:0.8205326581001282
step 301/334, epoch 197/501 --> loss:0.8314417779445649
step 51/334, epoch 198/501 --> loss:0.841212033033371
step 101/334, epoch 198/501 --> loss:0.8375533437728881
step 151/334, epoch 198/501 --> loss:0.8333852660655975
step 201/334, epoch 198/501 --> loss:0.8158939254283905
step 251/334, epoch 198/501 --> loss:0.8311333966255188
step 301/334, epoch 198/501 --> loss:0.829170184135437
step 51/334, epoch 199/501 --> loss:0.840660138130188
step 101/334, epoch 199/501 --> loss:0.8220796322822571
step 151/334, epoch 199/501 --> loss:0.8292119777202607
step 201/334, epoch 199/501 --> loss:0.8266094624996185
step 251/334, epoch 199/501 --> loss:0.8239884185791015
step 301/334, epoch 199/501 --> loss:0.8282046473026276
step 51/334, epoch 200/501 --> loss:0.8198062705993653
step 101/334, epoch 200/501 --> loss:0.8364202356338502
step 151/334, epoch 200/501 --> loss:0.8186865174770355
step 201/334, epoch 200/501 --> loss:0.8249909651279449
step 251/334, epoch 200/501 --> loss:0.8342457008361817
step 301/334, epoch 200/501 --> loss:0.8326731240749359
step 51/334, epoch 201/501 --> loss:0.8157893943786622
step 101/334, epoch 201/501 --> loss:0.8126971745491027
step 151/334, epoch 201/501 --> loss:0.8289087748527527
step 201/334, epoch 201/501 --> loss:0.832933543920517
step 251/334, epoch 201/501 --> loss:0.8389129745960235
step 301/334, epoch 201/501 --> loss:0.8408366298675537

##########train dataset##########
acc--> [97.68529090266024]
F1--> {'F1': [0.7760945246723476], 'precision': [0.6554063859087494], 'recall': [0.951276810170721]}
##########eval dataset##########
acc--> [97.33074985880631]
F1--> {'F1': [0.7433048139407474], 'precision': [0.6360043898863917], 'recall': [0.8941725205222525]}
step 51/334, epoch 202/501 --> loss:0.8309835350513458
step 101/334, epoch 202/501 --> loss:0.8386615681648254
step 151/334, epoch 202/501 --> loss:0.8245100331306457
step 201/334, epoch 202/501 --> loss:0.8243859791755677
step 251/334, epoch 202/501 --> loss:0.8367428874969483
step 301/334, epoch 202/501 --> loss:0.8250318479537964
step 51/334, epoch 203/501 --> loss:0.8377944469451905
step 101/334, epoch 203/501 --> loss:0.8340973007678986
step 151/334, epoch 203/501 --> loss:0.8232502603530883
step 201/334, epoch 203/501 --> loss:0.8261550760269165
step 251/334, epoch 203/501 --> loss:0.8273325538635254
step 301/334, epoch 203/501 --> loss:0.8175541496276856
step 51/334, epoch 204/501 --> loss:0.8366990458965301
step 101/334, epoch 204/501 --> loss:0.8305259788036347
step 151/334, epoch 204/501 --> loss:0.840572383403778
step 201/334, epoch 204/501 --> loss:0.8331937634944916
step 251/334, epoch 204/501 --> loss:0.8191860735416412
step 301/334, epoch 204/501 --> loss:0.8209187769889832
step 51/334, epoch 205/501 --> loss:0.8311618745326996
step 101/334, epoch 205/501 --> loss:0.8256304931640625
step 151/334, epoch 205/501 --> loss:0.8341166007518769
step 201/334, epoch 205/501 --> loss:0.8309577345848084
step 251/334, epoch 205/501 --> loss:0.8382104551792144
step 301/334, epoch 205/501 --> loss:0.8277037072181702
step 51/334, epoch 206/501 --> loss:0.8292892420291901
step 101/334, epoch 206/501 --> loss:0.837269995212555
step 151/334, epoch 206/501 --> loss:0.8264532494544983
step 201/334, epoch 206/501 --> loss:0.8168736279010773
step 251/334, epoch 206/501 --> loss:0.8370608186721802
step 301/334, epoch 206/501 --> loss:0.8353248000144958
step 51/334, epoch 207/501 --> loss:0.8316591548919677
step 101/334, epoch 207/501 --> loss:0.8409858226776123
step 151/334, epoch 207/501 --> loss:0.8245649218559266
step 201/334, epoch 207/501 --> loss:0.8273919498920441
step 251/334, epoch 207/501 --> loss:0.8304939472675323
step 301/334, epoch 207/501 --> loss:0.8293498909473419
step 51/334, epoch 208/501 --> loss:0.8260037791728974
step 101/334, epoch 208/501 --> loss:0.8161950933933259
step 151/334, epoch 208/501 --> loss:0.8279067480564117
step 201/334, epoch 208/501 --> loss:0.8312571001052856
step 251/334, epoch 208/501 --> loss:0.8413973033428193
step 301/334, epoch 208/501 --> loss:0.830364476442337
step 51/334, epoch 209/501 --> loss:0.8243064272403717
step 101/334, epoch 209/501 --> loss:0.8334691596031188
step 151/334, epoch 209/501 --> loss:0.8171739912033081
step 201/334, epoch 209/501 --> loss:0.8399426746368408
step 251/334, epoch 209/501 --> loss:0.8378728699684143
step 301/334, epoch 209/501 --> loss:0.8383522510528565
step 51/334, epoch 210/501 --> loss:0.8202248191833497
step 101/334, epoch 210/501 --> loss:0.8192178630828857
step 151/334, epoch 210/501 --> loss:0.8330398416519165
step 201/334, epoch 210/501 --> loss:0.8393311595916748
step 251/334, epoch 210/501 --> loss:0.836680201292038
step 301/334, epoch 210/501 --> loss:0.8374041748046875
step 51/334, epoch 211/501 --> loss:0.831114844083786
step 101/334, epoch 211/501 --> loss:0.8302434623241425
step 151/334, epoch 211/501 --> loss:0.8270093405246735
step 201/334, epoch 211/501 --> loss:0.830442625284195
step 251/334, epoch 211/501 --> loss:0.831076112985611
step 301/334, epoch 211/501 --> loss:0.8325740706920624

##########train dataset##########
acc--> [97.18760721773513]
F1--> {'F1': [0.7409205697156677], 'precision': [0.6058078996758306], 'recall': [0.9536169374796686]}
##########eval dataset##########
acc--> [96.82140734147086]
F1--> {'F1': [0.7099556628977907], 'precision': [0.5861485137615501], 'recall': [0.9000850627717861]}
step 51/334, epoch 212/501 --> loss:0.8295675480365753
step 101/334, epoch 212/501 --> loss:0.8297458064556121
step 151/334, epoch 212/501 --> loss:0.8279208636283875
step 201/334, epoch 212/501 --> loss:0.8320210444927215
step 251/334, epoch 212/501 --> loss:0.833317244052887
step 301/334, epoch 212/501 --> loss:0.8283367311954498
step 51/334, epoch 213/501 --> loss:0.8315562033653259
step 101/334, epoch 213/501 --> loss:0.8311912488937377
step 151/334, epoch 213/501 --> loss:0.8270257568359375
step 201/334, epoch 213/501 --> loss:0.8252205491065979
step 251/334, epoch 213/501 --> loss:0.8284871351718902
step 301/334, epoch 213/501 --> loss:0.8380743062496185
step 51/334, epoch 214/501 --> loss:0.8303568243980408
step 101/334, epoch 214/501 --> loss:0.8439455425739288
step 151/334, epoch 214/501 --> loss:0.8345370876789093
step 201/334, epoch 214/501 --> loss:0.8202824723720551
step 251/334, epoch 214/501 --> loss:0.8436435389518738
step 301/334, epoch 214/501 --> loss:0.8183858370780945
step 51/334, epoch 215/501 --> loss:0.8252222156524658
step 101/334, epoch 215/501 --> loss:0.8270503997802734
step 151/334, epoch 215/501 --> loss:0.8275040781497955
step 201/334, epoch 215/501 --> loss:0.8234837615489959
step 251/334, epoch 215/501 --> loss:0.8345429635047913
step 301/334, epoch 215/501 --> loss:0.8258480477333069
step 51/334, epoch 216/501 --> loss:0.8328279495239258
step 101/334, epoch 216/501 --> loss:0.8284503018856049
step 151/334, epoch 216/501 --> loss:0.8183736550807953
step 201/334, epoch 216/501 --> loss:0.8364529061317444
step 251/334, epoch 216/501 --> loss:0.8206811451911926
step 301/334, epoch 216/501 --> loss:0.8361187303066253
step 51/334, epoch 217/501 --> loss:0.8365356075763702
step 101/334, epoch 217/501 --> loss:0.8446388828754425
step 151/334, epoch 217/501 --> loss:0.8182421958446503
step 201/334, epoch 217/501 --> loss:0.8282901179790497
step 251/334, epoch 217/501 --> loss:0.8310944247245788
step 301/334, epoch 217/501 --> loss:0.827281082868576
step 51/334, epoch 218/501 --> loss:0.8225964415073395
step 101/334, epoch 218/501 --> loss:0.8371959888935089
step 151/334, epoch 218/501 --> loss:0.8227660524845123
step 201/334, epoch 218/501 --> loss:0.8176140213012695
step 251/334, epoch 218/501 --> loss:0.8421031403541565
step 301/334, epoch 218/501 --> loss:0.8286783409118652
step 51/334, epoch 219/501 --> loss:0.8260224962234497
step 101/334, epoch 219/501 --> loss:0.8413353931903839
step 151/334, epoch 219/501 --> loss:0.8180067431926727
step 201/334, epoch 219/501 --> loss:0.8244543468952179
step 251/334, epoch 219/501 --> loss:0.8398868012428283
step 301/334, epoch 219/501 --> loss:0.8191231608390808
step 51/334, epoch 220/501 --> loss:0.8280571568012237
step 101/334, epoch 220/501 --> loss:0.8349923288822174
step 151/334, epoch 220/501 --> loss:0.8253322470188141
step 201/334, epoch 220/501 --> loss:0.8282889795303344
step 251/334, epoch 220/501 --> loss:0.8295853459835052
step 301/334, epoch 220/501 --> loss:0.8402737653255463
step 51/334, epoch 221/501 --> loss:0.8484999763965607
step 101/334, epoch 221/501 --> loss:0.8394681179523468
step 151/334, epoch 221/501 --> loss:0.8099383306503296
step 201/334, epoch 221/501 --> loss:0.8298868143558502
step 251/334, epoch 221/501 --> loss:0.8301372063159943
step 301/334, epoch 221/501 --> loss:0.8230944836139679

##########train dataset##########
acc--> [96.62965474881159]
F1--> {'F1': [0.7072075424107295], 'precision': [0.5580492851714416], 'recall': [0.9652051735678318]}
##########eval dataset##########
acc--> [96.25214504131165]
F1--> {'F1': [0.678513588733309], 'precision': [0.539141533851128], 'recall': [0.9150814690703617]}
step 51/334, epoch 222/501 --> loss:0.8363039374351502
step 101/334, epoch 222/501 --> loss:0.8276595735549926
step 151/334, epoch 222/501 --> loss:0.8293380784988403
step 201/334, epoch 222/501 --> loss:0.8272288155555725
step 251/334, epoch 222/501 --> loss:0.845427211523056
step 301/334, epoch 222/501 --> loss:0.816776784658432
step 51/334, epoch 223/501 --> loss:0.8327022540569305
step 101/334, epoch 223/501 --> loss:0.8212983417510986
step 151/334, epoch 223/501 --> loss:0.8321374440193177
step 201/334, epoch 223/501 --> loss:0.8278127479553222
step 251/334, epoch 223/501 --> loss:0.8412613034248352
step 301/334, epoch 223/501 --> loss:0.8233494186401367
step 51/334, epoch 224/501 --> loss:0.8231967079639435
step 101/334, epoch 224/501 --> loss:0.8315449428558349
step 151/334, epoch 224/501 --> loss:0.8299686062335968
step 201/334, epoch 224/501 --> loss:0.8308716297149659
step 251/334, epoch 224/501 --> loss:0.8222719812393189
step 301/334, epoch 224/501 --> loss:0.8291312789916992
step 51/334, epoch 225/501 --> loss:0.8129773497581482
step 101/334, epoch 225/501 --> loss:0.8276214039325714
step 151/334, epoch 225/501 --> loss:0.8246486103534698
step 201/334, epoch 225/501 --> loss:0.8585571777820588
step 251/334, epoch 225/501 --> loss:0.8369466316699982
step 301/334, epoch 225/501 --> loss:0.8270391964912415
step 51/334, epoch 226/501 --> loss:0.8254076218605042
step 101/334, epoch 226/501 --> loss:0.8315430760383606
step 151/334, epoch 226/501 --> loss:0.8258489966392517
step 201/334, epoch 226/501 --> loss:0.833991848230362
step 251/334, epoch 226/501 --> loss:0.8357195806503296
step 301/334, epoch 226/501 --> loss:0.8376724290847778
step 51/334, epoch 227/501 --> loss:0.8301873290538788
step 101/334, epoch 227/501 --> loss:0.8346257686614991
step 151/334, epoch 227/501 --> loss:0.8093159759044647
step 201/334, epoch 227/501 --> loss:0.8262450385093689
step 251/334, epoch 227/501 --> loss:0.8401027727127075
step 301/334, epoch 227/501 --> loss:0.8275849759578705
step 51/334, epoch 228/501 --> loss:0.8175471949577332
step 101/334, epoch 228/501 --> loss:0.8524672269821167
step 151/334, epoch 228/501 --> loss:0.8406077480316162
step 201/334, epoch 228/501 --> loss:0.8125746357440948
step 251/334, epoch 228/501 --> loss:0.8218114590644836
step 301/334, epoch 228/501 --> loss:0.8318270063400268
step 51/334, epoch 229/501 --> loss:0.8188193356990814
step 101/334, epoch 229/501 --> loss:0.8356894946098328
step 151/334, epoch 229/501 --> loss:0.8195377647876739
step 201/334, epoch 229/501 --> loss:0.8274832105636597
step 251/334, epoch 229/501 --> loss:0.8390269935131073
step 301/334, epoch 229/501 --> loss:0.830934739112854
step 51/334, epoch 230/501 --> loss:0.8371721613407135
step 101/334, epoch 230/501 --> loss:0.8230826687812806
step 151/334, epoch 230/501 --> loss:0.8305679178237915
step 201/334, epoch 230/501 --> loss:0.8249320769309998
step 251/334, epoch 230/501 --> loss:0.8338878428936005
step 301/334, epoch 230/501 --> loss:0.8272174870967866
step 51/334, epoch 231/501 --> loss:0.8232754516601563
step 101/334, epoch 231/501 --> loss:0.8365187788009644
step 151/334, epoch 231/501 --> loss:0.8172658789157867
step 201/334, epoch 231/501 --> loss:0.8374847936630249
step 251/334, epoch 231/501 --> loss:0.8225727021694184
step 301/334, epoch 231/501 --> loss:0.8321950936317444

##########train dataset##########
acc--> [97.46080476512422]
F1--> {'F1': [0.7604283777656982], 'precision': [0.6314631140151311], 'recall': [0.9556062288432896]}
##########eval dataset##########
acc--> [97.07928688402437]
F1--> {'F1': [0.7264687037816384], 'precision': [0.6102464334073088], 'recall': [0.8973897095033753]}
step 51/334, epoch 232/501 --> loss:0.8312290632724761
step 101/334, epoch 232/501 --> loss:0.8335103332996369
step 151/334, epoch 232/501 --> loss:0.8366039073467255
step 201/334, epoch 232/501 --> loss:0.835543417930603
step 251/334, epoch 232/501 --> loss:0.8116131055355073
step 301/334, epoch 232/501 --> loss:0.8195631039142609
step 51/334, epoch 233/501 --> loss:0.8247248661518097
step 101/334, epoch 233/501 --> loss:0.8188957071304321
step 151/334, epoch 233/501 --> loss:0.8202195334434509
step 201/334, epoch 233/501 --> loss:0.8463045692443848
step 251/334, epoch 233/501 --> loss:0.8314346146583557
step 301/334, epoch 233/501 --> loss:0.8287864661216736
step 51/334, epoch 234/501 --> loss:0.8261996388435364
step 101/334, epoch 234/501 --> loss:0.8265075254440307
step 151/334, epoch 234/501 --> loss:0.844899080991745
step 201/334, epoch 234/501 --> loss:0.8221572148799896
step 251/334, epoch 234/501 --> loss:0.8201259064674378
step 301/334, epoch 234/501 --> loss:0.8266640126705169
step 51/334, epoch 235/501 --> loss:0.8249055528640747
step 101/334, epoch 235/501 --> loss:0.8308398354053498
step 151/334, epoch 235/501 --> loss:0.828432924747467
step 201/334, epoch 235/501 --> loss:0.8334491336345673
step 251/334, epoch 235/501 --> loss:0.8138437938690185
step 301/334, epoch 235/501 --> loss:0.8349843978881836
step 51/334, epoch 236/501 --> loss:0.8290055572986603
step 101/334, epoch 236/501 --> loss:0.8322883260250091
step 151/334, epoch 236/501 --> loss:0.8316688919067383
step 201/334, epoch 236/501 --> loss:0.8219703531265259
step 251/334, epoch 236/501 --> loss:0.8359417808055878
step 301/334, epoch 236/501 --> loss:0.8265458571910859
step 51/334, epoch 237/501 --> loss:0.8270909309387207
step 101/334, epoch 237/501 --> loss:0.8312939238548279
step 151/334, epoch 237/501 --> loss:0.8268501377105713
step 201/334, epoch 237/501 --> loss:0.8190967321395874
step 251/334, epoch 237/501 --> loss:0.8332140910625457
step 301/334, epoch 237/501 --> loss:0.8375701773166656
step 51/334, epoch 238/501 --> loss:0.8133493304252625
step 101/334, epoch 238/501 --> loss:0.8271431052684783
step 151/334, epoch 238/501 --> loss:0.8331249248981476
step 201/334, epoch 238/501 --> loss:0.8238548171520234
step 251/334, epoch 238/501 --> loss:0.8289899516105652
step 301/334, epoch 238/501 --> loss:0.8401744508743286
step 51/334, epoch 239/501 --> loss:0.8216231465339661
step 101/334, epoch 239/501 --> loss:0.8207285320758819
step 151/334, epoch 239/501 --> loss:0.8346710550785065
step 201/334, epoch 239/501 --> loss:0.8253579235076904
step 251/334, epoch 239/501 --> loss:0.8146318578720093
step 301/334, epoch 239/501 --> loss:0.8448656105995178
step 51/334, epoch 240/501 --> loss:0.8265877115726471
step 101/334, epoch 240/501 --> loss:0.8319899117946625
step 151/334, epoch 240/501 --> loss:0.8320562219619752
step 201/334, epoch 240/501 --> loss:0.8297782301902771
step 251/334, epoch 240/501 --> loss:0.8249471998214721
step 301/334, epoch 240/501 --> loss:0.832039806842804
step 51/334, epoch 241/501 --> loss:0.8297408866882324
step 101/334, epoch 241/501 --> loss:0.8242580139636994
step 151/334, epoch 241/501 --> loss:0.8366836726665496
step 201/334, epoch 241/501 --> loss:0.825591323375702
step 251/334, epoch 241/501 --> loss:0.8413949596881867
step 301/334, epoch 241/501 --> loss:0.8189913845062256

##########train dataset##########
acc--> [97.5042645654983]
F1--> {'F1': [0.7599012507501121], 'precision': [0.639327792310309], 'recall': [0.9365388514512062]}
##########eval dataset##########
acc--> [97.2085066667627]
F1--> {'F1': [0.7311409798229901], 'precision': [0.626273954305017], 'recall': [0.878204409487251]}
step 51/334, epoch 242/501 --> loss:0.817511602640152
step 101/334, epoch 242/501 --> loss:0.8397090196609497
step 151/334, epoch 242/501 --> loss:0.8387960934638977
step 201/334, epoch 242/501 --> loss:0.8341934156417846
step 251/334, epoch 242/501 --> loss:0.8291300511360169
step 301/334, epoch 242/501 --> loss:0.8167471492290497
step 51/334, epoch 243/501 --> loss:0.8243112766742706
step 101/334, epoch 243/501 --> loss:0.8266151547431946
step 151/334, epoch 243/501 --> loss:0.8300565445423126
step 201/334, epoch 243/501 --> loss:0.8353341555595398
step 251/334, epoch 243/501 --> loss:0.8144411516189575
step 301/334, epoch 243/501 --> loss:0.8358595776557922
step 51/334, epoch 244/501 --> loss:0.8287614226341248
step 101/334, epoch 244/501 --> loss:0.8198931539058685
step 151/334, epoch 244/501 --> loss:0.8295165014266968
step 201/334, epoch 244/501 --> loss:0.8347563886642456
step 251/334, epoch 244/501 --> loss:0.834519065618515
step 301/334, epoch 244/501 --> loss:0.8261198091506958
step 51/334, epoch 245/501 --> loss:0.8350261056423187
step 101/334, epoch 245/501 --> loss:0.8191381156444549
step 151/334, epoch 245/501 --> loss:0.8087374460697174
step 201/334, epoch 245/501 --> loss:0.8393064332008362
step 251/334, epoch 245/501 --> loss:0.8387272357940674
step 301/334, epoch 245/501 --> loss:0.8369213211536407
step 51/334, epoch 246/501 --> loss:0.8301590871810913
step 101/334, epoch 246/501 --> loss:0.8219071233272552
step 151/334, epoch 246/501 --> loss:0.8445588886737824
step 201/334, epoch 246/501 --> loss:0.8239432501792908
step 251/334, epoch 246/501 --> loss:0.8366855764389038
step 301/334, epoch 246/501 --> loss:0.8356735527515411
step 51/334, epoch 247/501 --> loss:0.8230226171016694
step 101/334, epoch 247/501 --> loss:0.8337200534343719
step 151/334, epoch 247/501 --> loss:0.8164400672912597
step 201/334, epoch 247/501 --> loss:0.8293818724155426
step 251/334, epoch 247/501 --> loss:0.821158641576767
step 301/334, epoch 247/501 --> loss:0.8300583362579346
step 51/334, epoch 248/501 --> loss:0.8289922869205475
step 101/334, epoch 248/501 --> loss:0.8186152315139771
step 151/334, epoch 248/501 --> loss:0.8354028964042663
step 201/334, epoch 248/501 --> loss:0.8372929167747497
step 251/334, epoch 248/501 --> loss:0.8350350034236907
step 301/334, epoch 248/501 --> loss:0.8202920758724213
step 51/334, epoch 249/501 --> loss:0.81759894490242
step 101/334, epoch 249/501 --> loss:0.8240881323814392
step 151/334, epoch 249/501 --> loss:0.8331876266002655
step 201/334, epoch 249/501 --> loss:0.8443293178081512
step 251/334, epoch 249/501 --> loss:0.817454160451889
step 301/334, epoch 249/501 --> loss:0.8360650742053986
step 51/334, epoch 250/501 --> loss:0.8194448292255402
step 101/334, epoch 250/501 --> loss:0.8337786519527435
step 151/334, epoch 250/501 --> loss:0.8348457300662995
step 201/334, epoch 250/501 --> loss:0.8308790946006774
step 251/334, epoch 250/501 --> loss:0.838314778804779
step 301/334, epoch 250/501 --> loss:0.82220458984375
step 51/334, epoch 251/501 --> loss:0.8265734982490539
step 101/334, epoch 251/501 --> loss:0.838268119096756
step 151/334, epoch 251/501 --> loss:0.828024011850357
step 201/334, epoch 251/501 --> loss:0.8201172137260437
step 251/334, epoch 251/501 --> loss:0.830918892621994
step 301/334, epoch 251/501 --> loss:0.8281551039218903

##########train dataset##########
acc--> [98.04713961154356]
F1--> {'F1': [0.8019200543693189], 'precision': [0.7006651098859101], 'recall': [0.9373972192107117]}
##########eval dataset##########
acc--> [97.65557116410211]
F1--> {'F1': [0.7633020324908951], 'precision': [0.6771248822459257], 'recall': [0.8746263060164544]}
step 51/334, epoch 252/501 --> loss:0.8408073174953461
step 101/334, epoch 252/501 --> loss:0.8250460886955261
step 151/334, epoch 252/501 --> loss:0.8243081593513488
step 201/334, epoch 252/501 --> loss:0.8358564412593842
step 251/334, epoch 252/501 --> loss:0.8229928696155548
step 301/334, epoch 252/501 --> loss:0.8229881429672241
step 51/334, epoch 253/501 --> loss:0.8239873576164246
step 101/334, epoch 253/501 --> loss:0.8334532427787781
step 151/334, epoch 253/501 --> loss:0.8351196706295013
step 201/334, epoch 253/501 --> loss:0.8168764472007751
step 251/334, epoch 253/501 --> loss:0.8340863192081451
step 301/334, epoch 253/501 --> loss:0.816541669368744
step 51/334, epoch 254/501 --> loss:0.8235386669635772
step 101/334, epoch 254/501 --> loss:0.8239640045166016
step 151/334, epoch 254/501 --> loss:0.8381328344345093
step 201/334, epoch 254/501 --> loss:0.8397137570381165
step 251/334, epoch 254/501 --> loss:0.8266514301300049
step 301/334, epoch 254/501 --> loss:0.828615493774414
step 51/334, epoch 255/501 --> loss:0.8265461897850037
step 101/334, epoch 255/501 --> loss:0.8181832242012024
step 151/334, epoch 255/501 --> loss:0.8338731908798218
step 201/334, epoch 255/501 --> loss:0.8362976348400116
step 251/334, epoch 255/501 --> loss:0.8225160813331605
step 301/334, epoch 255/501 --> loss:0.8345304548740387
step 51/334, epoch 256/501 --> loss:0.8223308074474335
step 101/334, epoch 256/501 --> loss:0.8227714836597443
step 151/334, epoch 256/501 --> loss:0.832190387248993
step 201/334, epoch 256/501 --> loss:0.8348722803592682
step 251/334, epoch 256/501 --> loss:0.8332664716243744
step 301/334, epoch 256/501 --> loss:0.8227329742908478
step 51/334, epoch 257/501 --> loss:0.832884042263031
step 101/334, epoch 257/501 --> loss:0.8182503962516785
step 151/334, epoch 257/501 --> loss:0.8309295320510864
step 201/334, epoch 257/501 --> loss:0.8207443010807037
step 251/334, epoch 257/501 --> loss:0.8309664642810821
step 301/334, epoch 257/501 --> loss:0.8326108753681183
step 51/334, epoch 258/501 --> loss:0.8402420914173127
step 101/334, epoch 258/501 --> loss:0.8144017732143403
step 151/334, epoch 258/501 --> loss:0.8324384891986847
step 201/334, epoch 258/501 --> loss:0.8125844717025756
step 251/334, epoch 258/501 --> loss:0.8304687786102295
step 301/334, epoch 258/501 --> loss:0.8302351582050324
step 51/334, epoch 259/501 --> loss:0.825055570602417
step 101/334, epoch 259/501 --> loss:0.835743328332901
step 151/334, epoch 259/501 --> loss:0.8274196422100067
step 201/334, epoch 259/501 --> loss:0.8238375604152679
step 251/334, epoch 259/501 --> loss:0.8274217700958252
step 301/334, epoch 259/501 --> loss:0.8268648266792298
step 51/334, epoch 260/501 --> loss:0.823439576625824
step 101/334, epoch 260/501 --> loss:0.8152007114887238
step 151/334, epoch 260/501 --> loss:0.826858845949173
step 201/334, epoch 260/501 --> loss:0.8335973596572877
step 251/334, epoch 260/501 --> loss:0.8382292699813843
step 301/334, epoch 260/501 --> loss:0.8254921591281891
step 51/334, epoch 261/501 --> loss:0.8325909054279328
step 101/334, epoch 261/501 --> loss:0.8176232028007507
step 151/334, epoch 261/501 --> loss:0.8353079795837403
step 201/334, epoch 261/501 --> loss:0.8286717784404755
step 251/334, epoch 261/501 --> loss:0.8284174501895905
step 301/334, epoch 261/501 --> loss:0.8314711582660675

##########train dataset##########
acc--> [97.98038512702722]
F1--> {'F1': [0.7985140980154212], 'precision': [0.6892270294758307], 'recall': [0.9490042452363954]}
##########eval dataset##########
acc--> [97.56942775722328]
F1--> {'F1': [0.7584088430633832], 'precision': [0.6648059774362374], 'recall': [0.8827023275496166]}
step 51/334, epoch 262/501 --> loss:0.8314443075656891
step 101/334, epoch 262/501 --> loss:0.8257929587364197
step 151/334, epoch 262/501 --> loss:0.8290288972854615
step 201/334, epoch 262/501 --> loss:0.8266854226589203
step 251/334, epoch 262/501 --> loss:0.8215725123882294
step 301/334, epoch 262/501 --> loss:0.8290410840511322
step 51/334, epoch 263/501 --> loss:0.8301568424701691
step 101/334, epoch 263/501 --> loss:0.822218165397644
step 151/334, epoch 263/501 --> loss:0.8307417476177216
step 201/334, epoch 263/501 --> loss:0.8198562455177307
step 251/334, epoch 263/501 --> loss:0.8348604261875152
step 301/334, epoch 263/501 --> loss:0.8418617069721221
step 51/334, epoch 264/501 --> loss:0.8248734104633332
step 101/334, epoch 264/501 --> loss:0.8302538645267487
step 151/334, epoch 264/501 --> loss:0.8425930178165436
step 201/334, epoch 264/501 --> loss:0.8199335622787476
step 251/334, epoch 264/501 --> loss:0.821735680103302
step 301/334, epoch 264/501 --> loss:0.8384461092948914
step 51/334, epoch 265/501 --> loss:0.8141596436500549
step 101/334, epoch 265/501 --> loss:0.8269522249698639
step 151/334, epoch 265/501 --> loss:0.8108982074260712
step 201/334, epoch 265/501 --> loss:0.8293586134910583
step 251/334, epoch 265/501 --> loss:0.8576167142391204
step 301/334, epoch 265/501 --> loss:0.834052095413208
step 51/334, epoch 266/501 --> loss:0.8320206689834595
step 101/334, epoch 266/501 --> loss:0.8307185590267181
step 151/334, epoch 266/501 --> loss:0.8372199749946594
step 201/334, epoch 266/501 --> loss:0.8274316799640655
step 251/334, epoch 266/501 --> loss:0.8197993433475494
step 301/334, epoch 266/501 --> loss:0.8271737217903137
step 51/334, epoch 267/501 --> loss:0.828073924779892
step 101/334, epoch 267/501 --> loss:0.827651686668396
step 151/334, epoch 267/501 --> loss:0.8166863524913788
step 201/334, epoch 267/501 --> loss:0.8324739706516265
step 251/334, epoch 267/501 --> loss:0.8343049550056457
step 301/334, epoch 267/501 --> loss:0.8325030326843261
step 51/334, epoch 268/501 --> loss:0.8217744171619416
step 101/334, epoch 268/501 --> loss:0.8409235763549805
step 151/334, epoch 268/501 --> loss:0.8162340533733368
step 201/334, epoch 268/501 --> loss:0.826213139295578
step 251/334, epoch 268/501 --> loss:0.8225798404216766
step 301/334, epoch 268/501 --> loss:0.8375137579441071
step 51/334, epoch 269/501 --> loss:0.8157300198078156
step 101/334, epoch 269/501 --> loss:0.8235963201522827
step 151/334, epoch 269/501 --> loss:0.8243956530094146
step 201/334, epoch 269/501 --> loss:0.8355301260948181
step 251/334, epoch 269/501 --> loss:0.8284381115436554
step 301/334, epoch 269/501 --> loss:0.8437615084648132
step 51/334, epoch 270/501 --> loss:0.8299638235569
step 101/334, epoch 270/501 --> loss:0.8512142515182495
step 151/334, epoch 270/501 --> loss:0.8261648106575012
step 201/334, epoch 270/501 --> loss:0.8353700888156891
step 251/334, epoch 270/501 --> loss:0.8196164107322693
step 301/334, epoch 270/501 --> loss:0.8229301011562348
step 51/334, epoch 271/501 --> loss:0.8260684776306152
step 101/334, epoch 271/501 --> loss:0.8095079576969146
step 151/334, epoch 271/501 --> loss:0.8289398062229156
step 201/334, epoch 271/501 --> loss:0.8315337800979614
step 251/334, epoch 271/501 --> loss:0.829557329416275
step 301/334, epoch 271/501 --> loss:0.8281948661804199

##########train dataset##########
acc--> [97.14052753140686]
F1--> {'F1': [0.740256555975539], 'precision': [0.5999506112331183], 'recall': [0.966235024944852]}
##########eval dataset##########
acc--> [96.70226863883799]
F1--> {'F1': [0.7039389047118432], 'precision': [0.5751388689780927], 'recall': [0.9070903867624116]}
step 51/334, epoch 272/501 --> loss:0.8206036794185638
step 101/334, epoch 272/501 --> loss:0.8259726572036743
step 151/334, epoch 272/501 --> loss:0.8204618465900421
step 201/334, epoch 272/501 --> loss:0.8352619338035584
step 251/334, epoch 272/501 --> loss:0.8334076094627381
step 301/334, epoch 272/501 --> loss:0.8267270660400391
step 51/334, epoch 273/501 --> loss:0.8374222350120545
step 101/334, epoch 273/501 --> loss:0.846523541212082
step 151/334, epoch 273/501 --> loss:0.8280218732357025
step 201/334, epoch 273/501 --> loss:0.8164349043369293
step 251/334, epoch 273/501 --> loss:0.8254843366146087
step 301/334, epoch 273/501 --> loss:0.815510083436966
step 51/334, epoch 274/501 --> loss:0.8270283150672912
step 101/334, epoch 274/501 --> loss:0.8352459013462067
step 151/334, epoch 274/501 --> loss:0.8211637771129608
step 201/334, epoch 274/501 --> loss:0.8405506587028504
step 251/334, epoch 274/501 --> loss:0.8180578422546386
step 301/334, epoch 274/501 --> loss:0.830892585515976
step 51/334, epoch 275/501 --> loss:0.8338448357582092
step 101/334, epoch 275/501 --> loss:0.8330771434307098
step 151/334, epoch 275/501 --> loss:0.8228093588352203
step 201/334, epoch 275/501 --> loss:0.8358930730819703
step 251/334, epoch 275/501 --> loss:0.814243016242981
step 301/334, epoch 275/501 --> loss:0.8235928773880005
step 51/334, epoch 276/501 --> loss:0.8405910289287567
step 101/334, epoch 276/501 --> loss:0.8325195109844208
step 151/334, epoch 276/501 --> loss:0.8123820185661316
step 201/334, epoch 276/501 --> loss:0.819298015832901
step 251/334, epoch 276/501 --> loss:0.8348826479911804
step 301/334, epoch 276/501 --> loss:0.8246124076843262
step 51/334, epoch 277/501 --> loss:0.829409111738205
step 101/334, epoch 277/501 --> loss:0.8323825788497925
step 151/334, epoch 277/501 --> loss:0.8378117704391479
step 201/334, epoch 277/501 --> loss:0.8209275555610657
step 251/334, epoch 277/501 --> loss:0.8252170395851135
step 301/334, epoch 277/501 --> loss:0.8234047043323517
step 51/334, epoch 278/501 --> loss:0.8308738088607788
step 101/334, epoch 278/501 --> loss:0.8252287685871125
step 151/334, epoch 278/501 --> loss:0.848417534828186
step 201/334, epoch 278/501 --> loss:0.8237145531177521
step 251/334, epoch 278/501 --> loss:0.8231282556056976
step 301/334, epoch 278/501 --> loss:0.82849609375
step 51/334, epoch 279/501 --> loss:0.8313642454147339
step 101/334, epoch 279/501 --> loss:0.8338908350467682
step 151/334, epoch 279/501 --> loss:0.8381465554237366
step 201/334, epoch 279/501 --> loss:0.8205916631221771
step 251/334, epoch 279/501 --> loss:0.82875066280365
step 301/334, epoch 279/501 --> loss:0.8179009854793549
step 51/334, epoch 280/501 --> loss:0.8202956390380859
step 101/334, epoch 280/501 --> loss:0.8417273652553559
step 151/334, epoch 280/501 --> loss:0.8428424799442291
step 201/334, epoch 280/501 --> loss:0.8125092780590057
step 251/334, epoch 280/501 --> loss:0.8251252174377441
step 301/334, epoch 280/501 --> loss:0.8284159755706787
step 51/334, epoch 281/501 --> loss:0.8347637856006622
step 101/334, epoch 281/501 --> loss:0.8294546580314637
step 151/334, epoch 281/501 --> loss:0.832141387462616
step 201/334, epoch 281/501 --> loss:0.8200480079650879
step 251/334, epoch 281/501 --> loss:0.8310806667804718
step 301/334, epoch 281/501 --> loss:0.8144728147983551

##########train dataset##########
acc--> [97.30060979315783]
F1--> {'F1': [0.7506868888801799], 'precision': [0.6148037003579131], 'recall': [0.9636934993525179]}
##########eval dataset##########
acc--> [96.88043602703311]
F1--> {'F1': [0.7147029714783686], 'precision': [0.5909316930937724], 'recall': [0.904074426527564]}
step 51/334, epoch 282/501 --> loss:0.8221602368354798
step 101/334, epoch 282/501 --> loss:0.8288020181655884
step 151/334, epoch 282/501 --> loss:0.8291550016403199
step 201/334, epoch 282/501 --> loss:0.8229716908931732
step 251/334, epoch 282/501 --> loss:0.8248763477802277
step 301/334, epoch 282/501 --> loss:0.8373484873771667
step 51/334, epoch 283/501 --> loss:0.8209062957763672
step 101/334, epoch 283/501 --> loss:0.8344036865234375
step 151/334, epoch 283/501 --> loss:0.8317879807949066
step 201/334, epoch 283/501 --> loss:0.8232135748863221
step 251/334, epoch 283/501 --> loss:0.8306812572479249
step 301/334, epoch 283/501 --> loss:0.8323037576675415
step 51/334, epoch 284/501 --> loss:0.8242813563346862
step 101/334, epoch 284/501 --> loss:0.8319783508777618
step 151/334, epoch 284/501 --> loss:0.8158020043373108
step 201/334, epoch 284/501 --> loss:0.8225957190990448
step 251/334, epoch 284/501 --> loss:0.8263367140293121
step 301/334, epoch 284/501 --> loss:0.8436568224430084
step 51/334, epoch 285/501 --> loss:0.8162627172470093
step 101/334, epoch 285/501 --> loss:0.8435830128192902
step 151/334, epoch 285/501 --> loss:0.8338715553283691
step 201/334, epoch 285/501 --> loss:0.8114371502399444
step 251/334, epoch 285/501 --> loss:0.8178828930854798
step 301/334, epoch 285/501 --> loss:0.8363530933856964
step 51/334, epoch 286/501 --> loss:0.8201001334190369
step 101/334, epoch 286/501 --> loss:0.827408812046051
step 151/334, epoch 286/501 --> loss:0.8167099571228027
step 201/334, epoch 286/501 --> loss:0.8282080328464508
step 251/334, epoch 286/501 --> loss:0.841362110376358
step 301/334, epoch 286/501 --> loss:0.8213036489486695
step 51/334, epoch 287/501 --> loss:0.8400788915157318
step 101/334, epoch 287/501 --> loss:0.8216373109817505
step 151/334, epoch 287/501 --> loss:0.8126023542881012
step 201/334, epoch 287/501 --> loss:0.842226254940033
step 251/334, epoch 287/501 --> loss:0.8323321032524109
step 301/334, epoch 287/501 --> loss:0.8248809039592743
step 51/334, epoch 288/501 --> loss:0.8257681548595428
step 101/334, epoch 288/501 --> loss:0.8270925271511078
step 151/334, epoch 288/501 --> loss:0.8348758256435395
step 201/334, epoch 288/501 --> loss:0.8240628266334533
step 251/334, epoch 288/501 --> loss:0.828590407371521
step 301/334, epoch 288/501 --> loss:0.8325589072704315
step 51/334, epoch 289/501 --> loss:0.8163923156261444
step 101/334, epoch 289/501 --> loss:0.8314681601524353
step 151/334, epoch 289/501 --> loss:0.8287239265441895
step 201/334, epoch 289/501 --> loss:0.825342892408371
step 251/334, epoch 289/501 --> loss:0.8293391525745392
step 301/334, epoch 289/501 --> loss:0.8378270471096039
step 51/334, epoch 290/501 --> loss:0.841909019947052
step 101/334, epoch 290/501 --> loss:0.8221653604507446
step 151/334, epoch 290/501 --> loss:0.838116729259491
step 201/334, epoch 290/501 --> loss:0.8200821650028228
step 251/334, epoch 290/501 --> loss:0.834642186164856
step 301/334, epoch 290/501 --> loss:0.817088748216629
step 51/334, epoch 291/501 --> loss:0.8340822184085845
step 101/334, epoch 291/501 --> loss:0.8286666703224183
step 151/334, epoch 291/501 --> loss:0.8362419080734252
step 201/334, epoch 291/501 --> loss:0.8241272759437561
step 251/334, epoch 291/501 --> loss:0.8295151424407959
step 301/334, epoch 291/501 --> loss:0.8244356727600097

##########train dataset##########
acc--> [91.8294013373424]
F1--> {'F1': [0.5002606853725142], 'precision': [0.3370752715258285], 'recall': [0.9697547126275979]}
##########eval dataset##########
acc--> [91.56723289455759]
F1--> {'F1': [0.48536381787319616], 'precision': [0.3296304212519443], 'recall': [0.920059062102743]}
step 51/334, epoch 292/501 --> loss:0.825160584449768
step 101/334, epoch 292/501 --> loss:0.8325105893611908
step 151/334, epoch 292/501 --> loss:0.8249831163883209
step 201/334, epoch 292/501 --> loss:0.8427747464179993
step 251/334, epoch 292/501 --> loss:0.8334880816936493
step 301/334, epoch 292/501 --> loss:0.8173138535022736
step 51/334, epoch 293/501 --> loss:0.8273422789573669
step 101/334, epoch 293/501 --> loss:0.8361677587032318
step 151/334, epoch 293/501 --> loss:0.8445199382305145
step 201/334, epoch 293/501 --> loss:0.8174803495407105
step 251/334, epoch 293/501 --> loss:0.828377171754837
step 301/334, epoch 293/501 --> loss:0.8226792395114899
step 51/334, epoch 294/501 --> loss:0.8258401036262513
step 101/334, epoch 294/501 --> loss:0.818021000623703
step 151/334, epoch 294/501 --> loss:0.8352357542514801
step 201/334, epoch 294/501 --> loss:0.8220395088195801
step 251/334, epoch 294/501 --> loss:0.8309874534606934
step 301/334, epoch 294/501 --> loss:0.8372095501422883
step 51/334, epoch 295/501 --> loss:0.8326546239852906
step 101/334, epoch 295/501 --> loss:0.8317378044128418
step 151/334, epoch 295/501 --> loss:0.8233988153934478
step 201/334, epoch 295/501 --> loss:0.8165446174144745
step 251/334, epoch 295/501 --> loss:0.8231305348873138
step 301/334, epoch 295/501 --> loss:0.8328720986843109
step 51/334, epoch 296/501 --> loss:0.8242146015167237
step 101/334, epoch 296/501 --> loss:0.8337822365760803
step 151/334, epoch 296/501 --> loss:0.8260508000850677
step 201/334, epoch 296/501 --> loss:0.8333778929710388
step 251/334, epoch 296/501 --> loss:0.8300489711761475
step 301/334, epoch 296/501 --> loss:0.8264938867092133
step 51/334, epoch 297/501 --> loss:0.8183998787403106
step 101/334, epoch 297/501 --> loss:0.8270971918106079
step 151/334, epoch 297/501 --> loss:0.8289393198490143
step 201/334, epoch 297/501 --> loss:0.8473276937007904
step 251/334, epoch 297/501 --> loss:0.8151963555812836
step 301/334, epoch 297/501 --> loss:0.8371396422386169
step 51/334, epoch 298/501 --> loss:0.8224048161506653
step 101/334, epoch 298/501 --> loss:0.8261340236663819
step 151/334, epoch 298/501 --> loss:0.8230296921730041
step 201/334, epoch 298/501 --> loss:0.8450069642066955
step 251/334, epoch 298/501 --> loss:0.8170662951469422
step 301/334, epoch 298/501 --> loss:0.8256097626686096
step 51/334, epoch 299/501 --> loss:0.8233560967445374
step 101/334, epoch 299/501 --> loss:0.8313375234603881
step 151/334, epoch 299/501 --> loss:0.8396914267539978
step 201/334, epoch 299/501 --> loss:0.8310832047462463
step 251/334, epoch 299/501 --> loss:0.8345811867713928
step 301/334, epoch 299/501 --> loss:0.8210248243808747
step 51/334, epoch 300/501 --> loss:0.827940776348114
step 101/334, epoch 300/501 --> loss:0.843400856256485
step 151/334, epoch 300/501 --> loss:0.8335442781448364
step 201/334, epoch 300/501 --> loss:0.8253583776950836
step 251/334, epoch 300/501 --> loss:0.812573584318161
step 301/334, epoch 300/501 --> loss:0.8304715991020203
step 51/334, epoch 301/501 --> loss:0.8324187719821929
step 101/334, epoch 301/501 --> loss:0.8323454678058624
step 151/334, epoch 301/501 --> loss:0.8197232758998871
step 201/334, epoch 301/501 --> loss:0.8170281231403351
step 251/334, epoch 301/501 --> loss:0.8247311985492707
step 301/334, epoch 301/501 --> loss:0.8434873068332672

##########train dataset##########
acc--> [97.79646064978292]
F1--> {'F1': [0.7862041832328855], 'precision': [0.6653282804887275], 'recall': [0.960766746124161]}
##########eval dataset##########
acc--> [97.33664979041276]
F1--> {'F1': [0.7436685721424066], 'precision': [0.636675643770317], 'recall': [0.8938994795219345]}
step 51/334, epoch 302/501 --> loss:0.8384906053543091
step 101/334, epoch 302/501 --> loss:0.8361682307720184
step 151/334, epoch 302/501 --> loss:0.8308246505260467
step 201/334, epoch 302/501 --> loss:0.8299398136138916
step 251/334, epoch 302/501 --> loss:0.8180337870121002
step 301/334, epoch 302/501 --> loss:0.826837375164032
step 51/334, epoch 303/501 --> loss:0.834436514377594
step 101/334, epoch 303/501 --> loss:0.8200016570091248
step 151/334, epoch 303/501 --> loss:0.831633276939392
step 201/334, epoch 303/501 --> loss:0.8382593262195587
step 251/334, epoch 303/501 --> loss:0.8396284914016724
step 301/334, epoch 303/501 --> loss:0.8236311984062195
step 51/334, epoch 304/501 --> loss:0.8204953849315644
step 101/334, epoch 304/501 --> loss:0.8459111797809601
step 151/334, epoch 304/501 --> loss:0.8288722360134124
step 201/334, epoch 304/501 --> loss:0.83700155377388
step 251/334, epoch 304/501 --> loss:0.8171750938892365
step 301/334, epoch 304/501 --> loss:0.8269591069221497
step 51/334, epoch 305/501 --> loss:0.8431409776210785
step 101/334, epoch 305/501 --> loss:0.8174279487133026
step 151/334, epoch 305/501 --> loss:0.8307617211341858
step 201/334, epoch 305/501 --> loss:0.8426072812080383
step 251/334, epoch 305/501 --> loss:0.8237846767902375
step 301/334, epoch 305/501 --> loss:0.8225703811645508
step 51/334, epoch 306/501 --> loss:0.8341630530357361
step 101/334, epoch 306/501 --> loss:0.8324852883815765
step 151/334, epoch 306/501 --> loss:0.8333904409408569
step 201/334, epoch 306/501 --> loss:0.8224885642528534
step 251/334, epoch 306/501 --> loss:0.8208704042434692
step 301/334, epoch 306/501 --> loss:0.8247998332977295
step 51/334, epoch 307/501 --> loss:0.830548802614212
step 101/334, epoch 307/501 --> loss:0.8384057784080505
step 151/334, epoch 307/501 --> loss:0.8128612327575684
step 201/334, epoch 307/501 --> loss:0.8217947459220887
step 251/334, epoch 307/501 --> loss:0.8320922994613648
step 301/334, epoch 307/501 --> loss:0.8271522796154023
step 51/334, epoch 308/501 --> loss:0.8372670638561249
step 101/334, epoch 308/501 --> loss:0.8311544454097748
step 151/334, epoch 308/501 --> loss:0.826054366827011
step 201/334, epoch 308/501 --> loss:0.8227523684501648
step 251/334, epoch 308/501 --> loss:0.845962620973587
step 301/334, epoch 308/501 --> loss:0.8139495861530304
step 51/334, epoch 309/501 --> loss:0.8237400758266449
step 101/334, epoch 309/501 --> loss:0.8249019885063171
step 151/334, epoch 309/501 --> loss:0.8285658407211304
step 201/334, epoch 309/501 --> loss:0.8239318180084229
step 251/334, epoch 309/501 --> loss:0.8381618845462799
step 301/334, epoch 309/501 --> loss:0.8157294404506683
step 51/334, epoch 310/501 --> loss:0.8244110524654389
step 101/334, epoch 310/501 --> loss:0.8346020436286926
step 151/334, epoch 310/501 --> loss:0.8343968284130097
step 201/334, epoch 310/501 --> loss:0.825291109085083
step 251/334, epoch 310/501 --> loss:0.8271120500564575
step 301/334, epoch 310/501 --> loss:0.8249186432361603
step 51/334, epoch 311/501 --> loss:0.8270959484577179
step 101/334, epoch 311/501 --> loss:0.8093541467189789
step 151/334, epoch 311/501 --> loss:0.8405210316181183
step 201/334, epoch 311/501 --> loss:0.815925076007843
step 251/334, epoch 311/501 --> loss:0.8419437158107758
step 301/334, epoch 311/501 --> loss:0.827729595899582

##########train dataset##########
acc--> [96.93050520402812]
F1--> {'F1': [0.723365166569202], 'precision': [0.5834208241470058], 'recall': [0.951648064062595]}
##########eval dataset##########
acc--> [96.58095525839386]
F1--> {'F1': [0.6929102937194747], 'precision': [0.5662891990666498], 'recall': [0.8924789868064791]}
step 51/334, epoch 312/501 --> loss:0.817342678308487
step 101/334, epoch 312/501 --> loss:0.8353414130210877
step 151/334, epoch 312/501 --> loss:0.8362048959732056
step 201/334, epoch 312/501 --> loss:0.8421344423294067
step 251/334, epoch 312/501 --> loss:0.8219963419437408
step 301/334, epoch 312/501 --> loss:0.8261023664474487
step 51/334, epoch 313/501 --> loss:0.8273069179058075
step 101/334, epoch 313/501 --> loss:0.8366569018363953
step 151/334, epoch 313/501 --> loss:0.8353232502937317
step 201/334, epoch 313/501 --> loss:0.8391427528858185
step 251/334, epoch 313/501 --> loss:0.8140719449520111
step 301/334, epoch 313/501 --> loss:0.822000595331192
step 51/334, epoch 314/501 --> loss:0.8172023165225982
step 101/334, epoch 314/501 --> loss:0.8304028189182282
step 151/334, epoch 314/501 --> loss:0.8335339522361755
step 201/334, epoch 314/501 --> loss:0.8301639819145202
step 251/334, epoch 314/501 --> loss:0.833789529800415
step 301/334, epoch 314/501 --> loss:0.8295173001289368
step 51/334, epoch 315/501 --> loss:0.8190957260131836
step 101/334, epoch 315/501 --> loss:0.8374207317829132
step 151/334, epoch 315/501 --> loss:0.811849639415741
step 201/334, epoch 315/501 --> loss:0.8361225509643555
step 251/334, epoch 315/501 --> loss:0.8342647647857666
step 301/334, epoch 315/501 --> loss:0.8367346906661988
step 51/334, epoch 316/501 --> loss:0.8192337620258331
step 101/334, epoch 316/501 --> loss:0.828485289812088
step 151/334, epoch 316/501 --> loss:0.8279408192634583
step 201/334, epoch 316/501 --> loss:0.828105092048645
step 251/334, epoch 316/501 --> loss:0.8237549090385436
step 301/334, epoch 316/501 --> loss:0.8376936519145965
step 51/334, epoch 317/501 --> loss:0.8224895834922791
step 101/334, epoch 317/501 --> loss:0.8395463991165161
step 151/334, epoch 317/501 --> loss:0.834652270078659
step 201/334, epoch 317/501 --> loss:0.8330935943126678
step 251/334, epoch 317/501 --> loss:0.8127676808834076
step 301/334, epoch 317/501 --> loss:0.8316914069652558
step 51/334, epoch 318/501 --> loss:0.8278391647338867
step 101/334, epoch 318/501 --> loss:0.8345052468776702
step 151/334, epoch 318/501 --> loss:0.8183888685703278
step 201/334, epoch 318/501 --> loss:0.8248511135578156
step 251/334, epoch 318/501 --> loss:0.8362476658821106
step 301/334, epoch 318/501 --> loss:0.8423968005180359
step 51/334, epoch 319/501 --> loss:0.8264351511001586
step 101/334, epoch 319/501 --> loss:0.8228155696392059
step 151/334, epoch 319/501 --> loss:0.8406305170059204
step 201/334, epoch 319/501 --> loss:0.8121298241615296
step 251/334, epoch 319/501 --> loss:0.8308674252033234
step 301/334, epoch 319/501 --> loss:0.8310887062549591
step 51/334, epoch 320/501 --> loss:0.8250189244747161
step 101/334, epoch 320/501 --> loss:0.8241822361946106
step 151/334, epoch 320/501 --> loss:0.833770444393158
step 201/334, epoch 320/501 --> loss:0.8289166581630707
step 251/334, epoch 320/501 --> loss:0.8247089958190919
step 301/334, epoch 320/501 --> loss:0.8269488608837128
step 51/334, epoch 321/501 --> loss:0.8361424851417542
step 101/334, epoch 321/501 --> loss:0.8184474563598633
step 151/334, epoch 321/501 --> loss:0.8326974821090698
step 201/334, epoch 321/501 --> loss:0.8302611863613129
step 251/334, epoch 321/501 --> loss:0.8353057742118836
step 301/334, epoch 321/501 --> loss:0.8193195557594299

##########train dataset##########
acc--> [98.28017570572563]
F1--> {'F1': [0.8210039087959465], 'precision': [0.7316110343075979], 'recall': [0.9352954595568062]}
##########eval dataset##########
acc--> [97.82560653886117]
F1--> {'F1': [0.7736805874222725], 'precision': [0.7031649247768371], 'recall': [0.8599279591808957]}
step 51/334, epoch 322/501 --> loss:0.8400317180156708
step 101/334, epoch 322/501 --> loss:0.8079613542556763
step 151/334, epoch 322/501 --> loss:0.8440223908424378
step 201/334, epoch 322/501 --> loss:0.8429737484455109
step 251/334, epoch 322/501 --> loss:0.8211009800434113
step 301/334, epoch 322/501 --> loss:0.8183898198604583
step 51/334, epoch 323/501 --> loss:0.819774956703186
step 101/334, epoch 323/501 --> loss:0.8272460079193116
step 151/334, epoch 323/501 --> loss:0.8216817033290863
step 201/334, epoch 323/501 --> loss:0.8385046458244324
step 251/334, epoch 323/501 --> loss:0.8319291925430298
step 301/334, epoch 323/501 --> loss:0.8352944028377532
step 51/334, epoch 324/501 --> loss:0.8210181653499603
step 101/334, epoch 324/501 --> loss:0.8247734522819519
step 151/334, epoch 324/501 --> loss:0.821056203842163
step 201/334, epoch 324/501 --> loss:0.8243067955970764
step 251/334, epoch 324/501 --> loss:0.8359449219703674
step 301/334, epoch 324/501 --> loss:0.8267723643779754
step 51/334, epoch 325/501 --> loss:0.8155176365375518
step 101/334, epoch 325/501 --> loss:0.82089834690094
step 151/334, epoch 325/501 --> loss:0.8368044936656952
step 201/334, epoch 325/501 --> loss:0.8487826907634735
step 251/334, epoch 325/501 --> loss:0.8205483269691467
step 301/334, epoch 325/501 --> loss:0.8205036580562591
step 51/334, epoch 326/501 --> loss:0.8441899597644806
step 101/334, epoch 326/501 --> loss:0.8223861753940582
step 151/334, epoch 326/501 --> loss:0.8285052156448365
step 201/334, epoch 326/501 --> loss:0.8199585938453674
step 251/334, epoch 326/501 --> loss:0.8320480465888977
step 301/334, epoch 326/501 --> loss:0.8186212539672851
step 51/334, epoch 327/501 --> loss:0.8294074249267578
step 101/334, epoch 327/501 --> loss:0.8417258083820343
step 151/334, epoch 327/501 --> loss:0.8354440796375274
step 201/334, epoch 327/501 --> loss:0.8259357464313507
step 251/334, epoch 327/501 --> loss:0.82824746966362
step 301/334, epoch 327/501 --> loss:0.8214497482776641
step 51/334, epoch 328/501 --> loss:0.8305664956569672
step 101/334, epoch 328/501 --> loss:0.8324302995204925
step 151/334, epoch 328/501 --> loss:0.8360651683807373
step 201/334, epoch 328/501 --> loss:0.8291607320308685
step 251/334, epoch 328/501 --> loss:0.8237057721614838
step 301/334, epoch 328/501 --> loss:0.8139425849914551
step 51/334, epoch 329/501 --> loss:0.8361562490463257
step 101/334, epoch 329/501 --> loss:0.8309133303165436
step 151/334, epoch 329/501 --> loss:0.8332375848293304
step 201/334, epoch 329/501 --> loss:0.8287531411647797
step 251/334, epoch 329/501 --> loss:0.8260431599617004
step 301/334, epoch 329/501 --> loss:0.8154647660255432
step 51/334, epoch 330/501 --> loss:0.8310567712783814
step 101/334, epoch 330/501 --> loss:0.8230768883228302
step 151/334, epoch 330/501 --> loss:0.8426793956756592
step 201/334, epoch 330/501 --> loss:0.8258855414390563
step 251/334, epoch 330/501 --> loss:0.825033940076828
step 301/334, epoch 330/501 --> loss:0.8273514139652253
step 51/334, epoch 331/501 --> loss:0.8284030902385712
step 101/334, epoch 331/501 --> loss:0.8180212497711181
step 151/334, epoch 331/501 --> loss:0.8324200141429902
step 201/334, epoch 331/501 --> loss:0.8315576553344727
step 251/334, epoch 331/501 --> loss:0.8246068084239959
step 301/334, epoch 331/501 --> loss:0.8284595501422882

##########train dataset##########
acc--> [98.0517921976949]
F1--> {'F1': [0.8023053089496486], 'precision': [0.7012315522141396], 'recall': [0.937436494928489]}
##########eval dataset##########
acc--> [97.62682042826269]
F1--> {'F1': [0.7587434153193572], 'precision': [0.6767018089488437], 'recall': [0.8634353314423339]}
step 51/334, epoch 332/501 --> loss:0.8310544347763061
step 101/334, epoch 332/501 --> loss:0.8382444930076599
step 151/334, epoch 332/501 --> loss:0.8137047696113586
step 201/334, epoch 332/501 --> loss:0.8253882539272308
step 251/334, epoch 332/501 --> loss:0.8399058640003204
step 301/334, epoch 332/501 --> loss:0.8230610609054565
step 51/334, epoch 333/501 --> loss:0.8287046313285827
step 101/334, epoch 333/501 --> loss:0.8302556264400482
step 151/334, epoch 333/501 --> loss:0.8261177980899811
step 201/334, epoch 333/501 --> loss:0.8195791637897492
step 251/334, epoch 333/501 --> loss:0.8359701883792877
step 301/334, epoch 333/501 --> loss:0.8238268649578094
step 51/334, epoch 334/501 --> loss:0.8380163824558258
step 101/334, epoch 334/501 --> loss:0.8169889008998871
step 151/334, epoch 334/501 --> loss:0.8306241202354431
step 201/334, epoch 334/501 --> loss:0.8312101936340333
step 251/334, epoch 334/501 --> loss:0.8289174807071685
step 301/334, epoch 334/501 --> loss:0.8216064143180847
step 51/334, epoch 335/501 --> loss:0.8356755006313324
step 101/334, epoch 335/501 --> loss:0.8325383734703063
step 151/334, epoch 335/501 --> loss:0.8126345777511597
step 201/334, epoch 335/501 --> loss:0.8295252132415771
step 251/334, epoch 335/501 --> loss:0.8375301516056061
step 301/334, epoch 335/501 --> loss:0.8272149229049682
step 51/334, epoch 336/501 --> loss:0.8391704297065735
step 101/334, epoch 336/501 --> loss:0.818492455482483
step 151/334, epoch 336/501 --> loss:0.8275705969333649
step 201/334, epoch 336/501 --> loss:0.8197126471996308
step 251/334, epoch 336/501 --> loss:0.8249301087856292
step 301/334, epoch 336/501 --> loss:0.8297085773944854
step 51/334, epoch 337/501 --> loss:0.8354652619361878
step 101/334, epoch 337/501 --> loss:0.8178642535209656
step 151/334, epoch 337/501 --> loss:0.8459777235984802
step 201/334, epoch 337/501 --> loss:0.8208190894126892
step 251/334, epoch 337/501 --> loss:0.8223800384998321
step 301/334, epoch 337/501 --> loss:0.8198856008052826
step 51/334, epoch 338/501 --> loss:0.8391195333003998
step 101/334, epoch 338/501 --> loss:0.8283417642116546
step 151/334, epoch 338/501 --> loss:0.8180472803115845
step 201/334, epoch 338/501 --> loss:0.8250859582424164
step 251/334, epoch 338/501 --> loss:0.8328228580951691
step 301/334, epoch 338/501 --> loss:0.8264781057834625
step 51/334, epoch 339/501 --> loss:0.8231629776954651
step 101/334, epoch 339/501 --> loss:0.8186496603488922
step 151/334, epoch 339/501 --> loss:0.8382642638683319
step 201/334, epoch 339/501 --> loss:0.8248480045795441
step 251/334, epoch 339/501 --> loss:0.8372458136081695
step 301/334, epoch 339/501 --> loss:0.8199663043022156
step 51/334, epoch 340/501 --> loss:0.8409201264381408
step 101/334, epoch 340/501 --> loss:0.823210586309433
step 151/334, epoch 340/501 --> loss:0.8336983752250672
step 201/334, epoch 340/501 --> loss:0.8330020141601563
step 251/334, epoch 340/501 --> loss:0.8255051100254058
step 301/334, epoch 340/501 --> loss:0.821828738451004
step 51/334, epoch 341/501 --> loss:0.8171490609645844
step 101/334, epoch 341/501 --> loss:0.8261426484584808
step 151/334, epoch 341/501 --> loss:0.8384725141525269
step 201/334, epoch 341/501 --> loss:0.8197722220420838
step 251/334, epoch 341/501 --> loss:0.8329083967208862
step 301/334, epoch 341/501 --> loss:0.8381450033187866

##########train dataset##########
acc--> [97.68041518461035]
F1--> {'F1': [0.7785528667226048], 'precision': [0.6516198293660428], 'recall': [0.9669164145563767]}
##########eval dataset##########
acc--> [97.1962949630926]
F1--> {'F1': [0.7351687322680976], 'precision': [0.6211869877616226], 'recall': [0.9003940871166031]}
step 51/334, epoch 342/501 --> loss:0.8368062281608581
step 101/334, epoch 342/501 --> loss:0.8306696772575378
step 151/334, epoch 342/501 --> loss:0.8211023044586182
step 201/334, epoch 342/501 --> loss:0.8287297058105468
step 251/334, epoch 342/501 --> loss:0.8243446326255799
step 301/334, epoch 342/501 --> loss:0.8265535795688629
step 51/334, epoch 343/501 --> loss:0.8160626852512359
step 101/334, epoch 343/501 --> loss:0.8214477944374085
step 151/334, epoch 343/501 --> loss:0.8364148318767548
step 201/334, epoch 343/501 --> loss:0.8279962921142578
step 251/334, epoch 343/501 --> loss:0.8421372938156128
step 301/334, epoch 343/501 --> loss:0.8226335513591766
step 51/334, epoch 344/501 --> loss:0.8289538002014161
step 101/334, epoch 344/501 --> loss:0.8299598848819733
step 151/334, epoch 344/501 --> loss:0.8299887871742249
step 201/334, epoch 344/501 --> loss:0.8279582035541534
step 251/334, epoch 344/501 --> loss:0.8351519727706909
step 301/334, epoch 344/501 --> loss:0.8194230604171753
step 51/334, epoch 345/501 --> loss:0.82835662484169
step 101/334, epoch 345/501 --> loss:0.8163284504413605
step 151/334, epoch 345/501 --> loss:0.8402218186855316
step 201/334, epoch 345/501 --> loss:0.8269996964931488
step 251/334, epoch 345/501 --> loss:0.8234420454502106
step 301/334, epoch 345/501 --> loss:0.830150032043457
step 51/334, epoch 346/501 --> loss:0.8343346416950226
step 101/334, epoch 346/501 --> loss:0.8364392650127411
step 151/334, epoch 346/501 --> loss:0.8246905636787415
step 201/334, epoch 346/501 --> loss:0.8277785634994507
step 251/334, epoch 346/501 --> loss:0.8126275861263275
step 301/334, epoch 346/501 --> loss:0.8429298424720764
step 51/334, epoch 347/501 --> loss:0.8349262082576752
step 101/334, epoch 347/501 --> loss:0.8342273378372193
step 151/334, epoch 347/501 --> loss:0.8200328636169434
step 201/334, epoch 347/501 --> loss:0.8409311485290527
step 251/334, epoch 347/501 --> loss:0.817293438911438
step 301/334, epoch 347/501 --> loss:0.815809452533722
step 51/334, epoch 348/501 --> loss:0.8276753151416778
step 101/334, epoch 348/501 --> loss:0.822046115398407
step 151/334, epoch 348/501 --> loss:0.8366858661174774
step 201/334, epoch 348/501 --> loss:0.8123083984851838
step 251/334, epoch 348/501 --> loss:0.8294967925548553
step 301/334, epoch 348/501 --> loss:0.8417465507984161
step 51/334, epoch 349/501 --> loss:0.8278337180614471
step 101/334, epoch 349/501 --> loss:0.8201599442958831
step 151/334, epoch 349/501 --> loss:0.821603034734726
step 201/334, epoch 349/501 --> loss:0.8356616294384003
step 251/334, epoch 349/501 --> loss:0.8322083473205566
step 301/334, epoch 349/501 --> loss:0.8310393977165222
step 51/334, epoch 350/501 --> loss:0.8343598556518554
step 101/334, epoch 350/501 --> loss:0.8290408217906952
step 151/334, epoch 350/501 --> loss:0.8289557898044586
step 201/334, epoch 350/501 --> loss:0.8357149517536163
step 251/334, epoch 350/501 --> loss:0.8170884501934051
step 301/334, epoch 350/501 --> loss:0.8155625295639038
step 51/334, epoch 351/501 --> loss:0.8161171913146973
step 101/334, epoch 351/501 --> loss:0.8296678054332733
step 151/334, epoch 351/501 --> loss:0.8343454551696777
step 201/334, epoch 351/501 --> loss:0.8293365001678467
step 251/334, epoch 351/501 --> loss:0.8192433762550354
step 301/334, epoch 351/501 --> loss:0.8282388806343078

##########train dataset##########
acc--> [97.95747319991092]
F1--> {'F1': [0.7992579672142972], 'precision': [0.6824994161468892], 'recall': [0.9642242981290416]}
##########eval dataset##########
acc--> [97.44817244828609]
F1--> {'F1': [0.7509666390072667], 'precision': [0.6493930300857254], 'recall': [0.8902203755906131]}
step 51/334, epoch 352/501 --> loss:0.8197907507419586
step 101/334, epoch 352/501 --> loss:0.8272988963127136
step 151/334, epoch 352/501 --> loss:0.8287517476081848
step 201/334, epoch 352/501 --> loss:0.8275825333595276
step 251/334, epoch 352/501 --> loss:0.8300258100032807
step 301/334, epoch 352/501 --> loss:0.8270159113407135
step 51/334, epoch 353/501 --> loss:0.8142369842529297
step 101/334, epoch 353/501 --> loss:0.8265109896659851
step 151/334, epoch 353/501 --> loss:0.8308987772464752
step 201/334, epoch 353/501 --> loss:0.8308459043502807
step 251/334, epoch 353/501 --> loss:0.8339039456844329
step 301/334, epoch 353/501 --> loss:0.833305881023407
step 51/334, epoch 354/501 --> loss:0.8348519909381866
step 101/334, epoch 354/501 --> loss:0.8212013661861419
step 151/334, epoch 354/501 --> loss:0.8249534511566162
step 201/334, epoch 354/501 --> loss:0.8220719850063324
step 251/334, epoch 354/501 --> loss:0.8261907029151917
step 301/334, epoch 354/501 --> loss:0.8313032960891724
step 51/334, epoch 355/501 --> loss:0.8291465508937835
step 101/334, epoch 355/501 --> loss:0.833233299255371
step 151/334, epoch 355/501 --> loss:0.8182119238376617
step 201/334, epoch 355/501 --> loss:0.823639738559723
step 251/334, epoch 355/501 --> loss:0.8297355103492737
step 301/334, epoch 355/501 --> loss:0.8393221890926361
step 51/334, epoch 356/501 --> loss:0.8371153450012208
step 101/334, epoch 356/501 --> loss:0.8248726177215576
step 151/334, epoch 356/501 --> loss:0.8176191830635071
step 201/334, epoch 356/501 --> loss:0.8400394654273987
step 251/334, epoch 356/501 --> loss:0.8220198559761047
step 301/334, epoch 356/501 --> loss:0.8190329551696778
step 51/334, epoch 357/501 --> loss:0.8301250433921814
step 101/334, epoch 357/501 --> loss:0.8333350479602813
step 151/334, epoch 357/501 --> loss:0.8223497545719147
step 201/334, epoch 357/501 --> loss:0.830182044506073
step 251/334, epoch 357/501 --> loss:0.8228515088558197
step 301/334, epoch 357/501 --> loss:0.8297525072097778
step 51/334, epoch 358/501 --> loss:0.8255283761024476
step 101/334, epoch 358/501 --> loss:0.8180432307720185
step 151/334, epoch 358/501 --> loss:0.836055519580841
step 201/334, epoch 358/501 --> loss:0.8303826022148132
step 251/334, epoch 358/501 --> loss:0.8288078165054321
step 301/334, epoch 358/501 --> loss:0.8445694303512573
step 51/334, epoch 359/501 --> loss:0.827647180557251
step 101/334, epoch 359/501 --> loss:0.8446315515041352
step 151/334, epoch 359/501 --> loss:0.8222759532928466
step 201/334, epoch 359/501 --> loss:0.8244924128055573
step 251/334, epoch 359/501 --> loss:0.8237792670726776
step 301/334, epoch 359/501 --> loss:0.8271000242233276
step 51/334, epoch 360/501 --> loss:0.8326794755458832
step 101/334, epoch 360/501 --> loss:0.8340059566497803
step 151/334, epoch 360/501 --> loss:0.8168418455123901
step 201/334, epoch 360/501 --> loss:0.8313564741611481
step 251/334, epoch 360/501 --> loss:0.8259936225414276
step 301/334, epoch 360/501 --> loss:0.832241586446762
step 51/334, epoch 361/501 --> loss:0.8178262495994568
step 101/334, epoch 361/501 --> loss:0.8264986085891723
step 151/334, epoch 361/501 --> loss:0.8354714262485504
step 201/334, epoch 361/501 --> loss:0.8220074355602265
step 251/334, epoch 361/501 --> loss:0.8320172047615051
step 301/334, epoch 361/501 --> loss:0.8238205134868621

##########train dataset##########
acc--> [97.08280216498538]
F1--> {'F1': [0.7367648292364675], 'precision': [0.594679732236243], 'recall': [0.9680770493252356]}
##########eval dataset##########
acc--> [96.65858952078587]
F1--> {'F1': [0.7000628714210149], 'precision': [0.5719173053332308], 'recall': [0.9022320175152371]}
step 51/334, epoch 362/501 --> loss:0.8183656120300293
step 101/334, epoch 362/501 --> loss:0.8188350594043732
step 151/334, epoch 362/501 --> loss:0.82523042678833
step 201/334, epoch 362/501 --> loss:0.81823690533638
step 251/334, epoch 362/501 --> loss:0.8346485280990601
step 301/334, epoch 362/501 --> loss:0.8379878294467926
step 51/334, epoch 363/501 --> loss:0.8192663514614105
step 101/334, epoch 363/501 --> loss:0.825968713760376
step 151/334, epoch 363/501 --> loss:0.8367152678966522
step 201/334, epoch 363/501 --> loss:0.8243887031078339
step 251/334, epoch 363/501 --> loss:0.835787490606308
step 301/334, epoch 363/501 --> loss:0.8163739991188049
step 51/334, epoch 364/501 --> loss:0.8334216225147247
step 101/334, epoch 364/501 --> loss:0.8250858116149903
step 151/334, epoch 364/501 --> loss:0.8316307938098908
step 201/334, epoch 364/501 --> loss:0.832568427324295
step 251/334, epoch 364/501 --> loss:0.8114761900901795
step 301/334, epoch 364/501 --> loss:0.8346349072456359
step 51/334, epoch 365/501 --> loss:0.8333451533317566
step 101/334, epoch 365/501 --> loss:0.8232583224773407
step 151/334, epoch 365/501 --> loss:0.8331662607192993
step 201/334, epoch 365/501 --> loss:0.8373706376552582
step 251/334, epoch 365/501 --> loss:0.8142664885520935
step 301/334, epoch 365/501 --> loss:0.8196591663360596
step 51/334, epoch 366/501 --> loss:0.8336621141433715
step 101/334, epoch 366/501 --> loss:0.8254234576225281
step 151/334, epoch 366/501 --> loss:0.8303264713287354
step 201/334, epoch 366/501 --> loss:0.8251780426502228
step 251/334, epoch 366/501 --> loss:0.8296974718570709
step 301/334, epoch 366/501 --> loss:0.8297164118289948
step 51/334, epoch 367/501 --> loss:0.837612589597702
step 101/334, epoch 367/501 --> loss:0.8201942408084869
step 151/334, epoch 367/501 --> loss:0.8362337052822113
step 201/334, epoch 367/501 --> loss:0.8341402208805084
step 251/334, epoch 367/501 --> loss:0.8221622323989868
step 301/334, epoch 367/501 --> loss:0.8141907334327698
step 51/334, epoch 368/501 --> loss:0.8234962224960327
step 101/334, epoch 368/501 --> loss:0.8286637425422668
step 151/334, epoch 368/501 --> loss:0.8302898252010346
step 201/334, epoch 368/501 --> loss:0.8242257440090179
step 251/334, epoch 368/501 --> loss:0.8282118713855744
step 301/334, epoch 368/501 --> loss:0.8446122682094575
step 51/334, epoch 369/501 --> loss:0.8340164041519165
step 101/334, epoch 369/501 --> loss:0.8246612966060638
step 151/334, epoch 369/501 --> loss:0.8166027784347534
step 201/334, epoch 369/501 --> loss:0.8340870428085327
step 251/334, epoch 369/501 --> loss:0.8204421186447144
step 301/334, epoch 369/501 --> loss:0.8437861609458923
step 51/334, epoch 370/501 --> loss:0.8242578685283661
step 101/334, epoch 370/501 --> loss:0.8377631378173828
step 151/334, epoch 370/501 --> loss:0.808125262260437
step 201/334, epoch 370/501 --> loss:0.842458621263504
step 251/334, epoch 370/501 --> loss:0.8264151418209076
step 301/334, epoch 370/501 --> loss:0.8227038788795471
step 51/334, epoch 371/501 --> loss:0.8431487512588501
step 101/334, epoch 371/501 --> loss:0.8255649673938751
step 151/334, epoch 371/501 --> loss:0.8231470668315888
step 201/334, epoch 371/501 --> loss:0.817281733751297
step 251/334, epoch 371/501 --> loss:0.8364469623565673
step 301/334, epoch 371/501 --> loss:0.8300520944595337

##########train dataset##########
acc--> [97.2838345653848]
F1--> {'F1': [0.7509587552144515], 'precision': [0.612190990800567], 'recall': [0.9710923933781471]}
##########eval dataset##########
acc--> [96.83632370888904]
F1--> {'F1': [0.711400188963202], 'precision': [0.5872305914294209], 'recall': [0.9021771931062366]}
step 51/334, epoch 372/501 --> loss:0.8209375333786011
step 101/334, epoch 372/501 --> loss:0.8319037556648254
step 151/334, epoch 372/501 --> loss:0.8203598010540009
step 201/334, epoch 372/501 --> loss:0.8164996933937073
step 251/334, epoch 372/501 --> loss:0.8389341974258423
step 301/334, epoch 372/501 --> loss:0.8329025089740754
step 51/334, epoch 373/501 --> loss:0.8188707828521729
step 101/334, epoch 373/501 --> loss:0.826570428609848
step 151/334, epoch 373/501 --> loss:0.8361655724048614
step 201/334, epoch 373/501 --> loss:0.8175094330310821
step 251/334, epoch 373/501 --> loss:0.8245622384548187
step 301/334, epoch 373/501 --> loss:0.8408663845062256
step 51/334, epoch 374/501 --> loss:0.8294061815738678
step 101/334, epoch 374/501 --> loss:0.8330677711963653
step 151/334, epoch 374/501 --> loss:0.8379084038734436
step 201/334, epoch 374/501 --> loss:0.8235734558105469
step 251/334, epoch 374/501 --> loss:0.8242546021938324
step 301/334, epoch 374/501 --> loss:0.830146621465683
step 51/334, epoch 375/501 --> loss:0.8276323902606965
step 101/334, epoch 375/501 --> loss:0.8231389188766479
step 151/334, epoch 375/501 --> loss:0.8270321357250213
step 201/334, epoch 375/501 --> loss:0.8341742503643036
step 251/334, epoch 375/501 --> loss:0.8332421064376831
step 301/334, epoch 375/501 --> loss:0.8229334259033203
step 51/334, epoch 376/501 --> loss:0.8320583117008209
step 101/334, epoch 376/501 --> loss:0.8223814499378205
step 151/334, epoch 376/501 --> loss:0.85740065574646
step 201/334, epoch 376/501 --> loss:0.8161119389533996
step 251/334, epoch 376/501 --> loss:0.835344010591507
step 301/334, epoch 376/501 --> loss:0.8163821005821228
step 51/334, epoch 377/501 --> loss:0.8244260025024414
step 101/334, epoch 377/501 --> loss:0.8331141316890717
step 151/334, epoch 377/501 --> loss:0.8157645261287689
step 201/334, epoch 377/501 --> loss:0.8257134687900544
step 251/334, epoch 377/501 --> loss:0.8296876728534699
step 301/334, epoch 377/501 --> loss:0.83854297041893
step 51/334, epoch 378/501 --> loss:0.818028370141983
step 101/334, epoch 378/501 --> loss:0.8375206911563873
step 151/334, epoch 378/501 --> loss:0.8163317549228668
step 201/334, epoch 378/501 --> loss:0.8252301549911499
step 251/334, epoch 378/501 --> loss:0.8347990381717681
step 301/334, epoch 378/501 --> loss:0.8391200423240661
step 51/334, epoch 379/501 --> loss:0.8325213086605072
step 101/334, epoch 379/501 --> loss:0.8323146486282349
step 151/334, epoch 379/501 --> loss:0.8289579272270202
step 201/334, epoch 379/501 --> loss:0.8359281527996063
step 251/334, epoch 379/501 --> loss:0.8347789919376374
step 301/334, epoch 379/501 --> loss:0.8125665318965912
step 51/334, epoch 380/501 --> loss:0.8134321928024292
step 101/334, epoch 380/501 --> loss:0.8145599830150604
step 151/334, epoch 380/501 --> loss:0.8239741766452789
step 201/334, epoch 380/501 --> loss:0.8372989618778228
step 251/334, epoch 380/501 --> loss:0.8266259431838989
step 301/334, epoch 380/501 --> loss:0.8465629887580871
step 51/334, epoch 381/501 --> loss:0.8269439566135407
step 101/334, epoch 381/501 --> loss:0.8282088315486908
step 151/334, epoch 381/501 --> loss:0.8392092037200928
step 201/334, epoch 381/501 --> loss:0.8308234024047851
step 251/334, epoch 381/501 --> loss:0.8170874917507172
step 301/334, epoch 381/501 --> loss:0.8240422618389129

##########train dataset##########
acc--> [97.70637972773875]
F1--> {'F1': [0.7797125968836643], 'precision': [0.6552505086741185], 'recall': [0.9625584378921254]}
##########eval dataset##########
acc--> [97.17352692744423]
F1--> {'F1': [0.7313863977714303], 'precision': [0.6206080002749176], 'recall': [0.8903201405714984]}
step 51/334, epoch 382/501 --> loss:0.8250282740592957
step 101/334, epoch 382/501 --> loss:0.8304013919830322
step 151/334, epoch 382/501 --> loss:0.8235169887542725
step 201/334, epoch 382/501 --> loss:0.8257711231708527
step 251/334, epoch 382/501 --> loss:0.825725998878479
step 301/334, epoch 382/501 --> loss:0.8281624019145966
step 51/334, epoch 383/501 --> loss:0.8178352117538452
step 101/334, epoch 383/501 --> loss:0.836519432067871
step 151/334, epoch 383/501 --> loss:0.8303652286529541
step 201/334, epoch 383/501 --> loss:0.8246598172187806
step 251/334, epoch 383/501 --> loss:0.819741667509079
step 301/334, epoch 383/501 --> loss:0.8368422961235047
step 51/334, epoch 384/501 --> loss:0.835480625629425
step 101/334, epoch 384/501 --> loss:0.8213065075874328
step 151/334, epoch 384/501 --> loss:0.8203879141807556
step 201/334, epoch 384/501 --> loss:0.8268164241313934
step 251/334, epoch 384/501 --> loss:0.8254449367523193
step 301/334, epoch 384/501 --> loss:0.8304673933982849
step 51/334, epoch 385/501 --> loss:0.8355255806446076
step 101/334, epoch 385/501 --> loss:0.8397613561153412
step 151/334, epoch 385/501 --> loss:0.8259107995033265
step 201/334, epoch 385/501 --> loss:0.8258654880523681
step 251/334, epoch 385/501 --> loss:0.8331345653533936
step 301/334, epoch 385/501 --> loss:0.8198749995231629
step 51/334, epoch 386/501 --> loss:0.8304497051239014
step 101/334, epoch 386/501 --> loss:0.8425518846511841
step 151/334, epoch 386/501 --> loss:0.8158453941345215
step 201/334, epoch 386/501 --> loss:0.8327904391288757
step 251/334, epoch 386/501 --> loss:0.8265844416618348
step 301/334, epoch 386/501 --> loss:0.8211801290512085
step 51/334, epoch 387/501 --> loss:0.8277503085136414
step 101/334, epoch 387/501 --> loss:0.8483491098880768
step 151/334, epoch 387/501 --> loss:0.8275940907001496
step 201/334, epoch 387/501 --> loss:0.8263930010795594
step 251/334, epoch 387/501 --> loss:0.831977266073227
step 301/334, epoch 387/501 --> loss:0.811602703332901
step 51/334, epoch 388/501 --> loss:0.8206584250926972
step 101/334, epoch 388/501 --> loss:0.8282248759269715
step 151/334, epoch 388/501 --> loss:0.8244431555271149
step 201/334, epoch 388/501 --> loss:0.8291191589832306
step 251/334, epoch 388/501 --> loss:0.8253171169757842
step 301/334, epoch 388/501 --> loss:0.8246681785583496
step 51/334, epoch 389/501 --> loss:0.8288190865516663
step 101/334, epoch 389/501 --> loss:0.8283122217655182
step 151/334, epoch 389/501 --> loss:0.8221765029430389
step 201/334, epoch 389/501 --> loss:0.8206363785266876
step 251/334, epoch 389/501 --> loss:0.823462666273117
step 301/334, epoch 389/501 --> loss:0.8284457278251648
step 51/334, epoch 390/501 --> loss:0.8220792496204377
step 101/334, epoch 390/501 --> loss:0.8400884580612182
step 151/334, epoch 390/501 --> loss:0.8260514390468597
step 201/334, epoch 390/501 --> loss:0.8265925908088684
step 251/334, epoch 390/501 --> loss:0.8343858826160431
step 301/334, epoch 390/501 --> loss:0.8285737109184265
step 51/334, epoch 391/501 --> loss:0.8388939702510834
step 101/334, epoch 391/501 --> loss:0.8301837968826294
step 151/334, epoch 391/501 --> loss:0.8313477528095246
step 201/334, epoch 391/501 --> loss:0.8217591059207916
step 251/334, epoch 391/501 --> loss:0.8175962269306183
step 301/334, epoch 391/501 --> loss:0.8223362147808075

##########train dataset##########
acc--> [98.09505404002563]
F1--> {'F1': [0.807577924984643], 'precision': [0.7034370015271038], 'recall': [0.9479258927571114]}
##########eval dataset##########
acc--> [97.60767084823584]
F1--> {'F1': [0.759458870245303], 'precision': [0.6715783968846185], 'recall': [0.8738145958933371]}
step 51/334, epoch 392/501 --> loss:0.8324224185943604
step 101/334, epoch 392/501 --> loss:0.8191371071338653
step 151/334, epoch 392/501 --> loss:0.8412320613861084
step 201/334, epoch 392/501 --> loss:0.8282739353179932
step 251/334, epoch 392/501 --> loss:0.8220664429664611
step 301/334, epoch 392/501 --> loss:0.8260417020320893
step 51/334, epoch 393/501 --> loss:0.8357820880413055
step 101/334, epoch 393/501 --> loss:0.8203685021400452
step 151/334, epoch 393/501 --> loss:0.827244552373886
step 201/334, epoch 393/501 --> loss:0.8336150133609772
step 251/334, epoch 393/501 --> loss:0.8218074572086335
step 301/334, epoch 393/501 --> loss:0.8311867487430572
step 51/334, epoch 394/501 --> loss:0.8290877783298493
step 101/334, epoch 394/501 --> loss:0.8387304615974426
step 151/334, epoch 394/501 --> loss:0.8075847995281219
step 201/334, epoch 394/501 --> loss:0.827866507768631
step 251/334, epoch 394/501 --> loss:0.8302649641036988
step 301/334, epoch 394/501 --> loss:0.8288575434684753
step 51/334, epoch 395/501 --> loss:0.8297734224796295
step 101/334, epoch 395/501 --> loss:0.8314410722255707
step 151/334, epoch 395/501 --> loss:0.830984001159668
step 201/334, epoch 395/501 --> loss:0.8059081315994263
step 251/334, epoch 395/501 --> loss:0.8404936742782593
step 301/334, epoch 395/501 --> loss:0.834934594631195
step 51/334, epoch 396/501 --> loss:0.8246236634254456
step 101/334, epoch 396/501 --> loss:0.841594455242157
step 151/334, epoch 396/501 --> loss:0.8278637993335723
step 201/334, epoch 396/501 --> loss:0.8415759968757629
step 251/334, epoch 396/501 --> loss:0.817952378988266
step 301/334, epoch 396/501 --> loss:0.8262025439739227
step 51/334, epoch 397/501 --> loss:0.8296661818027496
step 101/334, epoch 397/501 --> loss:0.819892874956131
step 151/334, epoch 397/501 --> loss:0.830760782957077
step 201/334, epoch 397/501 --> loss:0.8273463344573975
step 251/334, epoch 397/501 --> loss:0.8302951335906983
step 301/334, epoch 397/501 --> loss:0.8330138826370239
step 51/334, epoch 398/501 --> loss:0.8277877724170685
step 101/334, epoch 398/501 --> loss:0.8262416517734528
step 151/334, epoch 398/501 --> loss:0.8300255906581878
step 201/334, epoch 398/501 --> loss:0.8355538034439087
step 251/334, epoch 398/501 --> loss:0.8255344998836517
step 301/334, epoch 398/501 --> loss:0.8175473582744598
step 51/334, epoch 399/501 --> loss:0.8400056171417236
step 101/334, epoch 399/501 --> loss:0.8301340055465698
step 151/334, epoch 399/501 --> loss:0.8267264699935913
step 201/334, epoch 399/501 --> loss:0.8193033313751221
step 251/334, epoch 399/501 --> loss:0.8235495543479919
step 301/334, epoch 399/501 --> loss:0.8292355871200562
step 51/334, epoch 400/501 --> loss:0.8251353323459625
step 101/334, epoch 400/501 --> loss:0.8306820011138916
step 151/334, epoch 400/501 --> loss:0.8295374703407288
step 201/334, epoch 400/501 --> loss:0.830624190568924
step 251/334, epoch 400/501 --> loss:0.821189215183258
step 301/334, epoch 400/501 --> loss:0.8261754894256592
step 51/334, epoch 401/501 --> loss:0.8307852625846863
step 101/334, epoch 401/501 --> loss:0.8201351571083069
step 151/334, epoch 401/501 --> loss:0.8390218949317932
step 201/334, epoch 401/501 --> loss:0.8270686185359954
step 251/334, epoch 401/501 --> loss:0.8299159002304077
step 301/334, epoch 401/501 --> loss:0.8224143695831299

##########train dataset##########
acc--> [98.0876429142618]
F1--> {'F1': [0.7977060265968474], 'precision': [0.7200758639180883], 'recall': [0.8941095445211329]}
##########eval dataset##########
acc--> [97.68405824195028]
F1--> {'F1': [0.7554736165674064], 'precision': [0.6948065761193439], 'recall': [0.827760393548406]}
step 51/334, epoch 402/501 --> loss:0.8232924580574036
step 101/334, epoch 402/501 --> loss:0.8221770250797271
step 151/334, epoch 402/501 --> loss:0.8252215826511383
step 201/334, epoch 402/501 --> loss:0.8327947926521301
step 251/334, epoch 402/501 --> loss:0.8385830223560333
step 301/334, epoch 402/501 --> loss:0.8211540114879609
step 51/334, epoch 403/501 --> loss:0.8277071809768677
step 101/334, epoch 403/501 --> loss:0.8359754037857056
step 151/334, epoch 403/501 --> loss:0.8184027481079101
step 201/334, epoch 403/501 --> loss:0.824726197719574
step 251/334, epoch 403/501 --> loss:0.8253597927093506
step 301/334, epoch 403/501 --> loss:0.8258516407012939
step 51/334, epoch 404/501 --> loss:0.8201545584201813
step 101/334, epoch 404/501 --> loss:0.833071506023407
step 151/334, epoch 404/501 --> loss:0.8269880318641663
step 201/334, epoch 404/501 --> loss:0.8250470983982087
step 251/334, epoch 404/501 --> loss:0.833162373304367
step 301/334, epoch 404/501 --> loss:0.8273919904232026
step 51/334, epoch 405/501 --> loss:0.8184545814990998
step 101/334, epoch 405/501 --> loss:0.8195484495162964
step 151/334, epoch 405/501 --> loss:0.8264160096645355
step 201/334, epoch 405/501 --> loss:0.8306131565570831
step 251/334, epoch 405/501 --> loss:0.8358364653587341
step 301/334, epoch 405/501 --> loss:0.8289412033557891
step 51/334, epoch 406/501 --> loss:0.8254288697242737
step 101/334, epoch 406/501 --> loss:0.8252356052398682
step 151/334, epoch 406/501 --> loss:0.8366605162620544
step 201/334, epoch 406/501 --> loss:0.8397768354415893
step 251/334, epoch 406/501 --> loss:0.8234419655799866
step 301/334, epoch 406/501 --> loss:0.8176545929908753
step 51/334, epoch 407/501 --> loss:0.8209090828895569
step 101/334, epoch 407/501 --> loss:0.8301452922821045
step 151/334, epoch 407/501 --> loss:0.8169728302955628
step 201/334, epoch 407/501 --> loss:0.829873389005661
step 251/334, epoch 407/501 --> loss:0.8327238070964813
step 301/334, epoch 407/501 --> loss:0.8221474719047547
step 51/334, epoch 408/501 --> loss:0.8321356403827668
step 101/334, epoch 408/501 --> loss:0.8089784801006317
step 151/334, epoch 408/501 --> loss:0.8323409104347229
step 201/334, epoch 408/501 --> loss:0.8185717153549195
step 251/334, epoch 408/501 --> loss:0.8336040687561035
step 301/334, epoch 408/501 --> loss:0.8326767539978027
step 51/334, epoch 409/501 --> loss:0.8255527353286743
step 101/334, epoch 409/501 --> loss:0.820497156381607
step 151/334, epoch 409/501 --> loss:0.8292409288883209
step 201/334, epoch 409/501 --> loss:0.824709312915802
step 251/334, epoch 409/501 --> loss:0.813028347492218
step 301/334, epoch 409/501 --> loss:0.8501644051074981
step 51/334, epoch 410/501 --> loss:0.8211997091770172
step 101/334, epoch 410/501 --> loss:0.8112407171726227
step 151/334, epoch 410/501 --> loss:0.8335132646560669
step 201/334, epoch 410/501 --> loss:0.8335353195667267
step 251/334, epoch 410/501 --> loss:0.8295760536193848
step 301/334, epoch 410/501 --> loss:0.8342692363262176
step 51/334, epoch 411/501 --> loss:0.8394327259063721
step 101/334, epoch 411/501 --> loss:0.8268673777580261
step 151/334, epoch 411/501 --> loss:0.8301725149154663
step 201/334, epoch 411/501 --> loss:0.8223324131965637
step 251/334, epoch 411/501 --> loss:0.8197422385215759
step 301/334, epoch 411/501 --> loss:0.8384229528903961

##########train dataset##########
acc--> [98.05692995269231]
F1--> {'F1': [0.8058085994328456], 'precision': [0.6964148922214842], 'recall': [0.9559879291264205]}
##########eval dataset##########
acc--> [97.57523157047343]
F1--> {'F1': [0.7571718527026375], 'precision': [0.6675052504782973], 'recall': [0.8746797405108606]}
step 51/334, epoch 412/501 --> loss:0.8293918788433075
step 101/334, epoch 412/501 --> loss:0.8161761939525605
step 151/334, epoch 412/501 --> loss:0.8158945655822754
step 201/334, epoch 412/501 --> loss:0.8251319205760956
step 251/334, epoch 412/501 --> loss:0.8229952228069305
step 301/334, epoch 412/501 --> loss:0.8445650541782379
step 51/334, epoch 413/501 --> loss:0.8324776828289032
step 101/334, epoch 413/501 --> loss:0.8333717834949493
step 151/334, epoch 413/501 --> loss:0.8345127999782562
step 201/334, epoch 413/501 --> loss:0.8272951924800873
step 251/334, epoch 413/501 --> loss:0.8223712718486786
step 301/334, epoch 413/501 --> loss:0.8173755240440369
step 51/334, epoch 414/501 --> loss:0.8203581869602203
step 101/334, epoch 414/501 --> loss:0.8190987503528595
step 151/334, epoch 414/501 --> loss:0.8244790244102478
step 201/334, epoch 414/501 --> loss:0.8448211240768433
step 251/334, epoch 414/501 --> loss:0.8278640246391297
step 301/334, epoch 414/501 --> loss:0.825601270198822
step 51/334, epoch 415/501 --> loss:0.8251289737224579
step 101/334, epoch 415/501 --> loss:0.8374044442176819
step 151/334, epoch 415/501 --> loss:0.8317152559757233
step 201/334, epoch 415/501 --> loss:0.8300119996070862
step 251/334, epoch 415/501 --> loss:0.8235202729701996
step 301/334, epoch 415/501 --> loss:0.8160971271991729
step 51/334, epoch 416/501 --> loss:0.8223223853111267
step 101/334, epoch 416/501 --> loss:0.8110538637638092
step 151/334, epoch 416/501 --> loss:0.8255485439300537
step 201/334, epoch 416/501 --> loss:0.8377164304256439
step 251/334, epoch 416/501 --> loss:0.8392441773414612
step 301/334, epoch 416/501 --> loss:0.8322953796386718
step 51/334, epoch 417/501 --> loss:0.8283044910430908
step 101/334, epoch 417/501 --> loss:0.8184980309009552
step 151/334, epoch 417/501 --> loss:0.8315428161621093
step 201/334, epoch 417/501 --> loss:0.824647126197815
step 251/334, epoch 417/501 --> loss:0.8252165484428405
step 301/334, epoch 417/501 --> loss:0.8334902226924896
step 51/334, epoch 418/501 --> loss:0.8325100183486939
step 101/334, epoch 418/501 --> loss:0.8264987587928772
step 151/334, epoch 418/501 --> loss:0.8222858273983001
step 201/334, epoch 418/501 --> loss:0.8250156593322754
step 251/334, epoch 418/501 --> loss:0.8442846608161926
step 301/334, epoch 418/501 --> loss:0.8101004135608673
step 51/334, epoch 419/501 --> loss:0.8119940447807312
step 101/334, epoch 419/501 --> loss:0.8377906739711761
step 151/334, epoch 419/501 --> loss:0.823141039609909
step 201/334, epoch 419/501 --> loss:0.8304227733612061
step 251/334, epoch 419/501 --> loss:0.8232778680324554
step 301/334, epoch 419/501 --> loss:0.8290475833415986
step 51/334, epoch 420/501 --> loss:0.8152791774272918
step 101/334, epoch 420/501 --> loss:0.8264598941802979
step 151/334, epoch 420/501 --> loss:0.8284384298324585
step 201/334, epoch 420/501 --> loss:0.8201180732250214
step 251/334, epoch 420/501 --> loss:0.8295049905776978
step 301/334, epoch 420/501 --> loss:0.8406427359580994
step 51/334, epoch 421/501 --> loss:0.8335203850269317
step 101/334, epoch 421/501 --> loss:0.8265392637252807
step 151/334, epoch 421/501 --> loss:0.8308837807178497
step 201/334, epoch 421/501 --> loss:0.8244794595241547
step 251/334, epoch 421/501 --> loss:0.8289885270595551
step 301/334, epoch 421/501 --> loss:0.8237158763408661

##########train dataset##########
acc--> [97.77085769475678]
F1--> {'F1': [0.7706527492544927], 'precision': [0.6806466400990224], 'recall': [0.8881034800518438]}
##########eval dataset##########
acc--> [97.30024763291877]
F1--> {'F1': [0.7265776589950985], 'precision': [0.6461086247131166], 'recall': [0.8299549142579756]}
step 51/334, epoch 422/501 --> loss:0.8328949761390686
step 101/334, epoch 422/501 --> loss:0.8252887654304505
step 151/334, epoch 422/501 --> loss:0.8260107731819153
step 201/334, epoch 422/501 --> loss:0.835144876241684
step 251/334, epoch 422/501 --> loss:0.8081237971782684
step 301/334, epoch 422/501 --> loss:0.8397749733924865
step 51/334, epoch 423/501 --> loss:0.8284245347976684
step 101/334, epoch 423/501 --> loss:0.8259399771690369
step 151/334, epoch 423/501 --> loss:0.8338226449489593
step 201/334, epoch 423/501 --> loss:0.8152625679969787
step 251/334, epoch 423/501 --> loss:0.8294366526603699
step 301/334, epoch 423/501 --> loss:0.8336612272262574
step 51/334, epoch 424/501 --> loss:0.8334130322933198
step 101/334, epoch 424/501 --> loss:0.8097600674629212
step 151/334, epoch 424/501 --> loss:0.8269566142559052
step 201/334, epoch 424/501 --> loss:0.8349006593227386
step 251/334, epoch 424/501 --> loss:0.8289807331562042
step 301/334, epoch 424/501 --> loss:0.8281412518024445
step 51/334, epoch 425/501 --> loss:0.8235698890686035
step 101/334, epoch 425/501 --> loss:0.8260415506362915
step 151/334, epoch 425/501 --> loss:0.8270529115200043
step 201/334, epoch 425/501 --> loss:0.8287366580963135
step 251/334, epoch 425/501 --> loss:0.8432836759090424
step 301/334, epoch 425/501 --> loss:0.8164708817005157
step 51/334, epoch 426/501 --> loss:0.818801851272583
step 101/334, epoch 426/501 --> loss:0.8149256372451782
step 151/334, epoch 426/501 --> loss:0.822125837802887
step 201/334, epoch 426/501 --> loss:0.830748085975647
step 251/334, epoch 426/501 --> loss:0.8325837218761444
step 301/334, epoch 426/501 --> loss:0.8380382442474366
step 51/334, epoch 427/501 --> loss:0.8246158564090729
step 101/334, epoch 427/501 --> loss:0.8288764262199402
step 151/334, epoch 427/501 --> loss:0.8271674859523773
step 201/334, epoch 427/501 --> loss:0.8280189883708954
step 251/334, epoch 427/501 --> loss:0.8374040925502777
step 301/334, epoch 427/501 --> loss:0.8294266247749329
step 51/334, epoch 428/501 --> loss:0.821226304769516
step 101/334, epoch 428/501 --> loss:0.8258429598808289
step 151/334, epoch 428/501 --> loss:0.8275459265708923
step 201/334, epoch 428/501 --> loss:0.8356994843482971
step 251/334, epoch 428/501 --> loss:0.8303329360485077
step 301/334, epoch 428/501 --> loss:0.8214791595935822
step 51/334, epoch 429/501 --> loss:0.8277603316307068
step 101/334, epoch 429/501 --> loss:0.8240116143226623
step 151/334, epoch 429/501 --> loss:0.8325900983810425
step 201/334, epoch 429/501 --> loss:0.8218511378765107
step 251/334, epoch 429/501 --> loss:0.8224198961257935
step 301/334, epoch 429/501 --> loss:0.8411787319183349
step 51/334, epoch 430/501 --> loss:0.8195784556865692
step 101/334, epoch 430/501 --> loss:0.8344737625122071
step 151/334, epoch 430/501 --> loss:0.8256731510162354
step 201/334, epoch 430/501 --> loss:0.822602572441101
step 251/334, epoch 430/501 --> loss:0.8243574452400207
step 301/334, epoch 430/501 --> loss:0.8411684942245483
step 51/334, epoch 431/501 --> loss:0.8307050895690918
step 101/334, epoch 431/501 --> loss:0.8186543011665344
step 151/334, epoch 431/501 --> loss:0.8222016382217407
step 201/334, epoch 431/501 --> loss:0.8283981716632843
step 251/334, epoch 431/501 --> loss:0.8221757292747498
step 301/334, epoch 431/501 --> loss:0.8352512240409851

##########train dataset##########
acc--> [98.38017112179543]
F1--> {'F1': [0.8317846550942243], 'precision': [0.7399348917073262], 'recall': [0.9496820396613187]}
##########eval dataset##########
acc--> [97.84863623003501]
F1--> {'F1': [0.7763907952271295], 'precision': [0.7048218646207088], 'recall': [0.8641491298040249]}
save model!
step 51/334, epoch 432/501 --> loss:0.8167579889297485
step 101/334, epoch 432/501 --> loss:0.8374852216243744
step 151/334, epoch 432/501 --> loss:0.8212115836143493
step 201/334, epoch 432/501 --> loss:0.8352771627902985
step 251/334, epoch 432/501 --> loss:0.8211158108711243
step 301/334, epoch 432/501 --> loss:0.8237394714355468
step 51/334, epoch 433/501 --> loss:0.8341566157341004
step 101/334, epoch 433/501 --> loss:0.8140504324436187
step 151/334, epoch 433/501 --> loss:0.8321438467502594
step 201/334, epoch 433/501 --> loss:0.8196741890907288
step 251/334, epoch 433/501 --> loss:0.8501364743709564
step 301/334, epoch 433/501 --> loss:0.8233652436733245
step 51/334, epoch 434/501 --> loss:0.8192339682579041
step 101/334, epoch 434/501 --> loss:0.8435488533973694
step 151/334, epoch 434/501 --> loss:0.8325001716613769
step 201/334, epoch 434/501 --> loss:0.8282957136631012
step 251/334, epoch 434/501 --> loss:0.8311186468601227
step 301/334, epoch 434/501 --> loss:0.8179784429073333
step 51/334, epoch 435/501 --> loss:0.8204622077941894
step 101/334, epoch 435/501 --> loss:0.8165365660190582
step 151/334, epoch 435/501 --> loss:0.8310929536819458
step 201/334, epoch 435/501 --> loss:0.825733528137207
step 251/334, epoch 435/501 --> loss:0.8377748620510101
step 301/334, epoch 435/501 --> loss:0.8266910064220429
step 51/334, epoch 436/501 --> loss:0.838925793170929
step 101/334, epoch 436/501 --> loss:0.818135005235672
step 151/334, epoch 436/501 --> loss:0.8200589442253112
step 201/334, epoch 436/501 --> loss:0.8181925737857818
step 251/334, epoch 436/501 --> loss:0.8352801728248597
step 301/334, epoch 436/501 --> loss:0.8252861380577088
step 51/334, epoch 437/501 --> loss:0.823145616054535
step 101/334, epoch 437/501 --> loss:0.8237630009651185
step 151/334, epoch 437/501 --> loss:0.8343166720867157
step 201/334, epoch 437/501 --> loss:0.8290925264358521
step 251/334, epoch 437/501 --> loss:0.8245906817913056
step 301/334, epoch 437/501 --> loss:0.8306927037239075
step 51/334, epoch 438/501 --> loss:0.8340849208831788
step 101/334, epoch 438/501 --> loss:0.8322153890132904
step 151/334, epoch 438/501 --> loss:0.8283900988101959
step 201/334, epoch 438/501 --> loss:0.8145205163955689
step 251/334, epoch 438/501 --> loss:0.8153996133804321
step 301/334, epoch 438/501 --> loss:0.8279577898979187
step 51/334, epoch 439/501 --> loss:0.8225259733200073
step 101/334, epoch 439/501 --> loss:0.8392726612091065
step 151/334, epoch 439/501 --> loss:0.8192490446567535
step 201/334, epoch 439/501 --> loss:0.8196608066558838
step 251/334, epoch 439/501 --> loss:0.8313142538070679
step 301/334, epoch 439/501 --> loss:0.8244477951526642
step 51/334, epoch 440/501 --> loss:0.828740462064743
step 101/334, epoch 440/501 --> loss:0.8194206833839417
step 151/334, epoch 440/501 --> loss:0.8303042066097259
step 201/334, epoch 440/501 --> loss:0.8392928349971771
step 251/334, epoch 440/501 --> loss:0.8235198128223419
step 301/334, epoch 440/501 --> loss:0.8325628304481506
step 51/334, epoch 441/501 --> loss:0.8268089926242829
step 101/334, epoch 441/501 --> loss:0.814417542219162
step 151/334, epoch 441/501 --> loss:0.8262833333015442
step 201/334, epoch 441/501 --> loss:0.8304063856601716
step 251/334, epoch 441/501 --> loss:0.8250138342380524
step 301/334, epoch 441/501 --> loss:0.8325351369380951

##########train dataset##########
acc--> [98.07859949284013]
F1--> {'F1': [0.8091593679608096], 'precision': [0.6961806181961236], 'recall': [0.9659248213914919]}
##########eval dataset##########
acc--> [97.50921895184203]
F1--> {'F1': [0.754759759433641], 'precision': [0.6569400509106531], 'recall': [0.8868206444927618]}
step 51/334, epoch 442/501 --> loss:0.8188078725337982
step 101/334, epoch 442/501 --> loss:0.8215858542919159
step 151/334, epoch 442/501 --> loss:0.826944751739502
step 201/334, epoch 442/501 --> loss:0.8357109248638153
step 251/334, epoch 442/501 --> loss:0.8387068712711334
step 301/334, epoch 442/501 --> loss:0.8207697772979736
step 51/334, epoch 443/501 --> loss:0.8329091167449951
step 101/334, epoch 443/501 --> loss:0.83168137550354
step 151/334, epoch 443/501 --> loss:0.8314976572990418
step 201/334, epoch 443/501 --> loss:0.8338477432727813
step 251/334, epoch 443/501 --> loss:0.8180726385116577
step 301/334, epoch 443/501 --> loss:0.8278258216381073
step 51/334, epoch 444/501 --> loss:0.8333198821544647
step 101/334, epoch 444/501 --> loss:0.8233648467063904
step 151/334, epoch 444/501 --> loss:0.828535578250885
step 201/334, epoch 444/501 --> loss:0.8213071846961975
step 251/334, epoch 444/501 --> loss:0.8211241352558136
step 301/334, epoch 444/501 --> loss:0.8360232722759247
step 51/334, epoch 445/501 --> loss:0.8355732262134552
step 101/334, epoch 445/501 --> loss:0.82832200050354
step 151/334, epoch 445/501 --> loss:0.8220495080947876
step 201/334, epoch 445/501 --> loss:0.8257641196250916
step 251/334, epoch 445/501 --> loss:0.8190488183498382
step 301/334, epoch 445/501 --> loss:0.8300438559055329
step 51/334, epoch 446/501 --> loss:0.8294033086299897
step 101/334, epoch 446/501 --> loss:0.8301183640956878
step 151/334, epoch 446/501 --> loss:0.8219360828399658
step 201/334, epoch 446/501 --> loss:0.8230228590965271
step 251/334, epoch 446/501 --> loss:0.8229635965824127
step 301/334, epoch 446/501 --> loss:0.8375935876369476
step 51/334, epoch 447/501 --> loss:0.8261642217636108
step 101/334, epoch 447/501 --> loss:0.8325604581832886
step 151/334, epoch 447/501 --> loss:0.8427615284919738
step 201/334, epoch 447/501 --> loss:0.8094838070869446
step 251/334, epoch 447/501 --> loss:0.8297223544120789
step 301/334, epoch 447/501 --> loss:0.8271188652515411
step 51/334, epoch 448/501 --> loss:0.8090129494667053
step 101/334, epoch 448/501 --> loss:0.8305513834953309
step 151/334, epoch 448/501 --> loss:0.8297704005241394
step 201/334, epoch 448/501 --> loss:0.8189948093891144
step 251/334, epoch 448/501 --> loss:0.8368755722045899
step 301/334, epoch 448/501 --> loss:0.8330529975891113
step 51/334, epoch 449/501 --> loss:0.8280678617954255
step 101/334, epoch 449/501 --> loss:0.8205688297748566
step 151/334, epoch 449/501 --> loss:0.822462215423584
step 201/334, epoch 449/501 --> loss:0.8444918775558472
step 251/334, epoch 449/501 --> loss:0.8253300762176514
step 301/334, epoch 449/501 --> loss:0.82339839220047
step 51/334, epoch 450/501 --> loss:0.8309582984447479
step 101/334, epoch 450/501 --> loss:0.8293345367908478
step 151/334, epoch 450/501 --> loss:0.8175723206996918
step 201/334, epoch 450/501 --> loss:0.8214364218711853
step 251/334, epoch 450/501 --> loss:0.828251531124115
step 301/334, epoch 450/501 --> loss:0.8325775039196014
step 51/334, epoch 451/501 --> loss:0.8305254209041596
step 101/334, epoch 451/501 --> loss:0.8209377455711365
step 151/334, epoch 451/501 --> loss:0.8334685301780701
step 201/334, epoch 451/501 --> loss:0.8245058143138886
step 251/334, epoch 451/501 --> loss:0.8147876477241516
step 301/334, epoch 451/501 --> loss:0.8362980365753174

##########train dataset##########
acc--> [98.08795758745208]
F1--> {'F1': [0.8098525700816164], 'precision': [0.6974023470176012], 'recall': [0.9655511254860772]}
##########eval dataset##########
acc--> [97.55920717200347]
F1--> {'F1': [0.7576292147024084], 'precision': [0.663635858011731], 'recall': [0.882654452713588]}
step 51/334, epoch 452/501 --> loss:0.8368636393547058
step 101/334, epoch 452/501 --> loss:0.8459815061092377
step 151/334, epoch 452/501 --> loss:0.8108891999721527
step 201/334, epoch 452/501 --> loss:0.8266226410865783
step 251/334, epoch 452/501 --> loss:0.8190350639820099
step 301/334, epoch 452/501 --> loss:0.826469076871872
step 51/334, epoch 453/501 --> loss:0.8222468650341034
step 101/334, epoch 453/501 --> loss:0.8273536705970764
step 151/334, epoch 453/501 --> loss:0.8344739496707916
step 201/334, epoch 453/501 --> loss:0.8382014548778534
step 251/334, epoch 453/501 --> loss:0.8337608349323272
step 301/334, epoch 453/501 --> loss:0.8089106047153473
step 51/334, epoch 454/501 --> loss:0.8378594422340393
step 101/334, epoch 454/501 --> loss:0.8268740570545197
step 151/334, epoch 454/501 --> loss:0.8217612361907959
step 201/334, epoch 454/501 --> loss:0.8183219718933106
step 251/334, epoch 454/501 --> loss:0.840430713891983
step 301/334, epoch 454/501 --> loss:0.8132880675792694
step 51/334, epoch 455/501 --> loss:0.8356082081794739
step 101/334, epoch 455/501 --> loss:0.8180128574371338
step 151/334, epoch 455/501 --> loss:0.8260631585121154
step 201/334, epoch 455/501 --> loss:0.819953008890152
step 251/334, epoch 455/501 --> loss:0.8304592728614807
step 301/334, epoch 455/501 --> loss:0.8281114637851715
step 51/334, epoch 456/501 --> loss:0.8233613669872284
step 101/334, epoch 456/501 --> loss:0.8319361579418182
step 151/334, epoch 456/501 --> loss:0.8374460458755493
step 201/334, epoch 456/501 --> loss:0.8229350864887237
step 251/334, epoch 456/501 --> loss:0.8292954361438751
step 301/334, epoch 456/501 --> loss:0.8226875984668731
step 51/334, epoch 457/501 --> loss:0.8253334820270538
step 101/334, epoch 457/501 --> loss:0.8191248667240143
step 151/334, epoch 457/501 --> loss:0.8413255417346954
step 201/334, epoch 457/501 --> loss:0.8333263218402862
step 251/334, epoch 457/501 --> loss:0.8340702629089356
step 301/334, epoch 457/501 --> loss:0.8230784583091736
step 51/334, epoch 458/501 --> loss:0.8246418595314026
step 101/334, epoch 458/501 --> loss:0.8291927361488343
step 151/334, epoch 458/501 --> loss:0.8314453530311584
step 201/334, epoch 458/501 --> loss:0.8316755640506744
step 251/334, epoch 458/501 --> loss:0.834043835401535
step 301/334, epoch 458/501 --> loss:0.8210117423534393
step 51/334, epoch 459/501 --> loss:0.8260206770896912
step 101/334, epoch 459/501 --> loss:0.8212320387363434
step 151/334, epoch 459/501 --> loss:0.834075790643692
step 201/334, epoch 459/501 --> loss:0.839633719921112
step 251/334, epoch 459/501 --> loss:0.8154699325561523
step 301/334, epoch 459/501 --> loss:0.8290465033054352
step 51/334, epoch 460/501 --> loss:0.8374347007274627
step 101/334, epoch 460/501 --> loss:0.8213976848125458
step 151/334, epoch 460/501 --> loss:0.8221612906455994
step 201/334, epoch 460/501 --> loss:0.8442186200618744
step 251/334, epoch 460/501 --> loss:0.8318658947944642
step 301/334, epoch 460/501 --> loss:0.806250935792923
step 51/334, epoch 461/501 --> loss:0.8158362984657288
step 101/334, epoch 461/501 --> loss:0.8382868504524231
step 151/334, epoch 461/501 --> loss:0.8224571800231933
step 201/334, epoch 461/501 --> loss:0.8214060401916504
step 251/334, epoch 461/501 --> loss:0.832888857126236
step 301/334, epoch 461/501 --> loss:0.8256795644760132

##########train dataset##########
acc--> [96.49936088158381]
F1--> {'F1': [0.701516848028968], 'precision': [0.5476983577125566], 'recall': [0.9754919521062974]}
##########eval dataset##########
acc--> [95.94660214133968]
F1--> {'F1': [0.6596217848671801], 'precision': [0.5177113859860605], 'recall': [0.9087264706749507]}
step 51/334, epoch 462/501 --> loss:0.8245064091682434
step 101/334, epoch 462/501 --> loss:0.8282214164733886
step 151/334, epoch 462/501 --> loss:0.8273543632030487
step 201/334, epoch 462/501 --> loss:0.8358566927909851
step 251/334, epoch 462/501 --> loss:0.8245653259754181
step 301/334, epoch 462/501 --> loss:0.8225640058517456
step 51/334, epoch 463/501 --> loss:0.8204682779312134
step 101/334, epoch 463/501 --> loss:0.8328245985507965
step 151/334, epoch 463/501 --> loss:0.8372484564781189
step 201/334, epoch 463/501 --> loss:0.8123322427272797
step 251/334, epoch 463/501 --> loss:0.8355484700202942
step 301/334, epoch 463/501 --> loss:0.8294058477878571
step 51/334, epoch 464/501 --> loss:0.8263808381557465
step 101/334, epoch 464/501 --> loss:0.8308044028282165
step 151/334, epoch 464/501 --> loss:0.8248982465267182
step 201/334, epoch 464/501 --> loss:0.8324007332324982
step 251/334, epoch 464/501 --> loss:0.8201302313804626
step 301/334, epoch 464/501 --> loss:0.8266927969455719
step 51/334, epoch 465/501 --> loss:0.8233119714260101
step 101/334, epoch 465/501 --> loss:0.8283099043369293
step 151/334, epoch 465/501 --> loss:0.8334316122531891
step 201/334, epoch 465/501 --> loss:0.8261073970794678
step 251/334, epoch 465/501 --> loss:0.8265491008758545
step 301/334, epoch 465/501 --> loss:0.8212851476669312
step 51/334, epoch 466/501 --> loss:0.8259394049644471
step 101/334, epoch 466/501 --> loss:0.8308391201496125
step 151/334, epoch 466/501 --> loss:0.8229650175571441
step 201/334, epoch 466/501 --> loss:0.8320010721683502
step 251/334, epoch 466/501 --> loss:0.834389042854309
step 301/334, epoch 466/501 --> loss:0.8256241858005524
step 51/334, epoch 467/501 --> loss:0.8294945347309113
step 101/334, epoch 467/501 --> loss:0.8251291942596436
step 151/334, epoch 467/501 --> loss:0.8118656551837922
step 201/334, epoch 467/501 --> loss:0.844796656370163
step 251/334, epoch 467/501 --> loss:0.826201753616333
step 301/334, epoch 467/501 --> loss:0.822287826538086
step 51/334, epoch 468/501 --> loss:0.8226058304309845
step 101/334, epoch 468/501 --> loss:0.8301298081874847
step 151/334, epoch 468/501 --> loss:0.8355978786945343
step 201/334, epoch 468/501 --> loss:0.8148880386352539
step 251/334, epoch 468/501 --> loss:0.8185597932338715
step 301/334, epoch 468/501 --> loss:0.8414979195594787
step 51/334, epoch 469/501 --> loss:0.8283932876586914
step 101/334, epoch 469/501 --> loss:0.8184835886955262
step 151/334, epoch 469/501 --> loss:0.825067194700241
step 201/334, epoch 469/501 --> loss:0.8273697340488434
step 251/334, epoch 469/501 --> loss:0.8302536368370056
step 301/334, epoch 469/501 --> loss:0.8259916341304779
step 51/334, epoch 470/501 --> loss:0.8321747219562531
step 101/334, epoch 470/501 --> loss:0.8284242296218872
step 151/334, epoch 470/501 --> loss:0.8260872721672058
step 201/334, epoch 470/501 --> loss:0.8275305151939392
step 251/334, epoch 470/501 --> loss:0.8371095812320709
step 301/334, epoch 470/501 --> loss:0.8207571935653687
step 51/334, epoch 471/501 --> loss:0.8350359129905701
step 101/334, epoch 471/501 --> loss:0.8268995022773743
step 151/334, epoch 471/501 --> loss:0.8173899590969086
step 201/334, epoch 471/501 --> loss:0.8248061037063599
step 251/334, epoch 471/501 --> loss:0.8372066521644592
step 301/334, epoch 471/501 --> loss:0.8380950450897217

##########train dataset##########
acc--> [98.33307942067263]
F1--> {'F1': [0.8302783475537188], 'precision': [0.7275140367474447], 'recall': [0.9668630972607413]}
##########eval dataset##########
acc--> [97.72265910684462]
F1--> {'F1': [0.7700865894870177], 'precision': [0.6831172625744991], 'recall': [0.8824439578700171]}
step 51/334, epoch 472/501 --> loss:0.8285755622386932
step 101/334, epoch 472/501 --> loss:0.8272221231460571
step 151/334, epoch 472/501 --> loss:0.8228255808353424
step 201/334, epoch 472/501 --> loss:0.8400878417491913
step 251/334, epoch 472/501 --> loss:0.8288655555248261
step 301/334, epoch 472/501 --> loss:0.8179046595096588
step 51/334, epoch 473/501 --> loss:0.8243812239170074
step 101/334, epoch 473/501 --> loss:0.8287845659255981
step 151/334, epoch 473/501 --> loss:0.8241246628761292
step 201/334, epoch 473/501 --> loss:0.8223281598091126
step 251/334, epoch 473/501 --> loss:0.8203424227237701
step 301/334, epoch 473/501 --> loss:0.8338405859470367
step 51/334, epoch 474/501 --> loss:0.8253260326385498
step 101/334, epoch 474/501 --> loss:0.8234562337398529
step 151/334, epoch 474/501 --> loss:0.8325814318656921
step 201/334, epoch 474/501 --> loss:0.8267930042743683
step 251/334, epoch 474/501 --> loss:0.8230074059963226
step 301/334, epoch 474/501 --> loss:0.8340557909011841
step 51/334, epoch 475/501 --> loss:0.8254111349582672
step 101/334, epoch 475/501 --> loss:0.822791451215744
step 151/334, epoch 475/501 --> loss:0.8291101598739624
step 201/334, epoch 475/501 --> loss:0.8081048858165741
step 251/334, epoch 475/501 --> loss:0.8363848114013672
step 301/334, epoch 475/501 --> loss:0.8408221328258514
step 51/334, epoch 476/501 --> loss:0.8113735401630402
step 101/334, epoch 476/501 --> loss:0.8361294913291931
step 151/334, epoch 476/501 --> loss:0.8281984615325928
step 201/334, epoch 476/501 --> loss:0.8138816785812378
step 251/334, epoch 476/501 --> loss:0.8218939542770386
step 301/334, epoch 476/501 --> loss:0.8302820456027985
step 51/334, epoch 477/501 --> loss:0.8307277226448059
step 101/334, epoch 477/501 --> loss:0.8167768228054046
step 151/334, epoch 477/501 --> loss:0.8324071073532104
step 201/334, epoch 477/501 --> loss:0.8308607113361358
step 251/334, epoch 477/501 --> loss:0.8236940848827362
step 301/334, epoch 477/501 --> loss:0.8291065001487732
step 51/334, epoch 478/501 --> loss:0.8273783850669861
step 101/334, epoch 478/501 --> loss:0.8325596058368683
step 151/334, epoch 478/501 --> loss:0.8320067024230957
step 201/334, epoch 478/501 --> loss:0.813954039812088
step 251/334, epoch 478/501 --> loss:0.8240747201442719
step 301/334, epoch 478/501 --> loss:0.8261761629581451
step 51/334, epoch 479/501 --> loss:0.8275760924816131
step 101/334, epoch 479/501 --> loss:0.8348482978343964
step 151/334, epoch 479/501 --> loss:0.8347686862945557
step 201/334, epoch 479/501 --> loss:0.8168473827838898
step 251/334, epoch 479/501 --> loss:0.8324123179912567
step 301/334, epoch 479/501 --> loss:0.8211568009853363
step 51/334, epoch 480/501 --> loss:0.8300832557678223
step 101/334, epoch 480/501 --> loss:0.8446720540523529
step 151/334, epoch 480/501 --> loss:0.8233303487300873
step 201/334, epoch 480/501 --> loss:0.83853409409523
step 251/334, epoch 480/501 --> loss:0.8196142196655274
step 301/334, epoch 480/501 --> loss:0.8092669093608856
step 51/334, epoch 481/501 --> loss:0.8214194023609162
step 101/334, epoch 481/501 --> loss:0.8272166991233826
step 151/334, epoch 481/501 --> loss:0.8302325820922851
step 201/334, epoch 481/501 --> loss:0.8340819978713989
step 251/334, epoch 481/501 --> loss:0.8296552348136902
step 301/334, epoch 481/501 --> loss:0.8229903817176819

##########train dataset##########
acc--> [97.95315874440585]
F1--> {'F1': [0.8000928055136121], 'precision': [0.6801993505532189], 'recall': [0.9713098004169661]}
##########eval dataset##########
acc--> [97.34394877809423]
F1--> {'F1': [0.7442368575053584], 'precision': [0.6374004759816959], 'recall': [0.894113217499559]}
step 51/334, epoch 482/501 --> loss:0.8154316020011901
step 101/334, epoch 482/501 --> loss:0.8391849648952484
step 151/334, epoch 482/501 --> loss:0.8268001103401184
step 201/334, epoch 482/501 --> loss:0.8238181757926941
step 251/334, epoch 482/501 --> loss:0.8212654566764832
step 301/334, epoch 482/501 --> loss:0.8288713228702546
step 51/334, epoch 483/501 --> loss:0.8182066178321838
step 101/334, epoch 483/501 --> loss:0.8293741488456726
step 151/334, epoch 483/501 --> loss:0.8203638100624084
step 201/334, epoch 483/501 --> loss:0.8299967801570892
step 251/334, epoch 483/501 --> loss:0.8246266007423401
step 301/334, epoch 483/501 --> loss:0.8390921485424042
step 51/334, epoch 484/501 --> loss:0.8373583126068115
step 101/334, epoch 484/501 --> loss:0.821916573047638
step 151/334, epoch 484/501 --> loss:0.8310582423210144
step 201/334, epoch 484/501 --> loss:0.8241303265094757
step 251/334, epoch 484/501 --> loss:0.822569659948349
step 301/334, epoch 484/501 --> loss:0.8288468897342682
step 51/334, epoch 485/501 --> loss:0.8281354653835297
step 101/334, epoch 485/501 --> loss:0.8328623509407044
step 151/334, epoch 485/501 --> loss:0.8418923008441925
step 201/334, epoch 485/501 --> loss:0.8290102386474609
step 251/334, epoch 485/501 --> loss:0.8084848272800446
step 301/334, epoch 485/501 --> loss:0.8195533001422882
step 51/334, epoch 486/501 --> loss:0.8210354828834534
step 101/334, epoch 486/501 --> loss:0.8126289522647858
step 151/334, epoch 486/501 --> loss:0.8204633927345276
step 201/334, epoch 486/501 --> loss:0.8406856572628021
step 251/334, epoch 486/501 --> loss:0.8389772295951843
step 301/334, epoch 486/501 --> loss:0.830152233839035
step 51/334, epoch 487/501 --> loss:0.8142904615402222
step 101/334, epoch 487/501 --> loss:0.8205537056922912
step 151/334, epoch 487/501 --> loss:0.8306011652946472
step 201/334, epoch 487/501 --> loss:0.8458659672737121
step 251/334, epoch 487/501 --> loss:0.8133887326717377
step 301/334, epoch 487/501 --> loss:0.839943231344223
step 51/334, epoch 488/501 --> loss:0.8375792372226715
step 101/334, epoch 488/501 --> loss:0.8290322685241699
step 151/334, epoch 488/501 --> loss:0.8235973465442658
step 201/334, epoch 488/501 --> loss:0.8385056638717652
step 251/334, epoch 488/501 --> loss:0.8212764728069305
step 301/334, epoch 488/501 --> loss:0.8227482771873474
step 51/334, epoch 489/501 --> loss:0.8269180560111999
step 101/334, epoch 489/501 --> loss:0.8257119166851044
step 151/334, epoch 489/501 --> loss:0.8248665308952332
step 201/334, epoch 489/501 --> loss:0.8193772923946381
step 251/334, epoch 489/501 --> loss:0.8323790085315704
step 301/334, epoch 489/501 --> loss:0.8346213388442993
step 51/334, epoch 490/501 --> loss:0.8364154243469238
step 101/334, epoch 490/501 --> loss:0.8225342416763306
step 151/334, epoch 490/501 --> loss:0.8173652565479279
step 201/334, epoch 490/501 --> loss:0.8340307712554932
step 251/334, epoch 490/501 --> loss:0.8293033850193023
step 301/334, epoch 490/501 --> loss:0.8239960467815399
step 51/334, epoch 491/501 --> loss:0.8399499654769897
step 101/334, epoch 491/501 --> loss:0.8222011661529541
step 151/334, epoch 491/501 --> loss:0.8328799688816071
step 201/334, epoch 491/501 --> loss:0.8244857919216156
step 251/334, epoch 491/501 --> loss:0.8096483933925629
step 301/334, epoch 491/501 --> loss:0.8272027349472046

##########train dataset##########
acc--> [97.36410369128996]
F1--> {'F1': [0.7570768614314562], 'precision': [0.6191837286661782], 'recall': [0.9740004245026931]}
##########eval dataset##########
acc--> [96.75930620594997]
F1--> {'F1': [0.7065450840464397], 'precision': [0.5804488934269831], 'recall': [0.9026476019789563]}
step 51/334, epoch 492/501 --> loss:0.830713096857071
step 101/334, epoch 492/501 --> loss:0.8234666001796722
step 151/334, epoch 492/501 --> loss:0.8444912028312683
step 201/334, epoch 492/501 --> loss:0.8220460927486419
step 251/334, epoch 492/501 --> loss:0.8250759208202362
step 301/334, epoch 492/501 --> loss:0.8317670583724975
step 51/334, epoch 493/501 --> loss:0.8154132628440857
step 101/334, epoch 493/501 --> loss:0.8364263153076172
step 151/334, epoch 493/501 --> loss:0.8361871683597565
step 201/334, epoch 493/501 --> loss:0.8173913669586181
step 251/334, epoch 493/501 --> loss:0.8347983944416046
step 301/334, epoch 493/501 --> loss:0.8249790453910828
step 51/334, epoch 494/501 --> loss:0.8257515263557434
step 101/334, epoch 494/501 --> loss:0.8354789972305298
step 151/334, epoch 494/501 --> loss:0.8360610485076905
step 201/334, epoch 494/501 --> loss:0.8118786036968231
step 251/334, epoch 494/501 --> loss:0.8227988898754119
step 301/334, epoch 494/501 --> loss:0.83165403008461
step 51/334, epoch 495/501 --> loss:0.8392912983894348
step 101/334, epoch 495/501 --> loss:0.8384066867828369
step 151/334, epoch 495/501 --> loss:0.8243107104301453
step 201/334, epoch 495/501 --> loss:0.8256762933731079
step 251/334, epoch 495/501 --> loss:0.8291892027854919
step 301/334, epoch 495/501 --> loss:0.808254418373108
step 51/334, epoch 496/501 --> loss:0.8322414314746857
step 101/334, epoch 496/501 --> loss:0.8142258477210998
step 151/334, epoch 496/501 --> loss:0.8305331289768219
step 201/334, epoch 496/501 --> loss:0.8260720121860504
step 251/334, epoch 496/501 --> loss:0.8385635697841645
step 301/334, epoch 496/501 --> loss:0.8151518476009368
step 51/334, epoch 497/501 --> loss:0.8258664524555206
step 101/334, epoch 497/501 --> loss:0.8315146481990814
step 151/334, epoch 497/501 --> loss:0.8235106110572815
step 201/334, epoch 497/501 --> loss:0.823764009475708
step 251/334, epoch 497/501 --> loss:0.818851318359375
step 301/334, epoch 497/501 --> loss:0.8323816239833832
step 51/334, epoch 498/501 --> loss:0.8182787442207337
step 101/334, epoch 498/501 --> loss:0.8292501306533814
step 151/334, epoch 498/501 --> loss:0.8236472404003143
step 201/334, epoch 498/501 --> loss:0.8284325718879699
step 251/334, epoch 498/501 --> loss:0.8209583592414856
step 301/334, epoch 498/501 --> loss:0.8413331544399262
step 51/334, epoch 499/501 --> loss:0.8291296410560608
step 101/334, epoch 499/501 --> loss:0.8205818712711335
step 151/334, epoch 499/501 --> loss:0.8247822499275208
step 201/334, epoch 499/501 --> loss:0.829080239534378
step 251/334, epoch 499/501 --> loss:0.8242595100402832
step 301/334, epoch 499/501 --> loss:0.830463525056839
step 51/334, epoch 500/501 --> loss:0.8363732123374938
step 101/334, epoch 500/501 --> loss:0.8346000337600707
step 151/334, epoch 500/501 --> loss:0.8216658568382263
step 201/334, epoch 500/501 --> loss:0.8267415225505829
step 251/334, epoch 500/501 --> loss:0.8250773978233338
step 301/334, epoch 500/501 --> loss:0.8186612868309021
step 51/334, epoch 501/501 --> loss:0.838988003730774
step 101/334, epoch 501/501 --> loss:0.8121478569507599
step 151/334, epoch 501/501 --> loss:0.8329742574691772
step 201/334, epoch 501/501 --> loss:0.8284136128425598
step 251/334, epoch 501/501 --> loss:0.8328504383563995
step 301/334, epoch 501/501 --> loss:0.8250984454154968

##########train dataset##########
acc--> [98.317342900494]
F1--> {'F1': [0.82926712610492], 'precision': [0.7247467950966388], 'recall': [0.9690282814329918]}
##########eval dataset##########
acc--> [97.69793265968664]
F1--> {'F1': [0.768225494575801], 'precision': [0.6800276587745168], 'recall': [0.882723794008352]}
